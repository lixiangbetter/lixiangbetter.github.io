<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>java集合笔记</title>
    <url>/2020/06/20/java%E9%9B%86%E5%90%88%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<h1 id="java集合笔记"><a href="#java集合笔记" class="headerlink" title="java集合笔记"></a>java集合笔记</h1><h2 id="集合概述"><a href="#集合概述" class="headerlink" title="集合概述"></a>集合概述</h2><h4 id="什么是集合"><a href="#什么是集合" class="headerlink" title="什么是集合"></a>什么是集合</h4><p>集合框架：用于存储数据的容器。</p>
<p>集合框架是为表示和操作集合而规定的一种统一的标准的体系结构。<br>任何集合框架都包含三大块内容：对外的接口、接口的实现和对集合运算的算法。</p>
<p>接口：表示集合的抽象数据类型。接口允许我们操作集合时不必关注具体实现，从而达到“多态”。在面向对象编程语言中，接口通常用来形成规范。</p>
<p>实现：集合接口的具体实现，是重用性很高的数据结构。</p>
<p>算法：在一个实现了某个集合框架中的接口的对象身上完成某种有用的计算的方法，例如查找、排序等。这些算法通常是多态的，因为相同的方法可以在同一个接口被多个类实现时有不同的表现。事实上，算法是可复用的函数。<br>它减少了程序设计的辛劳。</p>
<p>集合框架通过提供有用的数据结构和算法使你能集中注意力于你的程序的重要部分上，而不是为了让程序能正常运转而将注意力于低层设计上。<br>通过这些在无关API之间的简易的互用性，使你免除了为改编对象或转换代码以便联合这些API而去写大量的代码。 它提高了程序速度和质量。</p>
<h4 id="集合的特点"><a href="#集合的特点" class="headerlink" title="集合的特点"></a>集合的特点</h4><p>集合的特点主要有如下两点：</p>
<ul>
<li>对象封装数据，对象多了也需要存储。集合用于存储对象。</li>
<li>对象的个数确定可以使用数组，对象的个数不确定的可以用集合。因为集合是可变长度的。</li>
</ul>
<h4 id="集合和数组的区别"><a href="#集合和数组的区别" class="headerlink" title="集合和数组的区别"></a>集合和数组的区别</h4><ul>
<li>数组是固定长度的；集合可变长度的。</li>
<li>数组可以存储基本数据类型，也可以存储引用数据类型；集合只能存储引用数据类型。</li>
<li>数组存储的元素必须是同一个数据类型；集合存储的对象可以是不同数据类型。</li>
</ul>
<h4 id="使用集合框架的好处"><a href="#使用集合框架的好处" class="headerlink" title="使用集合框架的好处"></a>使用集合框架的好处</h4><ul>
<li>容量自增长；</li>
<li>提供了高性能的数据结构和算法，使编码更轻松，提高了程序速度和质量；</li>
<li>允许不同 API 之间的互操作，API之间可以来回传递集合；</li>
<li>可以方便地扩展或改写集合，提高代码复用性和可操作性。</li>
<li>通过使用JDK自带的集合类，可以降低代码维护和学习新API成本。</li>
</ul>
<h4 id="常用的集合类有哪些？"><a href="#常用的集合类有哪些？" class="headerlink" title="常用的集合类有哪些？"></a>常用的集合类有哪些？</h4><p>Map接口和Collection接口是所有集合框架的父接口：</p>
<p>Collection接口的子接口包括：Set接口和List接口<br>Map接口的实现类主要有：HashMap、TreeMap、Hashtable、ConcurrentHashMap以及Properties等<br>Set接口的实现类主要有：HashSet、TreeSet、LinkedHashSet等<br>List接口的实现类主要有：ArrayList、LinkedList、Stack以及Vector等</p>
<h4 id="List，Set，Map三者的区别？List、Set、Map-是否继承自-Collection-接口？List、Map、Set-三个接口存取元素时，各有什么特点？"><a href="#List，Set，Map三者的区别？List、Set、Map-是否继承自-Collection-接口？List、Map、Set-三个接口存取元素时，各有什么特点？" class="headerlink" title="List，Set，Map三者的区别？List、Set、Map 是否继承自 Collection 接口？List、Map、Set 三个接口存取元素时，各有什么特点？"></a>List，Set，Map三者的区别？List、Set、Map 是否继承自 Collection 接口？List、Map、Set 三个接口存取元素时，各有什么特点？</h4><p><img src="//yoursite.com/2020/06/20/java集合笔记/aHR0cHM6Ly9pbWcyMDE4LmNuYmxvZ3MuY29tL290aGVyLzE0MDgxODMvMjAxOTExLzE0MDgxODMtMjAxOTExMTkxODQxNDk1NTktMTU3MTU5NTY2OC5qcGc.jpeg" alt></p>
<h4 id="集合框架底层数据结构"><a href="#集合框架底层数据结构" class="headerlink" title="集合框架底层数据结构"></a>集合框架底层数据结构</h4><p>Collection</p>
<ol>
<li>List</li>
</ol>
<ul>
<li>Arraylist： Object数组</li>
<li>Vector： Object数组</li>
<li>LinkedList： 双向循环链表</li>
</ul>
<ol start="2">
<li>Set</li>
</ol>
<ul>
<li>HashSet（无序，唯一）：底层采用 HashMap 来保存元素</li>
<li>LinkedHashSet： LinkedHashSet 继承于HashSet，并且其内部是通过 LinkedHashMap 来实现的。</li>
<li>TreeSet（有序，唯一）： 红黑树(自平衡的排序二叉树。)</li>
</ul>
<p>Map</p>
<ul>
<li>HashMap： JDK1.8之前HashMap由数组+链表组成的；JDK1.8以后链表+红黑树；</li>
<li>LinkedHashMap：LinkedHashMap 继承自 HashMap，所以它的底层仍然是基于拉链式散列结构即由数组和链表或红黑树组成。另外，LinkedHashMap 在上面结构的基础上，增加了一条双向链表，使得上面的结构可以保持键值对的插入顺序。同时通过对链表进行相应的操作，实现了访问顺序相关逻辑。</li>
<li>HashTable： 数组+链表组成的，数组是 HashMap 的主体，链表则是主要为了解决哈希冲突而存在</li>
<li>TreeMap： 红黑树（自平衡的排序二叉树）</li>
</ul>
<h4 id="哪些集合类是线程安全的？"><a href="#哪些集合类是线程安全的？" class="headerlink" title="哪些集合类是线程安全的？"></a>哪些集合类是线程安全的？</h4><ul>
<li>vector：就比arraylist多了个同步化机制（线程安全），因为效率较低，现在已经不太建议使用。在web应用中，特别是前台页面，往往效率（页面响应速度）是优先考虑的。</li>
<li>statck：堆栈类，先进后出。</li>
<li>hashtable：就比hashmap多了个线程安全。</li>
<li>enumeration：枚举，相当于迭代器。</li>
</ul>
<h4 id="Java集合的快速失败机制-“fail-fast”？"><a href="#Java集合的快速失败机制-“fail-fast”？" class="headerlink" title="Java集合的快速失败机制 “fail-fast”？"></a>Java集合的快速失败机制 “fail-fast”？</h4><p>是java集合的一种错误检测机制，当多个线程对集合进行结构上的改变的操作时，有可能会产生 fail-fast 机制。</p>
<p>例如：假设存在两个线程（线程1、线程2），线程1通过Iterator在遍历集合A中的元素，在某个时候线程2修改了集合A的结构（是结构上面的修改，而不是简单的修改集合元素的内容），那么这个时候程序就会抛出 ConcurrentModificationException 异常，从而产生fail-fast机制。</p>
<p>原因：迭代器在遍历时直接访问集合中的内容，并且在遍历过程中使用一个 modCount 变量。集合在被遍历期间如果内容发生变化，就会改变modCount的值。每当迭代器使用hashNext()/next()遍历下一个元素之前，都会检测modCount变量是否为expectedmodCount值，是的话就返回遍历；否则抛出异常，终止遍历。</p>
<p>解决办法：</p>
<p>在遍历过程中，所有涉及到改变modCount值得地方全部加上synchronized。</p>
<p>使用CopyOnWriteArrayList来替换ArrayList</p>
<h4 id="怎么确保一个集合不能被修改？"><a href="#怎么确保一个集合不能被修改？" class="headerlink" title="怎么确保一个集合不能被修改？"></a>怎么确保一个集合不能被修改？</h4><p>可以使用 Collections. unmodifiableCollection(Collection c) 方法来创建一个只读集合，这样改变集合的任何操作都会抛出 Java. lang. UnsupportedOperationException 异常。</p>
<h2 id="Collection接口"><a href="#Collection接口" class="headerlink" title="Collection接口"></a>Collection接口</h2><h3 id="List接口"><a href="#List接口" class="headerlink" title="List接口"></a>List接口</h3><h4 id="迭代器-Iterator-是什么？"><a href="#迭代器-Iterator-是什么？" class="headerlink" title="迭代器 Iterator 是什么？"></a>迭代器 Iterator 是什么？</h4><p>Iterator 接口提供遍历任何 Collection 的接口。迭代器取代了 Java 集合框架中的 Enumeration，迭代器允许调用者在迭代过程中移除元素。</p>
<h4 id="Iterator-怎么使用？有什么特点？"><a href="#Iterator-怎么使用？有什么特点？" class="headerlink" title="Iterator 怎么使用？有什么特点？"></a>Iterator 怎么使用？有什么特点？</h4><p>Iterator 的特点是只能单向遍历，但是更加安全，因为它可以确保，在当前遍历的集合元素被更改的时候，就会抛出 ConcurrentModificationException 异常。</p>
<h4 id="如何边遍历边移除-Collection-中的元素？"><a href="#如何边遍历边移除-Collection-中的元素？" class="headerlink" title="如何边遍历边移除 Collection 中的元素？"></a>如何边遍历边移除 Collection 中的元素？</h4><p>边遍历边修改 Collection 的唯一正确方式是使用 Iterator.remove() 方法</p>
<h4 id="Iterator-和-ListIterator-有什么区别？"><a href="#Iterator-和-ListIterator-有什么区别？" class="headerlink" title="Iterator 和 ListIterator 有什么区别？"></a>Iterator 和 ListIterator 有什么区别？</h4><ul>
<li>Iterator 可以遍历 Set 和 List 集合，而 ListIterator 只能遍历 List。</li>
<li>Iterator 只能单向遍历，而 ListIterator 可以双向遍历（向前/后遍历）。</li>
<li>ListIterator 实现 Iterator 接口，然后添加了一些额外的功能，比如添加一个元素、替换一个元素、获取前面或后面元素的索引位置。</li>
</ul>
<h4 id="遍历一个-List-有哪些不同的方式？每种方法的实现原理是什么？Java-中-List-遍历的最佳实践是什么？"><a href="#遍历一个-List-有哪些不同的方式？每种方法的实现原理是什么？Java-中-List-遍历的最佳实践是什么？" class="headerlink" title="遍历一个 List 有哪些不同的方式？每种方法的实现原理是什么？Java 中 List 遍历的最佳实践是什么？"></a>遍历一个 List 有哪些不同的方式？每种方法的实现原理是什么？Java 中 List 遍历的最佳实践是什么？</h4><ol>
<li>for 循环遍历，基于计数器。在集合外部维护一个计数器，然后依次读取每一个位置的元素，当读取到最后一个元素后停止。</li>
<li>迭代器遍历，Iterator。Iterator 是面向对象的一个设计模式，目的是屏蔽不同数据集合的特点，统一遍历集合的接口。Java 在 Collections 中支持了 Iterator 模式。</li>
<li>foreach 循环遍历。foreach 内部也是采用了 Iterator 的方式实现，使用时不需要显式声明 Iterator 或计数器。优点是代码简洁，不易出错；缺点是只能做简单的遍历，不能在遍历过程中操作数据集合，例如删除、替换。</li>
</ol>
<p>推荐的做法就是，支持 Random Access 的列表可用 for 循环遍历，否则建议用 Iterator 或 foreach 遍历。</p>
<h4 id="说一下-ArrayList-的优缺点"><a href="#说一下-ArrayList-的优缺点" class="headerlink" title="说一下 ArrayList 的优缺点"></a>说一下 ArrayList 的优缺点</h4><p>优点：随机访问快</p>
<p>缺点：插入删除需复制，耗费性能</p>
<h4 id="如何实现数组和-List-之间的转换？"><a href="#如何实现数组和-List-之间的转换？" class="headerlink" title="如何实现数组和 List 之间的转换？"></a>如何实现数组和 List 之间的转换？</h4><ul>
<li>数组转 List：使用 Arrays. asList(array) 进行转换。</li>
<li>List 转数组：使用 List 自带的 toArray() 方法。</li>
</ul>
<h4 id="ArrayList-和-LinkedList-的区别是什么？"><a href="#ArrayList-和-LinkedList-的区别是什么？" class="headerlink" title="ArrayList 和 LinkedList 的区别是什么？"></a>ArrayList 和 LinkedList 的区别是什么？</h4><p>数据结构实现：ArrayList 动态数组，而 LinkedList 双向链表<br>随机访问效率：ArrayList 更好<br>增加和删除效率：LinkedList 更好<br>内存空间占用：LinkedList 比 ArrayList 更占内存，因为 LinkedList 的节点除了存储数据，还存储了两个引用<br>线程安全：都不保证线程安全；<br>综合来说，在需要频繁读取集合中的元素时，更推荐使用 ArrayList，而在插入和删除操作较多时，更推荐使用 LinkedList。</p>
<h4 id="ArrayList-和-Vector-的区别是什么？"><a href="#ArrayList-和-Vector-的区别是什么？" class="headerlink" title="ArrayList 和 Vector 的区别是什么？"></a>ArrayList 和 Vector 的区别是什么？</h4><ul>
<li>线程安全：Vector 使用了 Synchronized 来实现线程同步，是线程安全的，而 ArrayList 是非线程安全的。</li>
<li>性能：ArrayList 在性能方面要优于 Vector。</li>
<li>扩容：ArrayList 和 Vector 都会根据实际的需要动态的调整容量，只不过在 Vector 扩容每次会增加 1 倍，而 ArrayList 只会增加 50%。</li>
</ul>
<h4 id="插入数据时，ArrayList、LinkedList、Vector谁速度较快？阐述-ArrayList、Vector、LinkedList-的存储性能和特性？"><a href="#插入数据时，ArrayList、LinkedList、Vector谁速度较快？阐述-ArrayList、Vector、LinkedList-的存储性能和特性？" class="headerlink" title="插入数据时，ArrayList、LinkedList、Vector谁速度较快？阐述 ArrayList、Vector、LinkedList 的存储性能和特性？"></a>插入数据时，ArrayList、LinkedList、Vector谁速度较快？阐述 ArrayList、Vector、LinkedList 的存储性能和特性？</h4><p>ArrayList、Vector 底层数组方式存储数据。</p>
<p>LinkedList 双向链表，LinkedList 插入速度较快。</p>
<h4 id="多线程场景下如何使用-ArrayList？"><a href="#多线程场景下如何使用-ArrayList？" class="headerlink" title="多线程场景下如何使用 ArrayList？"></a>多线程场景下如何使用 ArrayList？</h4><p>ArrayList 不是线程安全的，如果遇到多线程场景，可以通过 Collections 的 synchronizedList 方法将其转换成线程安全的容器后再使用</p>
<h4 id="为什么-ArrayList-的-elementData-加上-transient-修饰？"><a href="#为什么-ArrayList-的-elementData-加上-transient-修饰？" class="headerlink" title="为什么 ArrayList 的 elementData 加上 transient 修饰？"></a>为什么 ArrayList 的 elementData 加上 transient 修饰？</h4><p>每次序列化时，先调用 defaultWriteObject() 方法序列化 ArrayList 中的非 transient 元素，然后遍历 elementData，只序列化已存入的元素，这样既加快了序列化的速度，又减小了序列化之后的文件大小。</p>
<h4 id="List-和-Set-的区别"><a href="#List-和-Set-的区别" class="headerlink" title="List 和 Set 的区别"></a>List 和 Set 的区别</h4><p>list: 有序 元素可重复 多个null 有索引 for和iterator 检索低效 插入删除高效 </p>
<p>set: 无序 不可重复 一个null iterator 查找高效 插入删除低效</p>
<h3 id="Set接口"><a href="#Set接口" class="headerlink" title="Set接口"></a>Set接口</h3><h4 id="说一下-HashSet-的实现原理？"><a href="#说一下-HashSet-的实现原理？" class="headerlink" title="说一下 HashSet 的实现原理？"></a>说一下 HashSet 的实现原理？</h4><p>HashSet: 底层HashMap hashmap的value统一为PRESENT 底层调用hashmap的方法 hashset不允许重复</p>
<h4 id="HashSet如何检查重复？HashSet是如何保证数据不可重复的？"><a href="#HashSet如何检查重复？HashSet是如何保证数据不可重复的？" class="headerlink" title="HashSet如何检查重复？HashSet是如何保证数据不可重复的？"></a>HashSet如何检查重复？HashSet是如何保证数据不可重复的？</h4><p>检查重复，不仅比较hash值，还要结合equals方法</p>
<p>值作为hashmap的key，所以不会重复</p>
<h3 id="Queue"><a href="#Queue" class="headerlink" title="Queue"></a>Queue</h3><h4 id="BlockingQueue是什么？"><a href="#BlockingQueue是什么？" class="headerlink" title="BlockingQueue是什么？"></a>BlockingQueue是什么？</h4><p>阻塞队列  在进行检索或移除一个元素的时候，它会等待队列变为非空；当在添加一个元素时，它会等待队列中的可用空间</p>
<h4 id="在-Queue-中-poll-和-remove-有什么区别？"><a href="#在-Queue-中-poll-和-remove-有什么区别？" class="headerlink" title="在 Queue 中 poll()和 remove()有什么区别？"></a>在 Queue 中 poll()和 remove()有什么区别？</h4><p>相同点：返回第一元素，并删除</p>
<p>不同点：没有元素，poll返回null remove抛出异常NoSuchElementException</p>
<h2 id="Map接口"><a href="#Map接口" class="headerlink" title="Map接口"></a>Map接口</h2><h4 id="说一下-HashMap-的实现原理？"><a href="#说一下-HashMap-的实现原理？" class="headerlink" title="说一下 HashMap 的实现原理？"></a>说一下 HashMap 的实现原理？</h4><p>概述： HashMap是基于哈希表的Map接口的非同步实现</p>
<p>数组和链表的结合体</p>
<p>1.用key的hashcode作hash计算下标</p>
<p>2.(1)key相同，覆盖原始值；(2)key不同（出现冲突），key-value放入链表</p>
<p>Jdk 1.8中对HashMap的实现做了优化，当链表中的节点数据超过八个之后，该链表会转为红黑树来提高查询效率，从原来的O(n)到O(logn)</p>
<h4 id="HashMap在JDK1-7和JDK1-8中有哪些不同？HashMap的底层实现"><a href="#HashMap在JDK1-7和JDK1-8中有哪些不同？HashMap的底层实现" class="headerlink" title="HashMap在JDK1.7和JDK1.8中有哪些不同？HashMap的底层实现"></a>HashMap在JDK1.7和JDK1.8中有哪些不同？HashMap的底层实现</h4><p>JDK1.8之前: 数据+链表； 之后：链表长度大于阈值（默认为8），链表转为红黑树</p>
<h4 id="HashMap的put方法的具体流程？"><a href="#HashMap的put方法的具体流程？" class="headerlink" title="HashMap的put方法的具体流程？"></a>HashMap的put方法的具体流程？</h4><p>①.判断键值对数组table是否为空或为null，否则执行resize()进行扩容；</p>
<p>②.根据键值key计算hash值得到插入的数组索引i，如果table[i]==null，直接新建节点添加，转向⑥，如果table[i]不为空，转向③；</p>
<p>③.判断table[i]的首个元素是否和key一样，如果相同直接覆盖value，否则转向④，这里的相同指的是hashCode以及equals；</p>
<p>④.判断table[i] 是否为treeNode，即table[i] 是否是红黑树，如果是红黑树，则直接在树中插入键值对，否则转向⑤；</p>
<p>⑤.遍历table[i]，判断链表长度是否大于8，大于8的话把链表转换为红黑树，在红黑树中执行插入操作，否则进行链表的插入操作；遍历过程中若发现key已经存在直接覆盖value即可；</p>
<p>⑥.插入成功后，判断实际存在的键值对数量size是否超多了最大容量threshold，如果超过，进行扩容。</p>
<h4 id="HashMap的扩容操作是怎么实现的？"><a href="#HashMap的扩容操作是怎么实现的？" class="headerlink" title="HashMap的扩容操作是怎么实现的？"></a>HashMap的扩容操作是怎么实现的？</h4><p>①.在jdk1.8中，resize方法是在hashmap中的键值对大于阀值时或者初始化时，就调用resize方法进行扩容；</p>
<p>②.每次扩展的时候，都是扩展2倍；</p>
<p>③.扩展后Node对象的位置要么在原位置，要么移动到原偏移量两倍的位置。</p>
<h4 id="HashMap是怎么解决哈希冲突的？"><a href="#HashMap是怎么解决哈希冲突的？" class="headerlink" title="HashMap是怎么解决哈希冲突的？"></a>HashMap是怎么解决哈希冲突的？</h4><p>什么是哈希：<strong>就是把任意长度的输入通过散列算法，变换成固定长度的输出，该输出就是散列值（哈希值）</strong></p>
<p>基本特性：<strong>根据同一散列函数计算出的散列值如果不同，那么输入值肯定也不同。但是，根据同一散列函数计算出的散列值如果相同，输入值不一定相同</strong></p>
<p>什么是哈希冲突：<strong>当两个不同的输入值，根据同一散列函数计算出相同的散列值的现象，我们就把它叫做碰撞（哈希碰撞）</strong></p>
<p>HashMap的数据结构：<strong>数组的特点是：寻址容易，插入和删除困难；链表的特点是：寻址困难，但插入和删除容易</strong></p>
<p>hash()函数：<strong>与自己右移16位进行异或运算（高低位异或）</strong></p>
<h4 id="能否使用任何类作为-Map-的-key？"><a href="#能否使用任何类作为-Map-的-key？" class="headerlink" title="能否使用任何类作为 Map 的 key？"></a>能否使用任何类作为 Map 的 key？</h4><p>可以，考虑一下几点：</p>
<p>1.重写了 equals() 方法，也应该重写 hashCode() 方法。</p>
<p>2.遵循与 equals() 和 hashCode() 相关的规则。</p>
<p>3.用户自定义 Key 类最佳实践是使之为不可变的</p>
<h4 id="为什么HashMap中String、Integer这样的包装类适合作为Key？"><a href="#为什么HashMap中String、Integer这样的包装类适合作为Key？" class="headerlink" title="为什么HashMap中String、Integer这样的包装类适合作为Key？"></a>为什么HashMap中String、Integer这样的包装类适合作为Key？</h4><p>1.都是final类型，即不可变性，保证key的不可更改性，不会存在获取hash值不同的情况</p>
<p>2.内部已重写了<code>equals()</code>、<code>hashCode()</code>等方法，遵守了HashMap内部的规范</p>
<h4 id="如果使用Object作为HashMap的Key，应该怎么办呢？"><a href="#如果使用Object作为HashMap的Key，应该怎么办呢？" class="headerlink" title="如果使用Object作为HashMap的Key，应该怎么办呢？"></a>如果使用Object作为HashMap的Key，应该怎么办呢？</h4><p>重写<code>hashCode()</code>和<code>equals()</code>方法</p>
<h4 id="HashMap为什么不直接使用hashCode-处理后的哈希值直接作为table的下标？"><a href="#HashMap为什么不直接使用hashCode-处理后的哈希值直接作为table的下标？" class="headerlink" title="HashMap为什么不直接使用hashCode()处理后的哈希值直接作为table的下标？"></a>HashMap为什么不直接使用hashCode()处理后的哈希值直接作为table的下标？</h4><p><code>hashCode()</code>方法返回的是int整数类型，其范围为-(2 ^ 31)~(2 ^ 31 - 1)，约有40亿个映射空间；哈希值可能不在数组大小范围内，进而无法匹配存储位置</p>
<h4 id="HashMap-的长度为什么是2的幂次方"><a href="#HashMap-的长度为什么是2的幂次方" class="headerlink" title="HashMap 的长度为什么是2的幂次方"></a>HashMap 的长度为什么是2的幂次方</h4><p> hash%length==hash&amp;(length-1)的前提是 length 是2的 n 次方；</p>
<h4 id="HashMap-与-HashTable-有什么区别？"><a href="#HashMap-与-HashTable-有什么区别？" class="headerlink" title="HashMap 与 HashTable 有什么区别？"></a>HashMap 与 HashTable 有什么区别？</h4><p>1.线程安全 hashtable用synchronized修饰</p>
<p>2.效率 hashmap效率高</p>
<p>3.对null key的支持 hashmap可以 hashtable报错</p>
<p>4.Hashtable 默认大小11，之后扩充，容量为原来2n+1。HashMap 默认大小16。扩充，原来的2倍</p>
<h4 id="如何决定使用-HashMap-还是-TreeMap？"><a href="#如何决定使用-HashMap-还是-TreeMap？" class="headerlink" title="如何决定使用 HashMap 还是 TreeMap？"></a>如何决定使用 HashMap 还是 TreeMap？</h4><p>对于在Map中插入、删除和定位元素，HashMap最好。然而，对一个有序的key集合进行遍历，TreeMap更好</p>
<h4 id="HashMap-和-ConcurrentHashMap-的区别"><a href="#HashMap-和-ConcurrentHashMap-的区别" class="headerlink" title="HashMap 和 ConcurrentHashMap 的区别"></a>HashMap 和 ConcurrentHashMap 的区别</h4><p>1.JDK1.8之后ConcurrentHashMap启用了一种全新的方式实现,利用CAS算法。</p>
<p>2.hashmap允许null</p>
<h4 id="ConcurrentHashMap-和-Hashtable-的区别？"><a href="#ConcurrentHashMap-和-Hashtable-的区别？" class="headerlink" title="ConcurrentHashMap 和 Hashtable 的区别？"></a>ConcurrentHashMap 和 Hashtable 的区别？</h4><p>1.底层数据结构，ConcurrentHashMap：数组+链表/红黑二叉树；Hashtable：数组+链表</p>
<p>2.<strong>实现线程安全的方式（重要）</strong>：① 在JDK1.7的时候，ConcurrentHashMap（分段锁） 对整个桶数组进行了分割分段(Segment)，每一把锁只锁容器其中一部分数据，多线程访问容器里不同数据段的数据，就不会存在锁竞争，提高并发访问率。（默认分配16个Segment，比Hashtable效率提高16倍。） 到了 JDK1.8 的时候已经摒弃了Segment的概念，而是直接用 Node 数组+链表+红黑树的数据结构来实现，并发控制使用 synchronized 和 CAS 来操作。（JDK1.6以后 对 synchronized锁做了很多优化） 整个看起来就像是优化过且线程安全的 HashMap，虽然在JDK1.8中还能看到 Segment 的数据结构，但是已经简化了属性，只是为了兼容旧版本；② Hashtable(同一把锁) :使用 synchronized 来保证线程安全，效率非常低下。当一个线程访问同步方法时，其他线程也访问同步方法，可能会进入阻塞或轮询状态，如使用 put 添加元素，另一个线程不能使用 put 添加元素，也不能使用 get，竞争会越来越激烈效率越低。</p>
<h4 id="ConcurrentHashMap-底层具体实现知道吗？实现原理是什么？"><a href="#ConcurrentHashMap-底层具体实现知道吗？实现原理是什么？" class="headerlink" title="ConcurrentHashMap 底层具体实现知道吗？实现原理是什么？"></a>ConcurrentHashMap 底层具体实现知道吗？实现原理是什么？</h4><p>JDK1.7: ConcurrentHashMap采用Segment + HashEntry的方式进行实现</p>
<p>JDK1.8: synchronized只锁定当前链表或红黑二叉树的首节点</p>
<h4 id="Array-和-ArrayList-有何区别？"><a href="#Array-和-ArrayList-有何区别？" class="headerlink" title="Array 和 ArrayList 有何区别？"></a>Array 和 ArrayList 有何区别？</h4><p>Array 存储基本数据类型和对象，ArrayList 只能存储对象。<br>Array 是指定固定大小的，而 ArrayList 大小是自动扩展的。<br>Array 内置方法没有 ArrayList 多，比如 addAll、removeAll、iteration 等方法只有 ArrayList 有。</p>
<h4 id="如何实现-Array-和-List-之间的转换？"><a href="#如何实现-Array-和-List-之间的转换？" class="headerlink" title="如何实现 Array 和 List 之间的转换？"></a>如何实现 Array 和 List 之间的转换？</h4><ul>
<li>Array 转 List： Arrays. asList(array) ；</li>
<li>List 转 Array：List 的 toArray() 方法。</li>
</ul>
<h4 id="comparable-和-comparator的区别？"><a href="#comparable-和-comparator的区别？" class="headerlink" title="comparable 和 comparator的区别？"></a>comparable 和 comparator的区别？</h4><ul>
<li>comparable接口实际上是出自java.lang包，它有一个 compareTo(Object obj)方法用来排序</li>
<li>comparator接口实际上是出自 java.util 包，它有一个compare(Object obj1, Object obj2)方法用来排序</li>
</ul>
<h4 id="Collection-和-Collections-有什么区别？"><a href="#Collection-和-Collections-有什么区别？" class="headerlink" title="Collection 和 Collections 有什么区别？"></a>Collection 和 Collections 有什么区别？</h4><ul>
<li>java.util.Collection 是一个集合接口（集合类的一个顶级接口）。它提供了对集合对象进行基本操作的通用接口方法。Collection接口在Java 类库中有很多具体的实现。Collection接口的意义是为各种具体的集合提供了最大化的统一操作方式，其直接继承接口有List与Set。</li>
<li>Collections则是集合类的一个工具类/帮助类，其中提供了一系列静态方法，用于对集合中元素进行排序、搜索以及线程安全等各种操作。</li>
</ul>
<h4 id="TreeMap-和-TreeSet-在排序时如何比较元素？Collections-工具类中的-sort-方法如何比较元素？"><a href="#TreeMap-和-TreeSet-在排序时如何比较元素？Collections-工具类中的-sort-方法如何比较元素？" class="headerlink" title="TreeMap 和 TreeSet 在排序时如何比较元素？Collections 工具类中的 sort()方法如何比较元素？"></a>TreeMap 和 TreeSet 在排序时如何比较元素？Collections 工具类中的 sort()方法如何比较元素？</h4><ol>
<li>TreeSet 要求存放的对象所属的类必须实现 Comparable 接口，该接口提供了比较元素的 compareTo()方法，当插入元素时会回调该方法比较元素的大小。TreeMap 要求存放的键值对映射的键必须实现 Comparable 接口从而根据键对元素进 行排 序。</li>
<li>Collections 工具类的 sort 方法有两种重载的形式，</li>
</ol>
<p>第一种要求传入的待排序容器中存放的对象比较实现 Comparable 接口以实现元素的比较；</p>
<p>第二种不强制性的要求容器中的元素必须可比较，但是要求传入第二个参数，参数是Comparator 接口的子类型（需要重写 compare 方法实现元素的比较），相当于一个临时定义的排序规则，其实就是通过接口注入比较元素大小的算法，也是对回调模式的应用（Java 中对函数式编程的支持）。</p>
]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>Collections</tag>
      </tags>
  </entry>
  <entry>
    <title>初识alluxio</title>
    <url>/2020/01/12/%E5%88%9D%E8%AF%86alluxio/</url>
    <content><![CDATA[<p>Alluxio<br>    Tachyon 前身的名字  超光速粒子<br>    以内存为中心的分布式文件系统<br>        HDFS、S3….<br>    介于计算层和存储层之间<br>        计算层：Spark、Flink、MapReduce<br>    存储层在内存中的一个Cache系统<br>    Spark/Alluxio：AMPLab<br>    2012/12 0.1.0<br>    将计算和存储分离    移动计算优于移动数据</p>
<p>能够为我们带来什么？？？</p>
<pre><code>Flink能否替代Spark成为第三代/新一代执行引擎？
Hadoop真的凉了吗？那我还有必须学习Hadoop吗？
Flume吞吐量多少？Spark Application放多少资源？
如何保证数据不丢失

自动动手测试一下

时效性的要求是越来越高的

基于内存  Memory is King   Spark Flink

两面性</code></pre><p>1） 2 Spark Application 需要共享数据，必须通过写XX操作<br>2）基于JVM对数据进行缓存<br>    Spark Application = 1 Driver + N executor<br>3）2 Spark Application操作相同的数据<br>    HDFS ==&gt; WC ==&gt; SINK<br>    HDFS ==&gt; XXX ==&gt; SINK</p>
<p>Alluxio不是Apache的顶级项目<br>    <a href="https://www.alluxio.io/" target="_blank" rel="noopener">https://www.alluxio.io/</a><br>    <a href="https://github.com/Alluxio/alluxio" target="_blank" rel="noopener">https://github.com/Alluxio/alluxio</a></p>
<p>特点：<br>1）原生的API和文件系统的非常类似<br>2）兼容性   Hadoop   Spark  Flink<br>3）列式<br>4）底层文件系统是可插拔的<br>5）Web UI<br>6）Command line interaction<br>    hadoop/hdfs fs -ls …<br>    alluxio fs ….</p>
<p>Spark 两个不同角度的应用进行实战<br>    Spark 离线<br>    Spark 实时</p>
<p>Alluxio部署<br>    1）下载<br>    2）解压到app<br>    3）配置到系统环境变量<br>    4）conf/<br>        alluxio-site.properties<br>        masters<br>        workers<br>    5）格式化<br>    6）启动<br>    7）hadoop000:19999 可以看到Alluxio的Web UI</p>
<p>Alluxio常用的命令行参数<br>    alluxio fs<br>        ls lsr mkdir cat<br>        copyFromLocal copyToLocal mv<br>        pin<br>        count  location</p>
<p>Alluxio和HDFS整合</p>
<p>Alluxio和MapReduce整合</p>
<p>hadoop jar hadoop-mapreduce-examples-2.6.0-cdh5.15.1.jar wordcount -libjars /home/hadoop/app/alluxio-1.8.1/client/alluxio-1.8.1-client.jar alluxio://hadoop000:19998/alluxio/wc/input/hello.txt alluxio://hadoop000:19998/alluxio/wc/output</p>
<p>Alluxio和Spark整合</p>
<p>做了这几个与Alluxio的整合，业务逻辑根本没有发生变化，只是:</p>
<p>1) 环境上变化<br>2) hdfs ==&gt; alluxio</p>
]]></content>
      <categories>
        <category>bigdata</category>
      </categories>
      <tags>
        <tag>alluxio</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark优化笔记</title>
    <url>/2020/01/12/Spark%E4%BC%98%E5%8C%96%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<h5 id="优化杂谈"><a href="#优化杂谈" class="headerlink" title="优化杂谈"></a>优化杂谈</h5><p>优化点一：资源<br>    spark作业在运行的时候能占用多少资源：cpu、memory<br>    分配”足够多“的资源，在一定范围内，增加资源 和 性能提升 成正比的<br>    Spark on YARN 作业跑在规划好的YARN的队列中</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">./bin/spark-submit --class org.apache.spark.examples.SparkPi \</span><br><span class="line">    --master yarn \</span><br><span class="line">    --deploy-mode cluster \</span><br><span class="line">    --driver-memory 4g \    # Driver的内存</span><br><span class="line">    --executor-memory 2g \  # 每个Executor的内存</span><br><span class="line">    --executor-cores 1 \    # Executor的cpu core的数量</span><br><span class="line">    --queue thequeue \      # 运行在YARN的哪个队列上</span><br><span class="line">    --num-executors 3 \     # Executor的数量 </span><br><span class="line">    examples/jars/spark-examples*.jar \</span><br><span class="line">    10</span><br></pre></td></tr></table></figure>

<pre><code>送你们一句话：尽量将你的作业使用的资源调整到最大

YARN: pkspark  400G 100C
    50exe ==&gt; 
        executor-memory = 8G
        executor-cores  = 2C

num-executors + :    task的并行度  num*cores    
    4exe 2core = 8task
    8exe 2core = 16task
    100task 

executor-cores + : task的并行度

executor-memory + :
    能cache的数据多 ==&gt; 写入disk的次数会降低
    shuffle   IO
    JVM   GC

思考：Spark ETL HBase 运行在YARN之上</code></pre><p>调优之算子的选择<br>    map<br>        def map[U: ClassTag](f: T =&gt; U): RDD[U]</p>
<pre><code>mapPartitions
    def mapPartitions[U: ClassTag](
          f: Iterator[T] =&gt; Iterator[U],
          preservesPartitioning: Boolean = false): RDD[U]

transforamtion:转换算子

RDD = 2Partitions (2 * 1w = 2w)
    map  2w
    mapPartitions  2 </code></pre><p>QA：转换算子能生成Job吗？</p>
<pre><code>foreach 
    def foreach(f: T =&gt; Unit)

foreachPartitions
    def foreachPartition(f: Iterator[T] =&gt; Unit)

Action算子

送你们一句话：如果涉及到写数据库操作，
    建议采用带Partitions的，但是由于mapPartitions是一个transforamtion算子，所以建议采用foreachPartitions

    OOM
    使用之前：
        评估你要处理的RDD的数据量
        每个partition的数据量
        整个作业使用到的资源</code></pre><p>生产或者面试：Spark自定义排序</p>
<p>class 和 case class在使用层面有什么区别？？？</p>
<p>Spark Streaming对接Kafka数据<br>    对于Kafka来说，我们的Spark Streaming应用程序其实就是一个消费者</p>
<pre><code>1） Spark Streaming挂了，那么就没有办法去消费Kafka中的数据了，Kafka中的数据就会有积压
2） 高峰期的时候，由于你作业的资源并没有很好的设置，在某些批次中，很可能数据比较大

batch时间到了，那么Spark Streaming就会处理这个批次中的数据
假设：batch time 10s  就会出现10s你根本处理不过来整个批次的数据
后续批次的作业就会产生挤压，那么时效性就没有办法保证

==&gt; Kafka的限速
假设限速是100


10秒一个批次
    topic 是1个分区：10 * 1 * 100 = 1000
    topic 是3个分区：10 * 3 * 100 = 3000

要提升数据处理的吞吐量：提升Kafka的分区数    </code></pre><p>Spark Streaming对接Kafka数据进行处理时，能否保证仅处理一次的语义<br>    至少一次：可能数据消费重复<br>    至多一次：可能数据有丢失<br>    仅仅一次：不会有数据的丢失，也不会重复消费   ✅</p>
<pre><code>能？ 怎么做？
不能做到？还能用吗？</code></pre><p>广播<br>    join： shuffle/reduce join   mapjoin</p>
<p>val o = xxxx   // 20M  算子的外部变量<br>rdd.map(x =&gt; {</p>
<pre><code>//....
o</code></pre><p>})    </p>
<p>每个task都会获得一份变量o的副本</p>
<p>20executor  500task ==&gt; 500 * 20M = 10G</p>
<p>如果使用了广播变量：<br>    每个executor保存一个变量o的副本</p>
<pre><code>20 * 20m = 400M </code></pre>]]></content>
      <categories>
        <category>bigdata</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title>OLTP和OLAP</title>
    <url>/2020/01/05/OLTP%E5%92%8COLAP/</url>
    <content><![CDATA[<p>1 OLTP和OLAP</p>
<p>online transaction processing，联机事务处理。业务类系统主要供基层人员使用，进行一线业务操作，通常被称为联机事务处理。</p>
<p>online analytical processing，联机分析处理。数据分析的目标是探索并挖掘数据的价值，作为企业高层进行决策的参考。</p>
<p>从功能层面上来看，OLTP负责基本业务的正常运转，业务数据积累所产生的价值信息被OLAP所呈现，根据OLAP所产生的价值信息不断优化基本业务。</p>
<p>2 OLTP</p>
<p>OLTP负责基本业务的正常运转，因此使用基本的关系型数据库就可以了。比如Mysql。</p>
<p>3 OLAP</p>
<p>基本业务生成的数据越来越多，目前流行的是分布式的处理方案，即sql on hadoop。比如百度的关系数据仓储Palo。</p>
<p>MPP架构的数据仓储是典型的OLAP应用。</p>
<p>4 MPP</p>
<p>massively parallel processing，大规模并行处理。比如非共享数据库集群。</p>
<p>图示<br><img src="https://lixiangbetter.github.io/2020/01/05/OLTP%E5%92%8COLAP/2018072122222525.png" alt="not found"></p>
<p>图片转载：<a href="https://blog.csdn.net/qq_33414271/article/details/81149966" target="_blank" rel="noopener">https://blog.csdn.net/qq_33414271/article/details/81149966</a></p>
]]></content>
      <categories>
        <category>bigdata</category>
      </categories>
      <tags>
        <tag>bigdata</tag>
      </tags>
  </entry>
  <entry>
    <title>浅谈四层和七层负载均衡</title>
    <url>/2020/01/05/%E6%B5%85%E8%B0%88%E5%9B%9B%E5%B1%82%E5%92%8C%E4%B8%83%E5%B1%82%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1/</url>
    <content><![CDATA[<h4 id="浅谈四层和七层负载"><a href="#浅谈四层和七层负载" class="headerlink" title="浅谈四层和七层负载"></a>浅谈四层和七层负载</h4><p>关于负载均衡，经常听到四层负载均衡和七层负载均衡的说法，他们之间有什么关系和区别呢，今天就简单总结概括下。</p>
<h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><blockquote>
<p>用一句话来说，<strong>四层负载均衡就是工作在计算机网络OSI七层分层的第四层（传输层）的，七层负载军和则是工作在第七层（应用层）的</strong>。</p>
</blockquote>
<p>也就是说，四层负载均衡是<strong>基于IP+端口</strong>的负载均衡，七层负载均衡是<strong>基于URL</strong>等应用层信息的负载均衡。</p>
<p>同理，还有基于MAC地址的二层负载均衡和基于IP地址的三层负载均衡。</p>
<blockquote>
<ul>
<li><strong>二层负载均衡(mac)</strong><br> 一般是用<code>虚拟mac地址</code>方式,外部对虚拟MAC地址请求,负载均衡接收后分配后端实际的MAC地址响应。</li>
<li><strong>三层负载均衡(ip)</strong><br> 一般采用<code>虚拟IP地址</code>方式,外部对虚拟的ip地址请求,负载均衡接收后分配后端实际的IP地址响应。</li>
<li><strong>四层负载均衡(tcp)</strong><br> 用<code>虚拟ip+port</code>接收请求,再转发到对应的真实机器。</li>
<li><strong>七层负载均衡(http)</strong><br> 用<code>虚拟的url或主机名</code>接收请求,再转向相应的处理服务器。</li>
</ul>
</blockquote>
<p>在实际应用中,比较常见的就是四层负载及七层负载。这里也重点说下这两种负载。</p>
<p>  所谓的四到七层负载均衡，就是在对后台的服务器进行负载均衡时，<strong>依据四层的信息或七层的信息来决定怎么样转发流量</strong>。 比如四层的负载均衡，就是通过发布三层的IP地址（VIP），然后加四层的端口号，来决定哪些流量需要做负载均衡，对需要处理的流量进行NAT处理，转发至后台服务器，并记录下这个TCP或者UDP的流量是由哪台服务器处理的，后续这个连接的所有流量都同样转发到同一台服务器处理。七层的负载均衡，就是在四层的基础上（没有四层是绝对不可能有七层的），再考虑应用层的特征，比如同一个Web服务器的负载均衡，除了根据VIP加80端口辨别是否需要处理的流量，还可根据七层的URL、浏览器类别、语言来决定是否要进行负载均衡。举个例子，如果你的Web服务器分成两组，一组是中文语言的，一组是英文语言的，那么七层负载均衡就可以当用户来访问你的域名时，自动辨别用户语言，然后选择对应的语言服务器组进行负载均衡处理。</p>
<hr>
<h3 id="具体区别"><a href="#具体区别" class="headerlink" title="具体区别"></a>具体区别</h3><p>负载均衡器通常称为<strong>四层交换机</strong>或<strong>七层交换机</strong>。那么四层和七层两者到底区别在哪里？</p>
<h5 id="1-技术原理区别"><a href="#1-技术原理区别" class="headerlink" title="1. 技术原理区别"></a>1. 技术原理区别</h5><ul>
<li>所谓<strong>四层负载均衡</strong>，也就是主要通过报文中的目标地址和端口，再加上负载均衡设备设置的服务器选择方式，决定最终选择的内部服务器。</li>
</ul>
<p>  以常见的TCP为例，负载均衡设备在接收到第一个来自客户端的SYN 请求时，即通过上述方式选择一个最佳的服务器，并对报文中目标IP地址进行修改(改为后端服务器IP），直接转发给该服务器。TCP的连接建立，即<strong>三次握手是客户端和服务器直接建立的，负载均衡设备只是起到一个类似路由器的转发动作</strong>。在某些部署情况下，为保证服务器回包可以正确返回给负载均衡设备，在转发报文的同时可能还会对报文原来的源地址进行修改。<br><img src="/Users/lx/Documents/myblog/source/_posts/%E6%B5%85%E8%B0%88%E5%9B%9B%E5%B1%82%E5%92%8C%E4%B8%83%E5%B1%82%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1/1038472-618c1fc22f893b96.jpg" alt="not found"></p>
<p>四层和七层交换机原理</p>
<ul>
<li>所谓<strong>七层负载均衡</strong>，也称为“内容交换”，也就是主要通过报文中的真正有意义的应用层内容，再加上负载均衡设备设置的服务器选择方式，决定最终选择的内部服务器。</li>
</ul>
<p>  以常见的TCP为例，负载均衡设备如果要根据真正的应用层内容再选择服务器，只能先代理最终的服务器和客户端建立连接(三次握手)后，才可能接受到客户端发送的真正应用层内容的报文，然后再根据该报文中的特定字段，再加上负载均衡设备设置的服务器选择方式，决定最终选择的内部服务器。<strong>负载均衡设备在这种情况下，更类似于一个代理服务器</strong>。负载均衡和前端的客户端以及后端的服务器会分别建立TCP连接。所以从这个技术原理上来看，七层负载均衡明显的对负载均衡设备的要求更高，处理七层的能力也必然会低于四层模式的部署方式。</p>
<h5 id="2-应用场景区别"><a href="#2-应用场景区别" class="headerlink" title="2.应用场景区别"></a>2.应用场景区别</h5><p>  七层因为可以代理任意修改和处理用户的请求，所以可以使整个应用更加智能化和安全，代价就是设计和配置会更复杂。所以是否有必要使用七层负载均衡是一个需要权衡的问题。</p>
<p>  现在的7层负载均衡，主要还是着重于应用HTTP协议，所以其应用范围主要是众多的网站或者内部信息平台等基于B/S开发的系统。 4层负载均衡则对应其他TCP应用，例如基于C/S开发的ERP等系统。</p>
<p>原文链接：<a href="https://www.jianshu.com/p/04518b017c90" target="_blank" rel="noopener">https://www.jianshu.com/p/04518b017c90</a></p>
]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>负载均衡</tag>
      </tags>
  </entry>
  <entry>
    <title>storm创建DRPC远程客户端</title>
    <url>/2019/12/11/storm%E5%88%9B%E5%BB%BAdrpc%E8%BF%9C%E7%A8%8B%E5%AE%A2%E6%88%B7%E7%AB%AF/</url>
    <content><![CDATA[<h4 id="创建DRPC远程客户端"><a href="#创建DRPC远程客户端" class="headerlink" title="创建DRPC远程客户端"></a>创建DRPC远程客户端</h4><p>DRPCClient创建方法如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//conf map, drpc server, port no, timeout for the call</span></span><br><span class="line"><span class="keyword">new</span> DRPCClient(conf, <span class="string">"192.168.0.217"</span>, <span class="number">3772</span>, <span class="number">5000</span>);</span><br></pre></td></tr></table></figure>

<p>conf如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Config conf = <span class="keyword">new</span> Config();</span><br><span class="line">conf.setDebug(<span class="keyword">false</span>);</span><br></pre></td></tr></table></figure>

<p>这将产生下列这个错误：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">java.lang.NullPointerException</span><br><span class="line">java.lang.RuntimeException: java.lang.NullPointerException</span><br><span class="line">at backtype.storm.security.auth.AuthUtils.GetTransportPlugin(AuthUtils.java:<span class="number">230</span>)</span><br><span class="line">at backtype.storm.security.auth.ThriftClient.reconnect(ThriftClient.java:<span class="number">91</span>)</span><br></pre></td></tr></table></figure>

<p>如何添加下列这句话：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">conf.put(<span class="string">"storm.thrift.transport"</span>, <span class="string">"backtype.storm.security.auth.SimpleTransportPlugin"</span>);</span><br></pre></td></tr></table></figure>

<p>这将继续报这个错误：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Don<span class="string">'t know how to convert null to int</span></span><br><span class="line"><span class="string">java.lang.IllegalArgumentException: Don'</span>t know how to convert <span class="keyword">null</span> to <span class="keyword">int</span></span><br><span class="line">at backtype.storm.utils.Utils.getInt(Utils.java:<span class="number">420</span>)</span><br><span class="line">at backtype.storm.security.auth.ThriftClient.reconnect(ThriftClient.java:<span class="number">100</span>)</span><br></pre></td></tr></table></figure>

<p>在storm0.10之后就已经做了改进，使用map来传递配置参数。</p>
<p>正确做法：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Config conf = <span class="keyword">new</span> Config();</span><br><span class="line">Map defaultConfig = Utils.readDefaultConfig();</span><br><span class="line"></span><br><span class="line">defaultConfig.put(<span class="string">"storm.thrift.transport"</span>,<span class="string">"org.apache.storm.security.auth.SimpleTransportPlugin"</span>);</span><br><span class="line">defaultConfig.put(Config.STORM_EXHIBITOR_RETRY_TIMES, <span class="number">3</span>);</span><br><span class="line">defaultConfig.put(Config.STORM_EXHIBITOR_RETRY_INTERVAL, <span class="number">10</span>);</span><br><span class="line">defaultConfig.put(Config.STORM_EXHIBITOR_RETRY_INTERVAL_CEILING, <span class="number">20</span>);</span><br><span class="line">defaultConfig.put(Config.DRPC_MAX_BUFFER_SIZE, <span class="number">1048576</span>);</span><br><span class="line"></span><br><span class="line">conf.putAll(defaultConfig);</span><br><span class="line"></span><br><span class="line">DRPCClient drpcClient = <span class="keyword">new</span> DRPCClient(conf,<span class="string">"localhost"</span>, <span class="number">3772</span>,<span class="number">5000</span>);</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>bigdata</category>
      </categories>
      <tags>
        <tag>storm</tag>
      </tags>
  </entry>
  <entry>
    <title>研一前的暑假，深度学习初体验</title>
    <url>/2019/08/10/%E7%A0%94%E4%B8%80%E5%89%8D%E7%9A%84%E6%9A%91%E5%81%87%EF%BC%8C%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%88%9D%E4%BD%93%E9%AA%8C/</url>
    <content><![CDATA[<h1 id="选导师"><a href="#选导师" class="headerlink" title="选导师"></a>选导师</h1><p>&nbsp;&nbsp;&nbsp;&nbsp;在五月二十七号左右，再次来到一所新的城市，即将在这里度过我研究生的三年，之所以来这么早，是因为希望研究生阶段，能够跟着一个研究方向，自己比较感兴趣的导师。找的第一个导师比较偏学术一些，而我又希望自己在研究生阶段能够有一定的项目经历，所以在经过思考后，我和老师表明了我的想法，老师也表示理解，和我推荐了其他的导师。在找导师的过程中，发现比较热门的导师的特点有以下几点。在校刚来的几位导师中，比较年轻的导师无论能力还是学术都比较强，所有也很快就定下了人选。在来之前，自己也大致确立了方向，大数据。学校有个大数据院，看了相关的介绍，这一方向在校长的带领下，是发展的非常不错的。但是对于我看似来的早，实际上并不早的人来说，是轮不到我了。。此处省略一万字吧。最终找到了我现在的导师，导师管理的公司主要是做图像识别。也就属于当前的计算机视觉方向，被分配在公司的算法组，所以也就不得不走向了研究深度学习的方向。</p>
<h1 id="做事情"><a href="#做事情" class="headerlink" title="做事情"></a>做事情</h1><p>&nbsp;&nbsp;&nbsp;&nbsp;在简单的熟悉了公司之后，由于初来公司，并没有分配什么很具体的工作，所以自己在没事的时候，我选择继续考研之后做的事情，那就是学习springboot. 由于在公司里，没有很具体的工作，每天自己看视频自学，一个视频教程边看自己边跟着实践，也很快就完成了。 就这样，看了springboot企业微信点餐，springcloud升级企业微信点餐两个教程。就在这个时候，公司有准备让我写一些简单的接口。这时候随着对微服务的了解之后，我发现我对这个概念非常感兴趣。所以我不断去深入了解这一块的知识。</p>
<h1 id="深度学习相关"><a href="#深度学习相关" class="headerlink" title="深度学习相关"></a>深度学习相关</h1><p>&nbsp;&nbsp;&nbsp;&nbsp;在机器学习中，我们通常使用梯度下降来更新模型参数从而求解。损失函数关于模型参数的梯度指向一个可以降低损失函数值的方向，我们不断地沿着梯度的方向更新模型从而最小化损失函数。</p>
<h2 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;线性回归输出是一个连续值，因此适用于回归问题。回归问题在实际中很常见，如预测房屋价格、气温、销售额等连续值的问题。与回归问题不同，分类问题中模型的最终输出是一个离散值。我们所说的图像分类、垃圾邮件识别、疾病检测等输出为离散值的问题都属于分类问题的范畴。softmax回归则适用于分类问题。</p>
<h3 id="线性回归的基本要素"><a href="#线性回归的基本要素" class="headerlink" title="线性回归的基本要素"></a>线性回归的基本要素</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;我们以一个简单的房屋价格预测作为例子来解释线性回归的基本要素。这个应用的目标是预测一栋房子的售出价格（元）。我们知道这个价格取决于很多因素，如房屋状况、地段、市场行情等。为了简单起见，这里我们假设价格只取决于房屋状况的两个因素，即面积（平方米）和房龄（年）。接下来我们希望探索价格与这两个因素的具体关系。</p>
<h3 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;设房屋的面积为x1，房龄为x2，售出价格为y。我们需要建立基于输入x1和x2来计算输出y的表达式，也就是模型（model）。顾名思义，线性回归假设输出与各个输入之间是线性关系：<br>&nbsp;&nbsp;&nbsp;&nbsp;y^=x1w1+x2w2+b,<br>&nbsp;&nbsp;&nbsp;&nbsp;其中w1和w2是权重（weight），b是偏差（bias），且均为标量。它们是线性回归模型的参数（parameter）。模型输出ˆy是线性回归对真实价格y的预测或估计。我们通常允许它们之间有一定误差。</p>
<h3 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;接下来我们需要通过数据来寻找特定的模型参数值，使模型在数据上的误差尽可能小。这个过程叫作模型训练（model training）。下面我们介绍模型训练所涉及的3个要素。</p>
<h4 id="训练数据"><a href="#训练数据" class="headerlink" title="训练数据"></a>训练数据</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;我们通常收集一系列的真实数据，例如多栋房屋的真实售出价格和它们对应的面积和房龄。我们希望在这个数据上面寻找模型参数来使模型的预测价格与真实价格的误差最小。在机器学习术语里，该数据集被称为训练数据集（training data set）或训练集（training set），一栋房屋被称为一个样本（sample），其真实售出价格叫作标签（label），用来预测标签的两个因素叫作特征（feature）。特征用来表征样本的特点。<br>&nbsp;&nbsp;&nbsp;&nbsp;假设我们采集的样本数为n，索引为i的样本的特征为x(i)1和x(i)2，标签为y(i)。对于索引为i的房屋，线性回归模型的房屋价格预测表达式为<br>&nbsp;&nbsp;&nbsp;&nbsp;y(i)=x1(i)w1+x2(i)w2+b.</p>
<h4 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;在模型训练中，我们需要衡量价格预测值与真实值之间的误差。通常我们会选取一个非负数作为误差，且数值越小表示误差越小。一个常用的选择是平方函数。<br>    给定训练数据集，这个误差只与模型参数相关，因此我们将它记为以模型参数为参数的函数。在机器学习里，将衡量误差的函数称为损失函数（loss function）。<br>&nbsp;&nbsp;&nbsp;&nbsp;通常，我们用训练数据集中所有样本误差的平均来衡量模型预测的质量.<br>&nbsp;&nbsp;&nbsp;&nbsp;在模型训练中，我们希望找出一组模型参数，记为w∗1,w∗2,b∗，来使训练样本平均损失最小.</p>
<h4 id="优化算法"><a href="#优化算法" class="headerlink" title="优化算法"></a>优化算法</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;当模型和损失函数形式较为简单时，上面的误差最小化问题的解可以直接用公式表达出来。这类解叫作解析解（analytical solution）。本节使用的线性回归和平方误差刚好属于这个范畴。然而，大多数深度学习模型并没有解析解，只能通过优化算法有限次迭代模型参数来尽可能降低损失函数的值。这类解叫作数值解（numerical solution）。<br>&nbsp;&nbsp;&nbsp;&nbsp;在求数值解的优化算法中，小批量随机梯度下降（mini-batch stochastic gradient descent）在深度学习中被广泛使用。它的算法很简单：先选取一组模型参数的初始值，如随机选取；接下来对参数进行多次迭代，使每次迭代都可能降低损失函数的值。在每次迭代中，先随机均匀采样一个由固定数目训练数据样本所组成的小批量（mini-batch）β，然后求小批量中数据样本的平均损失有关模型参数的导数（梯度），最后用此结果与预先设定的一个正数的乘积作为模型参数在本次迭代的减小量。<br>&nbsp;&nbsp;&nbsp;&nbsp;在迭代的过程中，β代表每个小批量中的样本个数（批量大小，batch size），η称作学习率（learning rate）并取正数。需要强调的是，这里的批量大小和学习率的值是人为设定的，并不是通过模型训练学出的，因此叫作超参数（hyperparameter）。我们通常所说的“调参”指的正是调节超参数，例如通过反复试错来找到超参数合适的值。</p>
<h3 id="模型预测"><a href="#模型预测" class="headerlink" title="模型预测"></a>模型预测</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;模型训练完成后，我们将模型参数w1, w2, b在优化算法停止时的值分别记作w<sub>1</sub>, w<sub>2</sub>, b。注意，这里我们得到的并不一定是最小化损失函数的最优解w*<sub>1</sub>, w*<sub>2</sub>，b*，而是对最优解的一个近似。然后，我们就可以使用学出的线性回归模型x1w1+ x2w2 + b来估算训练数据集以外任意一栋面积（平方米）为x1、房龄（年）为x2的房屋的价格了。这里的估算也叫作模型预测、模型推断或模型测试。</p>
]]></content>
      <categories>
        <category>计算机视觉</category>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title>Hello World</title>
    <url>/2019/03/11/hello-world/</url>
    <content><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>
]]></content>
  </entry>
</search>
