<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title>日常笔记-2021-11-12</title>
    <url>/2022/07/24/%E6%97%A5%E5%B8%B8%E7%AC%94%E8%AE%B0-2021-11-12/</url>
    <content><![CDATA[<h3 id="待整理"><a href="#待整理" class="headerlink" title="待整理"></a>待整理</h3><p>apache beam</p>
<p>数据迁移。自动、压缩、小文件</p>
<p>spark sql开发一个类似于presto的框架</p>
<h3 id="Git"><a href="#Git" class="headerlink" title="Git"></a>Git</h3><h4 id="Git-pull-强制拉取并覆盖本地代码"><a href="#Git-pull-强制拉取并覆盖本地代码" class="headerlink" title="Git pull 强制拉取并覆盖本地代码"></a>Git pull 强制拉取并覆盖本地代码</h4><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">git fetch --all</span><br><span class="line">git reset --hard origin/master</span><br><span class="line">git pull</span><br></pre></td></tr></table></figure>

<h3 id="python下载文件"><a href="#python下载文件" class="headerlink" title="python下载文件"></a>python下载文件</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">python -m SimpleHTTPServer 8111</span><br></pre></td></tr></table></figure>

<h3 id="指定用户"><a href="#指定用户" class="headerlink" title="指定用户"></a>指定用户</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">export HADOOP_USER_NAME=*xxxx*</span><br></pre></td></tr></table></figure>

<h3 id="火焰图阅读"><a href="#火焰图阅读" class="headerlink" title="火焰图阅读"></a>火焰图阅读</h3><p><a href="https://www.cnblogs.com/tcicy/p/8491899.html" target="_blank" rel="noopener">https://www.cnblogs.com/tcicy/p/8491899.html</a></p>
<h3 id="gc日志查看工具"><a href="#gc日志查看工具" class="headerlink" title="gc日志查看工具"></a>gc日志查看工具</h3><p><a href="https://gceasy.io/gc-index.jsp" target="_blank" rel="noopener">https://gceasy.io/gc-index.jsp</a></p>
<h3 id="远程debug"><a href="#远程debug" class="headerlink" title="远程debug"></a>远程debug</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">-agentlib:jdwp=transport=dt_socket,server=y,suspend=n,address=8001</span><br></pre></td></tr></table></figure>

<h3 id="case-when两种用法"><a href="#case-when两种用法" class="headerlink" title="case when两种用法"></a>case when两种用法</h3><p><a href="https://blog.csdn.net/qq_36501591/article/details/104005083" target="_blank" rel="noopener">https://blog.csdn.net/qq_36501591/article/details/104005083</a></p>
<h3 id="scala编译"><a href="#scala编译" class="headerlink" title="scala编译"></a>scala编译</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">mvn clean scala:compile compile package assembly:assembly</span><br></pre></td></tr></table></figure>

<h3 id="spark远程提交和调试"><a href="#spark远程提交和调试" class="headerlink" title="spark远程提交和调试"></a>spark远程提交和调试</h3><p><a href="https://blog.csdn.net/yiluohan0307/article/details/80048765" target="_blank" rel="noopener">https://blog.csdn.net/yiluohan0307/article/details/80048765</a></p>
<p><a href="https://mp.weixin.qq.com/s/Rwz5uAI-TfnTBpppsMTfBg" target="_blank" rel="noopener">https://mp.weixin.qq.com/s/Rwz5uAI-TfnTBpppsMTfBg</a></p>
<h3 id="quota的设置"><a href="#quota的设置" class="headerlink" title="quota的设置"></a>quota的设置</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">hdfs dfs -put hdfs://bipcluster08/tmp/lx/</span><br><span class="line">hdfs dfsadmin -setSpaceQuota 1KB</span><br><span class="line">hdfs dfsadmin –clrSpaceQuota hdfs://bipcluster08/tmp/lx/</span><br></pre></td></tr></table></figure>

<h3 id="NoSuchMethodError-org-apache-hadoop-io-retry-RetryUtils-getDefaultRetryPolicy"><a href="#NoSuchMethodError-org-apache-hadoop-io-retry-RetryUtils-getDefaultRetryPolicy" class="headerlink" title="NoSuchMethodError: org.apache.hadoop.io.retry.RetryUtils.getDefaultRetryPolicy"></a>NoSuchMethodError: org.apache.hadoop.io.retry.RetryUtils.getDefaultRetryPolicy</h3><figure class="highlight"><table><tr><td class="code"><pre><span class="line">&lt;dependency&gt;</span><br><span class="line">  &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;</span><br><span class="line">  &lt;artifactId&gt;hadoop-hdfs&lt;/artifactId&gt;</span><br><span class="line">  &lt;version&gt;2.8.1&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure>

<h3 id="hive建表常用"><a href="#hive建表常用" class="headerlink" title="hive建表常用"></a>hive建表常用</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">// 创建一个分区表</span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> vip_dp.dept_part(</span><br><span class="line">    deptno <span class="built_in">int</span>,</span><br><span class="line">    dname <span class="keyword">string</span>,</span><br><span class="line">    loc <span class="keyword">string</span></span><br><span class="line">)</span><br><span class="line">partitioned <span class="keyword">by</span> (dt <span class="keyword">string</span>)</span><br><span class="line"><span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> vip_dp.name_list(<span class="keyword">name</span> <span class="keyword">string</span>) location <span class="string">'hdfs://bipnormal/bip/hive_warehouse/vip_dp.db/name_list'</span>;</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">table</span> vip_dp.name_list <span class="keyword">values</span>(<span class="string">'金融'</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> default.dept_part(deptno <span class="built_in">int</span>,dname <span class="keyword">string</span>,loc <span class="keyword">string</span>) partitioned <span class="keyword">by</span> (dt <span class="keyword">string</span>) <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span>;</span><br><span class="line"><span class="keyword">insert</span> <span class="keyword">into</span> <span class="keyword">table</span> vip_dp.dept_part <span class="keyword">partition</span>(dt=<span class="string">'2021-08-01'</span>) <span class="keyword">values</span>(<span class="number">1</span>,<span class="string">'金融'</span>,<span class="string">'1F'</span>);</span><br><span class="line"><span class="keyword">insert</span> overwrite <span class="keyword">table</span> vip_dp.dept_part <span class="keyword">partition</span>(dt=<span class="string">'2021-08-01'</span>) <span class="keyword">values</span>(<span class="number">1</span>,<span class="string">'金融'</span>,<span class="string">'1F'</span>);</span><br><span class="line"></span><br><span class="line"><span class="keyword">alter</span> <span class="keyword">table</span> vip_dp.dept_part <span class="keyword">add</span> <span class="keyword">partition</span>(dt=<span class="string">'2021-08-01'</span>)</span><br></pre></td></tr></table></figure>

<h3 id="申请创建hdfs目录"><a href="#申请创建hdfs目录" class="headerlink" title="申请创建hdfs目录"></a>申请创建hdfs目录</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hadoop fs -mkdir -p hdfs://bipcluster07/bip/external_data/frontend_ai</span><br><span class="line">hadoop fs -chown -R vip_frontend_ai:hdfs hdfs://bipcluster07/bip/external_data/frontend_ai</span><br></pre></td></tr></table></figure>

<h3 id="删除block-pool"><a href="#删除block-pool" class="headerlink" title="删除block pool"></a>删除block pool</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">nohup rm -rf /home/vipshop/hard_disk/0/dfs/dn/current/BP-1203969992-10.208.50.21-1450855658517 &gt; 1450855658517_0.out &amp;</span><br></pre></td></tr></table></figure>

<h3 id="迁移ssd相关"><a href="#迁移ssd相关" class="headerlink" title="迁移ssd相关"></a>迁移ssd相关</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hdfs storagepolicies -setStoragePolicy -path $&#123;partition&#125; -policy $&#123;policy&#125;</span><br><span class="line">hdfs storagepolicies -getStoragePolicy -path hdfs://biphotdata/bip/hive_warehouse/vipcube.db/dm_sup_thematic_tool_goods_code</span><br></pre></td></tr></table></figure>

<h3 id="linux时间函数"><a href="#linux时间函数" class="headerlink" title="linux时间函数"></a>linux时间函数</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">date -d &quot;1000 +30 minute&quot; +&quot;%H%M&quot;</span><br></pre></td></tr></table></figure>

<h3 id="hive表权限相关"><a href="#hive表权限相关" class="headerlink" title="hive表权限相关"></a>hive表权限相关</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">GRANT</span> <span class="keyword">ALL</span> <span class="keyword">on</span> <span class="keyword">table</span> vipdmt.all_hotdata_rw_count <span class="keyword">to</span> <span class="keyword">user</span></span><br><span class="line"><span class="keyword">SHOW</span> <span class="keyword">GRANT</span> <span class="keyword">USER</span> u_dmt <span class="keyword">ON</span> <span class="keyword">table</span> vipdmt.all_hotdata_rw_count</span><br><span class="line"><span class="keyword">SHOW</span> <span class="keyword">GRANT</span> <span class="keyword">user</span> mllib <span class="keyword">ON</span> <span class="keyword">DATABASE</span> <span class="keyword">default</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">GRANT</span> <span class="keyword">SELECT</span> <span class="keyword">on</span> <span class="keyword">table</span> vipdmt.all_hotdata_rw_count <span class="keyword">to</span> <span class="keyword">role</span> <span class="keyword">public</span>;</span><br><span class="line"><span class="keyword">GRANT</span> <span class="keyword">SELECT</span> <span class="keyword">on</span> <span class="keyword">table</span> vipdmt.remove_tempdb_noprt_tbl_list_new <span class="keyword">to</span> <span class="keyword">role</span> <span class="keyword">public</span>;</span><br><span class="line"><span class="keyword">GRANT</span> <span class="keyword">SELECT</span> <span class="keyword">on</span> <span class="keyword">table</span> vipdmt.remove_tempdb_noprt_tbl_list_all <span class="keyword">to</span> <span class="keyword">role</span> <span class="keyword">public</span>;</span><br><span class="line"></span><br><span class="line"><span class="keyword">revoke</span> <span class="keyword">all</span> <span class="keyword">on</span> <span class="keyword">table</span> vipdmt.all_hotdata_rw_count <span class="keyword">from</span> <span class="keyword">user</span> <span class="keyword">public</span>;</span><br></pre></td></tr></table></figure>

<h3 id="spark-submit相关"><a href="#spark-submit相关" class="headerlink" title="spark submit相关"></a>spark submit相关</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">spark-submit  --class org.apache.spark.sql.TestSubmitApp \</span><br><span class="line">    --conf spark.eventLog.dir=hdfs://bipnormal/tmp/spark3 \</span><br><span class="line">    --master yarn \</span><br><span class="line">    /home/hdfs/xiang66.li/ec-1.0-SNAPSHOT-jar-with-dependencies.jar</span><br><span class="line"></span><br><span class="line">spark-submit  --class org.apache.spark.sql.EcAndFileCombine \</span><br><span class="line">    --conf spark.eventLog.dir=hdfs://bipnormal/tmp/spark3 \</span><br><span class="line">    --master yarn \</span><br><span class="line">    /home/hdfs/xiang66.li/ec-1.0-SNAPSHOT-jar-with-dependencies.jar 123456 654321 bip03_colddata_ec_or_combine_list root.basic_platform.online 0 8 4 true desc true bipnormal &apos;&apos; false true 20000 false true false false false</span><br></pre></td></tr></table></figure>

<h3 id="excel技巧"><a href="#excel技巧" class="headerlink" title="excel技巧"></a>excel技巧</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">=INDEX($A$1:$A$30,MATCH(L1,$B$1:$B$30,0),1)</span><br><span class="line">在b1-b30(key),a1-a30(value)中，找到与L1(key)对应的value</span><br></pre></td></tr></table></figure>

<h3 id="hadoop编译"><a href="#hadoop编译" class="headerlink" title="hadoop编译"></a>hadoop编译</h3><h5 id="存储"><a href="#存储" class="headerlink" title="存储"></a>存储</h5><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">export JAVA_HOME=/usr/share/java</span><br><span class="line">tar xvf build-tools/cmake-3.8.2-Linux-x86_64.tar.gz</span><br><span class="line">export PATH=$PWD/cmake-3.8.2-Linux-x86_64/bin:$PATH</span><br><span class="line">cmake -version</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> build</span></span><br><span class="line">mvn package -Pdist,src,native -Drequire.snappy -Dbundle.snappy -Dsnappy.lib=/usr/lib64 -DskipTests -Dtar -Dcontainer-executor.conf.dir=/etc/hadoop</span><br><span class="line"></span><br><span class="line">ts=$(date +%Y%m%d)</span><br><span class="line">commit_id=$&#123;sourcerevision:0:7&#125;</span><br><span class="line">cd hadoop-dist/target/</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> get spark yarn shuffle jar</span></span><br><span class="line">wget -O hadoop-3.1.3/share/hadoop/yarn/spark-2.1.0-vip-1.0.0-yarn-shuffle.jar  \</span><br><span class="line">http://cider.tools.vipshop.com/api/release/download/spark-yarn-shuffle/2.1.0-vip-1.0.0/spark-2.1.0-vip-1.0.0-yarn-shuffle.jar</span><br><span class="line"></span><br><span class="line">sudo chown -R hdfs:hdfs hadoop-3.1.3</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> cgroup</span></span><br><span class="line">sudo chown root:yarn hadoop-3.1.3/bin/container-executor</span><br><span class="line">sudo chmod 6050 hadoop-3.1.3/bin/container-executor</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> rename</span></span><br><span class="line">sudo mv hadoop-3.1.3  hadoop-3.1.3-vip-$&#123;buildbranch&#125;-$&#123;ts&#125;-$&#123;commit_id&#125;-os7</span><br><span class="line">sudo tar -zcf hadoop-3.1.3-vip-$&#123;buildbranch&#125;-$&#123;ts&#125;-$&#123;commit_id&#125;-os7.tar.gz  hadoop-3.1.3-vip-$&#123;buildbranch&#125;-$&#123;ts&#125;-$&#123;commit_id&#125;-os7</span><br></pre></td></tr></table></figure>

<h5 id="yarn"><a href="#yarn" class="headerlink" title="yarn"></a>yarn</h5><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash"> check</span></span><br><span class="line">ls /usr/share/</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> ui</span></span><br><span class="line">sudo mkdir -p ~/.m2/repository/com/github/eirslett/yarn/0.21.3</span><br><span class="line">sudo cp -f /usr/share/yarn-0.21.3.tar.gz ~/.m2/repository/com/github/eirslett/yarn/0.21.3/</span><br><span class="line"></span><br><span class="line">sudo mkdir -p  ~/.m2/repository/com/github/eirslett/node/5.12.0/</span><br><span class="line">sudo cp -f /usr/share/node-v5.12.0-linux-x64.tar.gz ~/.m2/repository/com/github/eirslett/node/5.12.0/node-5.12.0-linux-x64.tar.gz</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> speed up git</span></span><br><span class="line">git ls-remote --tags --heads https://github.com/DataTables/DataTables.git</span><br><span class="line">git config --global url."https://".insteadOf git://</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> build</span></span><br><span class="line">mvn package -Pdist,src,native -Drequire.snappy -Dbundle.snappy -Dsnappy.lib=/usr/lib64 -Dzstd.lib=/usr/local/lib   -Dbundle.zstd=true -DskipTests  -Dtar -Dcontainer-executor.conf.dir=/etc/hadoop </span><br><span class="line"></span><br><span class="line"></span><br><span class="line">HADOOP_HOME=$PWD</span><br><span class="line">echo $HADOOP_HOME</span><br><span class="line">find . -name *.so</span><br><span class="line"></span><br><span class="line">ls -lt $HADOOP_HOME/hadoop-hdfs-project/hadoop-hdfs-native-client/target/target/usr/local/lib/</span><br><span class="line">cp /home/jenkins/workspace/yarn-vip-centos7/hadoop-hdfs-project/hadoop-hdfs-native-client/target/target/usr/local/lib/libhdfs* /home/jenkins/workspace/yarn-vip-centos7/hadoop-dist/target/hadoop-3.2.0/lib/native/</span><br><span class="line"></span><br><span class="line">ts=$(date +%Y%m%d)</span><br><span class="line">commit_id=$&#123;sourcerevision:0:7&#125;</span><br><span class="line">cd hadoop-dist/target/</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> get spark yarn shuffle jar</span></span><br><span class="line">wget -O hadoop-3.2.0/share/hadoop/yarn/spark-3.0.1-SNAPSHOT-yarn-shuffle.jar  \</span><br><span class="line">http://mvn1.tools.vipshop.com/nexus/content/repositories/snapshots/com/vipshop/spark-shuffle/3.0.1-SNAPSHOT/spark-shuffle-3.0.1-20210106.025509-1.jar</span><br><span class="line"></span><br><span class="line">sudo chown -R hdfs:hdfs hadoop-3.2.0</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> cgroup</span></span><br><span class="line">sudo chown root:yarn hadoop-3.2.0/bin/container-executor</span><br><span class="line">sudo chmod 6050 hadoop-3.2.0/bin/container-executor</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> rename</span></span><br><span class="line">sudo mv hadoop-3.2.0  hadoop-3.2.0-vip-$&#123;buildbranch&#125;-$&#123;ts&#125;-$&#123;commit_id&#125;-os7</span><br><span class="line">sudo tar -zcf hadoop-3.2.0-vip-$&#123;buildbranch&#125;-$&#123;ts&#125;-$&#123;commit_id&#125;-os7.tar.gz  hadoop-3.2.0-vip-$&#123;buildbranch&#125;-$&#123;ts&#125;-$&#123;commit_id&#125;-os7</span><br></pre></td></tr></table></figure>

<h3 id="spark编译"><a href="#spark编译" class="headerlink" title="spark编译"></a>spark编译</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">mypwd=`echo $PWD`</span><br><span class="line">ls /usr/share/</span><br><span class="line"></span><br><span class="line">wget https://www.python.org/ftp/python/3.6.1/Python-3.6.1.tgz</span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="built_in">cd</span> build</span></span><br><span class="line">tar -zxvf Python-3.6.1.tgz</span><br><span class="line">cd Python-3.6.1</span><br><span class="line">sudo mkdir -p /usr/local/python3</span><br><span class="line">sudo ./configure --prefix=/usr/local/python3</span><br><span class="line">sudo make </span><br><span class="line">sudo make install</span><br><span class="line">sudo ln -s /usr/local/python3/bin/python3 /usr/bin/python3</span><br><span class="line">cd ../..</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> ui</span></span><br><span class="line">sudo mkdir -p ~/.m2/repository/com/github/eirslett/yarn/0.21.3</span><br><span class="line">sudo cp -f /usr/share/yarn-0.21.3.tar.gz ~/.m2/repository/com/github/eirslett/yarn/0.21.3/</span><br><span class="line"></span><br><span class="line">sudo mkdir -p  ~/.m2/repository/com/github/eirslett/node/5.12.0/</span><br><span class="line">sudo cp -f /usr/share/node-v5.12.0-linux-x64.tar.gz ~/.m2/repository/com/github/eirslett/node/5.12.0/node-5.12.0-linux-x64.tar.gz</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash">git ls-remote --tags --heads https://github.com/DataTables/DataTables.git</span></span><br><span class="line"><span class="meta">#</span><span class="bash">git config --global url.<span class="string">"https://"</span>.insteadOf git://</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> build</span></span><br><span class="line"><span class="meta">#</span><span class="bash">./build/mvn  -Pyarn  -Phive -Phive-thriftserver -Psparkr -DskipTests=<span class="literal">true</span> -Dmaven.compiler.source=1.8 -Dmaven.compiler.target=1.8</span></span><br><span class="line"><span class="meta">#</span><span class="bash">./build/mvn  -Pyarn  -Phive -Phive-thriftserver -DskipTests=<span class="literal">true</span> -Dmaven.compiler.source=1.8 -Dmaven.compiler.target=1.8</span></span><br><span class="line">ts=`date "+%Y-%m-%d-%H-%M-%S"`</span><br><span class="line">cd  `echo $mypwd`</span><br><span class="line">pwd</span><br><span class="line">./dev/make-distribution.sh --pip --tgz --name $&#123;ts&#125;-$&#123;buildbranch&#125; --mvn /usr/share/maven/bin/mvn  -Pyarn  -Phive -Phive-thriftserver -DskipTests=true -Dmaven.compiler.source=1.8 -Dmaven.compiler.target=1.8</span><br></pre></td></tr></table></figure>

<h3 id="hadoop-namenode切换"><a href="#hadoop-namenode切换" class="headerlink" title="hadoop namenode切换"></a>hadoop namenode切换</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">hdfs haadmin -transitionToActive --forcemanual nn1 将nn1强制转换为Active</span><br><span class="line">hdfs haadmin -transitionToStandby --forcemanual nn2 将nn2强制转换为standby</span><br></pre></td></tr></table></figure>

<h3 id="查看文件副本数字"><a href="#查看文件副本数字" class="headerlink" title="查看文件副本数字"></a>查看文件副本数字</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">bin/hadoop fs -stat "%o %r" /liangly/teradata/part-00099</span><br></pre></td></tr></table></figure>

<h3 id="ec命令"><a href="#ec命令" class="headerlink" title="ec命令"></a>ec命令</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hdfs ec -setPolicy -path $&#123;ec_dt_location&#125; -policy &apos;RS-6-3-1024k&apos;</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&lt;!-- add by lx--&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">  &lt;groupId&gt;org.apache.hadoop&lt;/groupId&gt;</span><br><span class="line">  &lt;artifactId&gt;hadoop-common&lt;/artifactId&gt;</span><br><span class="line">  &lt;version&gt;3.2.0&lt;/version&gt;</span><br><span class="line"></span><br><span class="line">  &lt;exclusions&gt;</span><br><span class="line">    &lt;exclusion&gt;</span><br><span class="line">      &lt;groupId&gt;com.fasterxml.jackson.module&lt;/groupId&gt;</span><br><span class="line">      &lt;artifactId&gt;*&lt;/artifactId&gt;</span><br><span class="line">    &lt;/exclusion&gt;</span><br><span class="line">    &lt;exclusion&gt;</span><br><span class="line">      &lt;groupId&gt;com.fasterxml.jackson.core&lt;/groupId&gt;</span><br><span class="line">      &lt;artifactId&gt;*&lt;/artifactId&gt;</span><br><span class="line">    &lt;/exclusion&gt;</span><br><span class="line">  &lt;/exclusions&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br><span class="line"></span><br><span class="line">&lt;!-- add by lx--&gt;</span><br><span class="line">&lt;dependency&gt;</span><br><span class="line">  &lt;groupId&gt;org.apache.spark&lt;/groupId&gt;</span><br><span class="line">  &lt;artifactId&gt;spark-yarn_2.12&lt;/artifactId&gt;</span><br><span class="line">  &lt;version&gt;3.0.1&lt;/version&gt;</span><br><span class="line">&lt;/dependency&gt;</span><br></pre></td></tr></table></figure>

<h3 id="启动网卡报错（Failed-to-start-LSB-Bring-up-down-networking-）汇总"><a href="#启动网卡报错（Failed-to-start-LSB-Bring-up-down-networking-）汇总" class="headerlink" title="启动网卡报错（Failed to start LSB: Bring up/down networking ）汇总"></a>启动网卡报错（Failed to start LSB: Bring up/down networking ）汇总</h3><p>链接：<a href="https://blog.51cto.com/u_11863547/1905929" target="_blank" rel="noopener">https://blog.51cto.com/u_11863547/1905929</a></p>
<h3 id="proxifier下载和使用"><a href="#proxifier下载和使用" class="headerlink" title="proxifier下载和使用"></a>proxifier下载和使用</h3><p>链接：<a href="https://www.macdo.cn/17452.html" target="_blank" rel="noopener">https://www.macdo.cn/17452.html</a></p>
<h3 id="一道算法题"><a href="#一道算法题" class="headerlink" title="一道算法题"></a>一道算法题</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">&#123;&#123;a,b&#125;,&#123;c&#125;,&#123;d,e&#125;&#125; =&gt; acd ace bcd bce</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.lx.code;</span><br><span class="line"><span class="keyword">import</span> java.util.Stack;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> lx</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@date</span> 2021/10/17 4:55 下午</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">JIETI</span> </span>&#123;</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">int</span>[][] a = <span class="keyword">new</span> <span class="keyword">int</span>[<span class="number">10</span>][<span class="number">10</span>];</span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> Stack&lt;Integer&gt; stack = <span class="keyword">new</span> Stack&lt;&gt;();</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">dfs</span><span class="params">(<span class="keyword">int</span> i, <span class="keyword">int</span> len)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (i == len) &#123;</span><br><span class="line">            System.out.println(<span class="number">1</span>);</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; a[i][j] != <span class="number">0</span>; j++) &#123;</span><br><span class="line">                stack.push(a[i][j]);</span><br><span class="line">                dfs(i + <span class="number">1</span>, len);</span><br><span class="line">                stack.pop();</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        a[<span class="number">0</span>][<span class="number">0</span>] = <span class="number">1</span>;</span><br><span class="line">        a[<span class="number">0</span>][<span class="number">1</span>] = <span class="number">2</span>;</span><br><span class="line">        a[<span class="number">1</span>][<span class="number">0</span>] = <span class="number">3</span>;</span><br><span class="line">        a[<span class="number">2</span>][<span class="number">0</span>] = <span class="number">4</span>;</span><br><span class="line">        a[<span class="number">2</span>][<span class="number">1</span>] = <span class="number">5</span>;</span><br><span class="line">        dfs(<span class="number">0</span>, <span class="number">3</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>work</category>
      </categories>
      <tags>
        <tag>vip</tag>
      </tags>
  </entry>
  <entry>
    <title>校园商铺平台总结</title>
    <url>/2021/06/09/%E6%A0%A1%E5%9B%AD%E5%95%86%E9%93%BA%E5%B9%B3%E5%8F%B0%E6%80%BB%E7%BB%93/</url>
    <content><![CDATA[<h2 id="校园商铺平台"><a href="#校园商铺平台" class="headerlink" title="校园商铺平台"></a>校园商铺平台</h2><h3 id="过滤器、拦截器、aop的区别"><a href="#过滤器、拦截器、aop的区别" class="headerlink" title="过滤器、拦截器、aop的区别"></a>过滤器、拦截器、aop的区别</h3><h4 id="过滤器"><a href="#过滤器" class="headerlink" title="过滤器"></a>过滤器</h4><p>过滤器拦截的是URL</p>
<p>Spring中自定义过滤器（Filter）一般只有一个方法，返回值是void，当请求到达web容器时，会探测当前请求地址是否配置有过滤器，有则调用该过滤器的方法（可能会有多个过滤器），然后才调用真实的业务逻辑，至此过滤器任务完成。过滤器并没有定义业务逻辑执行前、后等，仅仅是请求到达就执行。</p>
<h4 id="拦截器"><a href="#拦截器" class="headerlink" title="拦截器"></a>拦截器</h4><p>拦截器拦截的是URL</p>
<p>拦截器有三个方法，相对于过滤器更加细致，有被拦截逻辑执行前、后等。Spring中拦截器有三个方法：preHandle，postHandle，afterCompletion。</p>
<h4 id="AOP（面向切面）"><a href="#AOP（面向切面）" class="headerlink" title="AOP（面向切面）"></a>AOP（面向切面）</h4><p>面向切面拦截的是类的元数据（包、类、方法名、参数等）</p>
<p>相对于拦截器更加细致，而且非常灵活，拦截器只能针对URL做拦截，而AOP针对具体的代码，能够实现更加复杂的业务逻辑。</p>
]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>项目总结</tag>
      </tags>
  </entry>
  <entry>
    <title>java面试</title>
    <url>/2021/06/07/java%E9%9D%A2%E8%AF%95/</url>
    <content><![CDATA[<h2 id="计算机网络"><a href="#计算机网络" class="headerlink" title="计算机网络"></a>计算机网络</h2><h3 id="TCP的拥塞控制"><a href="#TCP的拥塞控制" class="headerlink" title="TCP的拥塞控制"></a>TCP的拥塞控制</h3><ol>
<li><strong>拥塞</strong></li>
</ol>
<p>拥塞：即对资源的需求超过了可用的资源。若网络中许多资源同时供应不足，网络的性能就要明显变坏，整个网络的吞吐量随之负荷的增大而下降。</p>
<p>  拥塞控制：<strong>防止过多的数据注入到网络中，这样可以使网络中的路由器或链路不致过载。</strong>拥塞控制所要做的都有一个<strong>前提：网络能够承受现有的网络负荷。</strong>拥塞控制是一个<strong>全局性的过程</strong>，涉及到所有的主机、路由器，以及与降低网络传输性能有关的所有因素。</p>
<p>  流量控制：指点对点通信量的控制，是端到端正的问题。流量控制所要做的就是抑制发送端发送数据的速率，以便使接收端来得及接收。</p>
<ol start="2">
<li><p><strong>几种拥塞控制方法</strong></p>
<p>慢开始( slow-start )、拥塞避免( congestion avoidance )、快重传( fast retransmit )和快恢复( fast recovery )。</p>
</li>
</ol>
<p>2.1 慢开始和拥塞避免</p>
<p>  发送方维持一个拥塞窗口 cwnd ( congestion window )的状态变量。拥塞窗口的大小取决于网络的拥塞程度，并且动态地在变化。发送方让自己的发送窗口等于拥塞。</p>
<p>  发送方控制拥塞窗口的原则是：只要网络没有出现拥塞，拥塞窗口就再增大一些，以便把更多的分组发送出去。但只要网络出现拥塞，拥塞窗口就减小一些，以减少注入到网络中的分组数。</p>
<p>  慢开始算法：当主机开始发送数据时，如果立即所大量数据字节注入到网络，那么就有可能引起网络拥塞，因为现在并不清楚网络的负荷情况。因此，较好的方法是先探测一下，即由小到大逐渐增大发送窗口，也就是说，由小到大逐渐增大拥塞窗口数值。通常在刚刚开始发送报文段时，先把拥塞窗口 cwnd 设置为一个最大报文段MSS的数值。而在每收到一个对新的报文段的确认后，把拥塞窗口增加至多一个MSS的数值。用这样的方法逐步增大发送方的拥塞窗口 cwnd ，可以使分组注入到网络的速率更加合理。</p>
<h2 id="数据库结构"><a href="#数据库结构" class="headerlink" title="数据库结构"></a>数据库结构</h2><h3 id="如何设计一个关系型数据库？"><a href="#如何设计一个关系型数据库？" class="headerlink" title="如何设计一个关系型数据库？"></a>如何设计一个关系型数据库？</h3><p><img src="https://lixiangbetter.github.io/2021/06/07/java%E9%9D%A2%E8%AF%95/1623466894807.jpg" alt></p>
<h4 id="程序实例模块"><a href="#程序实例模块" class="headerlink" title="程序实例模块"></a>程序实例模块</h4><ul>
<li>存储模块：逻辑关系转化成物理关系的存储管理</li>
<li>缓存机制：优化执行效率</li>
<li>SQL解析：进行SQL语句的解析</li>
<li>日志管理：记录操作日志</li>
<li>权限划分：进行多用户管理的权限划分</li>
<li>容灾机制：灾难恢复模块</li>
<li>索引管理：优化数据查询效率</li>
<li>锁管理：使得数据库支持并发操作</li>
</ul>
<h4 id="存储模块（文件系统）"><a href="#存储模块（文件系统）" class="headerlink" title="存储模块（文件系统）"></a>存储模块（文件系统）</h4><ul>
<li>磁盘或者固态硬盘存储所有数据</li>
</ul>
<p>链接：<a href="https://www.pianshen.com/article/65811074520/" target="_blank" rel="noopener">https://www.pianshen.com/article/65811074520/</a></p>
<h4 id="索引的数据结构"><a href="#索引的数据结构" class="headerlink" title="索引的数据结构"></a>索引的数据结构</h4><ul>
<li>生成索引，建立二叉查找树进行二分查找<ul>
<li>时间复杂度o(logn),可能变成o(n)</li>
<li>还要考虑io</li>
</ul>
</li>
<li>生成索引，建立b-tree结构进行查找<ul>
<li>平衡多路查找树</li>
</ul>
</li>
<li>生成索引，建立b+tree结构进行查找<ul>
<li>b树的变体</li>
</ul>
</li>
<li>生成索引，建立hash结构进行查找</li>
</ul>
<h3 id="myisam和innodb关于锁方面的区别是什么"><a href="#myisam和innodb关于锁方面的区别是什么" class="headerlink" title="myisam和innodb关于锁方面的区别是什么"></a>myisam和innodb关于锁方面的区别是什么</h3><ul>
<li>myisam支持表级锁，不支持行级锁</li>
<li>innodb都支持</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">// 共享锁</span><br><span class="line">select * from person_info_large where id = 3 lock in share mode;</span><br><span class="line">// 排它锁</span><br><span class="line">select * from person_info_large where id = 3 for update;</span><br></pre></td></tr></table></figure>

<p>Ps：</p>
<ul>
<li><p>innodb没有用到索引的时候，用的是表级锁</p>
</li>
<li><p>用到索引的时候，用的是行锁</p>
</li>
</ul>
<h3 id="数据库锁的分类"><a href="#数据库锁的分类" class="headerlink" title="数据库锁的分类"></a>数据库锁的分类</h3><ul>
<li>锁粒度划分：表级锁、行级锁、页级锁</li>
<li>锁级别划分：共享锁、排它锁</li>
<li>按加锁方式划分：自动锁、显式锁</li>
<li>按操作划分：DML锁（对数据上的操作，数据的增删改查）、DDL锁（表结构，alter table）</li>
<li>按使用方式划分：乐观锁（认为数据一般情况下，不会冲突，如果冲突则处理。实现方式：版本号、时间戳）、悲观锁（往往依赖数据库的锁机制，先取锁，再操作）</li>
</ul>
<h3 id="事务隔离级别以及各级别下的并发访问问题"><a href="#事务隔离级别以及各级别下的并发访问问题" class="headerlink" title="事务隔离级别以及各级别下的并发访问问题"></a>事务隔离级别以及各级别下的并发访问问题</h3><h4 id="事务并发访问引起的问题以及如何避免"><a href="#事务并发访问引起的问题以及如何避免" class="headerlink" title="事务并发访问引起的问题以及如何避免"></a>事务并发访问引起的问题以及如何避免</h4><ul>
<li>更新丢失–mysql所有事务隔离级别在数据库层面上均可避免</li>
<li>脏读–read-committed事务隔离级别以上可避免</li>
<li>不可重复读–repeatable-read事务隔离级别以上可避免</li>
<li>幻读–serializable事务隔离级别可避免</li>
</ul>
<h3 id="innodb可重复读隔离级别下如何避免幻读"><a href="#innodb可重复读隔离级别下如何避免幻读" class="headerlink" title="innodb可重复读隔离级别下如何避免幻读"></a>innodb可重复读隔离级别下如何避免幻读</h3><ul>
<li>表象：快照读（非阻塞读）–伪MVCC</li>
<li>内在：next-key锁（行锁+gap锁）</li>
</ul>
<p>当前读：select…lock in share mode,select…for update</p>
<p>当前读：update, delete,insert</p>
<h4 id="rc-rr级别下的innodb的非阻塞读如何实现"><a href="#rc-rr级别下的innodb的非阻塞读如何实现" class="headerlink" title="rc,rr级别下的innodb的非阻塞读如何实现"></a>rc,rr级别下的innodb的非阻塞读如何实现</h4><ul>
<li>数据行里的DB_TRX_ID、DB_ROLL_PTR、DB_ROW_ID字段</li>
<li>undo日志</li>
<li>read view</li>
</ul>
<h4 id="对主键索引或者唯一索引会用gap锁吗"><a href="#对主键索引或者唯一索引会用gap锁吗" class="headerlink" title="对主键索引或者唯一索引会用gap锁吗"></a>对主键索引或者唯一索引会用gap锁吗</h4><ul>
<li>如果where条件全部命中，则不会用gap锁，只会加记录锁</li>
<li>如果where条件部分命中或者全不命中，则用gap锁</li>
</ul>
<h4 id="gap锁会用在非唯一索引或者不走索引的当前读中"><a href="#gap锁会用在非唯一索引或者不走索引的当前读中" class="headerlink" title="gap锁会用在非唯一索引或者不走索引的当前读中"></a>gap锁会用在非唯一索引或者不走索引的当前读中</h4><ul>
<li>非唯一索引</li>
<li>不走索引</li>
</ul>
<h3 id="关键语法"><a href="#关键语法" class="headerlink" title="关键语法"></a>关键语法</h3><h3 id="having"><a href="#having" class="headerlink" title="having"></a>having</h3><ul>
<li>通常和group by 一起使用</li>
<li>Where过滤行，having过滤组</li>
<li>sql顺序：where&gt; group by &gt; having</li>
</ul>
<h2 id="Redis"><a href="#Redis" class="headerlink" title="Redis"></a>Redis</h2><ul>
<li>String</li>
</ul>
<p>String类是是二进制安全的。意思是redis的String可以包含任何数据。比如jpg图片或者序列化的对象。</p>
<p>String类型是Redis最基本的数据类型，一个redis中value最多可以是512M。</p>
<ul>
<li>List</li>
</ul>
<p>Redis列表是简单的字符串列表，按照插入顺序排序。你可以添加一个元素到列表的头部或者尾部。它的底层是一个链表。</p>
<p>　总结：</p>
<p>　　　　它是一个字符串链表，left，right都可以插入添加</p>
<p>　　　　如果键不存在，创建新的链表，如果键已经存在则新增内容</p>
<p>　　　　如果内容全部移除。对应的键也消失。</p>
<p>　　　　链表的操作无论是在头和尾效率都极高，但假如是对中间元素进行操作，效率就很惨淡。</p>
<ul>
<li>Set</li>
</ul>
<p>Redis的Set的histring类型的无序集合。他是通过HashTable实现的。</p>
<ul>
<li>Redis哈希（Hash）</li>
</ul>
<p>Redis hash是一个键值对集合。</p>
<p>Redis hash是一个string类型的field和value的映射表，hash特别适合用于存储对象。</p>
<p>类似Java里面Map&lt;String, Object&gt;。</p>
<ul>
<li>Redis有序集合Zset（sorted set）</li>
</ul>
<p>Redis zset和set一样也是string类型元素的集合，而且不允许重复的成员。</p>
<p>不同的是每个元素都会关联一个double类型的分数。</p>
<p>redis正是通过分数来为集合中的成员进行从小到大的排序。zset的成员是唯一的，但分数（score）却可以重复。</p>
<h3 id="在Redis里，如何从海量key中查询出某一个固定前缀所有的key？"><a href="#在Redis里，如何从海量key中查询出某一个固定前缀所有的key？" class="headerlink" title="在Redis里，如何从海量key中查询出某一个固定前缀所有的key？"></a>在Redis里，如何从海量key中查询出某一个固定前缀所有的key？</h3><p>每次只返回一小部分的键，这样不会阻塞服务器，一下子在网络传输大量数据</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="meta">@Autowired</span></span><br><span class="line"><span class="keyword">private</span> RedisTemplate redisTemplate;</span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">getKey</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">long</span> start = System.currentTimeMillis();</span><br><span class="line">        redisTemplate.keys(<span class="string">"cart*"</span>);</span><br><span class="line">        <span class="keyword">long</span> end = System.currentTimeMillis();</span><br><span class="line">        System.out.println(end - start);</span><br><span class="line">  </span><br><span class="line">  			<span class="comment">// 推荐的做法</span></span><br><span class="line">        RedisConnection connection = RedisConnectionUtils.getConnection(redisTemplate.getConnectionFactory());</span><br><span class="line">        Cursor&lt;<span class="keyword">byte</span>[]&gt; result = connection.scan(<span class="keyword">new</span> ScanOptions.ScanOptionsBuilder().count(<span class="number">10</span>).match(<span class="string">"cart*"</span>).build());</span><br><span class="line">        <span class="keyword">long</span> start1 = System.currentTimeMillis();</span><br><span class="line">        <span class="comment">//cursor有id和position这两个属性，id则对应 scan cursor 的cursor的值，poisition则是当前遍历到第几个</span></span><br><span class="line">        <span class="keyword">while</span> (result.hasNext()) &#123;<span class="comment">//这里可以改用for循环来获取指定数量的key</span></span><br><span class="line">            String key=<span class="keyword">new</span> String(result.next());</span><br><span class="line">            <span class="comment">//对key的操作，或者先放到一个集合里面，然后再进行后续操作</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">long</span> end1 = System.currentTimeMillis();</span><br><span class="line">        System.out.println(end1 - start1);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="Redis持久化方式"><a href="#Redis持久化方式" class="headerlink" title="Redis持久化方式"></a>Redis持久化方式</h3><h4 id="方式"><a href="#方式" class="headerlink" title="方式"></a>方式</h4><p>Redis支持RDB和AOF两种持久化机制，持久化功能有效地避免因进程退出造成的数据丢失问题，当下次重启时利用之前持久化的文件即可实现数 据恢复。理解掌握持久化机制对于Redis运维非常重要</p>
<ol>
<li>RDB持久化<br>RDB持久化是把当前进程数据生成快照保存到硬盘的过程，触发RDB持久化过程分为手动触发和自动触发</li>
</ol>
<p>1）触发机制</p>
<p>手动触发分别对应save和bgsave命令</p>
<p>·save命令：阻塞当前Redis服务器，直到RDB过程完成为止，对于内存 比较大的实例会造成长时间阻塞，线上环境不建议使用</p>
<p>·bgsave命令：Redis进程执行fork操作创建子进程，RDB持久化过程由子 进程负责，完成后自动结束。阻塞只发生在fork阶段，一般时间很短</p>
<p>2）自动触发RDB的持久</p>
<p>1）使用save相关配置，如“save m n”。表示m秒内数据集存在n次修改 时，自动触发bgsave。</p>
<p>2）如果从节点执行全量复制操作，主节点自动执行bgsave生成RDB文件并发送给从节点，更多细节见6.3节介绍的复制原理。</p>
<p>3）执行debug reload命令重新加载Redis时，也会自动触发save操作。</p>
<p>4）默认情况下执行shutdown命令时，如果没有开启AOF持久化功能则 自动执行bgsave。</p>
<h4 id="RDB的优缺点"><a href="#RDB的优缺点" class="headerlink" title="RDB的优缺点"></a>RDB的优缺点</h4><p>RDB的优点：</p>
<ul>
<li><p>RDB是一个紧凑压缩的二进制文件，代表Redis在某个时间点上的数据 快照。非常适用于备份，全量复制等场景。比如每6小时执行bgsave备份， 并把RDB文件拷贝到远程机器或者文件系统中（如hdfs），用于灾难恢复。</p>
</li>
<li><p>Redis加载RDB恢复数据远远快于AOF的方式。</p>
</li>
</ul>
<p>RDB的缺点：</p>
<ul>
<li><p>RDB方式数据没办法做到实时持久化/秒级持久化。因为bgsave每次运 行都要执行fork操作创建子进程，属于重量级操作，频繁执行成本过高。</p>
</li>
<li><p>RDB文件使用特定二进制格式保存，Redis版本演进过程中有多个格式 的RDB版本，存在老版本Redis服务无法兼容新版RDB格式的问题。</p>
</li>
<li><p>针对RDB不适合实时持久化的问题，Redis提供了AOF持久化方式来解决。</p>
</li>
</ul>
<h4 id="AOF持久化"><a href="#AOF持久化" class="headerlink" title="AOF持久化"></a>AOF持久化</h4><p>AOF（append only file）持久化：以独立日志的方式记录每次写命令， 重启时再重新执行AOF文件中的命令达到恢复数据的目的。AOF的主要作用 是解决了数据持久化的实时性，目前已经是Redis持久化的主流方式</p>
<p>1）使用AOF<br>开启AOF功能需要设置配置：appendonly yes，默认不开启。AOF文件名 通过appendfilename配置设置，默认文件名是appendonly.aof。保存路径同 RDB持久化方式一致，通过dir配置指定。AOF的工作流程操作：命令写入 （append）、文件同步（sync）、文件重写（rewrite）、重启加载 （load）。</p>
<p>AOF过程：</p>
<p>1）所有的写入命令会追加到aof_buf（缓冲区）中。</p>
<p>2）AOF缓冲区根据对应的策略向硬盘做同步操作。</p>
<p>AOF为什么把命令追加到aof_buf中？Redis使用单线程响应命令，如 果每次写AOF文件命令都直接追加到硬盘，那么性能完全取决于当前硬盘负 载。先写入缓冲区aof_buf中，还有另一个好处，Redis可以提供多种缓冲区同步硬盘的策略，在性能和安全性方面做出平衡</p>
<p>3）随着AOF文件越来越大，需要定期对AOF文件进行重写，达到压缩的目的。</p>
<p>重写后的AOF文件为什么可以变小？有如下原因：</p>
<p>1）进程内已经超时的数据不再写入文件。</p>
<p>2）旧的AOF文件含有无效命令，如del key1、hdel key2、srem keys、set a111、set a222等。重写使用进程内数据直接生成，这样新的AOF文件只保留最终数据的写入命令。</p>
<p>3）多条写命令可以合并为一个，如：lpush list a、lpush list b、lpush list c可以转化为：lpush list a b c。</p>
<p>AOF重写降低了文件占用空间，除此之外，另一个目的是：更小的AOF 文件可以更快地被Redis加载。</p>
<p><strong>AOF重写过程可以手动触发和自动触发</strong>：</p>
<p>·手动触发：直接调用bgrewriteaof命令。</p>
<p>·自动触发：根据auto-aof-rewrite-min-size和auto-aof-rewrite-percentage参数确定自动触发时机</p>
<h4 id="加载AOF文件进行数据恢复"><a href="#加载AOF文件进行数据恢复" class="headerlink" title="加载AOF文件进行数据恢复"></a>加载AOF文件进行数据恢复</h4><p>流程说明：</p>
<p>1）AOF持久化开启且存在AOF文件时，优先加载AOF文件，打印如下日志：</p>
<p>* DB loaded from append only file: 5.841 seconds</p>
<p>2）AOF关闭或者AOF文件不存在时，加载RDB文件，打印如下日志：</p>
<p>* DB loaded from disk: 5.586 seconds</p>
<p>3）加载AOF/RDB文件成功后，Redis启动成功。</p>
<p>4）AOF/RDB文件存在错误时，Redis启动失败并打印错误信息。</p>
<h3 id="Redis管道技术（pipeline）"><a href="#Redis管道技术（pipeline）" class="headerlink" title="Redis管道技术（pipeline）"></a>Redis管道技术（pipeline）</h3><p>  作用：执行命令简单的，更加快速的发送给服务器；<br>   一个client可以通过一个socket连接发起多个请求命令。每个请求命令发出后client通常会阻塞并等待redis服务处理，redis处理完后请求命令后会将结果通过响应报文返回给client。如果没有pipeline那么redis就会处理完一个请求之后返回响应报文，client再发送下一个请求。</p>
<h4 id="Redis的同步机制"><a href="#Redis的同步机制" class="headerlink" title="Redis的同步机制"></a>Redis的同步机制</h4><p>全同步过程</p>
<ul>
<li>slave发送sync命令给master；</li>
<li>master启动一个后台进程，将Redis中的数据快照保存到文件中；</li>
<li>master将保存数据快照数据期间接收到的命令缓存起来；</li>
<li>master完成文件写操作后，将文件发送给slave；</li>
<li>使用新的AOF文件替换掉旧的AOF文件；</li>
<li>master将这期间收集的增量写命令发送给slave端。</li>
</ul>
<p>注：全量同步操作完成后，后续所有写操作都是在master上进行，slave上进行读操作，虽然master也能进行读操作，但一般不会使用；为了提升性能，一般都将读操作放在slave上，因此用户的写操作需要及时扩散到slave上，以保证数据最大程度上的同步。</p>
<p>增量同步过程</p>
<ul>
<li>master收到用户的操作指令，判断是否需要传播到slave；（一般增删改才需要）</li>
<li>将操作记录追加到AOF文件；</li>
<li>将操作传播到其他slave：a)对齐zhu主从库；b)往响应缓存写入指令 ;</li>
<li>将缓存中的数据发给slave。</li>
</ul>
<p>Redis sentinel：解决主从同步master宕机后的主从切换问题</p>
<p>监控：检查主从服务器是否运行正常；</p>
<p>提醒：通过API向管理员或者其他应用程序发送故障通知；</p>
<p>自动故障迁移：主从切换；</p>
<h3 id="Redis-的集群原理"><a href="#Redis-的集群原理" class="headerlink" title="Redis 的集群原理"></a>Redis 的集群原理</h3><h5 id="如何从海量的数据中尽快找到所需？"><a href="#如何从海量的数据中尽快找到所需？" class="headerlink" title="如何从海量的数据中尽快找到所需？"></a>如何从海量的数据中尽快找到所需？</h5><p>  分片：按照某种规则去划分数据，分散储存在多个节点上；<br>  常规的按照哈希划分无法实现节点的动态增减</p>
<h5 id="一致性哈希算法"><a href="#一致性哈希算法" class="headerlink" title="一致性哈希算法"></a>一致性哈希算法</h5><p>  <strong>对2^32取模，将哈希值空间组织成虚拟的圆环</strong></p>
<p>将数据key使用相同的函数hash计算出哈希值</p>
<h2 id="Linux知识考点"><a href="#Linux知识考点" class="headerlink" title="Linux知识考点"></a>Linux知识考点</h2><h3 id="Linux的体系结构"><a href="#Linux的体系结构" class="headerlink" title="Linux的体系结构"></a>Linux的体系结构</h3><ul>
<li>体系结构主要分为用户态（用户上层活动）和内核态</li>
<li>内核：本质是一段管理计算机硬件设备的程序</li>
<li>系统调用：内核的访问接口，是一种能再简化的操作</li>
<li>公用函数库：系统调用的组合拳</li>
<li>Shell：命令解释器，可编程</li>
</ul>
<h3 id="查找特定文件"><a href="#查找特定文件" class="headerlink" title="查找特定文件"></a>查找特定文件</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">语法 find path [options] params		</span><br><span class="line">// 精确查找</span><br><span class="line">find ~ -name &quot;target.java&quot;</span><br><span class="line">// 模糊查找， 并且忽略大小写</span><br><span class="line">find ~ -iname &quot;target*&quot;</span><br></pre></td></tr></table></figure>

<h3 id="检索文件内容"><a href="#检索文件内容" class="headerlink" title="检索文件内容"></a>检索文件内容</h3><h4 id="grep命令"><a href="#grep命令" class="headerlink" title="grep命令"></a>grep命令</h4><p>grep [pattern] [file] </p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">grep &quot;mooc&quot; target*</span><br></pre></td></tr></table></figure>

<h4 id="管道操作符-｜"><a href="#管道操作符-｜" class="headerlink" title="管道操作符 ｜"></a>管道操作符 ｜</h4><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">find ~ | grep &quot;target&quot;</span><br></pre></td></tr></table></figure>

<ul>
<li>前一个命令必须正确</li>
<li>右边必须能够接受标准输入流，否则数据会被抛弃</li>
<li>Sed,awk,grep,cut,head,top,less,more,wc,join,sort,split等</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">查看包含某个字段--&gt; 筛选出符合正则表达式的内容</span><br><span class="line">grep &quot;pattern\[true\]&quot; test.log | grep -o &quot;engine\[[0-9a-z]*\]&quot;</span><br><span class="line">过滤掉包含字符串的内容</span><br><span class="line">grep -v &quot;grep&quot;</span><br></pre></td></tr></table></figure>

<h3 id="对文件内容进行统计"><a href="#对文件内容进行统计" class="headerlink" title="对文件内容进行统计"></a>对文件内容进行统计</h3><h4 id="awk命令"><a href="#awk命令" class="headerlink" title="awk命令"></a>awk命令</h4><p>适合对列处理</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">语法 awk [options] &apos;cmd&apos; file</span><br></pre></td></tr></table></figure>

<ol>
<li>一次读取一行文本，按输入分隔符进行切片，切成多个组成部分</li>
<li>将切片直接保存在内建的变量中，$1,$2…($0表示行的全部)</li>
<li>支持对单个切片的判断，支持循环判断，默认分隔符为空格</li>
</ol>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">文件说明：</span><br><span class="line">netstat.txt</span><br><span class="line">文本内容为格式化数据：tcp	0	48	ip:ssh	</span><br><span class="line">命令：</span><br><span class="line">// 第一列 和 第四列输出</span><br><span class="line">awk '&#123;print $1,$4&#125;' netstat.txt</span><br><span class="line">// 判断</span><br><span class="line">awk '$1=="tcp" &amp;&amp; $2==1&#123;print $0&#125;' netstat.txt</span><br><span class="line">// 表头</span><br><span class="line">awk '($1=="tcp" &amp;&amp; $2==1) || NR==1 &#123;print $0&#125;' netstat.txt</span><br><span class="line"></span><br><span class="line">文件说明：</span><br><span class="line">test.txt</span><br><span class="line">sdfsd,123</span><br><span class="line">dsfsdf,456</span><br><span class="line">// 第2列</span><br><span class="line">awk -F "," 'print&#123; $2&#125;' test.txt</span><br><span class="line">// 统计</span><br><span class="line">grep "pattern\[true\]" test.log | grep -o "engine\[[0-9a-z]*\]" | awk 'enginearr[$1++]END&#123;for(i in enginearr)print i "\t" enginearr[i]&#125;'</span><br></pre></td></tr></table></figure>

<h3 id="批量替换文件中的内容"><a href="#批量替换文件中的内容" class="headerlink" title="批量替换文件中的内容"></a>批量替换文件中的内容</h3><h4 id="sed命令"><a href="#sed命令" class="headerlink" title="sed命令"></a>sed命令</h4><p>适合对行处理</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">语法：sed [option] &apos;sed command&apos; filename</span><br></pre></td></tr></table></figure>

<ol>
<li>全名stream editor,流编辑器</li>
<li>适合用于对文本的行内容进行处理</li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">sed &apos;s/^Str/String/&apos; replace.java</span><br><span class="line">// 会修改文件</span><br><span class="line">sed -i &apos;s/^Str/String/&apos; replace.java</span><br><span class="line">// $代表以什么结尾</span><br><span class="line">sed -i &apos;s/\.$/\;/&apos; replace.java</span><br><span class="line">// 全局替换加g（默认是替换第一个）</span><br><span class="line">sed -i &apos;s/Jack/me/g&apos; replace.java</span><br><span class="line">// 删除加d 删除空行</span><br><span class="line">sed -i &apos;/^ *$/d&apos; replace.java</span><br></pre></td></tr></table></figure>

<h2 id="Java底层知识：JVM"><a href="#Java底层知识：JVM" class="headerlink" title="Java底层知识：JVM"></a>Java底层知识：JVM</h2><h3 id="谈谈对java的理解"><a href="#谈谈对java的理解" class="headerlink" title="谈谈对java的理解"></a>谈谈对java的理解</h3><ul>
<li>平台无关性</li>
<li>GC</li>
<li>语言特性（泛型、反射、lambda表示）</li>
<li>面向对象（封装、继承、多态）</li>
<li>类库（集合库，并发库，网络库，io）</li>
<li>异常处理</li>
</ul>
<h3 id="Compile-Once-Run-anyway如何实现"><a href="#Compile-Once-Run-anyway如何实现" class="headerlink" title="Compile Once,Run anyway如何实现"></a>Compile Once,Run anyway如何实现</h3><p><img src="https://lixiangbetter.github.io/2021/06/07/java%E9%9D%A2%E8%AF%95/1623488323589.jpg" alt></p>
<h3 id="jvm如何加载-class文件"><a href="#jvm如何加载-class文件" class="headerlink" title="jvm如何加载.class文件"></a>jvm如何加载.class文件</h3><ul>
<li>ClassLoader: 依据特定格式，加载class文件到内存；</li>
<li>Execution Engine: 对命令进行解析；</li>
<li>Native Interface: 融合不同开发语言的原生库为java所用；</li>
<li>Runtime data area: Jim内存空间结构模型。</li>
</ul>
<h3 id="反射"><a href="#反射" class="headerlink" title="反射"></a>反射</h3><p>java反射机制在运行状态中，对于任意一个类，都能够知道这个类的所有属性和方法；对于任意一个对象，都能够调用它的任意方法和属性；这种动态获取信息以及动态调用对象方法的功能为java语言的反射机制。</p>
<h3 id="类从编译到执行的过程"><a href="#类从编译到执行的过程" class="headerlink" title="类从编译到执行的过程"></a>类从编译到执行的过程</h3><ul>
<li>编译器将robot.java源文件编译为robot.class字节码文件</li>
<li>ClassLoader将字节码转换为JVM中的Class&lt;Robot&gt;对象</li>
<li>JVM利用Class&lt;Robot&gt;对象实例化robot对象</li>
</ul>
<h3 id="ClassLoader"><a href="#ClassLoader" class="headerlink" title="ClassLoader"></a>ClassLoader</h3><p>ClassLoader在java中有着非常重要的作用，它主要工作在Class装载的加载阶段，其主要作用是从系统外部Class二进制数据流。它是java的核心组件，所有的Class都是由ClassLoader进行加载的，ClassLoader负责通过将Class文件中的二进制数据流装载进系统，然后交给JVM进行连接、初始化等操作。</p>
<ul>
<li>BootstrapClassLoader：c++编写，加载核心库java.*</li>
<li>ExtClassLoader:java编写，加载扩展库java.ext.*</li>
<li>AppClassLoader：java编写，加载程序所在目录</li>
<li>自定义ClassLoader：java编写，定制化加载</li>
</ul>
<p>自定义ClassLoader</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MyClassLoader</span> <span class="keyword">extends</span> <span class="title">ClassLoader</span> </span>&#123;</span><br><span class="line">	<span class="keyword">private</span> String path;</span><br><span class="line">  <span class="keyword">private</span> String classloaderName;</span><br><span class="line">  </span><br><span class="line">  <span class="function"><span class="keyword">public</span> <span class="title">MyClassLoader</span><span class="params">(String path,String classloaderName)</span></span>&#123;</span><br><span class="line">    <span class="keyword">this</span>.path = path;</span><br><span class="line">    <span class="keyword">this</span>.classloaderName = classloaderName;</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">// 用于寻找类文件</span></span><br><span class="line">  <span class="meta">@Override</span></span><br><span class="line">  <span class="function"><span class="keyword">public</span> Class <span class="title">findClass</span><span class="params">(String name)</span></span>&#123;</span><br><span class="line">    <span class="keyword">byte</span>[] b = loadClassData(name);</span><br><span class="line">    <span class="keyword">return</span> defineClass(name, b, <span class="number">0</span>, b.length);</span><br><span class="line">  &#125;</span><br><span class="line">  </span><br><span class="line">  <span class="comment">// 用于加载类文件</span></span><br><span class="line">  <span class="keyword">private</span> <span class="keyword">byte</span>[] loadClassData(String name)&#123;</span><br><span class="line">    name = path + name + <span class="string">".class"</span>;</span><br><span class="line">    InputStream in = <span class="keyword">null</span>;</span><br><span class="line">    ByteArrayOutputStream out = <span class="keyword">null</span>;</span><br><span class="line">    <span class="keyword">try</span>&#123;</span><br><span class="line">      in = <span class="keyword">new</span> FileInputStream(<span class="keyword">new</span> File(name));</span><br><span class="line">      out = <span class="keyword">new</span> ByteArrayOutputStream();</span><br><span class="line">      <span class="keyword">int</span> i = <span class="number">0</span>;</span><br><span class="line">      <span class="keyword">while</span>((i = in.read()) != -<span class="number">1</span>) &#123;</span><br><span class="line">        out.write(i);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; <span class="keyword">catch</span> (Exception e)&#123;</span><br><span class="line">      e.printStackTrace();</span><br><span class="line">    &#125; <span class="keyword">finally</span> &#123;</span><br><span class="line">      <span class="keyword">try</span> &#123;</span><br><span class="line">        out.close();</span><br><span class="line">        in.close();</span><br><span class="line">      &#125; <span class="keyword">catch</span> (Exception e)&#123;</span><br><span class="line">      	e.printStackTrace();</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> out.toByteArray();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="为什么使用双亲委派机制去加载类"><a href="#为什么使用双亲委派机制去加载类" class="headerlink" title="为什么使用双亲委派机制去加载类"></a>为什么使用双亲委派机制去加载类</h3><p>避免多份同样字节码加载</p>
<h3 id="loadClass和forName有什么区别"><a href="#loadClass和forName有什么区别" class="headerlink" title="loadClass和forName有什么区别"></a>loadClass和forName有什么区别</h3><h4 id="类加载方式"><a href="#类加载方式" class="headerlink" title="类加载方式"></a>类加载方式</h4><ul>
<li>隐式加载：new</li>
<li>显示加载：loadClass, forName等</li>
</ul>
<h4 id="类加载过程"><a href="#类加载过程" class="headerlink" title="类加载过程"></a>类加载过程</h4><p>加载–通过ClassLoader加载class文件字节码，生成class对象</p>
<p>链接</p>
<ul>
<li>校验：检查加载的class的正确性与安全性</li>
<li>准备：为类变量分配存储空间并设置类变量初始值</li>
<li>解析：JVM将常量池内的符号引用转换为直接引用</li>
</ul>
<p>初始化–执行类变量赋值和静态代码块</p>
<h4 id="区别"><a href="#区别" class="headerlink" title="区别"></a>区别</h4><ul>
<li>Class.forName得到的class是已经初始化完成的</li>
<li>ClassLoader.loadClass得到的class是没有链接的</li>
</ul>
<h3 id="java内存模型"><a href="#java内存模型" class="headerlink" title="java内存模型"></a>java内存模型</h3><p>线程私有：程序计数器、虚拟机栈、本地方法栈</p>
<p>线程共享：MetaSpace、java堆</p>
<p>程序计数器：存储下一条需要执行的字节码指令，地址</p>
<p>虚拟机栈：</p>
<ul>
<li>java方法执行的内存模型</li>
<li>包含多个栈帧：局部变量表、操作栈、动态连接、返回地址<ul>
<li>局部变量表：方法执行过程中的所有变量</li>
<li>操作栈：出栈、入栈</li>
</ul>
</li>
</ul>
<p><img src="https://lixiangbetter.github.io/2021/06/07/java%E9%9D%A2%E8%AF%95/1623499263968.jpg" alt></p>
<h3 id="递归为什么会引起stack-overflow异常"><a href="#递归为什么会引起stack-overflow异常" class="headerlink" title="递归为什么会引起stack overflow异常"></a>递归为什么会引起stack overflow异常</h3><ul>
<li>递归过深，栈帧数超出虚拟机栈深度</li>
</ul>
<h3 id="虚拟机栈过多会引发java-lang-OutOfMemoryError异常"><a href="#虚拟机栈过多会引发java-lang-OutOfMemoryError异常" class="headerlink" title="虚拟机栈过多会引发java.lang.OutOfMemoryError异常"></a>虚拟机栈过多会引发java.lang.OutOfMemoryError异常</h3><h3 id="元空间和永久代的区别"><a href="#元空间和永久代的区别" class="headerlink" title="元空间和永久代的区别"></a>元空间和永久代的区别</h3><ul>
<li>元空间使用本地内存，而永久代使用的是jvm内存</li>
<li>元空间没有字符串常量池</li>
</ul>
<p><img src="https://lixiangbetter.github.io/2021/06/07/java%E9%9D%A2%E8%AF%95/1623499910080.jpg" alt></p>
<h3 id="java堆"><a href="#java堆" class="headerlink" title="java堆"></a>java堆</h3><ul>
<li>对象实例的分配区域</li>
<li>gc管理的主要区域</li>
</ul>
<h3 id="JVM三大性能调优参数-xms-xmx-xss含义"><a href="#JVM三大性能调优参数-xms-xmx-xss含义" class="headerlink" title="JVM三大性能调优参数 -xms -xmx -xss含义"></a>JVM三大性能调优参数 -xms -xmx -xss含义</h3><ul>
<li>-xss: 规定了每个线程虚拟栈（堆栈）的大小</li>
<li>-xms：堆的初始值</li>
<li>-xmx：堆能达到的最大值</li>
</ul>
<h3 id="java内存模型中的堆和栈的区别–内存分配策略"><a href="#java内存模型中的堆和栈的区别–内存分配策略" class="headerlink" title="java内存模型中的堆和栈的区别–内存分配策略"></a>java内存模型中的堆和栈的区别–内存分配策略</h3><ul>
<li>静态存储：编译时确定每个数据目标在运行时的存储空间需求</li>
<li>栈式存储：数据区需求在编译时未知，运行时模块入口前确定</li>
<li>堆式存储：编译时或运行时模块入口都无法确定，动态分配</li>
</ul>
<h3 id="java内存模型中堆和栈的区别"><a href="#java内存模型中堆和栈的区别" class="headerlink" title="java内存模型中堆和栈的区别"></a>java内存模型中堆和栈的区别</h3><ul>
<li><p>联系：引用对象、数组时，栈里定义变量保存堆中目标的首地址</p>
</li>
<li><p>管理方式：栈自动释放，堆需要gc</p>
</li>
<li><p>空间大小：堆比栈小</p>
</li>
<li><p>碎片相关：堆产生的碎片远小于堆</p>
</li>
<li><p>分配方式：栈支持静态和动态分配，而堆仅支持动态分配</p>
</li>
<li><p>效率：栈的效率比堆高</p>
</li>
</ul>
<p><img src="https://lixiangbetter.github.io/2021/06/07/java%E9%9D%A2%E8%AF%95/1623501665746.jpg" alt></p>
<p>不同jdk版本之间的intern()方法的区别-jdk6 vs jdk6+</p>
<p><img src="https://lixiangbetter.github.io/2021/06/07/java%E9%9D%A2%E8%AF%95/1623502594261.jpg" alt></p>
<h2 id="Java底层知识：GC相关"><a href="#Java底层知识：GC相关" class="headerlink" title="Java底层知识：GC相关"></a>Java底层知识：GC相关</h2><h3 id="判定对象是否为垃圾的算法"><a href="#判定对象是否为垃圾的算法" class="headerlink" title="判定对象是否为垃圾的算法"></a>判定对象是否为垃圾的算法</h3><ul>
<li>引用计数法（缺点：无法检测循环引用的情况，导致内存泄漏）</li>
<li>可达性分析法</li>
</ul>
<h3 id="可以作为gc-root的对象"><a href="#可以作为gc-root的对象" class="headerlink" title="可以作为gc root的对象"></a>可以作为gc root的对象</h3><ul>
<li>虚拟机栈中引用的对象（栈帧中的本地变量表）</li>
<li>方法区中的常量引用的对象</li>
<li>方法区中的类静态属性引用的对象</li>
<li>本地方法栈中JNI(Native 方法)的引用对象</li>
<li>活跃线程的引用对象</li>
</ul>
<h3 id="垃圾回收算法"><a href="#垃圾回收算法" class="headerlink" title="垃圾回收算法"></a>垃圾回收算法</h3><h4 id="标记-清除"><a href="#标记-清除" class="headerlink" title="标记-清除"></a>标记-清除</h4><ul>
<li>标记：从根集合进行扫描，对存活的对象进行标记</li>
<li>清除：对堆内存从头到尾进行线性遍历，回收不可达对象内存</li>
</ul>
<p>特点：</p>
<ul>
<li>碎片化</li>
</ul>
<h4 id="复制"><a href="#复制" class="headerlink" title="复制"></a>复制</h4><ul>
<li>分为对象面和空闲面</li>
<li>对象在对象面上创建</li>
<li>存活的对象被从对象面复制到空闲面</li>
<li>将对象面所有对象内存清除</li>
</ul>
<p>特点：</p>
<ul>
<li>解决碎片化</li>
<li>顺序分配内存，简单高效</li>
<li>适用于对象存活率低的场景</li>
</ul>
<h4 id="标记整理"><a href="#标记整理" class="headerlink" title="标记整理"></a>标记整理</h4><ul>
<li>标记：从根集合进行扫描，对存活的对象进行标记</li>
<li>清除：移动所有存活的对象，且按照内存地址次序依次排列，然后将末端内存地址以后的内存全部回收。</li>
</ul>
<h4 id="gc的分类"><a href="#gc的分类" class="headerlink" title="gc的分类"></a>gc的分类</h4><ul>
<li>minor gc(复制)</li>
<li>full gc</li>
</ul>
<h4 id="分代收集算法"><a href="#分代收集算法" class="headerlink" title="分代收集算法"></a>分代收集算法</h4><p><img src="https://lixiangbetter.github.io/2021/06/07/java%E9%9D%A2%E8%AF%95/1623509722900.jpg" alt></p>
<h5 id="对象如何晋升到老年代"><a href="#对象如何晋升到老年代" class="headerlink" title="对象如何晋升到老年代"></a>对象如何晋升到老年代</h5><ul>
<li>经历一定minor次数依然存活的对象</li>
<li>Survivor区中存放不下的对象</li>
<li>新生成的大对象（-xx:+PretenuerSizeThreshold）</li>
</ul>
<h5 id="常用调优参数"><a href="#常用调优参数" class="headerlink" title="常用调优参数"></a>常用调优参数</h5><p><img src="https://lixiangbetter.github.io/2021/06/07/java%E9%9D%A2%E8%AF%95/1623510171598.jpg" alt></p>
<h4 id="触发full-gc的条件"><a href="#触发full-gc的条件" class="headerlink" title="触发full gc的条件"></a>触发full gc的条件</h4><p><img src="https://lixiangbetter.github.io/2021/06/07/java%E9%9D%A2%E8%AF%95/1623510420164.jpg" alt></p>
<h3 id="stop-the-world"><a href="#stop-the-world" class="headerlink" title="stop-the-world"></a>stop-the-world</h3><ul>
<li>JVM由于要执行gc而停止了应用程序的执行；</li>
<li>任何一种gc算法中都会发生</li>
<li>多数gc优化通过减少stop-the-world发生的时间来提高性能</li>
</ul>
<h3 id="safepoint"><a href="#safepoint" class="headerlink" title="safepoint"></a>safepoint</h3><ul>
<li>分析过程中对象引用关系不会发生变化的点</li>
<li>产生safepoint的地方：方法调用；循环跳转；异常跳转等</li>
<li>安全点数量得适中</li>
</ul>
<h3 id="jvm的运行模式"><a href="#jvm的运行模式" class="headerlink" title="jvm的运行模式"></a>jvm的运行模式</h3><ul>
<li>client(启动较快，轻量级) </li>
<li>Server（启动较慢，重量级虚拟机，做了很多优化，运行稳定后，性能更高）</li>
</ul>
<h3 id="年轻代收集器"><a href="#年轻代收集器" class="headerlink" title="年轻代收集器"></a>年轻代收集器</h3><h4 id="serial收集器（-xx-UseSerialGC-复制算法）"><a href="#serial收集器（-xx-UseSerialGC-复制算法）" class="headerlink" title="serial收集器（-xx:UseSerialGC,复制算法）"></a>serial收集器（-xx:UseSerialGC,复制算法）</h4><p><img src="https://lixiangbetter.github.io/2021/06/07/java%E9%9D%A2%E8%AF%95/1623552770819.jpg" alt></p>
<h4 id="parnew收集器（-xx-UseParNewGC-复制算法）"><a href="#parnew收集器（-xx-UseParNewGC-复制算法）" class="headerlink" title="parnew收集器（-xx:+UseParNewGC,复制算法）"></a>parnew收集器（-xx:+UseParNewGC,复制算法）</h4><p><img src="https://lixiangbetter.github.io/2021/06/07/java%E9%9D%A2%E8%AF%95/1623552847057.jpg" alt></p>
<h4 id="parallel-scavenge收集器（-xx-UseParallelGC，复制算法）"><a href="#parallel-scavenge收集器（-xx-UseParallelGC，复制算法）" class="headerlink" title="parallel scavenge收集器（-xx:+UseParallelGC，复制算法）"></a>parallel scavenge收集器（-xx:+UseParallelGC，复制算法）</h4><p>吞吐量 = 运行用户代码时间 / （运行用户代码时间 + 垃圾收集时间）</p>
<p><img src="https://lixiangbetter.github.io/2021/06/07/java%E9%9D%A2%E8%AF%95/1623552952259.jpg" alt></p>
<h3 id="老年代"><a href="#老年代" class="headerlink" title="老年代"></a>老年代</h3><h4 id="serial-old收集器（-xx-UseSerialOldGC，标记整理算法）"><a href="#serial-old收集器（-xx-UseSerialOldGC，标记整理算法）" class="headerlink" title="serial old收集器（-xx:+UseSerialOldGC，标记整理算法）"></a>serial old收集器（-xx:+UseSerialOldGC，标记整理算法）</h4><p><img src="https://lixiangbetter.github.io/2021/06/07/java%E9%9D%A2%E8%AF%95/1623553045570.jpg" alt></p>
<h4 id="parallel-old收集器（-xx：-UseParrallelOldGC，标记整理算法）"><a href="#parallel-old收集器（-xx：-UseParrallelOldGC，标记整理算法）" class="headerlink" title="parallel old收集器（-xx：+UseParrallelOldGC，标记整理算法）"></a>parallel old收集器（-xx：+UseParrallelOldGC，标记整理算法）</h4><p><img src="https://lixiangbetter.github.io/2021/06/07/java%E9%9D%A2%E8%AF%95/1623553222077.jpg" alt></p>
<h4 id="cms收集器（-xx-UseConcMarkSweepGC，标记-清除算法）"><a href="#cms收集器（-xx-UseConcMarkSweepGC，标记-清除算法）" class="headerlink" title="cms收集器（-xx:+UseConcMarkSweepGC，标记-清除算法）"></a>cms收集器（-xx:+UseConcMarkSweepGC，标记-清除算法）</h4><ul>
<li>初始标记：stop-the-world</li>
<li>并发标记：并发追溯标记，程序不会停顿</li>
<li>并发预清理：查找执行并发标记阶段从年轻代晋升到老年代的对象</li>
<li>重新标记：暂停虚拟机，扫描cms堆中剩余对象</li>
<li>并发清理：清理垃圾对象，程序不会停顿</li>
<li>并发重置：重置cms收集器的数据结构</li>
</ul>
<h3 id="g1收集器（-xx-UseG1GC，复制-标记-整理算法）年轻代-老年代"><a href="#g1收集器（-xx-UseG1GC，复制-标记-整理算法）年轻代-老年代" class="headerlink" title="g1收集器（-xx:+UseG1GC，复制+标记-整理算法）年轻代+老年代"></a>g1收集器（-xx:+UseG1GC，复制+标记-整理算法）年轻代+老年代</h3><p><img src="https://lixiangbetter.github.io/2021/06/07/java%E9%9D%A2%E8%AF%95/1623553847033.jpg" alt></p>
<h3 id="Object的finalize-方法的作用是否与c-的析构函数作用相同"><a href="#Object的finalize-方法的作用是否与c-的析构函数作用相同" class="headerlink" title="Object的finalize()方法的作用是否与c++的析构函数作用相同"></a>Object的finalize()方法的作用是否与c++的析构函数作用相同</h3><ul>
<li>与c++的析构函数不同，析构函数调用确定，而它的是不确定的</li>
<li>将未被引用的对象放置于f-queue队列</li>
<li>方法执行随时可能会被终止</li>
<li>给予对象最后一次重生的机会</li>
</ul>
<h3 id="java中的强引用、软引用、弱引用、虚引用有什么用"><a href="#java中的强引用、软引用、弱引用、虚引用有什么用" class="headerlink" title="java中的强引用、软引用、弱引用、虚引用有什么用"></a>java中的强引用、软引用、弱引用、虚引用有什么用</h3><p><img src="https://lixiangbetter.github.io/2021/06/07/java%E9%9D%A2%E8%AF%95/1623565480814.jpg" alt></p>
<h3 id="引用队列"><a href="#引用队列" class="headerlink" title="引用队列"></a>引用队列</h3><ul>
<li>无实际存储结构，存储逻辑依赖于内部节点之间的关系来表达</li>
<li>存储关联的且被gc的软引用，弱引用以及虚引用</li>
</ul>
<h2 id="Java多线程与并发"><a href="#Java多线程与并发" class="headerlink" title="Java多线程与并发"></a>Java多线程与并发</h2><p>链接：<a href="https://blog.csdn.net/qq_37128049/article/details/90080074" target="_blank" rel="noopener">https://blog.csdn.net/qq_37128049/article/details/90080074</a></p>
<h2 id="Java多线程与并发-原理"><a href="#Java多线程与并发-原理" class="headerlink" title="Java多线程与并发-原理"></a>Java多线程与并发-原理</h2><p>链接：<a href="https://blog.csdn.net/qq_37128049/article/details/90080074" target="_blank" rel="noopener">https://blog.csdn.net/qq_37128049/article/details/90080074</a></p>
<h3 id="reentrantlock的condition原理"><a href="#reentrantlock的condition原理" class="headerlink" title="reentrantlock的condition原理"></a>reentrantlock的condition原理</h3><p><a href="https://blog.csdn.net/zxd8080666/article/details/83214089" target="_blank" rel="noopener">https://blog.csdn.net/zxd8080666/article/details/83214089</a></p>
<h2 id="java常用类库与技巧"><a href="#java常用类库与技巧" class="headerlink" title="java常用类库与技巧"></a>java常用类库与技巧</h2><p>链接：<a href="https://blog.csdn.net/qq_37128049/article/details/90097438" target="_blank" rel="noopener">https://blog.csdn.net/qq_37128049/article/details/90097438</a></p>
<h3 id="Error和Exception的区别"><a href="#Error和Exception的区别" class="headerlink" title="Error和Exception的区别"></a>Error和Exception的区别</h3><p>Error: 程序无法处理的系统错误，编译器不做检查。</p>
<p>Exception: 程序可以处理的异常，捕获后可能恢复。</p>
<p>总结：前者是程序无法处理的错误，后者是可以处理的异常。</p>
<p>RuntimeException: 不可预知的，应当自行避免。空指针、数据越界</p>
<p>非RuntimeException: 可预知，从编译器校验的异常。</p>
<h3 id="Hashmap-put方法的逻辑"><a href="#Hashmap-put方法的逻辑" class="headerlink" title="Hashmap:put方法的逻辑"></a>Hashmap:put方法的逻辑</h3><p><img src="https://lixiangbetter.github.io/2021/06/07/java%E9%9D%A2%E8%AF%95/1623585479095.jpg" alt></p>
<h3 id="hash算法"><a href="#hash算法" class="headerlink" title="hash算法"></a>hash算法</h3><p><img src="https://lixiangbetter.github.io/2021/06/07/java%E9%9D%A2%E8%AF%95/1623585809460.jpg" alt></p>
<h3 id="ConcurrentHashmap-别的需要注意的点"><a href="#ConcurrentHashmap-别的需要注意的点" class="headerlink" title="ConcurrentHashmap:别的需要注意的点"></a>ConcurrentHashmap:别的需要注意的点</h3><ul>
<li>size()方法和mapping count()方法的异同，两者计算是否准确？</li>
<li>多线程环境下如何进行扩容？</li>
</ul>
<h2 id="Java框架-Spring"><a href="#Java框架-Spring" class="headerlink" title="Java框架-Spring"></a>Java框架-Spring</h2><p>链接：<a href="https://blog.csdn.net/qq_37128049/article/details/90140899" target="_blank" rel="noopener">https://blog.csdn.net/qq_37128049/article/details/90140899</a></p>
]]></content>
      <categories>
        <category>面经</category>
      </categories>
      <tags>
        <tag>java开发</tag>
      </tags>
  </entry>
  <entry>
    <title>大数据开发面试随笔记录</title>
    <url>/2021/06/03/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E9%9D%A2%E8%AF%95%E9%9A%8F%E7%AC%94%E8%AE%B0%E5%BD%95/</url>
    <content><![CDATA[<h2 id="大数据开发"><a href="#大数据开发" class="headerlink" title="大数据开发"></a>大数据开发</h2><h3 id="星型模型和雪花模型"><a href="#星型模型和雪花模型" class="headerlink" title="星型模型和雪花模型"></a>星型模型和雪花模型</h3><p>星型模型：是一种多维的数据关系，它由一个事实表（Fact Table）和一组维表（Dimension Table）组成。每个维表都有一个维作为主键，所有这些维的主键组合成事实表的主键。事实表的非主键属性称为事实（Fact），它们一般都是数值或其他可以进行计算的数据；</p>
<p>雪花型模型：当有一个或多个维表没有直接连接到事实表上，而是通过其他维表连接到事实表上时，其图解就像多个雪花连接在一起，故称雪花模型。</p>
<h3 id="google大数据论文"><a href="#google大数据论文" class="headerlink" title="google大数据论文"></a>google大数据论文</h3><p>①2003年，Google发布Google File System论文，这是一个可扩展的分布式文件系统，用于大型的、分布式的、对大量数据进行访问的应用。它运行于廉价的普通硬件上，提供容错功能。从根本上说：文件被分割成很多块，使用冗余的方式储存于商用机器集群上。<br>②紧随其后的就是2004年公布的 MapReduce论文，论文描述了大数据的分布式计算方式，主要思想是将任务分解然后在多台处理能力较弱的计算节点中同时处理，然后将结果合并从而完成大数据处理。<br>③最后就是谷歌发布于2006年的Bigtable，其启发了无数的NoSQL数据库，比如：Cassandra、HBase等等。Cassandra架构中有一半是模仿Bigtable，包括了数据模型、SSTables以及提前写日志（另一半是模仿Amazon的Dynamo数据库，使用点对点集群模式）。</p>
<h3 id="hadoop搭建过程中遇到哪些问题"><a href="#hadoop搭建过程中遇到哪些问题" class="headerlink" title="hadoop搭建过程中遇到哪些问题"></a>hadoop搭建过程中遇到哪些问题</h3><ol>
<li>Hadoop初始化错误</li>
</ol>
<p>INFO org.apache.hadoop.ipc.Client: Retryingconnect to server: uec-fe/16.157.63.10:9000. Already tried 0 time(s).</p>
<p>这种情况对数出现在启动namenode时成功，但是运行job时就会一直连接。</p>
<p>通过查看TaskTracker日志可以看到不停地Retryingconnect,但是一直连不上，有如下解决办法：</p>
<ul>
<li>在启动hadoop前没有格式化namenode，需要再每次启动前格式化namenode</li>
</ul>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">bin/hadoop namenode –format</span><br></pre></td></tr></table></figure>

<ul>
<li><p>如果格式化后还存在相同的问题：需要先停掉hadoop，hadoop默认配置把一些文件放到/tmp 下，我们需要删除所有</p>
<p>机器上的/tmp/hadoop-roor/(你的用户名)的文件，然后重新格式化后，启动服务。</p>
</li>
<li><p>也可以自定义tmp文件的位置，编辑conf/core-site.xml文件</p>
</li>
</ul>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">property</span>&gt;</span></span><br><span class="line">	<span class="tag">&lt;<span class="name">name</span>&gt;</span>hadoop.tmp.dir<span class="tag">&lt;/<span class="name">name</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">value</span>&gt;</span>/var/log/hadoop/tmp<span class="tag">&lt;/<span class="name">value</span>&gt;</span></span><br><span class="line">  <span class="tag">&lt;<span class="name">description</span>&gt;</span>Abase for other temporary directories<span class="tag">&lt;/<span class="name">description</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;/<span class="name">property</span>&gt;</span></span><br></pre></td></tr></table></figure>

<p>关闭hadoop，格式化namenode之后启动hadoop</p>
<ul>
<li>还有种可能是由于/etc/hosts没有编辑造成的，更改后重启</li>
</ul>
<ol start="2">
<li>Your DataNodes won’t start, and you seesomething like this in logs/<em>datanode</em>:</li>
</ol>
<p>解决办法：</p>
<p>bin/stop-all.sh rm–fr /tmp/hadoop-用户名 bin/hadoopnamenode -format bin/start-all.sh</p>
<ol start="3">
<li>INFO hdfs.DFSClient: Exception increateBlockOutputStream java.io.IOException:Bad connect ack with firstBadLink192.168.30.149:50010</li>
</ol>
<p>原因：可能有节点的防火墙开着，尝试关闭防火墙</p>
<p>/etc/init.d/iptables stop ###关闭防火墙</p>
<ol start="4">
<li>org.apache.hadoop.dfs.SafeModeException:Cannot delete …, Name node is in safe mode</li>
</ol>
<p>原因：hadoop的安全模式开启了，在分布式文件系统启动的时候，开始的时候会有安全模式，当分布式文件系统处于安全模式的情况下，文件系统中的内容不允许修改也不允许删除，直到安全模式结束。</p>
<p>安全模式主要是为了系统启动的时候检查各个DataNode上数据块的有效性，同时根据策略必要的复制或者删除部分数据块。运行期通过命令也可以进入安全模式。在实践过程中，系统启动的时候去修改和删除文件也会有安全模式不允许修改的出错提示，只需要等待一会儿即可</p>
<p>bin/hadoopd fs admin-safemode leave</p>
<ol start="5">
<li>File/home/hadoop/tmp/mapred/system/jobtracker.info could only be replicated to 0nodes, instead of 1最常见的错误！！我是这样，</li>
</ol>
<p>原因：当hadoop刚刚start-all时，datanode和namenode还没有进行连接，这个时候如果执行上传文件命令，会导致此命令，稍等一会问题就会解决，如果问题得不到解决，删除tmp/hadoop-你的用户名 文件重新格式化后启动。</p>
<ol start="6">
<li>ERROR org.apache.hadoop.hdfs.server.datanode.DataNode: java.net.UnknownHostException:hadoop149: hadoop149 atjava.net.InetAddress.getLocalHost(InetAddress.java:1426)。</li>
</ol>
<p>原因：可能防火墙原因（关闭防火墙：/etc/init.d/iptables stop），也有可能是/etc/hosts文件没有添加相关信息 例如添加：192.168.30.149 hadoop149</p>
<p>重启hadoop</p>
<h3 id="shell脚本"><a href="#shell脚本" class="headerlink" title="shell脚本"></a>shell脚本</h3><p>hadoop多台机器中执行同样的命令常用。</p>
<p>启动zookeeper</p>
<figure class="highlight bash"><table><tr><td class="code"><pre><span class="line"><span class="meta">#! /bin/bash</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">case</span> <span class="variable">$1</span> <span class="keyword">in</span></span><br><span class="line"><span class="string">"start"</span>)&#123;</span><br><span class="line">	<span class="keyword">for</span> i <span class="keyword">in</span> hadoop102 hadoop103 hadoop104</span><br><span class="line">	<span class="keyword">do</span></span><br><span class="line">		ssh <span class="variable">$i</span> <span class="string">"/opt/module/zookeeper-3.4.10/bin/zkServer.sh start"</span></span><br><span class="line">	<span class="keyword">done</span></span><br><span class="line">&#125;;;</span><br><span class="line"><span class="string">"stop"</span>)&#123;</span><br><span class="line">	<span class="keyword">for</span> i <span class="keyword">in</span> hadoop102 hadoop103 hadoop104</span><br><span class="line">	<span class="keyword">do</span></span><br><span class="line">		ssh <span class="variable">$i</span> <span class="string">"/opt/module/zookeeper-3.4.10/bin/zkServer.sh stop"</span></span><br><span class="line">	<span class="keyword">done</span></span><br><span class="line">&#125;;;</span><br><span class="line"><span class="string">"status"</span>)&#123;</span><br><span class="line">	<span class="keyword">for</span> i <span class="keyword">in</span> hadoop102 hadoop103 hadoop104</span><br><span class="line">	<span class="keyword">do</span></span><br><span class="line">		ssh <span class="variable">$i</span> <span class="string">"/opt/module/zookeeper-3.4.10/bin/zkServer.sh status"</span></span><br><span class="line">	<span class="keyword">done</span></span><br><span class="line">&#125;;;</span><br><span class="line"><span class="keyword">esac</span></span><br></pre></td></tr></table></figure>

<p>用于调度执行的shell脚本</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line"><span class="meta">#</span><span class="bash">!/bin/bash</span></span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 定义变量方便修改</span></span><br><span class="line">APP=gmall</span><br><span class="line">hive=/opt/module/hive/bin/hive</span><br><span class="line"></span><br><span class="line"><span class="meta">#</span><span class="bash"> 如果是输入的日期按照取输入日期；如果没输入日期取当前时间的前一天</span></span><br><span class="line">if [ -n "$1" ] ;then</span><br><span class="line">	do_date=$1</span><br><span class="line">else </span><br><span class="line">	do_date=`date -d "-1 day" +%F`  </span><br><span class="line">fi </span><br><span class="line"></span><br><span class="line">sql="</span><br><span class="line">set hive.exec.dynamic.partition.mode=nonstrict;</span><br><span class="line"></span><br><span class="line">insert overwrite table "$APP".dwd_start_log</span><br><span class="line">PARTITION (dt='$do_date')</span><br><span class="line">select </span><br><span class="line">    get_json_object(line,'$.mid') mid_id,</span><br><span class="line">    get_json_object(line,'$.uid') user_id,</span><br><span class="line">    get_json_object(line,'$.vc') version_code,</span><br><span class="line">    get_json_object(line,'$.vn') version_name,</span><br><span class="line">    get_json_object(line,'$.l') lang,</span><br><span class="line">    get_json_object(line,'$.sr') source,</span><br><span class="line">    get_json_object(line,'$.os') os,</span><br><span class="line">    get_json_object(line,'$.ar') area,</span><br><span class="line">    get_json_object(line,'$.md') model,</span><br><span class="line">    get_json_object(line,'$.ba') brand,</span><br><span class="line">    get_json_object(line,'$.sv') sdk_version,</span><br><span class="line">    get_json_object(line,'$.g') gmail,</span><br><span class="line">    get_json_object(line,'$.hw') height_width,</span><br><span class="line">    get_json_object(line,'$.t') app_time,</span><br><span class="line">    get_json_object(line,'$.nw') network,</span><br><span class="line">    get_json_object(line,'$.ln') lng,</span><br><span class="line">    get_json_object(line,'$.la') lat,</span><br><span class="line">    get_json_object(line,'$.entry') entry,</span><br><span class="line">    get_json_object(line,'$.open_ad_type') open_ad_type,</span><br><span class="line">    get_json_object(line,'$.action') action,</span><br><span class="line">    get_json_object(line,'$.loading_time') loading_time,</span><br><span class="line">    get_json_object(line,'$.detail') detail,</span><br><span class="line">    get_json_object(line,'$.extend1') extend1</span><br><span class="line">from "$APP".ods_start_log </span><br><span class="line">where dt='$do_date';</span><br><span class="line">"</span><br><span class="line"></span><br><span class="line"><span class="meta">$</span><span class="bash">hive -e <span class="string">"<span class="variable">$sql</span>"</span></span></span><br></pre></td></tr></table></figure>

<h3 id="vim命令"><a href="#vim命令" class="headerlink" title="vim命令"></a>vim命令</h3><p>〇、显示行号<br>当前文件  命令模式输入  set nu</p>
<p>:set nu</p>
<p>一、跳到指定行<br>①在正常模式下输入ngg 或者 nG，n为指定的行数，如12gg或者12G 跳转到第12行.</p>
<p>使用 gg 跳转到当前文件的第一行</p>
<p>使用 G 跳转光标到当前文件的最后一行</p>
<p>②在命令模式下输入行号n<br>:n</p>
<p>③打开文件即跳转到指定行，行号n<br>vim +n FileName</p>
<h3 id="hadoop搭建流程"><a href="#hadoop搭建流程" class="headerlink" title="hadoop搭建流程"></a>hadoop搭建流程</h3><p><a href="https://www.aboutyun.com/thread-6487-1-1.html" target="_blank" rel="noopener">https://www.aboutyun.com/thread-6487-1-1.html</a></p>
<h3 id="mapreduce求topn的问题"><a href="#mapreduce求topn的问题" class="headerlink" title="mapreduce求topn的问题"></a>mapreduce求topn的问题</h3><p>链接：<a href="https://www.cnblogs.com/hdc520/p/12900854.html" target="_blank" rel="noopener">https://www.cnblogs.com/hdc520/p/12900854.html</a></p>
]]></content>
      <categories>
        <category>bigdata</category>
      </categories>
      <tags>
        <tag>面试</tag>
      </tags>
  </entry>
  <entry>
    <title>微信点餐项目总结</title>
    <url>/2021/06/02/%E5%BE%AE%E4%BF%A1%E7%82%B9%E9%A4%90%E9%A1%B9%E7%9B%AE%E6%80%BB%E7%BB%93/</url>
    <content><![CDATA[<h2 id="微信点餐"><a href="#微信点餐" class="headerlink" title="微信点餐"></a>微信点餐</h2><h3 id="全局异常的处理"><a href="#全局异常的处理" class="headerlink" title="全局异常的处理"></a>全局异常的处理</h3><p>@ControllerAdvice</p>
<h3 id="AOP验证微信是否登录"><a href="#AOP验证微信是否登录" class="headerlink" title="AOP验证微信是否登录"></a>AOP验证微信是否登录</h3><p>@Aspect</p>
<p>定义一个切面，每一个请求调用，都需要判断是否携带有token.</p>
<h3 id="分布式session"><a href="#分布式session" class="headerlink" title="分布式session"></a>分布式session</h3><p>uuid作为token存储在redis中</p>
<h3 id="微信公众号授权过程"><a href="#微信公众号授权过程" class="headerlink" title="微信公众号授权过程"></a>微信公众号授权过程</h3><p>通过AppId和AppSecret和code==&gt;获得access_code</p>
<p>Access_code ==&gt; openId</p>
<p>转发到returnUrl?openid=***</p>
<h3 id="微信支付流程"><a href="#微信支付流程" class="headerlink" title="微信支付流程"></a>微信支付流程</h3><p>发起支付=&gt;收到微信的异步通知=&gt;根据payResponse返回的信息，判断并修改订单状态</p>
<p>退款=&gt;需要一个证书</p>
<p>完成订单后，可以进行微信模版消息推送</p>
<h3 id="redis分布式锁的实现"><a href="#redis分布式锁的实现" class="headerlink" title="redis分布式锁的实现"></a>redis分布式锁的实现</h3><p>key为productId，value为当前时间+超时时间</p>
<h3 id="redis缓存"><a href="#redis缓存" class="headerlink" title="redis缓存"></a>redis缓存</h3><p>@cacheable</p>
<p>缓存，但update,它并不更新</p>
<p>@<strong>cacheEvict</strong></p>
<p>执行下面的update，进而调用save<strong>方法之后</strong>，清除掉redis里的内容删除。</p>
<h2 id="微信点餐（微服务升级）"><a href="#微信点餐（微服务升级）" class="headerlink" title="微信点餐（微服务升级）"></a>微信点餐（微服务升级）</h2><h3 id="Ribbon-负载均衡"><a href="#Ribbon-负载均衡" class="headerlink" title="Ribbon: 负载均衡"></a>Ribbon: 负载均衡</h3><p>ServerList: 首选获取所有的服务列表</p>
<p>ServerListFilter：过滤掉一部分地址</p>
<p>通过IRule选择一个实例</p>
<h3 id="应用通信"><a href="#应用通信" class="headerlink" title="应用通信"></a>应用通信</h3><p>Feign的使用</p>
<h3 id="统一配置中心"><a href="#统一配置中心" class="headerlink" title="统一配置中心"></a>统一配置中心</h3><p>Spring Cloud Bus 自动刷新配置（rabbitmq）</p>
<p>git配置web-hook，访问/bus-refresh</p>
<h3 id="原始流程，下单"><a href="#原始流程，下单" class="headerlink" title="原始流程，下单"></a>原始流程，下单</h3><ol>
<li>查询商品信息（调用商品服务）</li>
<li>计算总价（生成订单详情）</li>
<li>商品服务扣库存（调用商品服务）</li>
<li>订单入库（生成订单）</li>
</ol>
<h3 id="异步扣减库存-修改为秒杀场景的思考"><a href="#异步扣减库存-修改为秒杀场景的思考" class="headerlink" title="异步扣减库存(修改为秒杀场景的思考)"></a>异步扣减库存(修改为秒杀场景的思考)</h3><p>1.读redis</p>
<p>2.减库存并将新值重新设置进redis</p>
<p>（1、2需要分布式锁，考虑多线程并发）</p>
<p>3.订单入库异常，手动回滚redis(try-catch)</p>
<h3 id="网关服务"><a href="#网关服务" class="headerlink" title="网关服务"></a>网关服务</h3><p>Cookie 和 动态路由</p>
<p>限流：令牌桶，放100个令牌</p>
<p>鉴权：</p>
<p>​    order/create 只能买家访问</p>
<p>​    order/finish 只能卖家访问</p>
<h3 id="服务容错"><a href="#服务容错" class="headerlink" title="服务容错"></a>服务容错</h3><p>服务降级：超市触发降级</p>
<p>断路器：错误率达到一定程度，触发断路器</p>
<p>​    10000秒</p>
<p>​    10滚动窗口中，有7次 70%&gt;60%,会触发断路器</p>
<p>​    60%</p>
<h3 id="rpc框架，dubbo简单理解"><a href="#rpc框架，dubbo简单理解" class="headerlink" title="rpc框架，dubbo简单理解"></a>rpc框架，dubbo简单理解</h3><p>dubbo作为rpc框架，实现的效果就是调用远程的方法就像在本地调用一样。如何做到呢？</p>
<p>就是本地有对远程方法的描述，包括方法名、参数、返回值，在dubbo中是远程和本地使用同样的接口；</p>
<p>然后呢，要有对网络通信的封装，要对调用方来说通信细节是完全不可见的，网络通信要做的就是将调用方法的属性通过一定的协议（简单来说就是消息格式）传递到服务端；</p>
<p>服务端按照协议解析出调用的信息；执行相应的方法；在将方法的返回值通过协议传递给客户端；客户端再解析；</p>
<p>在调用方式上又可以分为同步调用和异步调用；简单来说基本就这个过程。</p>
<h3 id="秒杀场景（交易性能优化）"><a href="#秒杀场景（交易性能优化）" class="headerlink" title="秒杀场景（交易性能优化）"></a>秒杀场景（交易性能优化）</h3><h4 id="性能瓶颈"><a href="#性能瓶颈" class="headerlink" title="性能瓶颈"></a>性能瓶颈</h4><ul>
<li>交易验证完全依赖数据库</li>
<li>库存行锁</li>
<li>后置处理逻辑</li>
</ul>
<p><img src="/Users/lx/Documents/myblog/source/_posts/%E5%BE%AE%E4%BF%A1%E7%82%B9%E9%A4%90%E9%A1%B9%E7%9B%AE%E6%80%BB%E7%BB%93/1623290749628.jpg" alt></p>
<h4 id="库存行锁优化"><a href="#库存行锁优化" class="headerlink" title="库存行锁优化"></a>库存行锁优化</h4><ul>
<li>扣减库存缓存化</li>
<li>异步同步数据库</li>
<li>库存数据库最终一致性保证</li>
</ul>
<h4 id="扣减库存缓存化"><a href="#扣减库存缓存化" class="headerlink" title="扣减库存缓存化"></a>扣减库存缓存化</h4><p>方案(1)：</p>
<ol>
<li>活动发布 同步库存进缓存</li>
<li>下单交易减缓存库存</li>
</ol>
<p>问题：</p>
<ol>
<li>数据库记录不一致</li>
</ol>
<p>方案(2)：</p>
<ol>
<li>活动发布 同步库存进缓存</li>
<li>下单交易减缓存库存</li>
<li>异步消息扣减数据库内库存</li>
</ol>
<p><img src="/Users/lx/Documents/myblog/source/_posts/%E5%BE%AE%E4%BF%A1%E7%82%B9%E9%A4%90%E9%A1%B9%E7%9B%AE%E6%80%BB%E7%BB%93/1623291711862.jpg" alt></p>
<h4 id="事务型消息"><a href="#事务型消息" class="headerlink" title="事务型消息"></a>事务型消息</h4><p>投递一个prepare状态的消息</p>
<p>问题：</p>
<ol>
<li>异步消息发送失败</li>
<li>扣减操作执行失败</li>
<li>下单失败无法正确回补库存</li>
</ol>
<p>本质：</p>
<p>没有库存流水</p>
<p>方案：</p>
<ol>
<li>引入库存操作流水</li>
<li>引入事务性消息机制</li>
</ol>
<h3 id="rocketmq如何保证消息不丢失？"><a href="#rocketmq如何保证消息不丢失？" class="headerlink" title="rocketmq如何保证消息不丢失？"></a>rocketmq如何保证消息不丢失？</h3><p>一、大体可以从三方面来说：</p>
<p>分别从Producer发送机制、Broker的持久化机制，以及消费者的offSet机制来最大程度保证消息不易丢失</p>
<ol>
<li>从Producer的视角来看：如果消息未能正确的存储在MQ中，或者消费者未能正确的消费到这条消息，都是消息丢失。</li>
<li>从Broker的视角来看：如果消息已经存在Broker里面了，如何保证不会丢失呢（宕机、磁盘崩溃）</li>
<li>从Consumer的视角来看：如果消息已经完成持久化了，但是Consumer取了，但是未消费成功且没有反馈，就是消息丢失</li>
</ol>
<p>从Producer分析：如何确保消息正确的发送到了Broker?</p>
<ol>
<li>默认情况下，可以通过同步的方式阻塞式的发送，check SendStatus，状态是OK，表示消息一定成功的投递到了Broker，状态超时或者失败，则会触发默认的2次重试。此方法的发送结果，可能Broker存储成功了，也可能没成功</li>
<li>采取事务消息的投递方式，并不能保证消息100%投递成功到了Broker，但是如果消息发送Ack失败的话，此消息会存储在CommitLog当中，但是对ConsumerQueue是不可见的。可以在日志中查看到这条异常的消息，严格意义上来讲，也并没有完全丢失</li>
<li>RocketMQ支持 日志的索引，如果一条消息发送之后超时，也可以通过查询日志的API，来check是否在Broker存储成功</li>
</ol>
<p>从Broker分析：如果确保接收到的消息不会丢失?</p>
<ol>
<li>消息支持持久化到Commitlog里面，即使宕机后重启，未消费的消息也是可以加载出来的</li>
<li>Broker自身支持同步刷盘、异步刷盘的策略，可以保证接收到的消息一定存储在本地的内存中</li>
<li>Broker集群支持 1主N从的策略，支持同步复制和异步复制的方式，同步复制可以保证即使Master 磁盘崩溃，消息仍然不会丢失。</li>
</ol>
<p>从Cunmser分析：如何确保拉取到的消息被成功消费？</p>
<ol>
<li>消费者可以根据自身的策略批量Pull消息</li>
<li>Consumer自身维护一个持久化的offset（对应MessageQueue里面的min offset），标记已经成功消费或者已经成功发回到broker的消息下标</li>
<li>如果Consumer消费失败，那么它会把这个消息发回给Broker，发回成功后，再更新自己的offset</li>
<li>如果Consumer消费失败，发回给broker时，broker挂掉了，那么Consumer会定时重试这个操作</li>
<li>如果Consumer和broker一起挂了，消息也不会丢失，因为consumer 里面的offset是定时持久化的，重启之后，继续拉取offset之前的消息到本地。</li>
</ol>
<h3 id="微服务之间分布式事务"><a href="#微服务之间分布式事务" class="headerlink" title="微服务之间分布式事务"></a>微服务之间分布式事务</h3><p>链接：使用spring cloud alibaba seata解决分布式事务</p>
]]></content>
      <categories>
        <category>项目总结</category>
      </categories>
      <tags>
        <tag>微信点餐</tag>
      </tags>
  </entry>
  <entry>
    <title>面试题随笔-21/6/1</title>
    <url>/2021/06/01/%E9%9D%A2%E8%AF%95%E9%A2%98%E9%9A%8F%E7%AC%94-21-6-1/</url>
    <content><![CDATA[<h2 id="今日面经"><a href="#今日面经" class="headerlink" title="今日面经"></a>今日面经</h2><p>链接：<a href="https://www.nowcoder.com/discuss/642608?source_id=profile_create_nctrack&amp;channel=-1" target="_blank" rel="noopener">https://www.nowcoder.com/discuss/642608?source_id=profile_create_nctrack&amp;channel=-1</a></p>
<h3 id="jvm内存空间"><a href="#jvm内存空间" class="headerlink" title="jvm内存空间"></a>jvm内存空间</h3><ul>
<li><p>方法区</p>
<p>方法区用于存储已被虚拟机加载的类信息、常量、静态变量、即时编译器编译后的代码等数据。<br>类的加载中提到了类加载的五个阶段。在加载阶段，会将字节流所代表的静态存储结构转化为方法区的运行时数据结构，在准备阶段，会将变量所使用的内存都将在方法区中进行分配。</p>
</li>
<li><p>程序计数器</p>
<p>java是多线程的，在线程切换回来后，它需要知道原先的执行位置在哪里。用来记录这个执行位置的，就是程序计数器。</p>
</li>
<li><p>虚拟机栈</p>
<p>虚拟机栈也是线程私有的，生命周期与线程相同。每个线程都有自己的虚拟机栈，如果这个线程执行了一个方法，就会创建一个栈帧，方法从调用直至执行完成的过程，就对应着一个栈帧在虚拟机栈中入栈到出栈的过程。</p>
</li>
<li><p>堆</p>
<p>堆内存区域的唯一目的就是存放对象实例，几乎所有的对象实例都在这里分配内存。</p>
</li>
<li><p>本地方法栈</p>
<p>底层native方法运行时，用的栈。</p>
</li>
</ul>
<h3 id="gc-垃圾回收器"><a href="#gc-垃圾回收器" class="headerlink" title="gc 垃圾回收器"></a>gc 垃圾回收器</h3><ol>
<li><p>串行：垃圾回收器 (Serial Garbage Collector)（单线程-串行垃圾回收器）</p>
<p>(1)串行垃圾回收器在进行垃圾回收时，它会持有所有应用程序的线程，冻结所有应用程序线程，使用单个垃圾回收线程来进行垃圾回收工作。</p>
<p>串行垃圾回收器是为单线程环境而设计的，如果你的程序不需要多线程，启动串行垃圾回收。</p>
<p>(2)串行收集器是最古老，最稳定以及效率高的收集器，可能会产生较长的停顿，只使用一个线程去回收。新生代、老年代使用串行回收；新生代复制算法、老年代标记-压缩；垃圾收集的过程中会Stop The World（服务暂停）。</p>
</li>
<li><p>串行：ParNew收集器（多线程-串行垃圾回收器）<br>ParNew收集器其实就是Serial收集器的多线程版本。新生代并行，老年代串行；新生代复制算法、老年代标记-压缩<br>使用方法：-XX:+UseParNewGC ParNew收集器</p>
</li>
<li><p>并行：Parallel收集器<br>Parallel Scavenge收集器类似ParNew收集器，Parallel收集器更关注系统的吞吐量。可以通过参数来打开自适应调节策略，虚拟机会根据当前系统的运行情况收集性能监控信息，动态调整这些参数以提供最合适的停顿时间或最大的吞吐量；也可以通过参数控制GC的时间不大于多少毫秒或者比例；新生代复制算法、老年代标记-压缩</p>
</li>
<li><p>并行：Parallel Old 收集器<br>Parallel Old是Parallel Scavenge收集器的老年代版本，使用多线程和“标记－整理”算法。这个收集器是在JDK 1.6中才开始提供</p>
</li>
<li><p>并发标记扫描CMS收集器</p>
<p>4个步骤：</p>
<p>初始标记（CMS initial mark）<br>并发标记（CMS concurrent mark）<br>重新标记（CMS remark）<br>并发清除（CMS concurrent sweep）<br>其中初始标记、重新标记这两个步骤仍然需要“Stop The World”。初始标记仅仅只是标记一下GC Roots能直接关联到的对象，速度很快，并发标记阶段就是进行GC Roots Tracing的过程，而重新标记阶段则是为了修正并发标记期间，因用户程序继续运作而导致标记产生变动的那一部分对象的标记记录，这个阶段的停顿时间一般会比初始标记阶段稍长一些，但远比并发标记的时间短。<br>由于整个过程中耗时最长的并发标记和并发清除过程中，收集器线程都可以与用户线程一起工作，所以总体上来说，CMS收集器的内存回收过程是与用户线程一起并发地执行。</p>
</li>
<li><p>G1(其主要目的是用来替换cms算法)</p>
<p>1、Region 区域化垃圾收集器：最大好处是化整为零，避免全内存扫描，只需要按照区域来进行扫描即可。</p>
<p>2、回收步骤：</p>
<p>3、四步过程：</p>
<ul>
<li>初始标记：标记一下GC Roots能直接关联到的对象，需要停顿线程，但耗时很短</li>
<li>并发标记：是从GC Root开始对堆中对象进行可达性分析，找出存活的对象，这阶段耗时较长，但可与用户程序并发执行</li>
<li>最终标记：修正在并发标记期间因用户程序继续运作而导致标记产生变动的那一部分标记记录</li>
<li>筛选回收：对各个Region的回收价值和成本进行排序，根据用户所期望的GC停顿时间来制定回收计划</li>
</ul>
<p>G1 整体采用标记-整理算法，局部是通过是通过复制算法，不会产生内存碎片。</p>
</li>
</ol>
<h3 id="类的加载，双亲委派，对象的生命周期"><a href="#类的加载，双亲委派，对象的生命周期" class="headerlink" title="类的加载，双亲委派，对象的生命周期"></a>类的加载，双亲委派，对象的生命周期</h3><ol>
<li><p>加载</p>
<p>加载阶段所要进行的工作：</p>
<ul>
<li>通过类的全限定名（包名 + 类名），获取到该类的.class文件的二进制字节流（二进制字节流加载到内存）</li>
<li>将二进制字节流所代表的静态存储结构，转化为方法区运行时的数据结构（映射成jvm能识别的结构）</li>
<li>在内存中生成一个代表该类的java.lang.Class对象，作为方法区中这个类的各种数据的访问入口（在内存中生成class对象）</li>
<li>通过反射可以得到此类的详细信息</li>
</ul>
</li>
<li><p>链接是指将加载阶段已经读入内存的类的二进制数据合并到 JVM 中，使之能够执行的过程，分为验证、准备、解析三个阶段</p>
<p>2.1 验证</p>
<p>​    确保class文件中的字节流包含的信息符合当前虚拟机的要求，保证这个被加载的class类的正确性，不会危害到虚拟机的安全</p>
<p>2.2 准备<br>   为类中的静态字段分配内存，并设置默认的初始值，比如 int a = 5 初始值是0。被final修饰的static字段不会设置，因为final在编译的时候就分配了</p>
<p>2.3 解析<br>   解析阶段的目的，是将常量池内的符号引用转换为直接引用的过程（将常量池内的符号引用解析成为实际引用）。如果符号引用指向一个未被加载的类，或者未被加载类的字段或方法，那么解析将触发这个类的加载（但未必触发这个类的链接以及初始化）</p>
<p>事实上，解析操作往往伴随着 JVM 在执行完初始化之后再执行</p>
<p>符号引用就是一组符号用来描述所引用的目标</p>
<p>直接引用就是直接指向目标的指针、相对偏移量或一个间接定位到目标的句柄</p>
</li>
<li><p>初始化</p>
<p>初始化就是执行类的构造器方法 init() 的过程（为类的静态变量赋予指定值，如上述的a被赋予5）</p>
<p>若该类具有父类，jvm会保证父类的 init() 先执行，然后再执行子类的 init()</p>
<p>如果类中存在初始化语句，就依次执行这些初始化语句，初始化语句指的是static修饰的语句</p>
</li>
</ol>
<h3 id="双亲委派机制"><a href="#双亲委派机制" class="headerlink" title="双亲委派机制"></a>双亲委派机制</h3><ol>
<li>什么是双亲委派机制</li>
</ol>
<p>当某个类加载器需要加载某个<code>.class</code>文件时，它首先把这个任务委托给他的上级类加载器，递归这个操作，如果上级的类加载器没有加载，自己才会去加载这个类。</p>
<ol start="2">
<li>类加载器的类别</li>
</ol>
<p>BootstrapClassLoader（启动类加载器）</p>
<p><code>c++</code>编写，加载<code>java</code>核心库 <code>java.*</code>,构造<code>ExtClassLoader</code>和<code>AppClassLoader</code>。由于引导类加载器涉及到虚拟机本地实现细节，开发者无法直接获取到启动类加载器的引用，所以不允许直接通过引用进行操作</p>
<p>ExtClassLoader （标准扩展类加载器）</p>
<p><code>java</code>编写，加载扩展库，如<code>classpath</code>中的<code>jre</code> ，<code>javax.*</code>或者<br> <code>java.ext.dir</code> 指定位置中的类，开发者可以直接使用标准扩展类加载器。</p>
<p>AppClassLoader（系统类加载器）</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">java`编写，加载程序所在的目录，如`user.dir`所在的位置的`class</span><br></pre></td></tr></table></figure>

<p>CustomClassLoader（用户自定义类加载器）</p>
<p><code>java</code>编写,用户自定义的类加载器,可加载指定路径的<code>class</code>文件</p>
<h3 id="集合，hashmap的实现，concurrenthashmap的实现"><a href="#集合，hashmap的实现，concurrenthashmap的实现" class="headerlink" title="集合，hashmap的实现，concurrenthashmap的实现"></a>集合，hashmap的实现，concurrenthashmap的实现</h3><p>HashMap：</p>
<ul>
<li>数据结构上的变化，数组+链表+红黑树</li>
<li>头插法可能引起链表中存在环，改为尾插法</li>
</ul>
<p>ConcurrentHashMap：</p>
<p>JDK1.7 ：</p>
<p>1.7中的 ConcurrentHashMap 是由 <code>Segment</code> 数组结构和 <code>HashEntry</code> 数组结构组成，即 ConcurrentHashMap 把哈希桶数组切分成小数组（Segment ），每个小数组有 n 个 HashEntry 组成。</p>
<p>如下图所示，首先将数据分为一段一段的存储，然后给每一段数据配一把锁，当一个线程占用锁访问其中一段数据时，其他段的数据也能被其他线程访问，实现了真正的并发访问。</p>
<p>Segment 继承了 ReentrantLock。</p>
<p> JDK1.8：</p>
<p>1.8中的ConcurrentHashMap 选择了与 HashMap 相同的<strong>Node数组+链表+红黑树</strong>结构；在锁的实现上，抛弃了原有的Segment 分段锁，采用<code>CAS + synchronized</code>实现更加细粒度的锁。</p>
<p>将锁的级别控制在了更细粒度的哈希桶数组元素级别，也就是说只需要锁住这个链表头节点（红黑树的根节点），就不会影响其他的哈希桶数组元素的读写，大大提高了并发度。</p>
<h3 id="synchornized的实现（新旧版本），对象头。"><a href="#synchornized的实现（新旧版本），对象头。" class="headerlink" title="synchornized的实现（新旧版本），对象头。"></a>synchornized的实现（新旧版本），对象头。</h3><p>链接：<a href="https://blog.csdn.net/javazejian/article/details/72828483" target="_blank" rel="noopener">https://blog.csdn.net/javazejian/article/details/72828483</a></p>
<h3 id="lock的实现，线程不安全问题。"><a href="#lock的实现，线程不安全问题。" class="headerlink" title="lock的实现，线程不安全问题。"></a>lock的实现，线程不安全问题。</h3><ol>
<li>ReentrantLock先通过CAS尝试获取锁;</li>
<li>如果此时锁已经被占用，该线程加入AQS队列并wait()</li>
<li>当前驱线程的锁被释放，挂在CLH队列为首的线程就会被notify()，然后继续CAS尝试获取锁，此时：<ul>
<li>非公平锁，如果有其他线程尝试lock()，有可能被其他刚好申请锁的线程<strong>抢占</strong>。</li>
<li>公平锁，只有在CLH<strong>队列头的线程</strong>才可以获取锁，<strong>新来的线程</strong>只能插入到队尾。</li>
</ul>
</li>
</ol>
<h3 id="volatile的原理"><a href="#volatile的原理" class="headerlink" title="volatile的原理"></a>volatile的原理</h3><p>保持内存可见性</p>
<blockquote>
<p>内存可见性（Memory Visibility）：所有线程都能看到共享内存的最新状态。</p>
</blockquote>
<p>防止指令重排</p>
<p>指令重排技术大大提高了程序执行效率，但同时也引入了一些问题。</p>
<p>初始化一个对象：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">memory = allocate();	<span class="comment">//1：分配对象的内存空间</span></span><br><span class="line">initInstance(memory);	<span class="comment">//2：初始化对象</span></span><br><span class="line">instance = memory;		<span class="comment">//3：设置instance指向刚分配的内存地址</span></span><br></pre></td></tr></table></figure>

<h3 id="线程池threadpoolexecutor的使用。"><a href="#线程池threadpoolexecutor的使用。" class="headerlink" title="线程池threadpoolexecutor的使用。"></a>线程池threadpoolexecutor的使用。</h3><ol>
<li>当线程数小于核心线程数时，创建线程。</li>
<li>当线程数大于等于核心线程数，且任务队列未满时，将任务放入任务队列。</li>
<li>当线程数大于等于核心线程数，且任务队列已满<ol>
<li>若线程数小于最大线程数，创建线程</li>
<li>若线程数等于最大线程数，抛出异常，拒绝任务</li>
</ol>
</li>
</ol>
<h3 id="IO，NIO，AIO，零拷贝的优势与应用"><a href="#IO，NIO，AIO，零拷贝的优势与应用" class="headerlink" title="IO，NIO，AIO，零拷贝的优势与应用"></a>IO，NIO，AIO，零拷贝的优势与应用</h3><p>IO 又称 BIO，blocking IO，就是单线程 IO，会阻塞线程。</p>
<p>NIO，non-blocking IO，就是不阻塞线程。通过 channel 和 selector 来完成多通道 IO。但它要定时遍历 selectKey 来查看哪个 channel 激活了。有点不优雅，才有了 AIO。</p>
<p>AIO：Asynchronous IO，用方法回调方式来处理。真正的异步 IO。</p>
<p>传统的io模型：</p>
<p><strong>「第一步」</strong>：将文件通过 <strong>「DMA」</strong> 技术从磁盘中拷贝到内核缓冲区</p>
<p><strong>「第二步」</strong>：将文件从内核缓冲区拷贝到用户进程缓冲区域中</p>
<p><strong>「第三步」</strong>：将文件从用户进程缓冲区中拷贝到 socket 缓冲区中</p>
<p><strong>「第四步」</strong>：将socket缓冲区中的文件通过 <strong>「DMA」</strong> 技术拷贝到网卡</p>
<p>DMA<strong>「原理」</strong>：</p>
<p>DMA 传输将数据<strong>「从一个地址空间复制到另外一个地址空间」</strong>。当 CPU 初始化这个传输动作，传输动作本身是由 DMA 控制器来实行和完成。</p>
<p>缺点：</p>
<p>因为是使用了内存映射的关系，所以零拷贝技术<strong>「无法对数据内容做更改」</strong>。</p>
<h3 id="聚集索引与非聚集索引"><a href="#聚集索引与非聚集索引" class="headerlink" title="聚集索引与非聚集索引"></a>聚集索引与非聚集索引</h3><blockquote>
<p>聚集索引一个表只能有一个，而非聚集索引一个表可以存在多个</p>
<p>聚集索引存储记录是物理上连续存在，而非聚集索引是逻辑上的连续，物理存储并不连续</p>
<p>聚集索引：物理存储按照索引排序；聚集索引是一种索引组织形式，索引的键值逻辑顺序决定了表数据行的物理存储顺序</p>
<p>非聚集索引：物理存储不按照索引排序；非聚集索引则就是普通索引了，仅仅只是对数据列创建相应的索引，不影响整个表的物理存储顺序.</p>
<p>索引是通过二叉树的数据结构来描述的，我们可以这么理解聚簇索引：索引的叶节点就是数据节点。而非聚簇索引的叶节点仍然是索引节点，只不过有一个指针指向对应的数据块。</p>
</blockquote>
<h3 id="ACID"><a href="#ACID" class="headerlink" title="ACID"></a>ACID</h3><p>1.原子性：事务是最小的执行单位，不允许分割。事务的原子性确保动作要么全部完成，要么完全不起作用；<br>2.一致性：执行事务前后，数据保持一致，多个事务对同一个数据读取的结果是相同的；<br>3.隔离性：并发访问数据库时，一个用户的事务不被其他事务所干扰，各并发事务之间数据库是独立的；<br>4.持久性：一个事务被提交之后。它对数据库中数据的改变是持久的，即使数据库发生故障也不应该对其有任何影响。</p>
<h3 id="事务隔离级别"><a href="#事务隔离级别" class="headerlink" title="事务隔离级别"></a>事务隔离级别</h3><p>事务隔离级别有4种，但是像Spring会提供给用户5种，来看一下：</p>
<p><strong>1、DEFAULT</strong></p>
<p>默认隔离级别，每种数据库支持的事务隔离级别不一样，如果Spring配置事务时将isolation设置为这个值的话，那么将使用底层数据库的默认事务隔离级别。顺便说一句，如果使用的MySQL，可以使用”<strong>select @@tx_isolation</strong>“来查看默认的事务隔离级别</p>
<p><strong>2、READ_UNCOMMITTED</strong></p>
<p>读未提交，即能够读取到没有被提交的数据，所以很明显这个级别的隔离机制无法解决脏读、不可重复读、幻读中的任何一种，因此很少使用</p>
<p><strong>3、READ_COMMITED</strong></p>
<p>读已提交，即能够读到那些已经提交的数据，自然能够防止脏读，但是无法限制不可重复读和幻读</p>
<p><strong>4、REPEATABLE_READ</strong></p>
<p>重复读取，即在数据读出来之后加锁，类似”select * from XXX for update”，明确数据读取出来就是为了更新用的，所以要加一把锁，防止别人修改它。REPEATABLE_READ的意思也类似，读取了一条数据，这个事务不结束，别的事务就不可以改这条记录，这样就解决了脏读、不可重复读的问题，但是幻读的问题还是无法解决</p>
<p><strong>5、SERLALIZABLE</strong></p>
<p>串行化，最高的事务隔离级别，不管多少事务，挨个运行完一个事务的所有子事务之后才可以执行另外一个事务里面的所有子事务，这样就解决了脏读、不可重复读和幻读的问题了</p>
<h3 id="InnoDB和myisam的锁区别"><a href="#InnoDB和myisam的锁区别" class="headerlink" title="InnoDB和myisam的锁区别"></a>InnoDB和myisam的锁区别</h3><p>InnoDB默认是行级锁，支持表锁，也就在InnoDB中更新某条数据会对行上锁，如果是排他锁，那么其他的事务访问这一行的数据需要等锁释放之后才能进行，而对其他行数据是没有影响的。</p>
<p>MyISAM默认是表级锁，不支持行级锁，也就是在MyISAM中进行某条数据更新时，会对整个表上锁，所有的其他事务对表中对数据进行访问或者更新的时候都必须等那个事务释放表锁。</p>
<p>需要注意的是，当InnoDB不走索引的时候，就会默认上表级锁了。</p>
<h3 id="InnoDB的锁分类"><a href="#InnoDB的锁分类" class="headerlink" title="InnoDB的锁分类"></a>InnoDB的锁分类</h3><h3 id="next-key锁的原理"><a href="#next-key锁的原理" class="headerlink" title="next key锁的原理"></a>next key锁的原理</h3><h3 id="大表优化"><a href="#大表优化" class="headerlink" title="大表优化"></a>大表优化</h3><p>链接：<a href="https://www.cnblogs.com/SimpleWu/p/10525946.html" target="_blank" rel="noopener">https://www.cnblogs.com/SimpleWu/p/10525946.html</a></p>
<h4 id="字段"><a href="#字段" class="headerlink" title="字段"></a><strong>字段</strong></h4><ul>
<li>尽量使用TINYINT、SMALLINT、MEDIUM_INT作为整数类型而非INT，如果非负则加上UNSIGNED</li>
<li>VARCHAR的长度只分配真正需要的空间</li>
<li>单表不要有太多字段，建议在20以内</li>
</ul>
<p>索引</p>
<ul>
<li>索引并不是越多越好，要根据查询有针对性的创建，考虑在WHERE和ORDER BY命令上涉及的列建立索引，可根据EXPLAIN来查看是否用了索引还是全表扫描</li>
<li>应尽量避免在WHERE子句中对字段进行NULL值判断，否则将导致引擎放弃使用索引而进行全表扫描</li>
<li>字符字段只建前缀索引</li>
<li>字符字段最好不要做主键</li>
<li>不用外键，由程序保证约束</li>
<li>尽量不用UNIQUE，由程序保证约束</li>
</ul>
<h4 id="查询SQL"><a href="#查询SQL" class="headerlink" title="查询SQL"></a><strong>查询SQL</strong></h4><ul>
<li>可通过开启慢查询日志来找出较慢的SQL</li>
<li>不做列运算：SELECT id WHERE age + 1 = 10，任何对列的操作都将导致表扫描，它包括数据库教程函数、计算表达式等等，查询时要尽可能将操作移至等号右边</li>
<li>sql语句尽可能简单：一条sql只能在一个cpu运算；大语句拆小语句，减少锁时间；一条大sql可以堵死整个库</li>
<li>不用SELECT *</li>
<li>OR改写成IN：OR的效率是n级别，IN的效率是log(n)级别，in的个数建议控制在200以内</li>
</ul>
<h4 id="引擎"><a href="#引擎" class="headerlink" title="引擎"></a><strong>引擎</strong></h4><p>目前广泛使用的是MyISAM和InnoDB两种引擎：</p>
<h5 id="MyISAM"><a href="#MyISAM" class="headerlink" title="MyISAM"></a>MyISAM</h5><p>MyISAM引擎是MySQL 5.1及之前版本的默认引擎，它的特点是：</p>
<ul>
<li>不支持行锁，读取时对需要读到的所有表加锁，写入时则对表加排它锁</li>
<li>不支持事务</li>
<li>不支持外键</li>
<li>不支持崩溃后的安全恢复</li>
<li>在表有读取查询的同时，支持往表中插入新纪录</li>
<li>支持BLOB和TEXT的前500个字符索引，支持全文索引</li>
<li>支持延迟更新索引，极大提升写入性能</li>
<li>对于不会进行修改的表，支持压缩表，极大减少磁盘空间占用</li>
</ul>
<h5 id="InnoDB"><a href="#InnoDB" class="headerlink" title="InnoDB"></a>InnoDB</h5><p>InnoDB在MySQL 5.5后成为默认索引，它的特点是：</p>
<ul>
<li>支持行锁，采用MVCC来支持高并发</li>
<li>支持事务</li>
<li>支持外键</li>
<li>支持崩溃后的安全恢复</li>
<li>不支持全文索引</li>
</ul>
<p>总体来讲，MyISAM适合SELECT密集型的表，而InnoDB适合INSERT和UPDATE密集型的表</p>
<h3 id="mvcc"><a href="#mvcc" class="headerlink" title="mvcc"></a>mvcc</h3><p>MVCC(Multi Version Concurrency Control的简称)，代表多版本并发控制。与MVCC相对的，是基于锁的并发控制，Lock-Based Concurrency Control)。<br> MVCC最大的优势：读不加锁，读写不冲突。在读多写少的OLTP应用中，读写不冲突是非常重要的，极大的增加了系统的并发性能。</p>
<p><strong>MVCC是通过在每行记录后面保存两个隐藏的列来实现的。这两个列，一个保存了行的创建时间，一个保存行的过期时间（或删除时间）。当然存储的并不是实际的时间值，而是系统版本号（system version number)。每开始一个新的事务，系统版本号都会自动递增。事务开始时刻的系统版本号会作为事务的版本号，用来和查询到的每行记录的版本号进行比较。</strong><br> 下面看一下在REPEATABLE READ隔离级别下，MVCC具体是如何操作的。</p>
<ul>
<li><p>SELECT</p>
<p>InnoDB会根据以下两个条件检查每行记录：</p>
<ol>
<li>InnoDB只查找版本早于当前事务版本的数据行（也就是，行的系统版本号小于或等于事务的系统版本号），这样可以确保事务读取的行，要么是在事务开始前已经存在的，要么是事务自身插入或者修改过的。</li>
<li>行的删除版本要么未定义，要么大于当前事务版本号。这可以确保事务读取到的行，在事务开始之前未被删除。</li>
</ol>
<p>只有符合上述两个条件的记录，才能返回作为查询结果</p>
</li>
<li><p>INSERT</p>
<p>InnoDB为新插入的每一行保存当前系统版本号作为行版本号。</p>
</li>
<li><p>DELETE</p>
<p>InnoDB为删除的每一行保存当前系统版本号作为行删除标识。</p>
</li>
<li><p>UPDATE</p>
<p>InnoDB为插入一行新记录，保存当前系统版本号作为行版本号，同时保存当前系统版本号到原来的行作为行删除标识。<br> 保存这两个额外系统版本号，使大多数读操作都可以不用加锁。这样设计使得读数据操作很简单，性能很好，并且也能保证只会读取到符合标准的行，不足之处是每行记录都需要额外的存储空间，需要做更多的行检查工作，以及一些额外的维护工作。</p>
</li>
</ul>
<h2 id="兴盛"><a href="#兴盛" class="headerlink" title="兴盛"></a>兴盛</h2><h3 id="如何设计数据库？合理安排他们的关联性？"><a href="#如何设计数据库？合理安排他们的关联性？" class="headerlink" title="如何设计数据库？合理安排他们的关联性？"></a>如何设计数据库？合理安排他们的关联性？</h3><p>1.需求分析阶段（常用自顶向下）</p>
<p>需求收集和分析，得到数据字典和数据流图。</p>
<pre><code>进行数据库设计首先必须准确了解和分析用户需求（包括数据与处理）。需求分析是整个设计过程的基础，也是最困难，最耗时的一步。需求分析是否做得充分和准确，决定了在其上构建数据库大厦的速度与质量。需求分析做的不好，会导致整个数据库设计返工重做。
 分析方法常用SA(Structured  Analysis) 结构化分析方法，SA方法从最上层的系统组织结构入手，采用自顶向下，逐层分解的方式分析系统。</code></pre><p>2.概念结构设计阶段（常用自底向上）</p>
<p>对用户需求综合、归纳与抽象，形成概念模型，用E-R图表示。</p>
   <figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">概念结构设计是整个数据库设计的关键，它通过对用户需求进行综合，归纳与抽象，形成了一个独立于具体DBMS的概念模型。</span><br></pre></td></tr></table></figure>

<p>3.逻辑结构设计阶段</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">任务：将基本E-R图转换为与选用DBMS产品所支持的数据模型相符合的逻辑结构。</span><br><span class="line">过程：</span><br><span class="line">	将概念结构转换为现有DBMS支持的关系、网状或层次模型中的某一种数据模型；</span><br><span class="line">	从功能和性能要求上对转换的模型进行评价，看它是否满足用户要求；</span><br><span class="line">	对数据模型进行优化。</span><br></pre></td></tr></table></figure>

<p>4.物理设计</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">就是根据所选择的关系型数据库的特点对逻辑模型进行存储结构设计。它涉及的内容包含以下4方面：</span><br><span class="line">1. 定义数据库、表及字段的命名规范；</span><br><span class="line">2. 选择合适的存储引擎；</span><br><span class="line">3. 为表中的字段选择合适的数据类型；</span><br><span class="line">4. 建立数据库结构。</span><br></pre></td></tr></table></figure>

<h3 id="E-R图设计"><a href="#E-R图设计" class="headerlink" title="E-R图设计"></a>E-R图设计</h3><p>在设计E-R图时，一般使用先局部后全局的方法。</p>
<ul>
<li>选择局部应用：根据某个系统的具体情况，在多层的数据流图中选择一个适当层次的数据流图作为设计分E-R图的出发点 。</li>
<li>逐一设计分E-R图：将数据字典中的数据抽取出来，参照数据流图，设计出E-R图，再作必要的调整。</li>
<li>调整原则：为简化图的处置，现实世界中的事物能作为属性对待的，尽量作为属性对待。作为“属性”，不能再具有描述的性质，也不能与其他实体具有联系。</li>
</ul>
<p>冲突：</p>
<p>1、属性冲突</p>
<p>属性域冲突，即<strong>属性值的类型、取值范围或取值集合不同。</strong><br>例如零件号，有的厂商把它定义为整数类型，有的部门把它定义为字符类型。<br>属性取值单位冲突。<br>例如，零件的重量有的以公斤为单位，有的以斤为单位，有的以克为单位。</p>
<p>2、命名冲突</p>
<p>同名异义，即不同意义的对象在不同的局部应用中具有相同的名字。</p>
<p>3、结构冲突</p>
<p>同一对象在不同应用中具有不同的抽象。</p>
<h3 id="设计模式的6大原则"><a href="#设计模式的6大原则" class="headerlink" title="设计模式的6大原则"></a>设计模式的6大原则</h3><p><strong>1.单一职责</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">一个类只负责一个功能领域中的相应职责，或者可以定义为：就一个类而言，应该只有一个引起它变化的原因。</span><br></pre></td></tr></table></figure>

<p><strong>2.开闭原则（Open Close Principle）</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">一个软件实体应当对扩展开放，对修改关闭。即软件实体应尽量在不修改原有代码的情况下进行扩展。</span><br></pre></td></tr></table></figure>

<p><strong>3.里氏代换原则（Liskov Substitution Principle）</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">任何基类可以出现的地方，子类一定可以出现。</span><br></pre></td></tr></table></figure>

<p><strong>4.依赖倒转原则（Dependence Inversion Principle）</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">抽象（接口/抽象类）不应该依赖于细节，细节应该依赖于抽象（接口/抽象类）。</span><br><span class="line">高层模块不应该依赖于低层模块，两者都应该依赖其抽象。</span><br></pre></td></tr></table></figure>

<p><strong>5.接口隔离原则（Interface Segregation Principle）</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">类间的依赖关系应该建立在最小的接口上</span><br></pre></td></tr></table></figure>

<p><strong>6.迪米特法则，又称最少知道原则（Demeter Principle）</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">一个对象应该对其他对象有最少的了解</span><br></pre></td></tr></table></figure>

<h3 id="zookeeper"><a href="#zookeeper" class="headerlink" title="zookeeper"></a>zookeeper</h3><p>watch机制</p>
<p>其实原理应该是很简单的，四个步骤：</p>
<ol>
<li>客户端注册Watcher到服务端;</li>
<li>服务端发生数据变更;<ol start="3">
<li>服务端通知客户端数据变更　　</li>
<li>客户端回调Watcher处理变更应对逻辑;</li>
</ol>
</li>
</ol>
<p>半数选举</p>
<h3 id="覆盖索引"><a href="#覆盖索引" class="headerlink" title="覆盖索引"></a>覆盖索引</h3><ul>
<li>解释一： 就是select的数据列只用从索引中就能够取得，不必从数据表中读取，换句话说查询列要被所使用的索引覆盖。</li>
<li>解释二： 索引是高效找到行的一个方法，当能通过检索索引就可以读取想要的数据，那就不需要再到数据表中读取行了。如果一个索引包含了（或覆盖了）满足查询语句中字段与条件的数据就叫 做覆盖索引。</li>
</ul>
<h3 id="explain-执行计划，参数详解"><a href="#explain-执行计划，参数详解" class="headerlink" title="explain 执行计划，参数详解"></a>explain 执行计划，参数详解</h3><p>一、 <strong>id</strong></p>
<p>SELECT识别符。这是SELECT的查询序列号</p>
<p><strong>我的理解是SQL执行的顺序的标识，SQL从大到小的执行</strong></p>
<ol>
<li><p>id相同时，执行顺序由上至下</p>
</li>
<li><p>如果是子查询，id的序号会递增，id值越大优先级越高，越先被执行</p>
</li>
<li><p>id如果相同，可以认为是一组，从上往下顺序执行；在所有组中，id值越大，优先级越高，越先执行</p>
</li>
</ol>
<p><strong>二、select_type</strong></p>
<p>   <strong><em>\</em>示查询中每个select子句的类型**</strong></p>
<p>(1) SIMPLE(简单SELECT，不使用UNION或子查询等)</p>
<p>(2) PRIMARY(子查询中最外层查询，查询中若包含任何复杂的子部分，最外层的select被标记为PRIMARY)</p>
<p>(3) UNION(UNION中的第二个或后面的SELECT语句) </p>
<p><strong>三、table</strong></p>
<p>显示这一步所访问数据库中表名称（显示这一行的数据是关于哪张表的），有时不是真实的表名字，可能是简称，例如上面的e，d，也可能是第几步执行的结果的简称</p>
<p><strong>四、type</strong></p>
<p>对表访问方式，表示MySQL在表中找到所需行的方式，又称“访问类型”。</p>
<p>常用的类型有： <strong>ALL、index、range、 ref、eq_ref、const、system、**</strong>NULL（从左到右，性能从差到好）**</p>
<p>ALL：Full Table Scan， MySQL将遍历全表以找到匹配的行</p>
<p>index: Full Index Scan，index与ALL区别为index类型只遍历索引树</p>
<p><strong>五、possible_keys</strong></p>
<p><strong>指出MySQL能使用哪个索引在表中找到记录，查询涉及到的字段上若存在索引，则该索引将被列出，但不一定被查询使用</strong></p>
<p><strong>六、Key</strong></p>
<p><strong>key列显示MySQL实际决定使用的键（索引），必然包含在possible_keys中</strong></p>
<p>如果没有选择索引，键是NULL。要想强制MySQL使用或忽视possible_keys列中的索引，在查询中使用FORCE INDEX、USE INDEX或者IGNORE INDEX。</p>
<p><strong>七、key_len</strong></p>
<p><strong>表示索引中使用的字节数，可通过该列计算查询中使用的索引的长度（key_len显示的值为索引字段的最大可能长度，并非实际使用长度，即key_len是根据表定义计算而得，不是通过表内检索出的）</strong></p>
<p>不损失精确性的情况下，长度越短越好 </p>
<p><strong>八、ref</strong></p>
<p><strong>列与索引的比较，表示上述表的连接匹配条件，即哪些列或常量被用于查找索引列上的值</strong></p>
<p><strong>九、rows</strong></p>
<p> <strong>估算出结果集行数，表示MySQL根据表统计信息及索引选用情况，估算的找到所需的记录所需要读取的行数</strong> </p>
<p><strong>十、Extra</strong></p>
<p><strong>该列包含MySQL解决查询的详细信息,有以下几种情况：</strong></p>
<p>Using where: 不用读取表中所有信息，仅通过索引就可以获取所需数据，这发生在对表的全部的请求列都是同一个索引的部分的时候，表示mysql服务器将在存储引擎检索行后再进行过滤</p>
<p>Using temporary：表示MySQL需要使用临时表来存储结果集，常见于排序和分组查询，常见 group by ; order by</p>
<p>Using filesort：当Query中包含 order by 操作，而且无法利用索引完成的排序操作称为“文件排序”</p>
<h3 id="java8新特性"><a href="#java8新特性" class="headerlink" title="java8新特性"></a>java8新特性</h3><p>Java8中有两大最为重要的改变。第一个是 Lambda 表达式；另外一个则是 Stream API(java.util.stream.*)。</p>
<p>Stream 是 Java8 中处理集合的关键抽象概念，它可以指定你希望对集合进行的操作，可以执行非常复杂的查找、过滤和映射数据等操作。使用Stream API 对集合数据进行操作，就类似于使用 SQL 执行的数据库查询。也可以使用 Stream API 来并行执行操作。简而言之，Stream API 提供了一种高效且易于使用的处理数据的方式。</p>
<p>创建stream的方式：</p>
<p><strong>（1）通过Collection系列集合提供的stream()方法或者parallelStream()方法来创建Stream。</strong></p>
<p><strong>（2）通过Arrays中的静态方法stream()获取数组流。</strong></p>
<p><strong>（3）通过Stream类的静态方法of()获取数组流。</strong></p>
<h3 id="指针和引用的区别"><a href="#指针和引用的区别" class="headerlink" title="指针和引用的区别"></a>指针和引用的区别</h3><p>指针：是编程语言中的一个对象，利用地址，它的值直接指向（points to）存在电脑存储器中另一个地方的值。由于通过地址能找到所需的变量单元，可以说，地址指向该变量单元。<br>引用：是某个已知变量 或对象的别名，它不是变量，自身没有值和地址，不占用内存空间。</p>
<ol>
<li><p>一个引用只能对应着一个变量（引用是专一的），一个指针变量却可以存放多个变量或对象的地址（指针是多变的）。<br>前面也说了，引用是一个一个变量或对象的别名。那么比如张三的别名是阿三，如果李四的别名也叫阿三，当有人叫阿三的时候，到底哪个阿三才会回应呢？这就容易引起混淆了。<br>指针是一块空间，里面的内容是可以替换的，指针里存的是a的地址，通过指针你可以访问到a，当有需要了，你把指针的值改为了b，你同意还是可以范文到b。</p>
</li>
<li><p>不存在空值的引用，但存在空值的指针<br>引用是一个已知对象或变量的别名，当这个变量都不存在，哪来的别名。</p>
</li>
<li><p>引用相对于指针更加安全<br>引用在定义的时候就被初始化了，并且引用的对象不可改变。</p>
</li>
</ol>
<h3 id="http1和http2的区别"><a href="#http1和http2的区别" class="headerlink" title="http1和http2的区别"></a>http1和http2的区别</h3><p>主要区别包括</p>
<ol>
<li>HTTP/2采用二进制格式而非文本格式</li>
<li>HTTP/2是完全多路复用的，而非有序并阻塞的——只需一个连接即可实现并行</li>
<li>使用报头压缩，HTTP/2降低了开销</li>
<li>HTTP/2让服务器可以将响应主动“推送”到客户端缓存中</li>
</ol>
<p>HTTP/2的新特性之一是基于流的流量控制。</p>
<p>不同于HTTP/1.1,只要客户端可以处理，服务端就会尽可能快的发送数据，HTTP/2提供了客户端调整传输速度的能力（服务端也可以）。WINDOW_UPDATE帧用来完成这件事情，每个帧告诉对方，发送方想要接收多少字节，它将发送一个WINDOW_UPDATE帧以指示其更新后的处理字节能力。</p>
<p>作用：防止发送方发送数据太多，耗尽接收方资源，从而使接收方来不及处理，造成数据溢出，造成丢包.</p>
<h3 id="进程和线程的区别"><a href="#进程和线程的区别" class="headerlink" title="进程和线程的区别"></a>进程和线程的区别</h3><p>一、关于进程和线程，首先从定义上理解就有所不同</p>
<p>​    1、进程是什么？<br>​      是具有一定独立功能的程序、它是系统进行资源分配和调度的一个独立单位，重点在系统调度和单独的单位，也就是说进程是可以独立运行的一段程序。<br>​    2、线程又是什么？<br>​       线程进程的一个实体，是CPU调度和分派的基本单位，他是比进程更小的能独立运行的基本单位，线程自己基本上不拥有系统资源。<br>​    在运行时，只是暂用一些计数器、寄存器和栈 。</p>
<p>二、他们之间的关系</p>
<p>​    1、一个线程只能属于一个进程，而一个进程可以有多个线程，但至少有一个线程（通常说的主线程）。<br>​    2、资源分配给进程，同一进程的所有线程共享该进程的所有资源。<br>​    3、线程在执行过程中，需要协作同步。不同进程的线程间要利用消息通信的办法实现同步。<br>​    4、处理机分给线程，即真正在处理机上运行的是线程。<br>​    5、线程是指进程内的一个执行单元，也是进程内的可调度实体。  </p>
<p>三、从三个角度来剖析二者之间的区别<br>    1、调度：线程作为调度和分配的基本单位，进程作为拥有资源的基本单位。<br>    2、并发性：不仅进程之间可以并发执行，同一个进程的多个线程之间也可以并发执行。<br>    3、拥有资源：进程是拥有资源的一个独立单位，线程不拥有系统资源，但可以访问隶属于进程的资源。</p>
<h3 id="并发和并行"><a href="#并发和并行" class="headerlink" title="并发和并行"></a>并发和并行</h3><ol>
<li><p>并发当有多个线程在操作时,如果系统只有一个CPU,则它根本不可能真正同时进行一个以上的线程，它只能把CPU运行时间划分成若干个时间段,再将时间 段分配给各个线程执行，在一个时间段的线程代码运行时，其它线程处于挂起状。.这种方式我们称之为并发(Concurrent)。</p>
</li>
<li><p>并行：当系统有一个以上CPU时,则线程的操作有可能非并发。当一个CPU执行一个线程时，另一个CPU可以执行另一个线程，两个线程互不抢占CPU资源，可以同时进行，这种方式我们称之为并行(Parallel)。</p>
</li>
<li><p>区别：</p>
<p>并发和并行是即相似又有区别的两个概念，并行是指两个或者多个事件在同一时刻发生；</p>
<p>而并发是指两个或多个事件在同一时间间隔内发生。</p>
<p>在多道程序环境下，并发性是指在一段时间内宏观上有多个程序在同时运行，但在单处理机系统中，每一时刻却仅能有一道程序执行，故微观上这些程序只能是分时地交替执行。</p>
<p>倘若在计算机系统中有多个处理机，则这些可以并发执行的程序便可被分配到多个处理机上，实现并行执行，即利用每个处理机来处理一个可并发执行的程序，这样，多个程序便可以同时执行。</p>
</li>
</ol>
<h3 id="如何理解泛型，它在底层是如何存储的？"><a href="#如何理解泛型，它在底层是如何存储的？" class="headerlink" title="如何理解泛型，它在底层是如何存储的？"></a>如何理解泛型，它在底层是如何存储的？</h3><p>自JDK 1.5 之后，Java 通过泛型解决了容器类型安全这一问题，而几乎所有人接触泛型也是通过Java的容器。那么泛型究竟是什么？<br><strong>泛型的本质是参数化类型</strong><br>也就是说，泛型就是将所操作的数据类型作为参数的一种语法。</p>
<h4 id="泛型的作用"><a href="#泛型的作用" class="headerlink" title="泛型的作用"></a>泛型的作用</h4><ul>
<li>使用泛型能写出更加灵活通用的代码</li>
<li>泛型将代码安全性检查提前到编译期</li>
<li>泛型能够省去类型强制转换</li>
</ul>
<p>为了实现以上功能，Java 设计者将泛型完全作为了<strong>语法糖</strong>加入了新的语法中，什么意思呢？也就是说泛型对于JVM来说是透明的，有泛型的和没有泛型的代码，通过编译器编译后所生成的二进制代码是完全相同的。</p>
<p>这个语法糖的实现被称为<strong>擦除</strong></p>
<h4 id="擦除的过程"><a href="#擦除的过程" class="headerlink" title="擦除的过程"></a>擦除的过程</h4><p><strong>泛型</strong>是为了将具体的类型作为参数传递给方法，类，接口。<br><strong>擦除</strong>是在代码运行过程中将具体的类型都抹除。</p>
<p>可以看到泛型就是在使用泛型代码的时候，将<strong>类型信息</strong>传递给具体的泛型代码。而经过编译后，生成的<code>.class</code>文件和原始的代码一模一样，就好像传递过来的<strong>类型信息</strong>又被擦除了一样。</p>
<h3 id="java是编译执行还是解释执行？"><a href="#java是编译执行还是解释执行？" class="headerlink" title="java是编译执行还是解释执行？"></a>java是编译执行还是解释执行？</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">解释执行：将编译好的字节码一行一行地翻译为机器码执行。</span><br><span class="line"></span><br><span class="line">（即时编译）编译执行：以方法为单位，将字节码一次性翻译为机器码后执行。</span><br></pre></td></tr></table></figure>

<p>Java采用的是解释和编译混合的模式。它首先通过javac将源码编译成字节码文件class.然后在运行的时候通过解释器或者JIT将字节码转换成最终的机器码。</p>
<p><strong>单独使用解释器的缺点：</strong></p>
<p>抛弃了JIT可能带来的性能优势。如果代码没有被JIT编译的话，再次运行时需要重复解析。</p>
<p><strong>单独使用JIT的缺点：</strong></p>
<p>需要将全部的代码编译成本地机器码。要花更多的时间，JVM启动会变慢非常多；</p>
<p>增加可执行代码的长度（字节码比JIT编译后的机器码小很多），这将导致页面调度，从而降低程序的速度。</p>
<p>有些JIT编译器的优化方式，比如分支预测，如果不进行profiling，往往并不能进行有效优化。</p>
<p>因此，HotSpot采用了惰性评估(Lazy Evaluation)的做法，根据二八定律，消耗大部分系统资源的只有那一小部分的代码（热点代码），而这也就是JIT所需要编译的部分。JVM会根据代码每次被执行的情况收集信息并相应地做出一些优化，因此执行的次数越多，它的速度就越快。</p>
<p>JDK 9引入了一种新的编译模式AOT(Ahead of Time Compilation)，它是直接将字节码编译成机器码，这样就避免了JIT预热等各方面的开销。JDK支持分层编译和AOT协作使用。</p>
<p>注：JIT为方法级，它会缓存编译过的字节码在CodeCache中，而不需要被重复解释。</p>
<p><img src="https://lixiangbetter.github.io/2021/06/01/%E9%9D%A2%E8%AF%95%E9%A2%98%E9%9A%8F%E7%AC%94-21-6-1/aHR0cDovL2hpLmNzZG4ubmV0L2F0dGFjaG1lbnQvMjAxMTA5LzE2LzBfMTMxNjE1OTYwNGRTNjAuZ2lm.gif" alt></p>
<p>Jit工作原理</p>
<p>当JIT编译启用时（默认是启用的），JVM读入.class文件解释后，将其发给JIT编译器。JIT编译器将字节码编译成本机机器代码。 </p>
<p>通常javac将程序源码编译，转换成java字节码，JVM通过解释字节码将其翻译成相应的机器指令，逐条读入，逐条解释翻译。非常显然，经过解释运行，其运行速度必定会比可运行的二进制字节码程序慢。为了提高运行速度，引入了JIT技术。 </p>
<p>在执行时JIT会把翻译过的机器码保存起来，已备下次使用，因此从理论上来说，採用该JIT技术能够，能够接近曾经纯编译技术。 </p>
<h3 id="TCP为什么是四次挥手，而不是三次？"><a href="#TCP为什么是四次挥手，而不是三次？" class="headerlink" title="TCP为什么是四次挥手，而不是三次？"></a>TCP为什么是四次挥手，而不是三次？</h3><p>因为TCP是全双工通信的</p>
<p>   （1）第一次挥手</p>
<p>​     因此当主动方发送断开连接的请求（即FIN报文）给被动方时，仅仅代表主动方不会再发送数据报文了，但主动方仍可以接收数据报文。</p>
<p>​    （2）第二次挥手</p>
<p>​     被动方此时有可能还有相应的数据报文需要发送，因此需要先发送ACK报文，告知主动方“我知道你想断开连接的请求了”。这样主动方便不会因为没有收到应答而继续发送断开连接的请求（即FIN报文）。</p>
<p>   （3）第三次挥手</p>
<p>​    被动方在处理完数据报文后，便发送给主动方FIN报文；这样可以保证数据通信正常可靠地完成。发送完FIN报文后，被动方进入LAST_ACK阶段（超时等待）。</p>
<p>   （4）第四挥手</p>
<p>​    如果主动方及时发送ACK报文进行连接中断的确认，这时被动方就直接释放连接，进入可用状态。</p>
<p>说法二：</p>
<p>因为TCP有个半关闭状态，假设A.B要释放连接，那么A发送一个释放连接报文给B，B收到后发送确认，这个时候A不发数据，但是B如果发数据A还是要接受，这叫半关闭。然后B还要发给A连接释放报文，然后A发确认，所以是4次。</p>
<h3 id="为什么是三次握手？"><a href="#为什么是三次握手？" class="headerlink" title="为什么是三次握手？"></a>为什么是三次握手？</h3><h4 id="一-资源浪费观点"><a href="#一-资源浪费观点" class="headerlink" title="一.资源浪费观点"></a>一.资源浪费观点</h4><p>引自《计算机网络》释疑与习题解答 谢希仁</p>
<p>如果只有两次握手，那么当客户端的SYN连接请求在网络中阻塞，导致客户端没有接收到ACK报文，就会重新发送SYN，由于没有第三次握手，服务器不清楚客户端是否收到了自己发送的建立连接的ACK确认信号，所以每收到一个SYN就只能主动建立一个连接，这会造成什么情况呢？如果客户端的SYN阻塞了，重复发送多次SYN报文，那么服务器在收到请求后就会建立多个冗余的无效链接，造成不必要的资源浪费。<br>即两次握手会造成消息滞留情况下，服务器重复接受无用的连接请求SYN报文，而造成重复分配资源。</p>
<h4 id="三、初始序列号"><a href="#三、初始序列号" class="headerlink" title="三、初始序列号"></a>三、初始序列号</h4><p>三次握手的本质是为了<strong>同步双方的初始序列号</strong>：</p>
<p>为了实现可靠数据传输，TCP 协议的通信双方，都必须维护一个序列号， 以标识发送出去的数据包中，哪些是已经被对方收到的。</p>
<p>三次握手的过程即是通信双方相互告知序列号起始值，并确认对方已经收到了序列号起始值的必经步骤。如果只是两次握手，至多只有连接发起方的起始序列号能被确认， 另一方选择的序列号则得不到确认。TCP建立连接的握手，实质上就是建立一个双向的可靠通信连接，一边一个来回，每一边都自带超时重传来确保可靠性（而不是靠握手的次数）。 </p>
<p>握手的作用，旨在确定两个双向的初始序列号，TCP用序列号来编址传输的字节，由于是两个方向的连接，所以需要两个序列号。</p>
<h3 id="如何理解代理，代理的作用？"><a href="#如何理解代理，代理的作用？" class="headerlink" title="如何理解代理，代理的作用？"></a>如何理解代理，代理的作用？</h3><p>链接：<a href="https://blog.csdn.net/liguangix/article/details/80858807" target="_blank" rel="noopener">https://blog.csdn.net/liguangix/article/details/80858807</a></p>
<h4 id="静态代理"><a href="#静态代理" class="headerlink" title="静态代理"></a>静态代理</h4><p><strong>1、静态代理</strong></p>
<p>静态代理：由程序员创建或特定工具自动生成源代码，也就是在编译时就已经将接口，被代理类，代理类等确定下来。在程序运行之前，代理类的.class文件就已经生成。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 创建Person接口</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> Gonjan</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">Person</span> </span>&#123;</span><br><span class="line">    <span class="comment">//上交班费</span></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">giveMoney</span><span class="params">()</span></span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>Student类实现Person接口。Student可以具体实施上交班费的动作。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Student</span> <span class="keyword">implements</span> <span class="title">Person</span> </span>&#123;</span><br><span class="line">    <span class="keyword">private</span> String name;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Student</span><span class="params">(String name)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.name = name;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">giveMoney</span><span class="params">()</span> </span>&#123;</span><br><span class="line">       System.out.println(name + <span class="string">"上交班费50元"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>StudentsProxy类，这个类也实现了Person接口，但是还另外持有一个学生类对象，由于实现了Peson接口，同时持有一个学生对象，那么他可以代理学生类对象执行上交班费（执行giveMoney()方法）行为。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * 学生代理类，也实现了Person接口，保存一个学生实体，这样既可以代理学生产生行为</span></span><br><span class="line"><span class="comment"> * <span class="doctag">@author</span> Gonjan</span></span><br><span class="line"><span class="comment"> *</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">StudentsProxy</span> <span class="keyword">implements</span> <span class="title">Person</span></span>&#123;</span><br><span class="line">    <span class="comment">//被代理的学生</span></span><br><span class="line">    Student stu;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">StudentsProxy</span><span class="params">(Person stu)</span> </span>&#123;</span><br><span class="line">        <span class="comment">// 只代理学生对象</span></span><br><span class="line">        <span class="keyword">if</span>(stu.getClass() == Student<span class="class">.<span class="keyword">class</span>) </span>&#123;</span><br><span class="line">            <span class="keyword">this</span>.stu = (Student)stu;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//代理上交班费，调用被代理学生的上交班费行为</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">giveMoney</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        stu.giveMoney();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>如何使用代理模式：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">StaticProxyTest</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        <span class="comment">//被代理的学生张三，他的班费上交有代理对象monitor（班长）完成</span></span><br><span class="line">        Person zhangsan = <span class="keyword">new</span> Student(<span class="string">"张三"</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//生成代理对象，并将张三传给代理对象</span></span><br><span class="line">        Person monitor = <span class="keyword">new</span> StudentsProxy(zhangsan);</span><br><span class="line"></span><br><span class="line">        <span class="comment">//班长代理上交班费</span></span><br><span class="line">        monitor.giveMoney();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><strong>2、动态代理</strong></p>
<p>代理类在程序运行时创建的代理方式被成为动态代理。 我们上面静态代理的例子中，代理类(studentProxy)是自己定义好的，在程序运行之前就已经编译完成。然而动态代理，代理类并不是在Java代码中定义的，而是在运行时根据我们在Java代码中的“指示”动态生成的。相比于静态代理， 动态代理的优势在于可以很方便的对代理类的函数进行统一的处理，而不用修改每个代理类中的方法。 比如说，想要在每个代理的方法前都加上一个处理方法：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">giveMoney</span><span class="params">()</span> </span>&#123;</span><br><span class="line">   <span class="comment">//调用被代理方法前加入处理方法</span></span><br><span class="line">   beforeMethod();</span><br><span class="line">   stu.giveMoney();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ul>
<li>创建一个InvocationHandler对象</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//创建一个与代理对象相关联的InvocationHandler</span></span><br><span class="line"> InvocationHandler stuHandler = <span class="keyword">new</span> MyInvocationHandler&lt;Person&gt;(stu);</span><br></pre></td></tr></table></figure>

<ul>
<li>使用Proxy类的getProxyClass静态方法生成一个动态代理类stuProxyClass</li>
</ul>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line">Class&lt;?&gt; stuProxyClass = Proxy.getProxyClass(Person.class.getClassLoader(), new Class&lt;?&gt;[] &#123;Person.class&#125;);</span><br></pre></td></tr></table></figure>

<ul>
<li>获得stuProxyClass 中一个带InvocationHandler参数的构造器constructor</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Constructor&lt;?&gt; constructor = PersonProxy.getConstructor(InvocationHandler<span class="class">.<span class="keyword">class</span>)</span>;</span><br></pre></td></tr></table></figure>

<ul>
<li>通过构造器constructor来创建一个动态实例stuProxy</li>
</ul>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Person stuProxy = (Person) cons.newInstance(stuHandler);</span><br></pre></td></tr></table></figure>

<p>就此，一个动态代理对象就创建完毕，当然，上面四个步骤可以通过Proxy类的newProxyInstances方法来简化：</p>
<figure class="highlight"><table><tr><td class="code"><pre><span class="line"><span class="comment">//创建一个与代理对象相关联的InvocationHandler</span></span><br><span class="line">InvocationHandler stuHandler = <span class="keyword">new</span> MyInvocationHandler&lt;Person&gt;(stu);</span><br><span class="line"><span class="comment">//创建一个代理对象stuProxy，代理对象的每个执行方法都会替换执行Invocation中的invoke方法</span></span><br><span class="line">Person stuProxy= (Person) Proxy.newProxyInstance(Person.class.getClassLoader(), new Class&lt;?&gt;[]&#123;Person.class&#125;, stuHandler)</span><br></pre></td></tr></table></figure>

<h2 id="小红书"><a href="#小红书" class="headerlink" title="小红书"></a>小红书</h2><h3 id="为什么索引要最左匹配原则？"><a href="#为什么索引要最左匹配原则？" class="headerlink" title="为什么索引要最左匹配原则？"></a>为什么索引要最左匹配原则？</h3><p>我们都知道索引的底层是一颗B+树，那么联合索引当然还是一颗B+树，只不过联合索引的健值数量不是一个，而是多个。构建一颗B+树只能根据一个值来构建，因此数据库依据联合索引最左的字段来构建B+树。<br>例子：假如创建一个（a,b)的联合索引，那么它的索引树是这样的</p>
<p><img src="https://lixiangbetter.github.io/2021/06/01/%E9%9D%A2%E8%AF%95%E9%A2%98%E9%9A%8F%E7%AC%94-21-6-1/1281680-20190117145740508-758737271.png" alt="img"><br>可以看到a的值是有顺序的，1，1，2，2，3，3，而b的值是没有顺序的1，2，1，4，1，2。所以b = 2这种查询条件没有办法利用索引，因为联合索引首先是按a排序的，b是无序的。</p>
<p>同时我们还可以发现在a值相等的情况下，b值又是按顺序排列的，但是这种顺序是相对的。所以最左匹配原则遇上范围查询就会停止，剩下的字段都无法使用索引。例如a = 1 and b = 2 a,b字段都可以使用索引，因为在a值确定的情况下b是相对有序的，而a&gt;1and b=2，a字段可以匹配上索引，但b值不可以，因为a的值是一个范围，在这个范围中b是无序的。</p>
<h3 id="tcp滑动窗口"><a href="#tcp滑动窗口" class="headerlink" title="tcp滑动窗口"></a>tcp滑动窗口</h3><p>首先明确：<br>1)TCP滑动窗口分为接受窗口，发送窗口</p>
<p>滑动窗口协议是<strong>传输层进行流控</strong>的一种措施，<strong>接收方通过通告发送方自己的窗口大小</strong>，从而控制发送方的发送速度，从而达到防止发送方发送速度过快而导致自己被淹没的目的。</p>
<p>对ACK的再认识，ack通常被理解为收到数据后给出的一个确认ACK，ACK包含两个非常重要的信息：</p>
<ol>
<li><p>一是期望接收到的下一字节的序号n，该n代表接收方已经接收到了前n-1字节数据，此时如果接收方收到第n+1字节数据而不是第n字节数据，接收方是不会发送序号为n+2的ACK的。</p>
<p>举个例子，假如接收端收到1-1024字节，它会发送一个确认号为1025的ACK,但是接下来收到的是2049-3072，它是不会发送确认号为3072的ACK,而依旧发送1025的ACK。</p>
</li>
<li><p>二是当前的窗口大小m，如此发送方在接收到ACK包含的这两个数据后就可以计算出还可以发送多少字节的数据给对方，假定当前发送方已发送到第x字节，则可以发送的字节数就是y=m-(x-n).这就是滑动窗口控制流量的基本原理。</p>
</li>
</ol>
<h3 id="spring事务是怎么设计的？"><a href="#spring事务是怎么设计的？" class="headerlink" title="spring事务是怎么设计的？"></a>spring事务是怎么设计的？</h3><p><strong>SingleThreadConnectionHolder</strong></p>
<p><img src="https://lixiangbetter.github.io/2021/06/01/%E9%9D%A2%E8%AF%95%E9%A2%98%E9%9A%8F%E7%AC%94-21-6-1/4943997-fdec898f986f367d.png" alt></p>
<p>本来线程不安全的，通过ThreadLocal这么封装一下，立刻就变成了线程的局部变量，不仅仅安全了，还保证了一个线程下面的操作拿到的Connection是同一个对象！这种思想，确实非常巧妙，这也是无锁编程思想的一种方式！</p>
<p><strong>一个线程中的一个事务的多个操作，使用的是同一个Connection！</strong></p>
<h3 id="b-数–相关面试题"><a href="#b-数–相关面试题" class="headerlink" title="b+数–相关面试题"></a>b+数–相关面试题</h3><p><strong>问题1：MySQL中存储索引用到的数据结构是B+树，B+树的查询时间跟树的高度有关，是log(n)，如果用hash存储，那么查询时间是O(1)。既然hash比B+树更快，为什么mysql用B+树来存储索引呢？</strong></p>
<p><strong>答：</strong>一、从内存角度上说，数据库中的索引一般时在磁盘上，数据量大的情况可能无法一次性装入内存，B+树的设计可以允许数据分批加载。</p>
<p>二、从业务场景上说，如果只选择一个数据那确实是hash更快，但是数据库中经常会选中多条这时候由于B+树索引有序，并且又有链表相连，它的查询效率比hash就快很多了。</p>
<p><strong>问题2：为什么不用红黑树或者二叉排序树？</strong></p>
<p><strong>答：</strong>树的查询时间跟树的高度有关，B+树是一棵多路搜索树可以降低树的高度，提高查找效率</p>
<p><strong>问题3：既然增加树的路数可以降低树的高度，那么无限增加树的路数是不是可以有最优的查找效率？</strong></p>
<p><strong>答：</strong>这样会形成一个有序数组，文件系统和数据库的索引都是存在硬盘上的，并且如果数据量大的话，不一定能一次性加载到内存中。有序数组没法一次性加载进内存，这时候B+树的多路存储威力就出来了，可以每次加载B+树的一个结点，然后一步步往下找，</p>
<p><strong>问题4：在内存中，红黑树比B树更优，但是涉及到磁盘操作B树就更优了，那么你能讲讲B+树吗？</strong></p>
<p>B+树是在B树的基础上进行改造，它的数据都在叶子结点，同时叶子结点之间还加了指针形成链表。</p>
<p>下面是一个4路B+树，它的数据都在叶子结点，并且有链表相连。</p>
<p><strong>问题5：为什么B+树要这样设计？</strong></p>
<p><strong>答：</strong>这个跟它的使用场景有关，B+树在数据库的索引中用得比较多，数据库中select数据，不一定只选一条，很多时候会选中多条，比如按照id进行排序后选100条。如果是多条的话，B树需要做局部的中序遍历，可能要跨层访问。而B+树由于所有数据都在叶子结点不用跨层，同时由于有链表结构，只需要找到首尾，通过链表就能把所有数据取出来了。</p>
<p>比如选出7到19只需要在叶子结点中就能找到。</p>
<h2 id="携程"><a href="#携程" class="headerlink" title="携程"></a>携程</h2><h3 id="java的内存模型"><a href="#java的内存模型" class="headerlink" title="java的内存模型"></a>java的内存模型</h3><p><strong>线程通信</strong>：</p>
<p>线程的通信是指线程之间以何种机制来交换信息。在命令式编程中，线程之间的通信机制有两种共享内存和消息传递。</p>
<p>在共享内存的并发模型里，线程之间共享程序的公共状态，线程之间通过写-读内存中的公共状态来隐式进行通信，典型的<strong>共享内存</strong>通信方式就是通过共享对象进行通信。</p>
<p>在消息传递的并发模型里，线程之间没有公共状态，线程之间必须通过<strong>明确的发送消息</strong>来显式进行通信，在java中典型的消息传递方式就是wait()和notify()。</p>
<p><strong>java内存模型（JMM）</strong></p>
<p>Java线程之间的通信采用的是过共享内存模型，这里提到的共享内存模型指的就是Java内存模型(简称JMM)，JMM决定一个线程对共享变量的写入何时对另一个线程可见。从抽象的角度来看，JMM定义了线程和主内存之间的抽象关系：线程之间的共享变量存储在主内存（main memory）中，每个线程都有一个私有的本地内存（local memory），本地内存中存储了该线程以读/写共享变量的副本。</p>
<p><img src="https://lixiangbetter.github.io/2021/06/01/%E9%9D%A2%E8%AF%95%E9%A2%98%E9%9A%8F%E7%AC%94-21-6-1/aHR0cDovL2ltZy5ibG9nLmNzZG4ubmV0LzIwMTYwOTIxMTgyMzM3OTA0.png" alt></p>
<h3 id="volatile关键字不支持原子性操作"><a href="#volatile关键字不支持原子性操作" class="headerlink" title="volatile关键字不支持原子性操作"></a>volatile关键字不支持原子性操作</h3><h3 id="AtomicInteger底层原理是cas"><a href="#AtomicInteger底层原理是cas" class="headerlink" title="AtomicInteger底层原理是cas"></a>AtomicInteger底层原理是cas</h3><h3 id="producer生产者保证安全，什么情况下会丢失数据，什么情况下会多数据？"><a href="#producer生产者保证安全，什么情况下会丢失数据，什么情况下会多数据？" class="headerlink" title="producer生产者保证安全，什么情况下会丢失数据，什么情况下会多数据？"></a>producer生产者保证安全，什么情况下会丢失数据，什么情况下会多数据？</h3><h3 id="spark-streaming消费kafka的两种方式"><a href="#spark-streaming消费kafka的两种方式" class="headerlink" title="spark streaming消费kafka的两种方式"></a>spark streaming消费kafka的两种方式</h3><ol>
<li>Receiver是使用高层次的consumer Api来实现的。</li>
</ol>
<p>receiver 接收的消息都是存储在spark Executor中的，然后spark启动job去处理那些消息;</p>
<p>然而，默认情况下，这种方式会因为底层的失败丢失数据。</p>
<p>如果要启用高可靠机制，让数据零丢失，就必须启用spark streaming的预写日志机制，（Write Ahead Log，WAL）。</p>
<p>该机制会同步的将kafka数据写入到分布式文件系统（如hdfs上）上的预写日志中。所以，即使底层节点出现问题，也可以</p>
<p>使用预写日志中的数据进行恢复，但是效率会下降。</p>
<ol start="2">
<li>direct这种方式会周期性的查询kafka，来获取每个topic+partition的每个offset,从而定义每个batch的offset的范围。</li>
</ol>
<p>当处理数据job启动时，就会使用简单的api来获取指定的offset范围的数据。</p>
<p>direct方式的优点：</p>
<p>1）简化并行读取</p>
<p>如果要读取多个partition,不需要创建多个输入Dstream然后对他们进行union操作，spark会创建跟kafka partition一样多的rdd partition,并行的从kafka读取数据。所以在kafka partition跟rdd partition之间，有一个一对一的映射。</p>
<p>2）高性能</p>
<p>如果要保证零数据丢失，在基于receiver的方式中需要开启WAL机制，这种方式效率低下，因为要保存两份数据，kafka本身就有高可靠的机制，会对数据复制一份，而这里又会复制一份到WAL中。而direct方式只要kafka中复制一份，就可以通过kafka的副本进行恢复。</p>
<p>3）一次且仅一次的事务机制</p>
<p>基于receiver的方式，是使用高层次api在zk中保存消费过的offset这是传统的消费方式。这种方式配合WAL实现数据零丢失，但是却无法保证数据消费仅一次，可能会处理两次。因为spark和zk之间可能是不同步的。direct方式自己追踪消费的offset,并保存在checkpoint中，spark自己一定是同步的，消息仅消费一次。</p>
<h3 id="spark-checkpoint"><a href="#spark-checkpoint" class="headerlink" title="spark checkpoint"></a>spark checkpoint</h3><ol>
<li>引入checkpoint机制原因</li>
</ol>
<p>Spark 在生产环境下经常会面临 Transformation 的 RDD 非常多(例如一个Job 中包含1万个RDD) 或者是具体的 Transformation 产生的 RDD 本身计算特别复杂和耗时(例如计算时常超过1个小时) , 这个时候如果可以对计算的过程进行复用，就可以极大的提升效率，此时我们必需考虑对计算结果的持久化。<br>如果采用 persists 把数据持久化在内存中的话，虽然最快速但是也是最不可靠的（内存清理）；如果放在磁盘上也不是完全可靠的，例如磁盘会损坏，系统管理员可能会清空磁盘。<br>Checkpoint 的产生就是为了相对而言更加可靠的持久化数据，在 Checkpoint 可以指定把数据放在本地并且是多副本的方式，在正常生产环境下通常放在 HDFS 上，借助HDFS 高可靠的特征来实现更可靠的数据持久化。</p>
<ol start="2">
<li>checkpoint在spark的两块应用</li>
</ol>
<p>（1）一块是在spark core中对RDD做checkpoint，将RDD数据保存到可靠存储（如HDFS）以便数据恢复；<br>通过将计算代价较大的 RDD checkpoint 一下，当下游 RDD 计算出错时，可以直接从 checkpoint 过的 RDD 那里读取数据继续算。<br>（2）应用在spark streaming中，使用checkpoint用来保存DStreamGraph以及相关配置信息，以便在Driver崩溃重启的时候能够接着之前进度继续进行处理（如之前waiting batch的job会在重启后继续处理）。</p>
<h3 id="垃圾回收器g1-cms"><a href="#垃圾回收器g1-cms" class="headerlink" title="垃圾回收器g1,cms"></a>垃圾回收器g1,cms</h3><h2 id="涂鸦智能一面："><a href="#涂鸦智能一面：" class="headerlink" title="涂鸦智能一面："></a>涂鸦智能一面：</h2><h3 id="kafka不丢失数据？"><a href="#kafka不丢失数据？" class="headerlink" title="kafka不丢失数据？"></a>kafka不丢失数据？</h3><p>1.生产者数据的不丢失<br>kafka的ack机制：在kafka发送数据的时候，每次发送消息都会有一个确认反馈机制，确保消息正常的能够被收到。</p>
<p>如果是同步模式：ack机制能够保证数据的不丢失，如果ack设置为0，风险很大，一般不建议设置为0</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">producer.type=sync </span><br><span class="line">request.required.acks=1</span><br></pre></td></tr></table></figure>

<p>如果是异步模式：通过buffer来进行控制数据的发送，有两个值来进行控制，时间阈值与消息的数量阈值，如果buffer满了</p>
<p>数据还没有发送出去，如果设置的是立即清理模式，风险很大，一定要设置为阻塞模式.</p>
<p>结论：producer有丢数据的可能，但是可以通过配置保证消息的不丢失</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">producer.type=async </span><br><span class="line">request.required.acks=1 </span><br><span class="line">queue.buffering.max.ms=5000 </span><br><span class="line">queue.buffering.max.messages=10000 </span><br><span class="line">queue.enqueue.timeout.ms = -1 </span><br><span class="line">batch.num.messages=200</span><br></pre></td></tr></table></figure>

<p>2.消费者数据的不丢失</p>
<p>通过offset commit 来保证数据的不丢失，kafka自己记录了每次消费的offset数值，下次继续消费的时候，接着上次的</p>
<p>offset进行消费即可。</p>
<h3 id="kafka不重复消费？（如何保证消息消费时的幂等性）"><a href="#kafka不重复消费？（如何保证消息消费时的幂等性）" class="headerlink" title="kafka不重复消费？（如何保证消息消费时的幂等性）"></a>kafka不重复消费？（如何保证消息消费时的幂等性）</h3><p>幂等性：<strong>其任意多次执行对资源本身所产生的影响均与一次执行的影响相同</strong>。</p>
<p>（1）比如你拿个数据要写库，你先根据主键查一下，如果这数据都有了，你就别插入了，update一下好吧 </p>
<p>（2）比如你是写redis，那没问题了，反正每次都是set，天然幂等性</p>
<p>（3）比如你不是上面两个场景，那做的稍微复杂一点，你需要让生产者发送每条数据的时候，里面加一个全局唯一的id，类似订单id之类的东西，然后你这里消费到了之后，先根据这个id去比如redis里查一下，之前消费过吗？如果没有消费过，你就处理，然后这个id写redis。如果消费过了，那你就别处理了，保证别重复处理相同的消息即可。</p>
<h3 id="Synchronized和ReentrantLock的区别"><a href="#Synchronized和ReentrantLock的区别" class="headerlink" title="Synchronized和ReentrantLock的区别"></a>Synchronized和ReentrantLock的区别</h3><h4 id="Synchronized原理："><a href="#Synchronized原理：" class="headerlink" title="Synchronized原理："></a>Synchronized原理：</h4><p>Synchronized进过编译，会在同步块的前后分别形成monitorenter和monitorexit这个两个字节码指令。在执行monitorenter指令时，首先要尝试获取对象锁。如果这个对象没被锁定，或者当前线程已经拥有了那个对象锁，把锁的计算器加1，相应的，在执行monitorexit指令时会将锁计算器就减1，当计算器为0时，锁就被释放了。如果获取对象锁失败，那当前线程就要阻塞，直到对象锁被另一个线程释放为止。</p>
<h4 id="ReentrantLock"><a href="#ReentrantLock" class="headerlink" title="ReentrantLock"></a>ReentrantLock</h4><p>由于ReentrantLock是java.util.concurrent包下提供的一套互斥锁，相比Synchronized，ReentrantLock类提供了一些高级功能，主要有以下3项：</p>
<pre><code>1.等待可中断，持有锁的线程长期不释放的时候，正在等待的线程可以选择放弃等待，这相当于Synchronized来说可以避免出现死锁的情况。通过lock.lockInterruptibly()来实现这个机制。

2.公平锁，多个线程等待同一个锁时，必须按照申请锁的时间顺序获得锁，Synchronized锁非公平锁，ReentrantLock默认的构造函数是创建的非公平锁，可以通过参数true设为公平锁，但公平锁表现的性能不是很好。</code></pre><p>公平锁、非公平锁的创建方式：</p>
<p>//创建一个非公平锁，默认是非公平锁Lock lock = new ReentrantLock();Lock lock = new ReentrantLock(false); //创建一个公平锁，构造传参trueLock lock = new ReentrantLock(true);</p>
<pre><code>3.锁绑定多个条件，一个ReentrantLock对象可以同时绑定多个对象。ReenTrantLock提供了一个Condition（条件）类，用来实现分组唤醒需要唤醒的线程们，而不是像synchronized要么随机唤醒一个线程要么唤醒全部线程。</code></pre><p>await() 和 signal():等待和唤醒</p>
<h3 id="内存溢出，如何去定位？"><a href="#内存溢出，如何去定位？" class="headerlink" title="内存溢出，如何去定位？"></a>内存溢出，如何去定位？</h3><h3 id="说说项目中你觉得可以体现你能力的点？"><a href="#说说项目中你觉得可以体现你能力的点？" class="headerlink" title="说说项目中你觉得可以体现你能力的点？"></a>说说项目中你觉得可以体现你能力的点？</h3><p>秒杀场景的设计</p>
<h3 id="什么是超卖？"><a href="#什么是超卖？" class="headerlink" title="什么是超卖？"></a>什么是超卖？</h3><p>多线程扣减库存（内存中，秒杀场景）时候，线程安全问题。</p>
<p>链接：<a href="https://blog.csdn.net/u010391342/article/details/84372342" target="_blank" rel="noopener">https://blog.csdn.net/u010391342/article/details/84372342</a></p>
<h2 id="阿里一面"><a href="#阿里一面" class="headerlink" title="阿里一面"></a>阿里一面</h2><h3 id="你的研究方向？"><a href="#你的研究方向？" class="headerlink" title="你的研究方向？"></a>你的研究方向？</h3><h3 id="java为什么可以“一次编译，到处运行”？"><a href="#java为什么可以“一次编译，到处运行”？" class="headerlink" title="java为什么可以“一次编译，到处运行”？"></a>java为什么可以“一次编译，到处运行”？</h3><h3 id="java的数据类型有哪些？"><a href="#java的数据类型有哪些？" class="headerlink" title="java的数据类型有哪些？"></a>java的数据类型有哪些？</h3><h3 id="封装、继承、多态"><a href="#封装、继承、多态" class="headerlink" title="封装、继承、多态"></a>封装、继承、多态</h3><p>链接：<a href="https://www.cnblogs.com/IzuruKamuku/p/14359762.html" target="_blank" rel="noopener">https://www.cnblogs.com/IzuruKamuku/p/14359762.html</a></p>
<h3 id="类加载过程？"><a href="#类加载过程？" class="headerlink" title="类加载过程？"></a>类加载过程？</h3><h3 id="双亲委派机制？"><a href="#双亲委派机制？" class="headerlink" title="双亲委派机制？"></a>双亲委派机制？</h3><h3 id="哪里用到过线程池？"><a href="#哪里用到过线程池？" class="headerlink" title="哪里用到过线程池？"></a>哪里用到过线程池？</h3><p>秒杀场景（队列化泄洪）</p>
<p>用一个newFixedThreadPool（20）</p>
<h3 id="Thread和runnable的区别？"><a href="#Thread和runnable的区别？" class="headerlink" title="Thread和runnable的区别？"></a>Thread和runnable的区别？</h3><h3 id="数据库有哪些索引？索引的结构？为什么最左匹配？"><a href="#数据库有哪些索引？索引的结构？为什么最左匹配？" class="headerlink" title="数据库有哪些索引？索引的结构？为什么最左匹配？"></a>数据库有哪些索引？索引的结构？为什么最左匹配？</h3><h2 id="金山云一面："><a href="#金山云一面：" class="headerlink" title="金山云一面："></a>金山云一面：</h2><h3 id="vim命令相关、linux命令"><a href="#vim命令相关、linux命令" class="headerlink" title="vim命令相关、linux命令"></a>vim命令相关、linux命令</h3><p>链接：<a href="https://lixiangbetter.github.io/2021/06/03/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E9%9D%A2%E8%AF%95%E9%9A%8F%E7%AC%94%E8%AE%B0%E5%BD%95/" target="_blank" rel="noopener">https://lixiangbetter.github.io/2021/06/03/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E9%9D%A2%E8%AF%95%E9%9A%8F%E7%AC%94%E8%AE%B0%E5%BD%95/</a></p>
<h3 id="Hadoop安装中遇到哪些问题"><a href="#Hadoop安装中遇到哪些问题" class="headerlink" title="Hadoop安装中遇到哪些问题"></a>Hadoop安装中遇到哪些问题</h3><p>链接：<a href="https://lixiangbetter.github.io/2021/06/03/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E9%9D%A2%E8%AF%95%E9%9A%8F%E7%AC%94%E8%AE%B0%E5%BD%95/" target="_blank" rel="noopener">https://lixiangbetter.github.io/2021/06/03/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E9%9D%A2%E8%AF%95%E9%9A%8F%E7%AC%94%E8%AE%B0%E5%BD%95/</a></p>
<h3 id="说说项目-大数据项目-怎么做的？"><a href="#说说项目-大数据项目-怎么做的？" class="headerlink" title="说说项目(大数据项目)怎么做的？"></a>说说项目(大数据项目)怎么做的？</h3><p>链接：<a href="https://lixiangbetter.github.io/2021/06/03/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E9%9D%A2%E8%AF%95%E9%9A%8F%E7%AC%94%E8%AE%B0%E5%BD%95/" target="_blank" rel="noopener">https://lixiangbetter.github.io/2021/06/03/%E5%A4%A7%E6%95%B0%E6%8D%AE%E5%BC%80%E5%8F%91%E9%9D%A2%E8%AF%95%E9%9A%8F%E7%AC%94%E8%AE%B0%E5%BD%95/</a></p>
<h2 id="唯品会一面："><a href="#唯品会一面：" class="headerlink" title="唯品会一面："></a>唯品会一面：</h2><h3 id="spark的yarn-client和yarn-cluster方式有什么区别"><a href="#spark的yarn-client和yarn-cluster方式有什么区别" class="headerlink" title="spark的yarn-client和yarn-cluster方式有什么区别"></a>spark的yarn-client和yarn-cluster方式有什么区别</h3><p>链接：<a href="https://lixiangbetter.github.io/2020/07/06/Spark笔记/" target="_blank" rel="noopener">https://lixiangbetter.github.io/2020/07/06/Spark笔记/</a></p>
<h3 id="thread-local的底层"><a href="#thread-local的底层" class="headerlink" title="thread local的底层"></a>thread local的底层</h3><p>非常好的一篇博客：<a href="https://www.cnblogs.com/tuyang1129/p/12713815.html" target="_blank" rel="noopener">https://www.cnblogs.com/tuyang1129/p/12713815.html</a></p>
<h3 id="线程池核心参数解释"><a href="#线程池核心参数解释" class="headerlink" title="线程池核心参数解释"></a>线程池核心参数解释</h3><h3 id="红黑树删除一个节点的过程"><a href="#红黑树删除一个节点的过程" class="headerlink" title="红黑树删除一个节点的过程"></a>红黑树删除一个节点的过程</h3><p>删除一个节点</p>
<h3 id="红黑树的性质"><a href="#红黑树的性质" class="headerlink" title="红黑树的性质"></a>红黑树的性质</h3><h3 id="where和having的区别"><a href="#where和having的区别" class="headerlink" title="where和having的区别"></a>where和having的区别</h3><h3 id="hadoop的shuffle和spark的shuffle的区别"><a href="#hadoop的shuffle和spark的shuffle的区别" class="headerlink" title="hadoop的shuffle和spark的shuffle的区别"></a>hadoop的shuffle和spark的shuffle的区别</h3><h3 id="jvm的g1垃圾回收器"><a href="#jvm的g1垃圾回收器" class="headerlink" title="jvm的g1垃圾回收器"></a>jvm的g1垃圾回收器</h3><h3 id="你平时有用到哪些集合？concurrent-hashmap原理？"><a href="#你平时有用到哪些集合？concurrent-hashmap原理？" class="headerlink" title="你平时有用到哪些集合？concurrent hashmap原理？"></a>你平时有用到哪些集合？concurrent hashmap原理？</h3><p>链接：<a href="https://blog.csdn.net/weixin_30819085/article/details/95117136" target="_blank" rel="noopener">https://blog.csdn.net/weixin_30819085/article/details/95117136</a></p>
<p>链接：<a href="https://blog.csdn.net/qq_41737716/article/details/90549847" target="_blank" rel="noopener">https://blog.csdn.net/qq_41737716/article/details/90549847</a></p>
<h3 id="aop的动态代理具体怎么写的？"><a href="#aop的动态代理具体怎么写的？" class="headerlink" title="aop的动态代理具体怎么写的？"></a>aop的动态代理具体怎么写的？</h3><p>链接：<a href="https://blog.csdn.net/liguangix/article/details/80858807" target="_blank" rel="noopener">https://blog.csdn.net/liguangix/article/details/80858807</a></p>
<h3 id="mysql的事务隔离级别，默认事务隔离级别"><a href="#mysql的事务隔离级别，默认事务隔离级别" class="headerlink" title="mysql的事务隔离级别，默认事务隔离级别"></a>mysql的事务隔离级别，默认事务隔离级别</h3>]]></content>
      <categories>
        <category>面经</category>
      </categories>
      <tags>
        <tag>大数据开发</tag>
      </tags>
  </entry>
  <entry>
    <title>大数据私房菜-21/5/13</title>
    <url>/2021/05/13/%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%A7%81%E6%88%BF%E8%8F%9C-21-5-13/</url>
    <content><![CDATA[<h2 id="大数据私房菜"><a href="#大数据私房菜" class="headerlink" title="大数据私房菜"></a>大数据私房菜</h2><h3 id="Kafka-api-low-level和high-level有什么区别"><a href="#Kafka-api-low-level和high-level有什么区别" class="headerlink" title="Kafka api low-level和high-level有什么区别"></a>Kafka api low-level和high-level有什么区别</h3><p>总的区别就是，high level 和 low level 的区别就是 high level屏蔽了一些细节，比如往哪个partition发，offset咋存往哪儿存，说白了只把kafka作为纯fifo使用，low level的话，自己可以自定义往哪个partition上发消息，包括存offset的处理都可以自己搞，去自定义。</p>
]]></content>
      <categories>
        <category>bigdata</category>
      </categories>
      <tags>
        <tag>大数据私房菜</tag>
      </tags>
  </entry>
  <entry>
    <title>秒杀项目回顾</title>
    <url>/2021/04/29/%E7%A7%92%E6%9D%80%E9%A1%B9%E7%9B%AE%E5%9B%9E%E9%A1%BE/</url>
    <content><![CDATA[<h3 id="默认内嵌tomcat配置"><a href="#默认内嵌tomcat配置" class="headerlink" title="默认内嵌tomcat配置"></a>默认内嵌tomcat配置</h3><ul>
<li>等待队列长度，默认100</li>
<li>最大可被连接数，默认10000</li>
<li>最大工作线程数，默认200</li>
<li>最小工作线程数，默认10</li>
</ul>
<h3 id="定制化内嵌tomcat开发"><a href="#定制化内嵌tomcat开发" class="headerlink" title="定制化内嵌tomcat开发"></a>定制化内嵌tomcat开发</h3><ul>
<li>keepAliveTimeOut: 多少毫秒后不响应的断开keepalive</li>
<li>MaxKeepAliveRequests: 多少次请求后keep alive断开失效</li>
</ul>
<h3 id="单web容器上限"><a href="#单web容器上限" class="headerlink" title="单web容器上限"></a>单web容器上限</h3><ul>
<li>4核cpu 8g内存单进程调度线程数800-1000以上后即花费巨大的时间在cpu调度上</li>
<li>等待队列长度</li>
</ul>
<h3 id="mysql数据库ops容量问题"><a href="#mysql数据库ops容量问题" class="headerlink" title="mysql数据库ops容量问题"></a>mysql数据库ops容量问题</h3><p>主键：千万级别数据 = 1-10毫秒</p>
<p>唯一索引：千万级别数据 = 10-100毫秒</p>
<p>非唯一索引：千万级别数据 = 100-1000毫秒</p>
<p>无索引：百万条数据 = 1000毫秒+</p>
<h3 id="mysql数据库tps容量问题"><a href="#mysql数据库tps容量问题" class="headerlink" title="mysql数据库tps容量问题"></a>mysql数据库tps容量问题</h3><p>非插入更新删除操作：同查询</p>
<p>插入操作：1w~10w tps（依赖配置优化，后续讲解）</p>
<h3 id="nginx高性能原因"><a href="#nginx高性能原因" class="headerlink" title="nginx高性能原因"></a>nginx高性能原因</h3><p>Epoll多路复用</p>
<p>Master worker进程模型</p>
<p>协程机制</p>
<ul>
<li>依附于线程的内存模型，切换开销小</li>
<li>遇阻塞及归还执行权，代码同步</li>
<li>无需加锁</li>
</ul>
]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>项目回顾</tag>
      </tags>
  </entry>
  <entry>
    <title>大数据私房菜-21/4/27</title>
    <url>/2021/04/27/%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%A7%81%E6%88%BF%E8%8F%9C-21-4-27/</url>
    <content><![CDATA[<h2 id="大数据私房菜"><a href="#大数据私房菜" class="headerlink" title="大数据私房菜"></a>大数据私房菜</h2><h3 id="Kafka为什么吞吐量大、速度快？"><a href="#Kafka为什么吞吐量大、速度快？" class="headerlink" title="Kafka为什么吞吐量大、速度快？"></a>Kafka为什么吞吐量大、速度快？</h3><p>一、顺序读写</p>
<p>Kafka就是使用了磁盘顺序读写来提升的性能。Kafka的message是不断追加到本地磁盘文件末尾的，而不是随机的写入，这使得Kafka写入吞吐量得到了显著提升 。</p>
<p>这种方法有一个缺陷—— 没有办法删除数据 ，所以Kafka是不会删除数据的，它会把所有的数据都保留下来，每个消费者（Consumer）对每个Topic都有一个offset用来表示 读取到了第几条数据 。</p>
<p>二、Page Cache</p>
<p>为了优化读写性能，Kafka利用了操作系统本身的Page Cache，就是利用操作系统自身的内存而不是JVM空间内存。这样做的好处有：</p>
<p>1避免Object消耗：如果是使用 Java 堆，Java对象的内存消耗比较大，通常是所存储数据的两倍甚至更多。</p>
<p>2避免GC问题：随着JVM中数据不断增多，垃圾回收将会变得复杂与缓慢，使用系统缓存就不会存在GC问题</p>
<p>相比于使用JVM或in-memory cache等数据结构，利用操作系统的Page Cache更加简单可靠。首先，操作系统层面的缓存利用率会更高，因为存储的都是紧凑的字节结构而不是独立的对象。其次，操作系统本身也对于Page Cache做了大量优化，提供了 write-behind、read-ahead以及flush等多种机制。再者，即使服务进程重启，系统缓存依然不会消失，避免了in-process cache重建缓存的过程。</p>
<p>通过操作系统的Page Cache，Kafka的读写操作基本上是基于内存的，读写速度得到了极大的提升。</p>
<p>三、零拷贝</p>
<p>通过这种 “零拷贝” 的机制，Page Cache 结合 sendfile 方法，Kafka消费端的性能也大幅提升。这也是为什么有时候消费端在不断消费数据时，我们并没有看到磁盘io比较高，此刻正是操作系统缓存在提供数据。</p>
<p>当Kafka客户端从服务器读取数据时，如果不使用零拷贝技术，那么大致需要经历这样的一个过程：</p>
<p>1.操作系统将数据从磁盘上读入到内核空间的读缓冲区中。</p>
<p>2.应用程序（也就是Kafka）从内核空间的读缓冲区将数据拷贝到用户空间的缓冲区中。</p>
<p>3.应用程序将数据从用户空间的缓冲区再写回到内核空间的socket缓冲区中。</p>
<p>4.操作系统将socket缓冲区中的数据拷贝到NIC缓冲区中，然后通过网络发送给客户端。</p>
<h3 id="kafka与传统的消息队列有什么不同"><a href="#kafka与传统的消息队列有什么不同" class="headerlink" title="kafka与传统的消息队列有什么不同"></a>kafka与传统的消息队列有什么不同</h3><p>1.首先kafka会将接收到的消息分区（partition），每个主题（topic）的消息有不同的分区，这样一方面消息的存储就不会受到单一服务器存储空间大小的限制，另一方面消息的处理也可以在多个服务器上并行。</p>
<p> 2.其次为了保证高可用，每个分区都会有一定数量的副本(replica)。这样如果有部分服务器不可用，副本所在的服务器就会接替上来，保证应用的持续性。</p>
<p> 3.然后保证分区内部消息的消费有序性。</p>
<p> 4.Kafka还具有consumer group的概念，每个分区只能被同一个group的一个consumer消费，但可以被多个group消费。</p>
<h3 id="hadoop的ha"><a href="#hadoop的ha" class="headerlink" title="hadoop的ha"></a>hadoop的ha</h3><p>Active NameNode 和 Standby NameNode：两台 NameNode 形成互备，一台处于 Active 状态，为主 NameNode，另外一台处于 Standby 状态，为备 NameNode，只有主 NameNode 才能对外提供读写服务。</p>
<p>主备切换控制器 ZKFailoverController：ZKFailoverController 作为独立的进程运行，对 NameNode 的主备切换进行总体控制。ZKFailoverController 能及时检测到 NameNode 的健康状况，在主 NameNode 故障时借助 Zookeeper 实现自动的主备选举和切换，当然 NameNode 目前也支持不依赖于 Zookeeper 的手动主备切换。</p>
<p>Zookeeper 集群：为主备切换控制器提供主备选举支持。</p>
<p>共享存储系统：共享存储系统是实现 NameNode 的高可用最为关键的部分，共享存储系统保存了 NameNode 在运行过程中所产生的 HDFS 的元数据。主 NameNode 和备NameNode 通过共享存储系统实现元数据同步。在进行主备切换的时候，新的主 NameNode 在确认元数据完全同步之后才能继续对外提供服务。</p>
<p>DataNode 节点：除了通过共享存储系统共享 HDFS 的元数据信息之外，主 NameNode 和备 NameNode 还需要共享 HDFS 的数据块和 DataNode 之间的映射关系。DataNode 会同时向主 NameNode 和备 NameNode 上报数据块的位置信息。</p>
<h3 id="HBase查询为什么这么快？"><a href="#HBase查询为什么这么快？" class="headerlink" title="HBase查询为什么这么快？"></a>HBase查询为什么这么快？</h3><p>首先数据量很大的时候，HBase会拆分成多个Region分配到多台RegionServer.<br>客户端通过meta信息定位到某台RegionServer（也可能是多台）,<br>通过Rowkey定位Region，这当中会先经过BlockCache，这边找不到的话，再经过MemStore和Hfile查询，这当中通过布隆过滤器过滤掉一些不需要查询的HFile。</p>
<p><strong>使用场景</strong></p>
<ul>
<li>单表数据量超千万，而且并发还挺高。</li>
<li>数据分析需求较弱，或者不需要那么灵活或者实时</li>
</ul>
<h3 id="用户登录表A，字段：user-id-device-id-login-date-求用户最大连续登录天数"><a href="#用户登录表A，字段：user-id-device-id-login-date-求用户最大连续登录天数" class="headerlink" title="用户登录表A，字段：user_id,device_id,login_date,求用户最大连续登录天数"></a>用户登录表A，字段：user_id,device_id,login_date,求用户最大连续登录天数</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span></span><br><span class="line">    t3.user_id,</span><br><span class="line">    <span class="keyword">max</span>(day_count) max_day</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">(</span><br><span class="line">    <span class="keyword">select</span></span><br><span class="line">        t2.user_id,</span><br><span class="line">        <span class="keyword">count</span>(date_diff) <span class="keyword">as</span> day_count</span><br><span class="line">    <span class="keyword">from</span></span><br><span class="line">    (</span><br><span class="line">        <span class="keyword">select</span></span><br><span class="line">            t1.user_id,</span><br><span class="line">            t1.device_id,</span><br><span class="line">            <span class="keyword">date_sub</span>(login_date,<span class="keyword">rank</span>) date_dif</span><br><span class="line">        <span class="keyword">from</span></span><br><span class="line">        (</span><br><span class="line">            <span class="keyword">select</span></span><br><span class="line">                A.user_id,</span><br><span class="line">                A.device_id,</span><br><span class="line">                A.login_date,</span><br><span class="line">                <span class="keyword">rank</span>() <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> user_id <span class="keyword">order</span> <span class="keyword">by</span> login_date) <span class="keyword">rank</span></span><br><span class="line">            <span class="keyword">from</span> A</span><br><span class="line">        ) t1</span><br><span class="line">    ) t2</span><br><span class="line">    <span class="keyword">group</span> <span class="keyword">by</span> user_id</span><br><span class="line">) t3</span><br></pre></td></tr></table></figure>

<h2 id="今日面经"><a href="#今日面经" class="headerlink" title="今日面经"></a>今日面经</h2><h3 id="synchronized与volatile的区别"><a href="#synchronized与volatile的区别" class="headerlink" title="synchronized与volatile的区别"></a>synchronized与volatile的区别</h3><ul>
<li>volatile是线程同步的轻量级实现，因此volatile性能好于synchronized</li>
<li>voaltile修饰变量，synchronized修饰方法和代码块</li>
<li>多线程访问volatile不会发生阻塞，但访问synchronized可能会阻塞</li>
<li>volatile可以保证数据的可见性，但不能保证原子性；而synchronized既可以保证原子性，也可以间接保证可见性。</li>
<li>volatile解决的是变量在多个线程之间的可见性，而Synchronized解决的是多个线程之间访问资源的同步性</li>
</ul>
<h3 id="hadoop是什么"><a href="#hadoop是什么" class="headerlink" title="hadoop是什么"></a>hadoop是什么</h3><p>Apache Hadoop软件库是一个框架，它允许使用简单的编程模型跨计算机群集分布式处理大型数据集（海量的数据）。</p>
<p>HADOOP的核心组件有：</p>
<p><strong>HDFS</strong>（分布式文件系统）</p>
<p><strong>YARN</strong>（运算资源调度系统）</p>
<p><strong>MAPREDUCE</strong>（分布式运算编程框架）</p>
<h3 id="ArrayList原理，为什么初始是10，为什么扩容1-5倍"><a href="#ArrayList原理，为什么初始是10，为什么扩容1-5倍" class="headerlink" title="ArrayList原理，为什么初始是10，为什么扩容1.5倍"></a>ArrayList原理，为什么初始是10，为什么扩容1.5倍</h3><p>优点：</p>
<p>ArrayList底层以数组实现，是一种随机访问模式，再加上它实现了RandomAccess接口，因此查找也就是get的时候非常快。<br>ArrayList在顺序添加一个元素的时候非常方便，只是往数组里面添加了一个元素而已。<br>根据下标遍历元素，效率高。</p>
<p>通过上述：我们大概可知当add一个元素时候的扩容流程。</p>
<p>添加一个元素，首先计算当前的list所需最小的容量大小，是否需要扩容等。当需要扩容时：</p>
<p>1.得到当前的ArrayList的容量(oldCapacity)。</p>
<p>2.计算除扩容后的新容量(newCapacity)，其值(oldCapacity + (oldCapacity &gt;&gt; 1))约是oldCapacity 的1.5倍。</p>
<p>这里采用的是移位运算(关于移位运算，后续会讲到)。</p>
<p>为什么采用这种方法呢？应该是出于效率的考虑。</p>
<p>3.当newCapacity小于所需最小容量，那么将所需最小容量赋值给newCapacity。</p>
<p>4.newCapacity大于ArrayList的所允许的最大容量,处理。</p>
<p>5.进行数据的复制，完成向ArrayList实例添加元素操作。</p>
<h3 id="b-树和b树"><a href="#b-树和b树" class="headerlink" title="b+树和b树"></a>b+树和b树</h3><ol>
<li><p>B+树中只有叶子节点会带有指向记录的指针（ROWID），而B树则所有节点都带有，在内部节点出现的索引项不会再出现在叶子节点中。</p>
</li>
<li><p>B+树中所有叶子节点都是通过指针连接在一起，而B树不会。</p>
</li>
</ol>
<p>B+树的优点：</p>
<ol>
<li><p>非叶子节点不会带上ROWID，这样，一个块中可以容纳更多的索引项，一是可以降低树的高度。二是一个内部节点可以定位更多的叶子节点。</p>
</li>
<li><p>叶子节点之间通过指针来连接，范围扫描将十分简单，而对于B树来说，则需要在叶子节点和内部节点不停的往返移动。</p>
</li>
</ol>
<h3 id="公平锁和非公平锁"><a href="#公平锁和非公平锁" class="headerlink" title="公平锁和非公平锁"></a>公平锁和非公平锁</h3><p>公平锁：多个线程按照申请锁的顺序去获得锁，线程会直接进入队列去排队，永远都是队列的第一位才能得到锁。</p>
<p>非公平锁：多个线程去获取锁的时候，会直接去尝试获取，获取不到，再去进入等待队列，如果能获取到，就直接获取到锁。</p>
<h3 id="redeuce怎么知道从哪里下载map输出的文件"><a href="#redeuce怎么知道从哪里下载map输出的文件" class="headerlink" title="redeuce怎么知道从哪里下载map输出的文件"></a>redeuce怎么知道从哪里下载map输出的文件</h3><p>Shuffle是MapReduce处理流程中的一个流程,它的每一个处理步骤是分散在各个maptask和reducetask节点上完成的,整体来看,分为3个操作:</p>
<p>\1.分区partition</p>
<p>\2.Sort根据key排序</p>
<p>\3.Combiner进行局部value的合并</p>
<p>详细流程</p>
<ol>
<li>maptask收集map方法输出的kv对,放到内存缓冲区中</li>
<li>从内存缓冲区不断溢出本地磁盘文件,可能会溢出多个文件</li>
<li>多个溢出文件会被合并成大的溢出文件</li>
<li>在溢出过程中,及合并的过程中,都要调用partition进行分组和针对key进行排序</li>
<li>reducetask根据自己的分区号,去各个maptask机器上去响应的结果分区数据</li>
<li>reducetask会取到同一个分区来自不同maptask的结果文件,reducetask会将这些文件再进行合并(归并排序)</li>
<li>合并成大文件后,Shullfe的过程也就结束了,后面进入reducetask的逻辑运算过程(从文件中取出一个一个的键值对组,调用用户自定义的reduce()方法)</li>
</ol>
<p>Shullfe中的缓冲区大小会影响到MapReduce程序的执行效率,原则上说,缓冲区越大,磁盘io的次数越少,执行速度就越快.缓冲区大大小可以通过参数调整,参数:io.sort.mb,默认为100M.</p>
<h3 id="spring-ioc和aop"><a href="#spring-ioc和aop" class="headerlink" title="spring ioc和aop"></a>spring ioc和aop</h3><p><strong>(1)IOC是什么?</strong></p>
<p>注:(Inversion Of Controll 控制反转) 对象之间的依赖关系由容器来建立。</p>
<p><strong>(2)DI是什么?</strong></p>
<p>注:(Dependency Injection 依赖注入) 容器通过调用set方法或者构造器来建立对象之间的 依赖关系。 IOC是目标，DI是手段。</p>
<p><strong>aop</strong></p>
<p>Spring AOP是其实是通过动态代理来实现业务逻辑的插入，是开发者在开发是不用关注其他与业务无关的点，通过代理的方式做到了插拔式操作。</p>
]]></content>
      <categories>
        <category>bigdata</category>
      </categories>
      <tags>
        <tag>大数据私房菜</tag>
      </tags>
  </entry>
  <entry>
    <title>大数据私房菜-21/4/24</title>
    <url>/2021/04/24/%E5%A4%A7%E6%95%B0%E6%8D%AE%E7%A7%81%E6%88%BF%E8%8F%9C-21-4-24/</url>
    <content><![CDATA[<h2 id="字节笔试"><a href="#字节笔试" class="headerlink" title="字节笔试"></a>字节笔试</h2><h3 id="哪些排序是稳定的？（多选）"><a href="#哪些排序是稳定的？（多选）" class="headerlink" title="哪些排序是稳定的？（多选）"></a>哪些排序是稳定的？（多选）</h3><p>冒泡排序，归并排序</p>
<p>不稳定：堆排序，快速排序</p>
<h3 id="http和https？（多选）"><a href="#http和https？（多选）" class="headerlink" title="http和https？（多选）"></a>http和https？（多选）</h3><p>tls</p>
<h3 id="zookeeper（多选）"><a href="#zookeeper（多选）" class="headerlink" title="zookeeper（多选）"></a>zookeeper（多选）</h3><p>高速缓存 命名服务 主节点选举 分布式锁</p>
<h3 id="进程和线程（多选）"><a href="#进程和线程（多选）" class="headerlink" title="进程和线程（多选）"></a>进程和线程（多选）</h3><p>线程有独立的地址空间 错</p>
<p>线程有独立的堆和栈 错 独立栈，共享堆</p>
<h3 id="999个节点的二叉树的高（单选）"><a href="#999个节点的二叉树的高（单选）" class="headerlink" title="999个节点的二叉树的高（单选）"></a>999个节点的二叉树的高（单选）</h3><p>10</p>
<h2 id="大数据私房菜"><a href="#大数据私房菜" class="headerlink" title="大数据私房菜"></a>大数据私房菜</h2><h3 id="大数据求top-n-map-reduce程序"><a href="#大数据求top-n-map-reduce程序" class="headerlink" title="大数据求top n map reduce程序"></a>大数据求top n map reduce程序</h3><p>链接：<a href="https://blog.csdn.net/qq_43193797/article/details/86367610" target="_blank" rel="noopener">https://blog.csdn.net/qq_43193797/article/details/86367610</a></p>
<h3 id="连续三天登陆的用户"><a href="#连续三天登陆的用户" class="headerlink" title="连续三天登陆的用户"></a>连续三天登陆的用户</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span></span><br><span class="line">    t2.user_id <span class="keyword">as</span> user_id,</span><br><span class="line">    <span class="keyword">count</span>(<span class="number">1</span>) <span class="keyword">as</span> times,</span><br><span class="line">    <span class="keyword">min</span>(t2.login_date) <span class="keyword">as</span> start_date,</span><br><span class="line">    <span class="keyword">max</span>(t2.login_date) <span class="keyword">as</span> end_date</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">(</span><br><span class="line">    <span class="keyword">select</span></span><br><span class="line">        t1.user_id,</span><br><span class="line">        t1.login_date,</span><br><span class="line">        <span class="keyword">date_sub</span>(t1.login_date, rn) <span class="keyword">as</span> date_diff</span><br><span class="line">    <span class="keyword">from</span></span><br><span class="line">    (</span><br><span class="line">        <span class="keyword">select</span></span><br><span class="line">            user_id,</span><br><span class="line">            login_date,</span><br><span class="line">            row_number() <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> user_id <span class="keyword">order</span> <span class="keyword">by</span> login_date <span class="keyword">asc</span>) <span class="keyword">as</span> rn</span><br><span class="line">        <span class="keyword">from</span></span><br><span class="line">        wedw_dw.t_login_info</span><br><span class="line">    ) t1</span><br><span class="line">) t2</span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> t2.user_id,t2.date_diff</span><br><span class="line"><span class="keyword">having</span> times &gt;= <span class="number">3</span></span><br></pre></td></tr></table></figure>

<h3 id="广告日志，求广告的曝光数和点击数"><a href="#广告日志，求广告的曝光数和点击数" class="headerlink" title="广告日志，求广告的曝光数和点击数"></a>广告日志，求广告的曝光数和点击数</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">package com.lx.spark</span><br><span class="line"></span><br><span class="line">import org.apache.spark.SparkContext</span><br><span class="line">import org.apache.spark.rdd.RDD</span><br><span class="line">import org.apache.spark.sql.SparkSession</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment"> * @author lx</span></span><br><span class="line"><span class="comment"> * @date 2021/4/26 3:32 下午</span></span><br><span class="line"><span class="comment"> */</span></span><br><span class="line">class AddLog &#123;</span><br><span class="line"></span><br><span class="line">  def main(args: Array[String]): Unit = &#123;</span><br><span class="line">    </span><br><span class="line">    val spark: SparkSession = SparkSession</span><br><span class="line">      .builder()</span><br><span class="line">      .master("local[2]")</span><br><span class="line">      .appName("AddLog")</span><br><span class="line">      .getOrCreate()</span><br><span class="line"></span><br><span class="line">    import spark.implicits._;</span><br><span class="line">    val sc: SparkContext = spark.sparkContext</span><br><span class="line"></span><br><span class="line">    val clickRDD: RDD[String] = sc.textFile("data/click.log")</span><br><span class="line">    val impRDD: RDD[String] = sc.textFile("data/imp.log")</span><br><span class="line"></span><br><span class="line">    val clickRes: RDD[(String, Int)] = clickRDD.map(line =&gt; &#123;</span><br><span class="line">      val arr: Array[String] = line.split("\\s+")</span><br><span class="line">      val adid: String = arr(3).substring(arr(3).lastIndexOf("=") + 1)</span><br><span class="line">      (adid, 1)</span><br><span class="line">    &#125;).reduceByKey(_ + _)</span><br><span class="line"></span><br><span class="line">    val impRes: RDD[(String, Int)] = impRDD.map(line =&gt; &#123;</span><br><span class="line">      val arr: Array[String] = line.split("\\s+")</span><br><span class="line">      val adid: String = arr(3).substring(arr(3).lastIndexOf("=") + 1)</span><br><span class="line">      (adid, 1)</span><br><span class="line">    &#125;).reduceByKey(_ + _)</span><br><span class="line"></span><br><span class="line">    // 保存至hdfs</span><br><span class="line">    clickRes.fullOuterJoin(impRes)</span><br><span class="line">      .map(x =&gt; x._1 + ","+x._2._1.getOrElse(0)+","+ x._2._2.getOrElse(0))</span><br><span class="line">      .repartition(1)</span><br><span class="line">      .saveAsTextFile("data/add_log")</span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="spark应用程序的执行命令"><a href="#spark应用程序的执行命令" class="headerlink" title="spark应用程序的执行命令"></a>spark应用程序的执行命令</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">spark-submit \</span><br><span class="line"><span class="comment">--class com.***.Application \</span></span><br><span class="line"><span class="comment">--master yarn \</span></span><br><span class="line"><span class="comment">--deploy-mode client \</span></span><br><span class="line"><span class="comment">--driver-memory 1g \</span></span><br><span class="line"><span class="comment">--executor-memory 2g \</span></span><br><span class="line"><span class="comment">--queue root.wedw \</span></span><br><span class="line"><span class="comment">--num-executors 200 \</span></span><br><span class="line"><span class="comment">--jars</span></span><br><span class="line">/home/lx<span class="comment">/***/</span>config-1.3.0.jar,/home/lx<span class="comment">/***/</span>elasticsearch-hadoop-hive.jar,/home/lx/sen.jar</span><br></pre></td></tr></table></figure>

<h3 id="创建rdd的方式"><a href="#创建rdd的方式" class="headerlink" title="创建rdd的方式"></a>创建rdd的方式</h3><p>1.并行化集合创建 RDD</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> arr = <span class="type">Array</span>(<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>)</span><br><span class="line"><span class="keyword">val</span> rdd = sc.parallelize(arr)</span><br><span class="line"><span class="keyword">val</span> sum = rdd.reduce(_+_)</span><br></pre></td></tr></table></figure>

<p>2.使用外界的数据源创建RDD，比如说本地文件系统，分布式文件系统HDFS等等。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> rdd = sc.textFile(<span class="string">"d://data.txt"</span>)</span><br><span class="line"><span class="keyword">val</span> wordCount = rdd.map(line =&gt; line.length).reduce(_+_)</span><br></pre></td></tr></table></figure>

<p>3.通过将已有RDD使用transform算子操作产生新的RDD。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> wordsRDD = lineWordRDD.flatMap(line =&gt; line.split(<span class="string">" "</span>))</span><br><span class="line"><span class="comment">// 打印RDD的内容</span></span><br><span class="line">wordsRDD.foreach(word =&gt; println(word))</span><br></pre></td></tr></table></figure>

<h3 id="创建dataset的方式"><a href="#创建dataset的方式" class="headerlink" title="创建dataset的方式"></a>创建dataset的方式</h3><p>创建Datasets 的三种方式</p>
<ul>
<li>由DataFrame 转化成为 Dataset</li>
<li>通过 SparkSession.createDataset() 直接创建</li>
<li>通过toDS 方法意识转换</li>
</ul>
<h3 id="如何优雅地kill掉yarn上的application"><a href="#如何优雅地kill掉yarn上的application" class="headerlink" title="如何优雅地kill掉yarn上的application"></a>如何优雅地kill掉yarn上的application</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">yarn application -kill &lt;applicationId&gt;</span><br></pre></td></tr></table></figure>

<h3 id="spark-on-yarn关键配置"><a href="#spark-on-yarn关键配置" class="headerlink" title="spark on yarn关键配置"></a>spark on yarn关键配置</h3><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">./bin/spark-submit \</span><br><span class="line">		--class org.apache.spark.examples.SparkPi \</span><br><span class="line">    --master yarn \</span><br><span class="line">    --deploy-mode cluster \</span><br><span class="line">    --driver-memory 4g \</span><br><span class="line">    --executor-memory 2g \</span><br><span class="line">    --executor-cores 1 \</span><br><span class="line">    --queue thequeue \</span><br><span class="line">    examples/jars/spark-examples*.jar \</span><br><span class="line">    10</span><br></pre></td></tr></table></figure>

<p>num-executors: 该参数用于设置Spark作业总共要用多少个Executor进程来执行</p>
<p>executor-memory: 是每个节点上占用的内存。</p>
<p>driver-memory: 实际分配的内存</p>
<p>executor-cores: 该参数用于设置每个Executor进程的CPU core数量，这个参数决定了每个Executor进程并行执行task线程的能力。</p>
<h3 id="partition分区的个数"><a href="#partition分区的个数" class="headerlink" title="partition分区的个数"></a>partition分区的个数</h3><p>根据 RDD 的创建方式分为两种情况：</p>
<p>​    1、从内存中创建 RDD：sc.parallelize(…)，那么默认的分区数量为该程序所分配的资源的 CPU 数量。<br>​    2、从 HDFS 文件创建：sc.textFile(…) 或 spark.sql(…)，每个 HDFS 文件以块（Block）的形式存储，Spark 读取时会根据具体数据格式对应的 InputFormat 进行解析，例如文本格式就用 TextInputFormat 进行解析，一般是将若干个 Block 合并为一个输入分片（InputSplit），而这个 InputSplit 数就是默认的分区数。</p>
<h3 id="scala中的函数和方法的区别"><a href="#scala中的函数和方法的区别" class="headerlink" title="scala中的函数和方法的区别"></a>scala中的函数和方法的区别</h3><p>链接：<a href="https://www.runoob.com/w3cnote/scala-different-function-method.html" target="_blank" rel="noopener">https://www.runoob.com/w3cnote/scala-different-function-method.html</a></p>
<p>1.方法不能作为单独的表达式而存在（参数为空的方法除外），而函数可以。</p>
<p>2.函数必须要有参数列表，而方法可以没有参数列表</p>
<p>3.方法名是方法调用，而函数名只是代表函数对象本身</p>
<p>4.在需要函数的地方，如果传递一个方法，会自动进行ETA展开（把方法转换为函数）</p>
<p>5.传名参数本质上是个方法</p>
<h3 id="如何监控spark-streaming"><a href="#如何监控spark-streaming" class="headerlink" title="如何监控spark streaming"></a>如何监控spark streaming</h3><ol>
<li>界面监控</li>
<li>程序监控</li>
</ol>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// uncomment these 2 lines if you want to see only your logs</span></span><br><span class="line">  <span class="comment">// Logger.getLogger("org").setLevel(Level.OFF)</span></span><br><span class="line">  <span class="comment">// Logger.getLogger("akka").setLevel(Level.OFF)</span></span><br><span class="line"></span><br><span class="line">  <span class="type">ApplicationProperties</span>.parse(args.toList)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="type">ApplicationProperties</span>.sparkMaster).setAppName(<span class="type">ApplicationProperties</span>.appName)</span><br><span class="line"></span><br><span class="line">  <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(sparkConf, <span class="type">Seconds</span>(<span class="type">ApplicationProperties</span>.batchInterval))</span><br><span class="line">  </span><br><span class="line">  <span class="comment">//添加监控</span></span><br><span class="line">  ssc.addStreamingListener(<span class="keyword">new</span> <span class="type">SparkMonitoringListener</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Create a DStream that will connect to hostname:port, like localhost:9999</span></span><br><span class="line">  <span class="keyword">val</span> lines = ssc.socketTextStream(<span class="type">ApplicationProperties</span>.socketStreamHost, <span class="type">ApplicationProperties</span>.socketStreamPort)</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Split each line into words</span></span><br><span class="line">  <span class="keyword">val</span> words = lines.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Count each word in each batch</span></span><br><span class="line">  <span class="keyword">val</span> pairs = words.map(word =&gt; (word, <span class="number">1</span>))</span><br><span class="line">  <span class="keyword">val</span> wordCounts = pairs.reduceByKey(_ + _)</span><br><span class="line"></span><br><span class="line">  <span class="comment">// Print the first ten elements of each RDD generated in this DStream to the console</span></span><br><span class="line">  wordCounts.print()</span><br><span class="line"></span><br><span class="line">  ssc.start()             <span class="comment">// Start the computation</span></span><br><span class="line">  ssc.awaitTermination()  <span class="comment">// Wait for the computation to terminate</span></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>Listener</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">case</span> <span class="class"><span class="keyword">class</span> <span class="title">SparkMonitoringListener</span>(<span class="params"></span>) <span class="keyword">extends</span> <span class="title">StreamingListener</span> </span>&#123;</span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onBatchStarted</span></span>(batchStarted: <span class="type">StreamingListenerBatchStarted</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    println(<span class="string">"&gt;&gt;&gt; Batch started...records in batch = "</span> + batchStarted.batchInfo.numRecords)</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">override</span> <span class="function"><span class="keyword">def</span> <span class="title">onBatchCompleted</span></span>(batchCompleted: <span class="type">StreamingListenerBatchCompleted</span>): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> start = batchCompleted.batchInfo.processingStartTime.get</span><br><span class="line">    <span class="keyword">val</span> end = batchCompleted.batchInfo.processingEndTime.get</span><br><span class="line">    <span class="keyword">val</span> batchTime = batchCompleted.batchInfo.batchTime</span><br><span class="line">    <span class="keyword">val</span> numRecords = batchCompleted.batchInfo.numRecords</span><br><span class="line">    println(<span class="string">"batch finished"</span>, start, end, end-start, batchTime.toString(), numRecords)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ol start="3">
<li>API监控</li>
</ol>
<p>SparkStreaming restAPI监控从spark2.2.0版本开始支持，目前不支持2.1.0</p>
<h3 id="scala多继承"><a href="#scala多继承" class="headerlink" title="scala多继承"></a>scala多继承</h3><p>通过实现trait</p>
<h3 id="flink和spark-streaming实时处理的区别"><a href="#flink和spark-streaming实时处理的区别" class="headerlink" title="flink和spark streaming实时处理的区别"></a>flink和spark streaming实时处理的区别</h3><p>SparkStreaming是基于微批处理的,所以他采用DirectDstream的方式根据计算出的每个partition要取数据的Offset范围,拉取一批数据形成Rdd进行批量处理,而且该Rdd和kafka的分区是一一对应的;</p>
<p>Flink是真正的流处理,他是基于事件触发机制进行处理,在KafkaConsumer拉取一批数据以后,Flink将其经过处理之后变成,逐个Record发送的事件触发式的流处理；</p>
<ol>
<li><p>数据模型</p>
<ul>
<li>spark 采用rdd，spark streaming的Stream实际上也是一组rdd的集合</li>
<li>flink的基本数据模型是数据流，以及事件的序列</li>
</ul>
</li>
<li><p>运行架构</p>
<ul>
<li>spark是批计算，将DAG划分为不同的stage,一个完成后才可以计算下一个</li>
<li>Flink是标准的流执行模式，一个事件在一个节点处理完成后可以直接发往下一个节点进行处理。</li>
</ul>
</li>
</ol>
<h3 id="spark-context作用"><a href="#spark-context作用" class="headerlink" title="spark context作用"></a>spark context作用</h3><p>SparkContext是开发Spark应用的入口，它负责和整个集群的交互，包括创建RDD等。<br>从本质上来说，SparkContext是Spark的对外接口，负责向调用这提供Spark的各种功能。</p>
<h3 id="spark-streaming读取kafka，direct-receiver两种模式，为什么选择direct-直连模式"><a href="#spark-streaming读取kafka，direct-receiver两种模式，为什么选择direct-直连模式" class="headerlink" title="spark streaming读取kafka，direct+receiver两种模式，为什么选择direct(直连模式)"></a>spark streaming读取kafka，direct+receiver两种模式，为什么选择direct(直连模式)</h3><ol>
<li><p>receiver模式</p>
<p>采用该模式，需要一个task一直处于接受数据的状态。spark streaming相当于kafka的消费者，接受来的数据备份到其他节点完成之后，会向zookeeper更新消费者offset。</p>
<p>当更新完消费者偏移量后，如果driver挂掉，driver下的executor也会挂掉，就会有数据丢失的问题。</p>
<p>如何解决？</p>
<p>开启WAL机制，预写日志机制。将数据备份一份到HDFS上，完成之后更新zookeeper的offset。</p>
<p>效率慢，延迟大。</p>
<p>可以创建多个Kafka输入DStream，使用不同的consumer group和topic，来通过多个receiver并行接收数据。</p>
</li>
<li><p>direct模式</p>
<p>direct模式将kafka看成存储数据的一方，spark streaming主动取数据，无需一个task一直占用接收数据。</p>
<p>生成的Stream中的RDD的并行度与读取的kafka的topic的partition个数一致。提高了计算时的并行度。</p>
<p>spark来管理offset，默认在内存中，可以设置checkpoint。</p>
</li>
</ol>
<h3 id="spark-core和spark-sql"><a href="#spark-core和spark-sql" class="headerlink" title="spark core和spark sql"></a>spark core和spark sql</h3><p>Spark SQL在Spark Core的基础上针对结构化数据处理进行很多优化和改进:</p>
<p>Spark SQL 支持很多种结构化数据源，可以让你跳过复杂的读取过程，轻松从各种数据源中读取数据。</p>
<p>当你使用SQL查询这些数据源中的数据并且只用到了一部分字段时，SparkSQL可以智能地只扫描这些用到的字段，而不是像SparkContext.hadoopFile中那样简单粗暴地扫描全部数据。</p>
<h3 id="一千万条短信，有重复，以文本文件的形式保存，一行一条数据，请用五分钟时间，找出重复出现最多的前10条。"><a href="#一千万条短信，有重复，以文本文件的形式保存，一行一条数据，请用五分钟时间，找出重复出现最多的前10条。" class="headerlink" title="一千万条短信，有重复，以文本文件的形式保存，一行一条数据，请用五分钟时间，找出重复出现最多的前10条。"></a>一千万条短信，有重复，以文本文件的形式保存，一行一条数据，请用五分钟时间，找出重复出现最多的前10条。</h3><p>方法一：哈希表法 hashtable 或者 concurrentHashmap</p>
<p>可以用哈希表的方法对1千万条分成若干组进行边扫描边建散列表。第一次扫描，取首字节，尾字节，中间随便两字节作为Hash Code，插入到hash table中。并记录其地址和信息长度和重复次数，1千万条信息，记录这几个信息还放得下。同Hash Code且等长就疑似相同，比较一下。相同记录只加1次进hash table，但将重复次数加1。一次扫描以后，已经记录各自的重复次数，进行第二次hash table的处理。用线性时间选择可在O（n）的级别上完成前10条的寻找。分组后每份中的top10必须保证各不相同，可hash来保证，也可直接按hash值的大小来分类。</p>
<h3 id="spark调优的8个方面"><a href="#spark调优的8个方面" class="headerlink" title="spark调优的8个方面"></a>spark调优的8个方面</h3><p>链接：<a href="https://zhuanlan.zhihu.com/p/54293797" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/54293797</a></p>
<h3 id="yarn-client和yarn-cluster"><a href="#yarn-client和yarn-cluster" class="headerlink" title="yarn-client和yarn-cluster"></a>yarn-client和yarn-cluster</h3><p>一般yarn-client用于测试环境调试程序；yarn-cluster用于生产环境。</p>
<p>yarn-client和yarn-cluster的区别就在于，Driver是运行在本地客户端，它的AM只是作为一个Executor启动器，并没有Driver进程。</p>
<h3 id="spark1-x和spark2-x区别"><a href="#spark1-x和spark2-x区别" class="headerlink" title="spark1.x和spark2.x区别"></a>spark1.x和spark2.x区别</h3><p>Spark 2.x新特性<br>1）Spark Core/SQL</p>
<p>在内存和CPU使用方面进一步优化Spark引擎性能，支持SQL 2003标准，支持子查询，对常用的SQL操作和DataFrame,性能有2-10倍的提升。</p>
<p>2）Spark Session</p>
<p>Spark2.0 中引入了 SparkSession 的概念，它为用户提供了一个统一的切入点来使用 Spark 的各项功能，统一了旧的SQLContext与HiveContext。用户不但可以使用 DataFrame 和Dataset 的各种 API，学习 Spark2 的难度也会大大降低。</p>
<p>3)统一 DataFrames 和 Datasets 的 API</p>
<p>它们都是提供给用户使用，包括各类操作接口的 API，1.3 版本引入 DataFrame，1.6版本引入Dataset，在 spark 2.0 中，把 dataframes 当作是一种特殊的 datasets，dataframes = datasets[row]，把两者统一为datasets。</p>
<p>4) strutured Streaming</p>
<p>Spark Streaming基于Spark SQL(DataFrame / Dataset )构建了high-level API，使得Spark Streaming充分受益Spark SQL的易用性和性能提升。</p>
<h3 id="spark-streaming-7-24小时一直运行"><a href="#spark-streaming-7-24小时一直运行" class="headerlink" title="spark streaming 7*24小时一直运行"></a>spark streaming 7*24小时一直运行</h3><ol>
<li><p>Driver高可用</p>
<p>Spark Streaming HA将Driver元数据写到checkpoint目录下</p>
</li>
</ol>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(conf, <span class="type">Seconds</span>(<span class="number">2</span>))</span><br><span class="line">ssc.checkpoint(checkpointDirectory)</span><br></pre></td></tr></table></figure>

<ol>
<li><p>RDD高可用</p>
<p>为了防止Executor异常退出导致数据丢失，Spark Streaming提供了WAL(预写日志)机制。</p>
<p>Receiver只要接收到数据，会立即将数据写入一份到高可用文件系统(一般是HDFS)上的checkpoint目录中</p>
</li>
</ol>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">conf.set(<span class="string">"spark.streaming.receiver.writeAheadLog.enable "</span>,<span class="string">"true"</span>);</span><br></pre></td></tr></table></figure>

<h3 id="spark-streaming是exactly-once-吗？"><a href="#spark-streaming是exactly-once-吗？" class="headerlink" title="spark streaming是exactly-once 吗？"></a>spark streaming是exactly-once 吗？</h3><p>基于direct stream的方法采用Kafka的简单消费者API，它的流程大大简化了。executor不再从Kafka中连续读取消息，也消除了receiver和WAL。还有一个改进就是Kafka分区与RDD分区是一一对应的，更可控。<br>driver进程只需要每次从Kafka获得批次消息的offset range，然后executor进程根据offset range去读取该批次对应的消息即可。由于offset在Kafka中能唯一确定一条消息，且在外部只能被Streaming程序本身感知到，因此消除了不一致性，达到了exactly once。<br>不过，由于它采用了简单消费者API，我们就需要自己来管理offset。否则一旦程序崩溃，整个流只能从earliest或者latest点恢复，这肯定是不稳妥的。offset管理在之前的文章中提到过，这里不再赘述。</p>
<p>Kafka作为输入源可以保证exactly once，那么处理逻辑呢？</p>
<p>答案是显然的，Spark Streaming的处理逻辑天生具备exactly once语义。<br>Spark RDD之所以被称为“弹性分布式数据集”，是因为它具有<strong>不可变、可分区、可并行计算、容错</strong>的特征。一个RDD只能由稳定的数据集生成，或者从其他RDD转换（transform）得来。如果在执行RDD lineage的过程中失败，那么只要源数据不发生变化，无论重新执行多少次lineage，都一定会得到同样的、确定的结果。</p>
<p>最后，我们还需要保证输出过程也符合exactly once语义。Spark Streaming的输出一般是靠foreachRDD()算子来实现，它默认是at least once的。如果输出过程中途出错，那么就会重复执行直到写入成功。为了让它符合exactly once，可以施加两种限制之一：<strong>幂等性写入</strong>（idempotent write）、<strong>事务性写入</strong>（transactional write）。</p>
]]></content>
      <categories>
        <category>bigdata</category>
      </categories>
      <tags>
        <tag>大数据私房菜</tag>
      </tags>
  </entry>
  <entry>
    <title>面试题随笔-21/4/19</title>
    <url>/2021/04/19/%E9%9D%A2%E8%AF%95%E9%A2%98%E9%9A%8F%E7%AC%94-21-4-19/</url>
    <content><![CDATA[<h2 id="面试总结"><a href="#面试总结" class="headerlink" title="面试总结"></a>面试总结</h2><h3 id="输入Scanner的一些总结"><a href="#输入Scanner的一些总结" class="headerlink" title="输入Scanner的一些总结"></a>输入Scanner的一些总结</h3><ol>
<li>需要无限循环读取的情况</li>
</ol>
<p>这种无需设置终止条件，while (in.hasNext())即可</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Scanner in = <span class="keyword">new</span> Scanner(System.in);</span><br><span class="line"></span><br><span class="line"><span class="keyword">while</span> (in.hasNext()) &#123;</span><br><span class="line">  <span class="keyword">int</span> a = in.nextInt();</span><br><span class="line">  <span class="keyword">int</span> b = in.nextInt();</span><br><span class="line">  System.out.println(<span class="string">"输出:"</span> + (a + b));</span><br><span class="line">&#125;</span><br><span class="line">示例输入<span class="number">1</span>：</span><br><span class="line"><span class="number">1</span> <span class="number">2</span></span><br><span class="line">输出:<span class="number">3</span></span><br><span class="line">结论：</span><br><span class="line">空格间隔，正确</span><br><span class="line">  </span><br><span class="line">示例输入<span class="number">2</span>：</span><br><span class="line"><span class="number">1</span></span><br><span class="line"><span class="number">2</span></span><br><span class="line">输出:<span class="number">3</span></span><br><span class="line">结论：</span><br><span class="line">回车换行，正确</span><br></pre></td></tr></table></figure>

<ol start="2">
<li>需要读取n行的情况，每一行又是一个数组的情况下</li>
</ol>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        Scanner in = <span class="keyword">new</span> Scanner(System.in);</span><br><span class="line">        <span class="keyword">int</span> n = in.nextInt();</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; n; i++) &#123;</span><br><span class="line">          	<span class="comment">// 用什么数据结构保存，因题目而定，例如二维数组、二叉树等</span></span><br><span class="line">            <span class="keyword">int</span>[] s = string2int(in.nextLine().split(<span class="string">" "</span>));</span><br><span class="line">          	<span class="comment">// 对每一行的业务逻辑</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">  	<span class="comment">// string数组转int数组</span></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">int</span>[] string2int(String[] s1) &#123;</span><br><span class="line">        <span class="keyword">int</span>[] ints = <span class="keyword">new</span> <span class="keyword">int</span>[s1.length];</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; s1.length; i++) &#123;</span><br><span class="line">            ints[i] = Integer.parseInt(s1[i]);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> ints;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<ol start="3">
<li>不知道输入多少行</li>
</ol>
<p>这种情况，当没有输入直接回车时，则停止输入。</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        Scanner in = <span class="keyword">new</span> Scanner(System.in);</span><br><span class="line">      </span><br><span class="line">        String line = <span class="keyword">null</span>;</span><br><span class="line">        <span class="keyword">while</span>(<span class="keyword">true</span>) &#123;</span><br><span class="line">            line = in.nextLine();</span><br><span class="line">          	<span class="comment">// 没有输入直接回车，则为空字符串</span></span><br><span class="line">            <span class="keyword">if</span> (line.equals(<span class="string">""</span>)) &#123;</span><br><span class="line">                <span class="keyword">break</span>;</span><br><span class="line">            &#125;</span><br><span class="line">          	<span class="comment">// 业务逻辑</span></span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="美团笔试题总结"><a href="#美团笔试题总结" class="headerlink" title="美团笔试题总结"></a>美团笔试题总结</h2><h3 id="排序问题，一个集合，有id和数值num两个数字，按照num从大到小排序，当num相同时，则id小的在前；query代表查询，append代表添加。"><a href="#排序问题，一个集合，有id和数值num两个数字，按照num从大到小排序，当num相同时，则id小的在前；query代表查询，append代表添加。" class="headerlink" title="排序问题，一个集合，有id和数值num两个数字，按照num从大到小排序，当num相同时，则id小的在前；query代表查询，append代表添加。"></a>排序问题，一个集合，有id和数值num两个数字，按照num从大到小排序，当num相同时，则id小的在前；query代表查询，append代表添加。</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main4</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 排序问题，从大到小，然后相同就id小的在前</span></span><br><span class="line"><span class="comment">     * 输入:</span></span><br><span class="line"><span class="comment">     * 9</span></span><br><span class="line"><span class="comment">     * query</span></span><br><span class="line"><span class="comment">     * append 1 10</span></span><br><span class="line"><span class="comment">     * query</span></span><br><span class="line"><span class="comment">     * append 2 20</span></span><br><span class="line"><span class="comment">     * query</span></span><br><span class="line"><span class="comment">     * append 3 15</span></span><br><span class="line"><span class="comment">     * query</span></span><br><span class="line"><span class="comment">     * append 1 10</span></span><br><span class="line"><span class="comment">     * query</span></span><br><span class="line"><span class="comment">     * 输出：</span></span><br><span class="line"><span class="comment">     * null</span></span><br><span class="line"><span class="comment">     * 1</span></span><br><span class="line"><span class="comment">     * 2 1</span></span><br><span class="line"><span class="comment">     * 2 3 1</span></span><br><span class="line"><span class="comment">     * 1 2 3</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> Map&lt;Integer, Integer&gt; map = <span class="keyword">new</span> ConcurrentHashMap&lt;&gt;();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> List&lt;String&gt; result = <span class="keyword">new</span> ArrayList&lt;String&gt;();</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        Scanner in = <span class="keyword">new</span> Scanner(System.in);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">int</span> _n;</span><br><span class="line">        _n = Integer.valueOf(in.nextLine());</span><br><span class="line"></span><br><span class="line">        String line = <span class="keyword">null</span>;</span><br><span class="line">        <span class="keyword">int</span> x = <span class="number">0</span>, y = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; _n; i++) &#123;</span><br><span class="line">            line = in.nextLine();</span><br><span class="line">            String[] command = line.split(<span class="string">" "</span>);</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (command[<span class="number">0</span>].equals(<span class="string">"query"</span>)) &#123;</span><br><span class="line">                query();</span><br><span class="line">            &#125; <span class="keyword">else</span> <span class="keyword">if</span> (command[<span class="number">0</span>].equals(<span class="string">"append"</span>)) &#123;</span><br><span class="line">                x = Integer.valueOf(command[<span class="number">1</span>]);</span><br><span class="line">                y = Integer.valueOf(command[<span class="number">2</span>]);</span><br><span class="line">                append(x, y);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (String result_tmp :</span><br><span class="line">                result) &#123;</span><br><span class="line">            System.out.println(result_tmp);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">append</span><span class="params">(<span class="keyword">int</span> x, <span class="keyword">int</span> y)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> (map.containsKey(x)) &#123;</span><br><span class="line">            map.put(x, map.get(x) + y);</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            map.put(x, y);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">query</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (map.size() == <span class="number">0</span>) &#123;</span><br><span class="line">            result.add(<span class="string">"null"</span>);</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            List&lt;Map.Entry&lt;Integer, Integer&gt;&gt; list = <span class="keyword">new</span> ArrayList&lt;Map.Entry&lt;Integer, Integer&gt;&gt;(map.entrySet());</span><br><span class="line">            Collections.sort(list, <span class="keyword">new</span> Comparator&lt;Map.Entry&lt;Integer, Integer&gt;&gt;() &#123;</span><br><span class="line">                <span class="meta">@Override</span></span><br><span class="line">                <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">compare</span><span class="params">(Map.Entry&lt;Integer, Integer&gt; o1, Map.Entry&lt;Integer, Integer&gt; o2)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">                    <span class="keyword">return</span> o2.getValue().compareTo(o1.getValue());</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;);</span><br><span class="line">            String s = <span class="string">""</span>;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">for</span> (Map.Entry&lt;Integer, Integer&gt; entry :</span><br><span class="line">                    list) &#123;</span><br><span class="line">                s += entry.getKey() + <span class="string">" "</span>;</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">            <span class="keyword">if</span> (!s.equals(<span class="string">""</span>)) &#123;</span><br><span class="line">                result.add(s);</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="兵力和问题"><a href="#兵力和问题" class="headerlink" title="兵力和问题"></a>兵力和问题</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> java.util.*;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main5</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 兵力和 输入 3 1 =&gt; 代表3个1级兵</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * 输入3 4 3，A有3种兵，B有4种兵，出战的兵的力量必须&gt;=3 所以A=3*4 = 12 ; B=1*3 + 2*5 = 13</span></span><br><span class="line"><span class="comment">     * B获胜</span></span><br><span class="line"><span class="comment">     * 输入：</span></span><br><span class="line"><span class="comment">     * 3 4 3</span></span><br><span class="line"><span class="comment">     * 3 1</span></span><br><span class="line"><span class="comment">     * 3 4</span></span><br><span class="line"><span class="comment">     * 4 2</span></span><br><span class="line"><span class="comment">     * 1 3</span></span><br><span class="line"><span class="comment">     * 2 5</span></span><br><span class="line"><span class="comment">     * 4 2</span></span><br><span class="line"><span class="comment">     * 2 1</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * 输出：</span></span><br><span class="line"><span class="comment">     * 12 13</span></span><br><span class="line"><span class="comment">     * B</span></span><br><span class="line"><span class="comment">     * <span class="doctag">@param</span> args</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line"></span><br><span class="line">        <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        Scanner in = <span class="keyword">new</span> Scanner(System.in);</span><br><span class="line"></span><br><span class="line">        String nmk;</span><br><span class="line">        nmk = in.nextLine();</span><br><span class="line"></span><br><span class="line">        String[] s = nmk.split(<span class="string">" "</span>);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">int</span> n = Integer.valueOf(s[<span class="number">0</span>]);</span><br><span class="line">        <span class="keyword">int</span> m = Integer.valueOf(s[<span class="number">1</span>]);</span><br><span class="line">        <span class="keyword">int</span> k = Integer.valueOf(s[<span class="number">2</span>]);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">int</span>[][] a_ints = <span class="keyword">new</span> <span class="keyword">int</span>[n][<span class="number">2</span>];</span><br><span class="line">        <span class="keyword">int</span>[][] b_ints = <span class="keyword">new</span> <span class="keyword">int</span>[m][<span class="number">2</span>];</span><br><span class="line"></span><br><span class="line">        String line = <span class="keyword">null</span>;</span><br><span class="line">        String[] s1 = <span class="keyword">null</span>;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">int</span> a_sum = <span class="number">0</span>, b_sum = <span class="number">0</span>;</span><br><span class="line">        <span class="comment">// 输入n行 A的兵力情况</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; n; i++) &#123;</span><br><span class="line">            line = in.nextLine();</span><br><span class="line">            s1 = line.split(<span class="string">" "</span>);</span><br><span class="line">            a_ints[i][<span class="number">0</span>] = Integer.parseInt(s1[<span class="number">0</span>]);</span><br><span class="line">            <span class="keyword">int</span> fight = Integer.parseInt(s1[<span class="number">1</span>]);</span><br><span class="line">            a_ints[i][<span class="number">1</span>] = fight;</span><br><span class="line">            <span class="comment">// 是否有资格出战，累加得出A的兵力和</span></span><br><span class="line">            <span class="keyword">if</span> (fight &gt;= k) &#123;</span><br><span class="line">                a_sum += fight * a_ints[i][<span class="number">0</span>];</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 输入m行 B的兵力情况</span></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; m; i++) &#123;</span><br><span class="line">            line = in.nextLine();</span><br><span class="line">            s1 = line.split(<span class="string">" "</span>);</span><br><span class="line">            b_ints[i][<span class="number">0</span>] = Integer.parseInt(s1[<span class="number">0</span>]);</span><br><span class="line">            <span class="keyword">int</span> fight = Integer.parseInt(s1[<span class="number">1</span>]);</span><br><span class="line">            b_ints[i][<span class="number">1</span>] = fight;</span><br><span class="line">            <span class="comment">// 是否有资格出战，累加得出B的兵力和</span></span><br><span class="line">            <span class="keyword">if</span> (fight &gt;= k) &#123;</span><br><span class="line">                b_sum += fight * b_ints[i][<span class="number">0</span>];</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        System.out.println(a_sum + <span class="string">" "</span> + b_sum);</span><br><span class="line">        <span class="keyword">if</span> (a_sum &gt; b_sum) &#123;</span><br><span class="line">            System.out.print(<span class="string">"A"</span>);</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">            System.out.print(<span class="string">"B"</span>);</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="车厢调度问题"><a href="#车厢调度问题" class="headerlink" title="车厢调度问题"></a>车厢调度问题</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Main6</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 车厢调度问题，1 代表追加车厢， 2 代表对两个车厢对调 例如 1 1 A =&gt; 将 1车厢 追加到 A列车后 2 A B =&gt; 将A,B车厢对调</span></span><br><span class="line"><span class="comment">     * 输入：</span></span><br><span class="line"><span class="comment">     * 7</span></span><br><span class="line"><span class="comment">     * 1 1 A</span></span><br><span class="line"><span class="comment">     * 1 2 B</span></span><br><span class="line"><span class="comment">     * 1 5 A</span></span><br><span class="line"><span class="comment">     * 1 3 C</span></span><br><span class="line"><span class="comment">     * 1 4 D</span></span><br><span class="line"><span class="comment">     * 2 A B</span></span><br><span class="line"><span class="comment">     * 2 B C</span></span><br><span class="line"><span class="comment">     *</span></span><br><span class="line"><span class="comment">     * 输出：</span></span><br><span class="line"><span class="comment">     * 3 1 5 2 4</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line">    <span class="comment">// 题目给定，列车&lt;=5 挂载车厢数量&lt;=1000，定义一个二维数组即可</span></span><br><span class="line">    <span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">int</span>[][] result = <span class="keyword">new</span> <span class="keyword">int</span>[<span class="number">5</span>][<span class="number">1000</span>];</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        Scanner in = <span class="keyword">new</span> Scanner(System.in);</span><br><span class="line"></span><br><span class="line">        <span class="keyword">int</span> n;</span><br><span class="line">        n = Integer.parseInt(in.nextLine());</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 用一个map存储每个列车，对应二维数组的下标</span></span><br><span class="line">        HashMap&lt;String, Integer&gt; indexHashMap = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">        indexHashMap.put(<span class="string">"A"</span>, <span class="number">0</span>);</span><br><span class="line">        indexHashMap.put(<span class="string">"B"</span>, <span class="number">0</span>);</span><br><span class="line">        indexHashMap.put(<span class="string">"C"</span>, <span class="number">0</span>);</span><br><span class="line">        indexHashMap.put(<span class="string">"D"</span>, <span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">        <span class="comment">// 用一个map存储每个列车，与相应的二维数组下标对应起来，初始为0，1，2，3</span></span><br><span class="line">        HashMap&lt;String, Integer&gt; intmap = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">        intmap.put(<span class="string">"A"</span>, <span class="number">0</span>);</span><br><span class="line">        intmap.put(<span class="string">"B"</span>, <span class="number">1</span>);</span><br><span class="line">        intmap.put(<span class="string">"C"</span>, <span class="number">2</span>);</span><br><span class="line">        intmap.put(<span class="string">"D"</span>, <span class="number">3</span>);</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">        String line = <span class="keyword">null</span>;</span><br><span class="line">        String[] s1 = <span class="keyword">null</span>;</span><br><span class="line">        <span class="keyword">int</span> index = <span class="number">0</span>;</span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; n; i++) &#123;</span><br><span class="line">            line = in.nextLine();</span><br><span class="line">            s1 = line.split(<span class="string">" "</span>);</span><br><span class="line">            <span class="keyword">if</span> (s1[<span class="number">0</span>].equals(<span class="string">"1"</span>)) &#123;</span><br><span class="line">                index = indexHashMap.get(s1[<span class="number">2</span>]);</span><br><span class="line">                result[intmap.get(s1[<span class="number">2</span>])][index] = Integer.parseInt(s1[<span class="number">1</span>]);</span><br><span class="line">                <span class="comment">// 下标+1</span></span><br><span class="line">                index++;</span><br><span class="line">                indexHashMap.put(s1[<span class="number">2</span>], index);</span><br><span class="line">            &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">                <span class="keyword">int</span> x = intmap.get(s1[<span class="number">1</span>]);</span><br><span class="line">                <span class="keyword">int</span> y = intmap.get(s1[<span class="number">2</span>]);</span><br><span class="line"></span><br><span class="line">                <span class="comment">// 车厢对调</span></span><br><span class="line">                swap_xy(x, y);</span><br><span class="line"></span><br><span class="line">                <span class="comment">// 将map中下标也对应替换</span></span><br><span class="line">                intmap.put(s1[<span class="number">1</span>], y);</span><br><span class="line">                intmap.put(s1[<span class="number">2</span>], x);</span><br><span class="line">            &#125;</span><br><span class="line"></span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; <span class="number">5</span>; i++) &#123;</span><br><span class="line">            <span class="keyword">for</span> (<span class="keyword">int</span> k = <span class="number">0</span>; k &lt; <span class="number">1000</span>; k++) &#123;</span><br><span class="line">                <span class="keyword">if</span> (result[i][k] != <span class="number">0</span>) &#123;</span><br><span class="line">                    System.out.print(result[i][k] + <span class="string">" "</span>);</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 车厢对调</span></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">swap_xy</span><span class="params">(<span class="keyword">int</span> x, <span class="keyword">int</span> y)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span>[] tmp = result[x];</span><br><span class="line">        result[x] = result[y];</span><br><span class="line">        result[y] = tmp;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>面经</category>
      </categories>
      <tags>
        <tag>大数据开发</tag>
      </tags>
  </entry>
  <entry>
    <title>面试题随笔-21/4/16</title>
    <url>/2021/04/16/%E9%9D%A2%E8%AF%95%E9%A2%98%E9%9A%8F%E7%AC%94-21-4-16/</url>
    <content><![CDATA[<h2 id="面经总结"><a href="#面经总结" class="headerlink" title="面经总结"></a>面经总结</h2><h3 id="hive的metastore选择"><a href="#hive的metastore选择" class="headerlink" title="hive的metastore选择"></a>hive的metastore选择</h3><p>hive的metastore存储方式：</p>
<p>一、使用derby数据库存储元数据。</p>
<p>使用derby存储方式时，运行hive会在当前目录生成一个derby文件和一个metastore_db目录。这种存储方式的弊端是在同一个目录下同时只能有一个hive客户端能使用数据库，否则会提示如下错误（这是一个很常见的错误）。</p>
<p>二、使用本机mysql服务器存储元数据。这种存储方式需要在本地运行一个mysql服务器，并作如下配置（下面两种使用mysql的方式，需要将mysql的jar包拷贝到$HIVE_HOME/lib目录下）。</p>
<p>三、使用远端mysql服务器存储元数据。这种存储方式需要在远端服务器运行一个mysql服务器，并且需要在Hive服务器启动meta服务。</p>
<h3 id="kafka中consumer和partition的对应关系"><a href="#kafka中consumer和partition的对应关系" class="headerlink" title="kafka中consumer和partition的对应关系"></a>kafka中consumer和partition的对应关系</h3><p>一个partition只能被同组的一个consumer消费，同组的consumer则起到均衡效果。</p>
<ol>
<li>消费者多于partition</li>
</ol>
<p>多余的消费者空闲</p>
<ol start="2">
<li>消费者少于和等于partition</li>
</ol>
<p>即消息在同一个组之间的消费者之间均分</p>
<ol start="3">
<li>多个消费者组</li>
</ol>
<p>即组与组之间的消息是否被消费是相互隔离互不影响的。</p>
<h3 id="synchronized实现原理"><a href="#synchronized实现原理" class="headerlink" title="synchronized实现原理"></a>synchronized实现原理</h3><p>详细理解</p>
<p>链接：<a href="https://blog.csdn.net/javazejian/article/details/72828483" target="_blank" rel="noopener">https://blog.csdn.net/javazejian/article/details/72828483</a></p>
<p>简单理解</p>
<p>链接：<a href="https://blog.csdn.net/jinjiniao1/article/details/91546512" target="_blank" rel="noopener">https://blog.csdn.net/jinjiniao1/article/details/91546512</a></p>
<h4 id="synchronized的实现原理和应用总结"><a href="#synchronized的实现原理和应用总结" class="headerlink" title="synchronized的实现原理和应用总结"></a>synchronized的实现原理和应用总结</h4><p>（1）synchronized同步代码块：synchronized关键字经过编译之后，会在同步代码块前后分别形成monitor enter和monitor exit字节码指令，在执行monitorenter指令的时候，首先尝试获取对象的锁，如果这个锁没有被锁定或者当前线程已经拥有了那个对象的锁，锁的计数器就加1，在执行monitorexit指令时会将锁的计数器减1，当减为0的时候就释放锁。如果获取对象锁一直失败，那当前线程就要阻塞等待，直到对象锁被另一个线程释放为止。</p>
<p>（2）同步方法：方法级的同步是隐式的，无须通过字节码指令来控制，JVM可以从方法常量池的方法表结构中的ACC_SYNCHRONIZED访问标志得知一个方法是否声明为同步方法。当方法调用的时，调用指令会检查方法的ACC_SYNCHRONIZED访问标志是否被设置，如果设置了，执行线程就要求先持有monitor对象，然后才能执行方法，最后当方法执行完（无论是正常完成还是非正常完成）时释放monitor对象。在方法执行期间，执行线程持有了管程，其他线程都无法再次获取同一个管程。</p>
<p>JVM规范中对monitorenter和monitorexit指令的描述如下：</p>
<p>monitorenter：</p>
<p>每个对象都有一个监视器锁(monitor)与之对应。当monitor被占用时就会处于锁定状态，线程执行monitorenter指令时尝试获取monitor的所有权，过程如下：<br>1、如果monitor的进入数为0，则该线程进入monitor，然后将进入数设置为1，该线程即为monitor的所有者。<br>2、如果线程已经占有该monitor，只是重新进入，则进入monitor的进入数加1.<br>3.如果其他线程已经占用了monitor，则该线程进入阻塞状态，直到monitor的进入数为0，再重新尝试获取monitor的所有权。</p>
<p>monitorexit：</p>
<p>执行monitorexit的线程必须是objectref所对应的monitor的所有者。<br>指令执行时，monitor的进入数减1，如果减1后进入数为0，那线程退出monitor，不再是这个monitor的所有者。其他被这个monitor阻塞的线程可以尝试去获取这个monitor的所有权。</p>
<p>同时我们还必须注意到的是在Java早期版本中，synchronized属于重量级锁，效率低下，因为监视器锁（monitor）是依赖于底层的操作系统的Mutex Lock来实现的，而操作系统实现线程之间的切换时需要从用户态转换到核心态，这个状态之间的转换需要相对比较长的时间，时间成本相对较高，这也是为什么早期的synchronized效率低的原因。庆幸的是在Java 6之后Java官方对从JVM层面对synchronized较大优化，所以现在的synchronized锁效率也优化得很不错了，Java 6之后，为了减少获得锁和释放锁所带来的性能消耗，引入了轻量级锁和偏向锁，接下来我们将简单了解一下Java官方在JVM层面对synchronized锁的优化。</p>
<h4 id="Java虚拟机对synchronized的优化"><a href="#Java虚拟机对synchronized的优化" class="headerlink" title="Java虚拟机对synchronized的优化"></a>Java虚拟机对synchronized的优化</h4><p>锁的状态总共有四种，无锁状态、偏向锁、轻量级锁和重量级锁。随着锁的竞争，锁可以从偏向锁升级到轻量级锁，再升级的重量级锁，但是锁的升级是单向的，也就是说只能从低到高升级，不会出现锁的降级，关于重量级锁，前面我们已详细分析过，下面我们将介绍偏向锁和轻量级锁以及JVM的其他优化手段，这里并不打算深入到每个锁的实现和转换过程更多地是阐述Java虚拟机所提供的每个锁的核心优化思想，毕竟涉及到具体过程比较繁琐，如需了解详细过程可以查阅《深入理解Java虚拟机原理》。</p>
<ol>
<li><p>偏向锁</p>
<p>偏向锁的核心思想是，如果一个线程获得了锁，那么锁就进入偏向模式，此时Mark Word 的结构也变为偏向锁结构，当这个线程再次请求锁时，无需再做任何同步操作，即获取锁的过程，这样就省去了大量有关锁申请的操作，从而也就提供程序的性能。</p>
</li>
<li><p>轻量级锁</p>
<p>轻量级锁能够提升程序性能的依据是“对绝大部分的锁，在整个同步周期内都不存在竞争”，注意这是经验数据。需要了解的是，轻量级锁所适应的场景是线程交替执行同步块的场合，如果存在同一时间访问同一锁的场合，就会导致轻量级锁膨胀为重量级锁。</p>
</li>
<li><p>自旋锁</p>
<p>这是基于在大多数情况下，线程持有锁的时间都不会太长，如果直接挂起操作系统层面的线程可能会得不偿失，毕竟操作系统实现线程之间的切换时需要从用户态转换到核心态，这个状态之间的转换需要相对比较长的时间，时间成本相对较高。</p>
</li>
<li><p>重量级锁</p>
<p>获取不到锁的线程进入阻塞，获取到锁的线程，在释放锁时会有唤醒操作。</p>
<p>重量级锁是依赖对象内部的monitor锁来实现的，而monitor又依赖操作系统的MutexLock(互斥锁)来实现的，所以重量级锁也被成为<strong>互斥锁</strong>。</p>
<p>当系统检查到锁是重量级锁之后，会把等待想要获得锁的线程进行<strong>阻塞</strong>，被阻塞的线程不会消耗cup。但是阻塞或者唤醒一个线程时，都需要操作系统来帮忙，这就需要从<strong>用户态</strong>转换到<strong>内核态</strong>，而转换状态是需要消耗很多时间的，有可能比用户执行代码的时间还要长。</p>
</li>
<li><p>锁消除</p>
<p>Java虚拟机在JIT编译时(可以简单理解为当某段代码即将第一次被执行时进行编译，又称即时编译)，通过对运行上下文的扫描，去除不可能存在共享资源竞争的锁，通过这种方式消除没有必要的锁，可以节省毫无意义的请求锁时间，</p>
</li>
</ol>
<h3 id="数据仓库分层"><a href="#数据仓库分层" class="headerlink" title="数据仓库分层"></a>数据仓库分层</h3><p>一、数据运营层：ODS（Operational Data Store）<br>“面向主题的”数据运营层，也叫ODS层，是最接近数据源中数据的一层，数据源中的数据，经过抽取、洗净、传输，也就说传说中的 ETL 之后，装入本层。本层的数据，总体上大多是按照源头业务系统的分类方式而分类的。</p>
<p>建议做过多的数据清洗工作，原封不动地接入原始数据即可，至于数据的去噪、去重、异常值处理等过程可以放在后面的DWD层来做。</p>
<p>二、数据仓库层：DW（Data Warehouse）<br>数据仓库层是我们在做数据仓库时要核心设计的一层，在这里，从 ODS 层中获得的数据按照主题建立各种数据模型。DW层又细分为 DWD（Data Warehouse Detail）层、DWM（Data WareHouse Middle）层和DWS（Data WareHouse Servce）层。</p>
<p>\1. 数据明细层：DWD（Data Warehouse Detail）</p>
<p>维度退化手法，将维度退化至事实表中，减少事实表和维表的关联。</p>
<p>部分的数据聚合，将相同主题的数据汇集到一张表中，提高数据的可用性。</p>
<p>\2. 数据中间层：DWM（Data WareHouse Middle）</p>
<p>轻度聚合操作，生成一系列的中间表，提升公共指标的复用性。</p>
<p>直观来讲，就是对通用的核心维度进行聚合操作，算出相应的统计指标。</p>
<p>\3. 数据服务层：DWS（Data WareHouse Servce）</p>
<p>又称数据集市或宽表。按照业务划分，如流量、订单、用户等，生成字段比较多的宽表，用于提供后续的业务查询，OLAP分析，数据分发等。</p>
<p>一般来讲，该层的数据表会相对比较少，一张表会涵盖比较多的业务内容，由于其字段较多，因此一般也会称该层的表为宽表。</p>
<p>在实际计算中，如果直接从DWD或者ODS计算出宽表的统计指标，会存在计算量太大并且维度太少的问题，因此一般的做法是，在DWM层先计算出多个小的中间表，然后再拼接成一张DWS的宽表。由于宽和窄的界限不易界定，也可以去掉DWM这一层，只留DWS层，将所有的数据在放在DWS亦可。</p>
<p>三、数据应用层：APP（Application）<br>应用层的需求来创建表。</p>
<h3 id="hadoop安装过程中遇到的问题"><a href="#hadoop安装过程中遇到的问题" class="headerlink" title="hadoop安装过程中遇到的问题"></a>hadoop安装过程中遇到的问题</h3><p>首先我使用的hadoop的cdn版本，比所有的组件都使用社区版，可能会出现版本问题。</p>
<p>其次是自己学习过程中，基本上以单结点，或者三节点集群为主，问题较少</p>
<ol>
<li>关闭防火墙</li>
<li>配置文件要细心</li>
</ol>
<p>namenode format 失败</p>
<p>切换到root用户，否则容易没有权限。</p>
<h3 id="hive工作原理"><a href="#hive工作原理" class="headerlink" title="hive工作原理"></a>hive工作原理</h3><p>1.词法分析/语法分析：使用antlr将SQL语句解析成抽象语法树-AST<br>2.语义分析：从Megastore获取模式信息，验证SQL语句中队表名,列名，以及数据类型的检查和隐式转换，以及Hive提供的函数和用户自定义的函数（UDF/UAF）<br>3.逻辑计划生产：生成逻辑计划-算子树<br>4.逻辑计划优化：对算子树进行优化，包括列剪枝，分区剪枝，谓词下推等<br>5.物理计划生成：将逻辑计划生产包含由MapReduce任务组成的DAG的物理计划<br>6.物理计划执行：将DAG发送到Hadoop集群进行执行<br>7.将查询结果返回</p>
<h3 id="除了Synchronized-还有什么可以保证多线程的安全"><a href="#除了Synchronized-还有什么可以保证多线程的安全" class="headerlink" title="除了Synchronized,还有什么可以保证多线程的安全"></a>除了Synchronized,还有什么可以保证多线程的安全</h3><p>ReentrantLock</p>
<p>ReentrantLock主要利用CAS+AQS队列来实现。它支持公平锁和非公平锁，两者的实现类似。</p>
<p>CAS：Compare and Swap，比较并交换。CAS有3个操作数：内存值V、预期值A、要修改的新值B。当且仅当预期值A和内存值V相同时，将内存值V修改为B，否则什么都不做。该操作是一个原子操作，被广泛的应用在Java的底层实现中。在Java中，CAS主要是由sun.misc.Unsafe这个类通过JNI调用CPU底层指令实现</p>
<p>AbstractQueuedSynchronizer简称AQS</p>
<p>是一个用于构建锁和同步容器的框架。事实上concurrent包内许多类都是基于AQS构建，例如ReentrantLock，Semaphore，CountDownLatch，ReentrantReadWriteLock，FutureTask等。AQS解决了在实现同步容器时设计的大量细节问题。</p>
<p><img src="https://lixiangbetter.github.io/2021/04/16/%E9%9D%A2%E8%AF%95%E9%A2%98%E9%9A%8F%E7%AC%94-21-4-16/20181104201101705.png" alt="img"></p>
<p>AQS使用一个FIFO的队列表示排队等待锁的线程，队列头节点称作“哨兵节点”或者“哑节点”，它不与任何线程关联。其他的节点与等待线程关联，每个节点维护一个等待状态waitStatus</p>
<p>ReentrantLock的基本实现可以概括为：先通过CAS尝试获取锁。如果此时已经有线程占据了锁，那就加入AQS队列并且被挂起。当锁被释放之后，排在CLH队列队首的线程会被唤醒，然后CAS再次尝试获取锁。在这个时候，如果：</p>
<p>非公平锁：如果同时还有另一个线程进来尝试获取，那么有可能会让这个线程抢先获取；</p>
<p>公平锁：如果同时还有另一个线程进来尝试获取，当它发现自己不是在队首的话，就会排到队尾，由队首的线程获取到锁。</p>
<h3 id="union和union-all的区别"><a href="#union和union-all的区别" class="headerlink" title="union和union all的区别"></a>union和union all的区别</h3><p>一、区别1：取结果的交集</p>
<p>1、union: 对两个结果集进行并集操作, 不包括重复行,相当于distinct, 同时进行默认规则的排序;</p>
<p>2、union all: 对两个结果集进行并集操作, 包括重复行, 即所有的结果全部显示, 不管是不是重复;</p>
<p>二、区别2：获取结果后的操作</p>
<p>1、union: 会对获取的结果进行排序操作</p>
<p>2、union all: 不会对获取的结果进行排序操作</p>
<h3 id="你是如何模拟数据？"><a href="#你是如何模拟数据？" class="headerlink" title="你是如何模拟数据？"></a>你是如何模拟数据？</h3><p>写数据上报的接口，接口中，使用kafkaproducer想相应的topic发送数据。</p>
<p>模拟的json数据，通过发送http的post请求。调用数据上报的接口。</p>
]]></content>
      <categories>
        <category>面经</category>
      </categories>
      <tags>
        <tag>大数据开发</tag>
      </tags>
  </entry>
  <entry>
    <title>面试题随笔-21/4/15</title>
    <url>/2021/04/15/%E9%9D%A2%E8%AF%95%E9%A2%98%E9%9A%8F%E7%AC%94-21-4-15/</url>
    <content><![CDATA[<h2 id="今日面经"><a href="#今日面经" class="headerlink" title="今日面经"></a>今日面经</h2><p>链接：<a href="https://www.nowcoder.com/discuss/468594?type=post&amp;order=time&amp;pos=&amp;page=1&amp;channel=-1&amp;source_id=search_post_nctrack&amp;subType=2" target="_blank" rel="noopener">https://www.nowcoder.com/discuss/468594?type=post&amp;order=time&amp;pos=&amp;page=1&amp;channel=-1&amp;source_id=search_post_nctrack&amp;subType=2</a></p>
<h3 id="一个大数-超过了Long的最大值-，你怎么存储"><a href="#一个大数-超过了Long的最大值-，你怎么存储" class="headerlink" title="一个大数(超过了Long的最大值)，你怎么存储"></a>一个大数(超过了Long的最大值)，你怎么存储</h3><p>String</p>
<h3 id="线程安全的理解"><a href="#线程安全的理解" class="headerlink" title="线程安全的理解"></a>线程安全的理解</h3><p><strong>同理，“线程安全”也不是指线程的安全，而是指内存的安全。为什么如此说呢？这和操作系统有关。</strong><br>*<br>*目前主流操作系统都是多任务的，即多个进程同时运行。为了保证安全，每个进程只能访问分配给自己的内存空间，而不能访问别的进程的，这是由操作系统保障的。</p>
<p><strong>在每个进程的内存空间中都会有一块特殊的公共区域，通常称为堆（内存）。进程内的所有线程都可以访问到该区域，这就是造成问题的潜在原因。</strong></p>
<p><strong>所以线程安全指的是，在堆内存中的数据由于可以被任何线程访问到，在没有限制的情况下存在被意外修改的风险。</strong></p>
<h3 id="kafka的消费者组是怎么回事，为什么有消费者组，作用是啥"><a href="#kafka的消费者组是怎么回事，为什么有消费者组，作用是啥" class="headerlink" title="kafka的消费者组是怎么回事，为什么有消费者组，作用是啥"></a>kafka的消费者组是怎么回事，为什么有消费者组，作用是啥</h3><p><strong>Consumer Group 是 Kafka 提供的可扩展且具有容错性的消费者机制</strong>。</p>
<ol>
<li><strong>Consumer Group 下可以有一个或多个 Consumer 实例。</strong>这里的实例可以是一个单独的进程，也可以是同一进程下的线程。在实际场景中，使用进程更为常见一些。</li>
<li><strong>Group ID 是一个字符串，在一个 Kafka 集群中，它标识唯一的一个 Consumer Group。</strong></li>
<li><strong>Consumer Group 下所有实例订阅的主题的单个分区，只能分配给组内的某个 Consumer 实例消费。</strong>这个分区当然也可以被其他的 Group 消费。</li>
</ol>
<h3 id="spark的repartiton底层原理，它与coalesce的区别"><a href="#spark的repartiton底层原理，它与coalesce的区别" class="headerlink" title="spark的repartiton底层原理，它与coalesce的区别"></a>spark的repartiton底层原理，它与coalesce的区别</h3><p>coalesce：</p>
<p>译文：</p>
<p>返回一个经过简化到numPartitions个分区的新RDD。这会导致一个窄依赖，例如：你将1000个分区转换成100个分区，这个过程不会发生shuffle，相反如果10个分区转换成100个分区将会发生shuffle。然而如果你想大幅度合并分区，例如合并成一个分区，这会导致你的计算在少数几个集群节点上计算（言外之意：并行度不够）。为了避免这种情况，你可以将第二个shuffle参数传递一个true，这样会在重新分区过程中多一步shuffle，这意味着上游的分区可以并行运行。</p>
<p>注意：第二个参数shuffle=true，将会产生多于之前的分区数目，例如你有一个个数较少的分区，假如是100，调用coalesce(1000, shuffle = true)将会使用一个  HashPartitioner产生1000个分区分布在集群节点上。这个（对于提高并行度）是非常有用的。</p>
<p>repartition：</p>
<p>返回一个恰好有numPartitions个分区的RDD，可以增加或者减少此RDD的并行度。内部，这将使用shuffle重新分布数据，如果你减少分区数，考虑使用coalesce，这样可以避免执行shuffle。</p>
<h3 id="有16T的有重复的数据求找到重复次数前k多的数"><a href="#有16T的有重复的数据求找到重复次数前k多的数" class="headerlink" title="有16T的有重复的数据求找到重复次数前k多的数"></a>有16T的有重复的数据求找到重复次数前k多的数</h3>]]></content>
      <categories>
        <category>面经</category>
      </categories>
      <tags>
        <tag>大数据开发</tag>
      </tags>
  </entry>
  <entry>
    <title>面试题随笔-21/4/14</title>
    <url>/2021/04/14/%E9%9D%A2%E8%AF%95%E9%A2%98%E9%9A%8F%E7%AC%94-21-4-14/</url>
    <content><![CDATA[<h2 id="今日面经2"><a href="#今日面经2" class="headerlink" title="今日面经2"></a>今日面经2</h2><p>链接：<a href="https://www.nowcoder.com/discuss/639062?type=post&amp;order=time&amp;pos=&amp;page=1&amp;channel=-1&amp;source_id=search_post_nctrack" target="_blank" rel="noopener">https://www.nowcoder.com/discuss/639062?type=post&amp;order=time&amp;pos=&amp;page=1&amp;channel=-1&amp;source_id=search_post_nctrack</a></p>
<h3 id="hashmap中hash的计算"><a href="#hashmap中hash的计算" class="headerlink" title="hashmap中hash的计算"></a>hashmap中hash的计算</h3><p>链接：<a href="https://www.cnblogs.com/mxxct/p/13857097.html" target="_blank" rel="noopener">https://www.cnblogs.com/mxxct/p/13857097.html</a></p>
<ol>
<li>hash 计算</li>
</ol>
<p>JDK1.8<br>HashMap源码</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">static final int hash(Object key) &#123;</span><br><span class="line">    int h;</span><br><span class="line">    return (key == null) ? 0 : (h = key.hashCode()) ^ (h &gt;&gt;&gt; 16);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>右移16位相当于将高16位移入到低16位，再与原hashcode做异或计算（位相同为0，不同为1）可以将高低位二进制特征混合起来 =&gt; 高16位没有发生变化，但是低16位改变了</p>
<p>拿到的hash值会参与hashmap中数组槽位的计算，计算公式：(n - 1) &amp; hash。</p>
<p>高区的16位很有可能会被数组槽位数的二进制码锁屏蔽，<strong>如果我们不做刚才移位异或运算，那么在计算槽位时将丢失高区特征</strong></p>
<p>虽然丢失了高区特征，不同hashcode也可以计算出不同的槽位来，但是如果两个hashcode很接近时，高区的特征差异可能会导致一次哈希碰撞。</p>
<ol start="2">
<li>使用异或运算的原因</li>
</ol>
<p>异或运算能更好的保留各部分的特征，如果采用 &amp; 运算计算出来的值会向0靠拢，采用 | 运算计算出来的值会向1靠拢</p>
<ol start="3">
<li>为什么槽位数必须使用2^n / 为什么要 &amp;length-1</li>
</ol>
<p>为了让哈希后的结果更加均匀，减少hash碰撞</p>
<ol start="4">
<li>扩容后Hash值计算</li>
</ol>
<p>length * 2，即新增的bit位是1，在 (n - 1) &amp; hash 时，只需要判断新增加的这一个bit位，如果是0的话，说明索引不变，如果变成1了，索引变成 原索引+扩容前的容量大小</p>
<h3 id="gc对象生命周期"><a href="#gc对象生命周期" class="headerlink" title="gc对象生命周期"></a>gc对象生命周期</h3><p>在 Java 中，堆被划分成两个不同的区域：新生代 ( Young )、老年代 ( Old )。新生代 ( Young ) 又被划分为</p>
<p>三个区域：Eden、From Survivor、To Survivor。</p>
<p>这样划分的目的是为了使 JVM 能够更好的管理堆内存中的对象，包括内存的分配以及回收。</p>
<p>老年代：</p>
<p>老年代主要存放在程序中生命周期长的对象。老年代因为其中对象比较稳定，所以Major GC不会频繁的执行。在进行Major GC之前通常都会先执行一次Minor GC,Minor GC执行完后可能会有新生代的对象晋升到老年代之中，然后导致老年代的空间不足才触发Major GC。</p>
<p>永久代：java7之前</p>
<p>永久存储区是一个常驻内存区域，用于存放JDK自身所携带的Class,Interface的元数据，也就是说它存储的是运行环境必须的类信息，被装载进此区域的数据是不会被垃圾回收器回收掉的，关闭JVM才会释放此区域所占用的内存。</p>
<p>java8以后是元空间。</p>
<p>元空间的本质和永久代类似，都是对JVM规范中方法区的实现。不过元空间与永久代之间最大的区别在于：元空间并不在虚拟机中，而是使用本地内存。因此，默认情况下，元空间的大小仅受本地内存限制，但可以通过以下参数来指定元空间的大小。</p>
<h3 id="gc判断对象存活的方法"><a href="#gc判断对象存活的方法" class="headerlink" title="gc判断对象存活的方法"></a>gc判断对象存活的方法</h3><h4 id="引用计数"><a href="#引用计数" class="headerlink" title="引用计数"></a>引用计数</h4><ol>
<li>优点：实现简单</li>
<li>缺点：难以解决对象循环引用的问题</li>
</ol>
<h4 id="可达性分析"><a href="#可达性分析" class="headerlink" title="可达性分析"></a>可达性分析</h4><ol>
<li>通过一系列的称为“GC Roots”的对象作为起始点，从这些节点考试向下探索，搜索所走过的路径称为“引用链”，当一个对象到GC Roots没有任何引用链相连（用图论的话来说，就是从GC Roots到这个对象不可达）时，则证明此对象是不可用的。</li>
</ol>
<h3 id="gc-root包括哪些？"><a href="#gc-root包括哪些？" class="headerlink" title="gc root包括哪些？"></a>gc root包括哪些？</h3><ol>
<li>虚拟机栈（栈帧中的本地变量表）中引用的对象；</li>
<li>方法区中类静态属性引用的对象；</li>
<li>方法区中常量引用的对象；</li>
<li>本地方法栈中JNI（即一般说的Native方法）引用的对象。</li>
</ol>
<h3 id="给你5张牌，其中包含两张大小王，判断是否是顺子"><a href="#给你5张牌，其中包含两张大小王，判断是否是顺子" class="headerlink" title="给你5张牌，其中包含两张大小王，判断是否是顺子"></a>给你5张牌，其中包含两张大小王，判断是否是顺子</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">isStraight</span><span class="params">(<span class="keyword">int</span>[] nums)</span> </span>&#123;</span><br><span class="line">        Set&lt;Integer&gt; repeat = <span class="keyword">new</span> HashSet&lt;&gt;();</span><br><span class="line">        <span class="keyword">int</span> max = <span class="number">0</span>, min = <span class="number">14</span>;</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> num : nums) &#123;</span><br><span class="line">            <span class="keyword">if</span>(num == <span class="number">0</span>) <span class="keyword">continue</span>; <span class="comment">// 跳过大小王</span></span><br><span class="line">            max = Math.max(max, num); <span class="comment">// 最大牌</span></span><br><span class="line">            min = Math.min(min, num); <span class="comment">// 最小牌</span></span><br><span class="line">            <span class="keyword">if</span>(repeat.contains(num)) <span class="keyword">return</span> <span class="keyword">false</span>; <span class="comment">// 若有重复，提前返回 false</span></span><br><span class="line">            repeat.add(num); <span class="comment">// 添加此牌至 Set</span></span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> max - min &lt; <span class="number">5</span>; <span class="comment">// 最大牌 - 最小牌 &lt; 5 则可构成顺子</span></span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="链表反转"><a href="#链表反转" class="headerlink" title="链表反转"></a>链表反转</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> ListNode <span class="title">reverseList</span><span class="params">(ListNode head)</span> </span>&#123;</span><br><span class="line">        ListNode prev = <span class="keyword">null</span>;</span><br><span class="line">        ListNode curr = head;</span><br><span class="line">        <span class="keyword">while</span> (curr != <span class="keyword">null</span>) &#123;</span><br><span class="line">            ListNode next = curr.next;</span><br><span class="line">            curr.next = prev;</span><br><span class="line">            prev = curr;</span><br><span class="line">            curr = next;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> prev;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="二叉树的遍历"><a href="#二叉树的遍历" class="headerlink" title="二叉树的遍历"></a>二叉树的遍历</h3><p>前中后序递归遍历</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 前序遍历</span></span><br><span class="line"><span class="comment">     * 递归</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">preOrder</span><span class="params">(BinaryNode&lt;AnyType&gt; Node)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="keyword">if</span> (Node != <span class="keyword">null</span>)</span><br><span class="line">  &#123;</span><br><span class="line">    System.out.print(Node.element + <span class="string">" "</span>);</span><br><span class="line">    preOrder(Node.left);</span><br><span class="line">    preOrder(Node.right);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 中序遍历</span></span><br><span class="line"><span class="comment">     * 递归</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">midOrder</span><span class="params">(BinaryNode&lt;AnyType&gt; Node)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="keyword">if</span> (Node != <span class="keyword">null</span>)</span><br><span class="line">  &#123;</span><br><span class="line">    midOrder(Node.left);</span><br><span class="line">    System.out.print(Node.element + <span class="string">" "</span>);</span><br><span class="line">    midOrder(Node.right);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 后序遍历</span></span><br><span class="line"><span class="comment">     * 递归</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">posOrder</span><span class="params">(BinaryNode&lt;AnyType&gt; Node)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  <span class="keyword">if</span> (Node != <span class="keyword">null</span>)</span><br><span class="line">  &#123;</span><br><span class="line">    posOrder(Node.left);</span><br><span class="line">    posOrder(Node.right);</span><br><span class="line">    System.out.print(Node.element + <span class="string">" "</span>);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>层序遍历</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">     * 层序遍历</span></span><br><span class="line"><span class="comment">     * 递归</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">levelOrder</span><span class="params">(BinaryNode&lt;AnyType&gt; Node)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">if</span> (Node == <span class="keyword">null</span>) &#123;</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">int</span> depth = depth(Node);</span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt;= depth; i++) &#123;</span><br><span class="line">    levelOrder(Node, i);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">levelOrder</span><span class="params">(BinaryNode&lt;AnyType&gt; Node, <span class="keyword">int</span> level)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">if</span> (Node == <span class="keyword">null</span> || level &lt; <span class="number">1</span>) &#123;</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (level == <span class="number">1</span>) &#123;</span><br><span class="line">    System.out.print(Node.element + <span class="string">"  "</span>);</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 左子树</span></span><br><span class="line">  levelOrder(Node.left, level - <span class="number">1</span>);</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 右子树</span></span><br><span class="line">  levelOrder(Node.right, level - <span class="number">1</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">depth</span><span class="params">(BinaryNode&lt;AnyType&gt; Node)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">if</span> (Node == <span class="keyword">null</span>) &#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">int</span> l = depth(Node.left);</span><br><span class="line">  <span class="keyword">int</span> r = depth(Node.right);</span><br><span class="line">  <span class="keyword">if</span> (l &gt; r) &#123;</span><br><span class="line">    <span class="keyword">return</span> l + <span class="number">1</span>;</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="keyword">return</span> r + <span class="number">1</span>;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/*</span></span><br><span class="line"><span class="comment">     * 层序遍历</span></span><br><span class="line"><span class="comment">     * 非递归</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">levelOrder1</span><span class="params">(BinaryNode&lt;AnyType&gt; Node)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">if</span> (Node == <span class="keyword">null</span>) &#123;</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  BinaryNode&lt;AnyType&gt; binaryNode;</span><br><span class="line">  Queue&lt;BinaryNode&gt; queue = <span class="keyword">new</span> LinkedList&lt;&gt;();</span><br><span class="line">  queue.add(Node);</span><br><span class="line"></span><br><span class="line">  <span class="keyword">while</span> (queue.size() != <span class="number">0</span>) &#123;</span><br><span class="line">    binaryNode = queue.poll();</span><br><span class="line"></span><br><span class="line">    System.out.print(binaryNode.element + <span class="string">"  "</span>);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (binaryNode.left != <span class="keyword">null</span>) &#123;</span><br><span class="line">      queue.offer(binaryNode.left);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (binaryNode.right != <span class="keyword">null</span>) &#123;</span><br><span class="line">      queue.offer(binaryNode.right);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>前中后序非递归遍历</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 前序遍历</span></span><br><span class="line"><span class="comment">     * 非递归</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">preOrder1</span><span class="params">(BinaryNode&lt;AnyType&gt; Node)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  Stack&lt;BinaryNode&gt; stack = <span class="keyword">new</span> Stack&lt;&gt;();</span><br><span class="line">  <span class="keyword">while</span>(Node != <span class="keyword">null</span> || !stack.empty())</span><br><span class="line">  &#123;</span><br><span class="line">    <span class="keyword">while</span>(Node != <span class="keyword">null</span>)</span><br><span class="line">    &#123;</span><br><span class="line">      System.out.print(Node.element + <span class="string">"   "</span>);</span><br><span class="line">      stack.push(Node);</span><br><span class="line">      Node = Node.left;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>(!stack.empty())</span><br><span class="line">    &#123;</span><br><span class="line">      Node = stack.pop();</span><br><span class="line">      Node = Node.right;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 中序遍历</span></span><br><span class="line"><span class="comment">     * 非递归</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">midOrder1</span><span class="params">(BinaryNode&lt;AnyType&gt; Node)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  Stack&lt;BinaryNode&gt; stack = <span class="keyword">new</span> Stack&lt;&gt;();</span><br><span class="line">  <span class="keyword">while</span>(Node != <span class="keyword">null</span> || !stack.empty())</span><br><span class="line">  &#123;</span><br><span class="line">    <span class="keyword">while</span> (Node != <span class="keyword">null</span>)</span><br><span class="line">    &#123;</span><br><span class="line">      stack.push(Node);</span><br><span class="line">      Node = Node.left;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span>(!stack.empty())</span><br><span class="line">    &#123;</span><br><span class="line">      Node = stack.pop();</span><br><span class="line">      System.out.print(Node.element + <span class="string">"   "</span>);</span><br><span class="line">      Node = Node.right;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">     * 后序遍历</span></span><br><span class="line"><span class="comment">     * 非递归</span></span><br><span class="line"><span class="comment">     */</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">posOrder1</span><span class="params">(BinaryNode&lt;AnyType&gt; Node)</span></span></span><br><span class="line"><span class="function"></span>&#123;</span><br><span class="line">  Stack&lt;BinaryNode&gt; stack1 = <span class="keyword">new</span> Stack&lt;&gt;();</span><br><span class="line">  Stack&lt;Integer&gt; stack2 = <span class="keyword">new</span> Stack&lt;&gt;();</span><br><span class="line">  <span class="keyword">int</span> i = <span class="number">1</span>;</span><br><span class="line">  <span class="keyword">while</span>(Node != <span class="keyword">null</span> || !stack1.empty())</span><br><span class="line">  &#123;</span><br><span class="line">    <span class="keyword">while</span> (Node != <span class="keyword">null</span>)</span><br><span class="line">    &#123;</span><br><span class="line">      stack1.push(Node);</span><br><span class="line">      stack2.push(<span class="number">0</span>);</span><br><span class="line">      Node = Node.left;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">while</span>(!stack1.empty() &amp;&amp; stack2.peek() == i)</span><br><span class="line">    &#123;</span><br><span class="line">      stack2.pop();</span><br><span class="line">      System.out.print(stack1.pop().element + <span class="string">"   "</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span>(!stack1.empty())</span><br><span class="line">    &#123;</span><br><span class="line">      stack2.pop();</span><br><span class="line">      stack2.push(<span class="number">1</span>);</span><br><span class="line">      Node = stack1.peek();</span><br><span class="line">      Node = Node.right;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>面经</category>
      </categories>
      <tags>
        <tag>大数据开发</tag>
      </tags>
  </entry>
  <entry>
    <title>面试题随笔-21/4/12</title>
    <url>/2021/04/12/%E9%9D%A2%E8%AF%95%E9%A2%98%E9%9A%8F%E7%AC%94-21-4-12/</url>
    <content><![CDATA[<h2 id="今日面经"><a href="#今日面经" class="headerlink" title="今日面经"></a>今日面经</h2><p>链接：<a href="https://www.nowcoder.com/discuss/636561?type=post&amp;order=time&amp;pos=&amp;page=1&amp;channel=-1&amp;source_id=search_post_nctrack&amp;subType=2" target="_blank" rel="noopener">https://www.nowcoder.com/discuss/636561?type=post&amp;order=time&amp;pos=&amp;page=1&amp;channel=-1&amp;source_id=search_post_nctrack&amp;subType=2</a></p>
<h3 id="线程和进程的区别"><a href="#线程和进程的区别" class="headerlink" title="线程和进程的区别"></a>线程和进程的区别</h3><p>1、进程是一段正在执行的程序，是资源分配的基本单元，而线程是CPU调度的基本单元。<br>2、进程间相互独立进程，进程之间不能共享资源，一个进程至少有一个线程，同一进程的各线程共享整个进程的资源（寄存器、堆栈、上下文）。<br>3、线程的创建和切换开销比进程小。</p>
<h3 id="死锁的条件"><a href="#死锁的条件" class="headerlink" title="死锁的条件"></a>死锁的条件</h3><p>互斥条件</p>
<p>不可剥夺条件</p>
<p>请求与保持条件</p>
<p>循环等待条件</p>
<h3 id="多线程和线程池的一些问题"><a href="#多线程和线程池的一些问题" class="headerlink" title="多线程和线程池的一些问题"></a>多线程和线程池的一些问题</h3><p>链接：<a href="https://blog.csdn.net/asd136912/article/details/87908629" target="_blank" rel="noopener">https://blog.csdn.net/asd136912/article/details/87908629</a></p>
<h4 id="使用Thread弊端"><a href="#使用Thread弊端" class="headerlink" title="使用Thread弊端"></a>使用Thread弊端</h4><p>Thread的弊端如下：</p>
<ol>
<li>每次new Thread新建对象性能差。</li>
<li>线程缺乏统一管理，可能无限制新建线程，相互之间竞争，及可能占用过多系统资源导致死机或oom。</li>
<li>缺乏更多功能，如定时执行、定期执行、线程中断。</li>
</ol>
<h4 id="线程池背景及优势"><a href="#线程池背景及优势" class="headerlink" title="线程池背景及优势"></a>线程池背景及优势</h4><p>什么时候使用线程池？</p>
<ul>
<li><p>单个任务处理时间比较短</p>
</li>
<li><p>需要处理的任务数量很大</p>
</li>
</ul>
<p>使用线程池的好处:</p>
<ul>
<li>降低资源消耗。通过重复利用已创建的线程降低线程创建和销毁造成的消耗。</li>
<li>提高响应速度。当任务到达时，任务可以不需要的等到线程创建就能立即执行。</li>
<li>提高线程的可管理性。线程是稀缺资源，如果无限制的创建，不仅会消耗系统资源，还会降低系统的稳定性，使用线程池可以进行统一的分配，调优和监控。</li>
</ul>
<h4 id="Executor解析"><a href="#Executor解析" class="headerlink" title="Executor解析"></a>Executor解析</h4><h5 id="线程池原理"><a href="#线程池原理" class="headerlink" title="线程池原理"></a>线程池原理</h5><p>Java通过Executors提供四种线程池，分别为：</p>
<ul>
<li>newCachedThreadPool创建一个可缓存线程池，如果线程池长度超过处理需要，可灵活回收空闲线程，若无可回收，则新建线程。</li>
<li>newFixedThreadPool 创建一个定长线程池，可控制线程最大并发数，超出的线程会在队列中等待。</li>
<li>newScheduledThreadPool 创建一个定长线程池，支持定时及周期性任务执行。</li>
<li>newSingleThreadExecutor 创建一个单线程化的线程池，它只会用唯一的工作线程来执行任务，保证所有任务按照指定顺序(FIFO, LIFO, 优先级)执行。</li>
</ul>
<p>核心参数的作用：</p>
<ul>
<li><code>corePoolSize</code> 为线程池的基本大小。</li>
<li><code>maximumPoolSize</code> 为线程池最大线程大小。</li>
<li><code>keepAliveTime</code> 和 <code>unit</code> 则是线程空闲后的存活时间。</li>
<li><code>workQueue</code> 用于存放任务的阻塞队列。</li>
<li><code>handler</code> 当队列和最大线程池都满了之后的饱和策略。</li>
</ul>
<p>线程池运行状态：</p>
<ol>
<li>RUNNING ：能接受新提交的任务，并且也能处理阻塞队列中的任务；</li>
<li>SHUTDOWN：关闭状态，不再接受新提交的任务，但却可以继续处理阻塞队列中已保存的任务。在线程池处于 RUNNING 状态时，调用shutdown()方法会使线程池进入到该状态。（finalize() 方法在执行过程中也会调用shutdown()方法进入该状态）；</li>
<li>STOP：不能接受新任务，也不处理队列中的任务，会中断正在处理任务的线程。在线程池处于 RUNNING 或 SHUTDOWN 状态时，调用 shutdownNow() 方法会使线程池进入到该状态；</li>
<li>TIDYING：如果所有的任务都已终止了，workerCount (有效线程数) 为0，线程池进入该状态后会调用 terminated() 方法进入TERMINATED 状态。</li>
<li>TERMINATED：在terminated() 方法执行完后进入该状态，默认terminated()方法中什么也没有做。</li>
</ol>
<p>进入TERMINATED的条件如下：</p>
<ul>
<li>线程池不是RUNNING状态；</li>
<li>线程池状态不是TIDYING状态或TERMINATED状态；</li>
<li>如果线程池状态是SHUTDOWN并且workerQueue为空；</li>
<li>workerCount为0；</li>
<li>设置TIDYING状态成功。</li>
</ul>
<h5 id="配置线程池"><a href="#配置线程池" class="headerlink" title="配置线程池"></a>配置线程池</h5><p>流程聊完了再来看看上文提到了几个核心参数应该如何配置呢？</p>
<p>有一点是肯定的，线程池肯定是不是越大越好。</p>
<p>通常我们是需要根据这批任务执行的性质来确定的。</p>
<ul>
<li>IO 密集型任务：由于线程并不是一直在运行，所以可以尽可能的多配置线程，比如 CPU 个数 * 2</li>
<li>CPU 密集型任务（大量复杂的运算）应当分配较少的线程，比如 CPU 个数相当的大小。</li>
</ul>
<p>当然这些都是经验值，最好的方式还是根据实际情况测试得出最佳配置。</p>
<h5 id="优雅关闭线程池"><a href="#优雅关闭线程池" class="headerlink" title="优雅关闭线程池"></a>优雅关闭线程池</h5><p>有运行任务自然也有关闭任务，从上文提到的 5 个状态就能看出如何来关闭线程池。</p>
<p>其实无非就是两个方法 shutdown()/shutdownNow()。</p>
<p>但他们有着重要的区别：</p>
<p>shutdown() 执行后停止接受新任务，会把队列的任务执行完毕。<br>shutdownNow() 也是停止接受新任务，但会中断所有的任务，将线程池状态变为 stop。</p>
<p><code>pool.awaitTermination(1, TimeUnit.SECONDS)</code> 会每隔一秒钟检查一次是否执行完毕（状态为 <code>TERMINATED</code>），当从 while 循环退出时就表明线程池已经完全终止了。</p>
<h4 id="Executor存在问题"><a href="#Executor存在问题" class="headerlink" title="Executor存在问题"></a>Executor存在问题</h4><p>使用Executors创建线程池可能会导致OOM(OutOfMemory ,内存溢出)。</p>
<p><code>newFixedThreadPool</code>中创建<code>LinkedBlockingQueue</code>时，并未指定容量。此时，<code>LinkedBlockingQueue</code>就是一个无边界队列，对于一个无边界队列来说，是可以不断的向队列中加入任务的，这种情况下就有可能因为任务过多而导致内存溢出问题。</p>
<p>解决方法</p>
<p>避免使用Executors创建线程池，主要是避免使用其中的默认实现，那么我们可以自己直接调用<code>ThreadPoolExecutor</code>的构造函数来自己创建线程池。</p>
<h3 id="jvm-内存模型"><a href="#jvm-内存模型" class="headerlink" title="jvm 内存模型"></a>jvm 内存模型</h3><p><strong>程序计数器(线程私有)：</strong></p>
<p>计数器记录的是虚拟机字节码指令的地址。存放下一条指令所在单元的地址的地方</p>
<p><strong>java 虚拟机栈</strong><br>也是线程私有的。<br>每个方法在执行的时候也会创建一个栈帧，存储了局部变量，操作数，动态链接，方法返回地址。</p>
<p><strong>本地方法栈（线程私有）</strong><br>和虚拟机栈类似，主要为虚拟机使用到的Native方法服务。也会抛出StackOverflowError 和OutOfMemoryError。</p>
<p><strong>Java堆（线程共享）</strong><br>被所有线程共享的一块内存区域，在虚拟机启动的时候创建，用于存放对象实例。</p>
<p><strong>方法区（线程共享）</strong><br>被所有方法线程共享的一块内存区域。<br>用于存储已经被虚拟机加载的类信息，常量，静态变量等。<br>这个区域的内存回收目标主要针对常量池的回收和堆类型的卸载。</p>
<h3 id="jvm调优"><a href="#jvm调优" class="headerlink" title="jvm调优"></a>jvm调优</h3><h4 id="堆大小设置"><a href="#堆大小设置" class="headerlink" title="堆大小设置"></a>堆大小设置</h4><p>年轻代的设置很关键<br>JVM中最大堆大小有三方面限制：相关操作系统的数据模型（32-bt还是64-bit）限制；系统的可用虚拟内存限制；系统的可用物理内存限制。<br>典型设置：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">java -Xmx3550m -Xms3550m -Xmn2g –Xss128k</span><br><span class="line"></span><br><span class="line">-Xmx3550m：设置JVM最大可用内存为3550M。</span><br><span class="line"></span><br><span class="line">-Xms3550m：设置JVM初始内存为3550m。此值可以设置与-Xmx相同，以避免每次垃圾回收完成后JVM重新分配内存。</span><br><span class="line"></span><br><span class="line">-Xmn2g：设置年轻代大小为2G。整个堆大小=年轻代大小 + 年老代大小 + 持久代大小。持久代一般固定大小为64m，所以增大年轻代后，将会减小年老代大小。此值对系统性能影响较大，Sun官方推荐配置为整个堆的3/8。</span><br><span class="line"></span><br><span class="line">-Xss128k：设置每个线程的堆栈大小。JDK5.0以后每个线程堆栈大小为1M，以前每个线程堆栈大小为256K。更具应用的线程所需内存大小进行调整。在相同物理内存下，减小这个值能生成更多的线程。但是操作系统对一个进程内的线程数还是有限制的，不能无限生成，经验值在3000~5000左右。</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">java -Xmx3550m -Xms3550m -Xss128k -XX:NewRatio=4 -XX:SurvivorRatio=4 -XX:MaxPermSize=16m -XX:MaxTenuringThreshold=0</span><br><span class="line"></span><br><span class="line">-XX:NewRatio=4:设置年轻代（包括Eden和两个Survivor区）与年老代的比值（除去持久代）。设置为4，则年轻代与年老代所占比值为1：4，年轻代占整个堆栈的1/5</span><br><span class="line"></span><br><span class="line">-XX:SurvivorRatio=4：设置年轻代中Eden区与Survivor区的大小比值。设置为4，则两个Survivor区与一个Eden区的比值为2:4，一个Survivor区占整个年轻代的1/6</span><br><span class="line"></span><br><span class="line">-XX:MaxPermSize=16m:设置持久代大小为16m。</span><br><span class="line"></span><br><span class="line">-XX:MaxTenuringThreshold=0：设置垃圾最大年龄。如果设置为0的话，则年轻代对象不经过Survivor区，直接进入年老代。对于年老代比较多的应用，可以提高效率。如果将此值设置为一个较大值，则年轻代对象会在Survivor区进行多次复制，这样可以增加对象再年轻代的存活时间，增加在年轻代即被回收的概论。</span><br></pre></td></tr></table></figure>

<p><strong>堆设置</strong></p>
<p>-Xms:初始堆大小<br>-Xmx:最大堆大小<br>-XX:NewSize=n:设置年轻代大小<br>-XX:NewRatio=n:设置年轻代和年老代的比值。如:为3，表示年轻代与年老代比值为1：3，年轻代占整个年轻代年老代和的1/4<br>-XX:SurvivorRatio=n:年轻代中Eden区与两个Survivor区的比值。注意Survivor区有两个。如：3，表示Eden：Survivor=3：2，一个Survivor区占整个年轻代的1/5<br>-XX:MaxPermSize=n:设置持久代大小</p>
<p><strong>收集器设置</strong></p>
<p>-XX:+UseSerialGC:设置串行收集器<br>-XX:+UseParallelGC:设置并行收集器<br>-XX:+UseParalledlOldGC:设置并行年老代收集器<br>-XX:+UseConcMarkSweepGC:设置并发收集器</p>
<h3 id="http和https的主要区别"><a href="#http和https的主要区别" class="headerlink" title="http和https的主要区别"></a>http和https的主要区别</h3><p>https协议需要到CA申请证书，一般免费证书较少，因而需要一定费用。</p>
<p>http是超文本传输协议，信息是明文传输，https则是具有安全性的ssl/tls加密传输协议。</p>
<h3 id="tcp协议"><a href="#tcp协议" class="headerlink" title="tcp协议"></a>tcp协议</h3><p>TCP（Transmission Control Protocol 传输控制协议）是一种面向连接的、可靠的、基于字节流的传输层通信协议.</p>
<h3 id="数组链表栈队列的区别"><a href="#数组链表栈队列的区别" class="headerlink" title="数组链表栈队列的区别"></a>数组<a href="https://www.nowcoder.com/jump/super-jump/word?word=链表" target="_blank" rel="noopener">链表</a>栈队列的区别</h3><p>联系：</p>
<ul>
<li>这四种数据结构都是线性表数据结构。</li>
</ul>
<p>区别：</p>
<ul>
<li>数组与链表是更加偏向数据存储方式的概念，数组在连续的空间中存储数据，随机读取效率高，但是数据添加删除的效率较低； 而链表可以在非连续的空间中存储数据，随机访问效率低，数据添加删除效率高。</li>
<li>队列和栈是描述数据存取方式的概念，队列是先进先出，而堆栈是后进先出；队列和栈都可以使用数组或者链表实现。</li>
</ul>
<h3 id="如何返回链表倒数第k个元素"><a href="#如何返回链表倒数第k个元素" class="headerlink" title="如何返回链表倒数第k个元素"></a>如何返回<a href="https://www.nowcoder.com/jump/super-jump/word?word=链表" target="_blank" rel="noopener">链表</a>倒数第k个元素</h3><p>双指针法：</p>
<p>让两个结点，同时指向首结点，其中一个结点向后移动k个位置。</p>
<p>然后两个结点同时后移，当前一个结点到末尾时，后一个结点就是倒数第k个元素。</p>
<h3 id="kafka的ISR机制"><a href="#kafka的ISR机制" class="headerlink" title="kafka的ISR机制"></a>kafka的ISR机制</h3><p>kafka的ISR机制被成为“不丢消息”机制。在说ISR机制前，先讲一下kafka的副本（replica）。</p>
<p><strong>kafka的Replica</strong></p>
<p>1.kafka的topic可以设置有N个副本（replica），副本数最好要小于broker的数量，也就是要保证一个broker上的replica最多有一个，所以可以用broker id指定Partition replica。</p>
<p>2.创建副本的单位是topic的分区，每个分区有1个leader和0到多个follower，我们把多个replica分为Lerder replica和follower replica。</p>
<p>3.当producer在向partition中写数据时，根据ack机制，默认ack=1，只会向leader中写入数据，然后leader中的数据会复制到其他的replica中，follower会周期性的从leader中pull数据，但是对于数据的读写操作都在leader replica中，follower副本只是当leader副本挂了后才重新选取leader，follower并不向外提供服务。</p>
<p><strong>kafka的“同步”</strong></p>
<p>kafka不是完全同步，也不是完全异步，是一种特殊的ISR（In Sync Replica）</p>
<p>1.leader会维持一个与其保持同步的replica集合，该集合就是ISR，每一个partition都有一个ISR，它时有leader动态维护。</p>
<p>2.我们要保证kafka不丢失message，就要保证ISR这组集合存活（至少有一个存活），并且消息commit成功。</p>
<h3 id="kafka原理"><a href="#kafka原理" class="headerlink" title="kafka原理"></a>kafka原理</h3><p>Kafka是最初由Linkedin公司开发，是一个分布式、支持分区的（partition）、多副本的（replica），基于zookeeper协调的分布式消息系统。</p>
<p><strong>Kafka的特性:</strong></p>
<p>- 高吞吐量、低延迟：kafka每秒可以处理几十万条消息，它的延迟最低只有几毫秒，每个topic可以分多个partition, consumer group 对partition进行consume操作。</p>
<p>- 可扩展性：kafka集群支持热扩展</p>
<p>- 持久性、可靠性：消息被持久化到本地磁盘，并且支持数据备份防止数据丢失</p>
<p>- 容错性：允许集群中节点失败（若副本数量为n,则允许n-1个节点失败）</p>
<p>- 高并发：支持数千个客户端同时读写</p>
<h3 id="求子数组和最大值"><a href="#求子数组和最大值" class="headerlink" title="求子数组和最大值"></a>求子数组和最大值</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Solution</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">maxSubArray</span><span class="params">(<span class="keyword">int</span>[] nums)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">int</span> res = nums[<span class="number">0</span>];</span><br><span class="line">        <span class="keyword">for</span>(<span class="keyword">int</span> i = <span class="number">1</span>; i &lt; nums.length; i++) &#123;</span><br><span class="line">            nums[i] += Math.max(nums[i - <span class="number">1</span>], <span class="number">0</span>);</span><br><span class="line">            res = Math.max(res, nums[i]);</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="keyword">return</span> res;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="mapreduce的原理"><a href="#mapreduce的原理" class="headerlink" title="mapreduce的原理"></a>mapreduce的原理</h3><ol>
<li><p>一个mr程序启动的时候，最先启动的是MRAppMaster，MRAppMaster启动后根据本次job的描述信息，计算出需要的maptask实例数量，然后向集群申请机器启动相应数量的maptask进程</p>
</li>
<li><p>maptask进程启动之后，根据给定的数据切片范围进行数据处理，主体流程为：</p>
</li>
</ol>
<ul>
<li>利用客户指定的inputformat来获取RecordReader读取数据，形成输入KV对</li>
<li>将输入KV对传递给客户定义的map()方法，做逻辑运算，并将map()方法输出的KV对收集到缓存</li>
<li>将缓存中的KV对按照K分区排序后不断溢写到磁盘文件</li>
</ul>
<ol start="3">
<li><p>MRAppMaster监控到所有maptask进程任务完成之后，会根据客户指定的参数启动相应数量的reducetask进程，并告知reducetask进程要处理的数据范围（数据分区）</p>
</li>
<li><p>Reducetask进程启动之后，根据MRAppMaster告知的待处理数据所在位置，从若干台maptask运行所在机器上获取到若干个maptask输出结果文件，并在本地进行重新归并排序，然后按照相同key的KV为一个组，调用客户定义的reduce()方法进行逻辑运算，并收集运算输出的结果KV，然后调用客户指定的outputformat将结果数据输出到外部存储。</p>
</li>
</ol>
]]></content>
      <categories>
        <category>面经</category>
      </categories>
      <tags>
        <tag>大数据开发</tag>
      </tags>
  </entry>
  <entry>
    <title>面试题随笔-21/4/11</title>
    <url>/2021/04/11/%E9%9D%A2%E8%AF%95%E9%A2%98%E9%9A%8F%E7%AC%94-21-4-11/</url>
    <content><![CDATA[<h2 id="今日面经"><a href="#今日面经" class="headerlink" title="今日面经"></a>今日面经</h2><p>链接：<a href="https://www.nowcoder.com/discuss/614626?source_id=profile_create_nctrack&amp;channel=-1" target="_blank" rel="noopener">https://www.nowcoder.com/discuss/614626?source_id=profile_create_nctrack&amp;channel=-1</a></p>
<h3 id="操作系统中线程、进程联系和区别，上下文切换时发生了什么"><a href="#操作系统中线程、进程联系和区别，上下文切换时发生了什么" class="headerlink" title="操作系统中线程、进程联系和区别，上下文切换时发生了什么"></a>操作系统中线程、进程联系和区别，上下文切换时发生了什么</h3><p>1、首先，需要搞明白什么是上下文切换？（面试题）<br>上下文切换就是从当前执行任务切换到另一个任务执行的过程。但是，为了确保下次能从正确的位置继续执行，在切换之前，会保存上一个任务的状态。</p>
<p>2、 然后，需要明白进程与线程的区别？（网上很多，这里简单说明）<br>1).线程是进程的一部分。 进程是表示资源分配的基本单位，又是调度运行的基本单位，是程序执行的一个实例; 线程是进程中执行运算的最小单位，即执行处理机调度的基本单位，是进程中的一个执行流。<br>2).内存空间不同。 每一个进程拥有自己独立的内存空间，而线程共享进程的内存空间。<br>进程上下文切换与线程上下文切换最主要的区别就是线程的切换虚拟空间内存是相同的（因为都是属于自己的进程），但是，进程切换的虚拟空间内存则是不同的。<br>同时，这两种上下文切换的处理都是通过操作系统内核来完成的。内核的这种切换过程伴随的最显著的性能损耗是将寄存器中的内容切换出。</p>
<p>3、线程上下文切换比进程上下文切换快的多。<br>补充：多线程是如何实现的？<br>主要是CPU通过给每个线程分配CPU时间片来实现多线程的。即使是单核处理器（CPU）也可以执行多线程处理。<br>时间片是CPU分配给各个线程的时间，因为时间片非常短，所以CPU通过不停地切换线程执行，让我们感觉多个线程时同时执行的，时间片一般是几十毫秒（ms）。</p>
<p>4、在进行上下文切换时，CPU通过时间片分配算法来循环执行任务，Java中的时间片分配算法有哪些？<br>最简单最常用的就是基于时间片轮转调度算法。 时间片轮转调度算法是非常公平的处理机分配方式，可以使就绪队列的每个进程每次仅运行一个时间片。<br>原理：在时间片轮转调度算法中，系统根据先来先服务的原则，将所有的就绪进程排成一个就绪队列，并且每隔一段时间产生一次中断，激活系统中的进程调度程序，完成一次处理机调度，把处理机分配给就绪队列队首进程，让其执行指令。当时间片结束或进程执行结束，系统再次将CPU分配给队首进程。</p>
<p>上下文的切换流程如下</p>
<p>（1）挂起一个进程，将这个进程在CPU中的状态（上下文信息）存储于内存的PCB中。</p>
<p>（2）在PCB中检索下一个进程的上下文并将其在CPU的寄存器中恢复。</p>
<p>（3）跳转到<a href="https://baike.baidu.com/item/程序计数器/3219536" target="_blank" rel="noopener">程序计数器</a>所指向的位置（即跳转到进程被中断时的代码行）并恢复该进程。 </p>
<p>时间片轮转方式使多个任务在同一CPU上的执行有了可能。</p>
<h3 id="协程说一下"><a href="#协程说一下" class="headerlink" title="协程说一下"></a>协程说一下</h3><p>概念</p>
<p>协程，又称微线程，纤程，英文名Coroutine。协程看上去也是子程序，但执行过程中，在子程序内部可中断，然后转而执行别的子程序，在适当的时候再返回来接着执行。</p>
<p><strong>进程</strong> 拥有自己独立的堆和栈，既不共享堆，亦不共享栈，进程由操作系统调度。<br><strong>线程</strong> 拥有自己独立的栈和共享的堆，共享堆，不共享栈，线程亦由操作系统调度。<br><strong>协程</strong> 和线程一样共享堆，不共享栈，协程由程序员在协程的代码里显示调度。</p>
<p>协程和线程区别</p>
<p>那和多线程比，协程最大的优势就是协程极高的执行效率。因为子程序切换不是线程切换，<br>而是由程序自身控制，因此，没有线程切换的开销，和多线程比，线程数量越多，协程的<br>性能优势就越明显。</p>
<p>第二大优势就是不需要多线程的锁机制，因为只有一个线程，也不存在同时写变量冲突，<br>在协程中控制共享资源不加锁，只需要判断状态就好了，所以执行效率比多线程高很多。</p>
<p> 事务四大特性</p>
<ol>
<li>原子性（Atomicity）</li>
<li>一致性（Consistency）</li>
<li>隔离性（Isolation）</li>
<li>持久性（Durability）</li>
</ol>
<h3 id="select语句的执行顺序"><a href="#select语句的执行顺序" class="headerlink" title="select语句的执行顺序"></a>select语句的执行顺序</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span></span><br><span class="line">	班级，<span class="keyword">avg</span>(数学成绩) <span class="keyword">as</span> 数据平均成绩</span><br><span class="line"><span class="keyword">from</span> 学生信息表</span><br><span class="line"><span class="keyword">where</span> 数学成绩 <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">null</span></span><br><span class="line"><span class="keyword">group</span> <span class="keyword">by</span> 班级</span><br><span class="line"><span class="keyword">having</span> 数学平均成绩 &gt; <span class="number">75</span></span><br><span class="line"><span class="keyword">order</span> <span class="keyword">by</span> 数学平均成绩 <span class="keyword">desc</span></span><br><span class="line"><span class="keyword">limit</span> <span class="number">3</span></span><br></pre></td></tr></table></figure>

<ol>
<li>首先执行 FROM 子句, 从<strong>学生成绩表</strong>中组装数据源的数据。</li>
<li>执行 WHERE 子句, 筛选<strong>学生成绩表</strong>中所有学生的数学成绩不为 NULL 的数据 。</li>
<li>执行 GROUP BY 子句, 把<strong>学生成绩表</strong>按 “<strong>班级</strong>“ 字段进行分组。</li>
<li>计算 avg 聚合函数, 按找每个班级分组求出 <strong>数学平均成绩</strong>。</li>
<li>执行 HAVING 子句, 筛选出班级 <strong>数学平均成绩</strong>大于 75 分的。</li>
<li>执行SELECT语句，返回数据，但别着急，还需要执行后面几个步骤。</li>
<li>执行 ORDER BY 子句, 把最后的结果按 “数学平均成绩” 进行排序。</li>
<li>执行LIMIT ，限制仅返回3条数据。结合ORDER BY 子句，即返回所有班级中数学平均成绩的前三的班级及其数学平均成绩。</li>
</ol>
<h3 id="Delete-from-table-where-id-gt-2-执行这条语句过程中发生了什么？（胡说一通）"><a href="#Delete-from-table-where-id-gt-2-执行这条语句过程中发生了什么？（胡说一通）" class="headerlink" title="Delete * from table where id&gt;2 执行这条语句过程中发生了什么？（胡说一通）"></a>Delete * from table where id&gt;2 执行这条语句过程中发生了什么？（胡说一通）</h3><p><img src="https://lixiangbetter.github.io/2021/04/11/%E9%9D%A2%E8%AF%95%E9%A2%98%E9%9A%8F%E7%AC%94-21-4-11/20191026190526122.png" alt></p>
<h3 id="数据库的索引"><a href="#数据库的索引" class="headerlink" title="数据库的索引"></a>数据库的索引</h3><p><strong>一、数据索引是干什么用的呢？</strong></p>
<p>数据库索引其实就是为了使查询数据效率快。</p>
<p><strong>二、数据库索引有哪些呢？</strong></p>
<ol>
<li>聚集索引（主键索引）：在数据库里面，所有行数都会按照主键索引进行排序。</li>
<li>非聚集索引：就是给普通字段加上索引。</li>
<li>联合索引：就是好几个字段组成的索引，称为联合索引。</li>
</ol>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">key ``&apos;idx_age_name_sex&apos;` `(``&apos;age&apos;``,``&apos;name&apos;``,``&apos;sex&apos;``)</span><br></pre></td></tr></table></figure>

<p> 联合索引遵从最左前缀原则</p>
<ol>
<li><p>脏读<br>  脏读： ==在不同的事务下，当前事务可以读到另外事务未提交的数据。==另外我们需要注意的是默认的MySQL隔离级别是REPEATABLE READ是不会发生脏读的，脏读发生的条件是需要事务的隔离级别为READ UNCOMMITTED，所以如果出现脏读，可能就是这种隔离级别导致的。</p>
</li>
<li><p>不可重复读（修改）</p>
</li>
</ol>
<p>  <strong>不可重复读：</strong> 是指在一个事务内多次读取同一集合的数据，但是多次读到的数据是不一样的，这就违反了数据库事务的一致性的原则。但是，这跟脏读还是有区别的，脏读的数据是没有提交的，但是不可重复读的数据是已经提交的数据。</p>
<ol start="3">
<li>丢失更新（幻读）新增或者删除<br>  <br>  事务在操作过程中进行两次查询，第二次查询的结果包含了第一次查询中未出现的数据或者缺少了第一次查 询中出现的数据（并不要求两次查询的 SQL 语句相同）。这是因为在两次查询过程中有另外一个事务插入数据造 成的</li>
</ol>
<p>幻读，可以锁表。解决</p>
<p>串行化</p>
<h3 id="数据库三大范式"><a href="#数据库三大范式" class="headerlink" title="数据库三大范式"></a>数据库三大范式</h3><p>1、1NF:字段不可分，每个字段是原子级别的，上节中看到第一个字段为ID，它就是ID不能在分成两个字段了，不能说我要把这个人的ID、名称、班级号都塞在一个字段里面，这个是不合适的，对以后的应用造成很大影响；</p>
<p>2、2NF:有主键，非主键字段依赖主键，ID字段就是主键，它能表示这一条数据是唯一的，有的读者朋友记性很好，“unique”表示唯一的、不允许重复的，确实它经常会修饰某个字段，保证该字段唯一性，然后再设置该字段为主键；</p>
<p>不存在部分依赖（消除非主属性与码的部分依赖）</p>
<p>3、3NF:非主键字段不能相互依赖。</p>
<p>（消除非主属性与码的传递依赖）</p>
<h3 id="线程池原理"><a href="#线程池原理" class="headerlink" title="线程池原理"></a>线程池原理</h3><p><strong>线程池的原理：</strong></p>
<p>其实线程池的原理很简单，类似于操作系统中的缓冲区的概念，它的流程如下：先启动若干数量的线程，并让这些线程都处于睡眠状态，当客户端有一个新请求时，就会唤醒线程池中的某一个睡眠线程，让它来处理客户端的这个请求，当处理完这个请求后，线程又处于睡眠状态。</p>
<h3 id="HashMap为什么线程不安全"><a href="#HashMap为什么线程不安全" class="headerlink" title="HashMap为什么线程不安全"></a>HashMap为什么线程不安全</h3><p>一直以来都知道HashMap是线程不安全的，但是到底为什么线程不安全，在多线程操作情况下什么时候线程不安全？</p>
<p>让我们先来了解一下HashMap的底层存储结构，HashMap底层是一个Entry数组，一旦发生Hash冲突的的时候，HashMap采用拉链法解决碰撞冲突，Entry内部的变量：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">final</span> Object key;</span><br><span class="line">Object value; </span><br><span class="line">Entry next; </span><br><span class="line"><span class="keyword">int</span> hash;</span><br></pre></td></tr></table></figure>

<p>​    通过Entry内部的next变量可以知道使用的是链表，这时候我们可以知道，如果多个线程，在某一时刻同时操作HashMap并执行put操作，而有大于两个key的hash值相同，如图中a1、a2，这个时候需要解决碰撞冲突，而解决冲突的办法上面已经说过，对于链表的结构在这里不再赘述，暂且不讨论是从链表头部插入还是从尾部初入，这个时候两个线程如果恰好都取到了对应位置的头结点e1，而最终的结果可想而知，a1、a2两个数据中势必会有一个会丢失.</p>
<p>​    put方法不是同步的，同时调用了addEntry方法：</p>
<p>​    addEntry方法依然不是同步的，所以导致了线程不安全出现伤处问题，其他类似操作不再说明，源码一看便知，下面主要说一下另一个非常重要的知识点，同样也是HashMap非线程安全的原因，我们知道在HashMap存在扩容的情况，对应的方法为HashMap中的resize方法：</p>
<p>​    可以看到扩容方法也不是同步的，通过代码我们知道在扩容过程中，会新生成一个新的容量的数组，然后对原数组的所有键值对重新进行计算和写入新的数组，之后指向新生成的数组。    </p>
<p>​    当多个线程同时检测到总数量超过门限值的时候就会同时调用resize操作，各自生成新的数组并rehash后赋给该map底层的数组table，结果最终只有最后一个线程生成的新数组被赋给table变量，其他线程的均会丢失。而且当某些线程已经完成赋值而其他线程刚开始的时候，就会用已经被赋值的table作为原始数组，这样也会有问题。</p>
<h2 id="今日面经2"><a href="#今日面经2" class="headerlink" title="今日面经2"></a>今日面经2</h2><p>链接：<a href="https://www.nowcoder.com/discuss/636764?source_id=profile_create_nctrack&amp;channel=-1" target="_blank" rel="noopener">https://www.nowcoder.com/discuss/636764?source_id=profile_create_nctrack&amp;channel=-1</a></p>
<h3 id="hashmap的原理"><a href="#hashmap的原理" class="headerlink" title="hashmap的原理"></a>hashmap的原理</h3><p>简单来说，HashMap由数组+链表组成的，数组是HashMap的主体，链表则是主要为了解决哈希冲突而存在的，如果定位到的数组位置不含链表（当前entry的next指向null）,那么对于查找，添加等操作很快，仅需一次寻址即可；如果定位到的数组包含链表，对于添加操作，其时间复杂度为O(n)，首先遍历链表，存在即覆盖，否则新增；对于查找操作来讲，仍需遍历链表，然后通过key对象的equals方法逐一比对查找。所以，性能考虑，HashMap中的链表出现越少，性能才会越好。</p>
<h3 id="线程池的参数"><a href="#线程池的参数" class="headerlink" title="线程池的参数"></a>线程池的参数</h3><p>一、corePoolSize 线程池核心线程大小</p>
<p>线程池中会维护一个最小的线程数量，即使这些线程处理空闲状态，他们也不会被销毁，除非设置了allowCoreThreadTimeOut。这里的最小线程数量即是corePoolSize。</p>
<p>二、maximumPoolSize 线程池最大线程数量</p>
<p>一个任务被提交到线程池以后，首先会找有没有空闲存活线程，如果有则直接将任务交给这个空闲线程来执行，如果没有则会缓存到工作队列（后面会介绍）中，如果工作队列满了，才会创建一个新线程，然后从工作队列的头部取出一个任务交由新线程来处理，而将刚提交的任务放入工作队列尾部。线程池不会无限制的去创建新线程，它会有一个最大线程数量的限制，这个数量即由maximunPoolSize指定。</p>
<p>三、keepAliveTime 空闲线程存活时间</p>
<p>一个线程如果处于空闲状态，并且当前的线程数量大于corePoolSize，那么在指定时间后，这个空闲线程会被销毁，这里的指定时间由keepAliveTime来设定</p>
<p>四、unit 空闲线程存活时间单位</p>
<p>keepAliveTime的计量单位</p>
<p>五、workQueue 工作队列</p>
<p>新任务被提交后，会先进入到此工作队列中，任务调度时再从队列中取出任务。jdk中提供了四种工作队列：</p>
<p>①ArrayBlockingQueue</p>
<p>基于数组的有界阻塞队列，按FIFO排序。</p>
<p>②LinkedBlockingQuene</p>
<p>基于链表的无界阻塞队列（其实最大容量为Interger.MAX），按照FIFO排序。</p>
<p>③SynchronousQuene</p>
<p>一个不缓存任务的阻塞队列，生产者放入一个任务必须等到消费者取出这个任务。</p>
<p>④PriorityBlockingQueue</p>
<p>具有优先级的无界阻塞队列，优先级通过参数Comparator实现。</p>
<p>六、threadFactory 线程工厂</p>
<p>创建一个新线程时使用的工厂，可以用来设定线程名、是否为daemon线程等等</p>
<p>七、handler 拒绝策略</p>
<p>①CallerRunsPolicy</p>
<p>该策略下，在调用者线程中直接执行被拒绝任务的run方法，除非线程池已经shutdown，则直接抛弃任务。</p>
<p>②AbortPolicy</p>
<p>该策略下，直接丢弃任务，并抛出RejectedExecutionException异常。</p>
<h3 id="UDP与TDP区别"><a href="#UDP与TDP区别" class="headerlink" title="UDP与TDP区别"></a>UDP与TDP区别</h3><p>主要是UDP没有连接状态、速度快，但数据容易丢失，TCP有连接状态、速度慢，但数据较安全，一般根据你对程序数据的安全性要求和传输速度权衡。</p>
<h3 id="Cookie和Session"><a href="#Cookie和Session" class="headerlink" title="Cookie和Session"></a>Cookie和Session</h3><ol>
<li>session 在服务器端，cookie 在客户端（浏览器）</li>
<li>session 默认被存在在服务器的一个文件里（不是内存）</li>
<li>session 的运行依赖 session id，而 session id 是存在 cookie 中的，也就是说，如果浏览器禁用了 cookie ，同时 session 也会失效（但是可以通过其它方式实现，比如在 url 中传递 session_id）</li>
<li>session 可以放在 文件、数据库、或内存中都可以。</li>
<li>用户验证这种场合一般会用 session </li>
</ol>
<p>因此，维持一个会话的核心就是客户端的唯一标识，即 session id</p>
<h3 id="索引的类别"><a href="#索引的类别" class="headerlink" title="索引的类别"></a>索引的类别</h3><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">唯一索引：   UNIQUE     例如：create unique index stusno on student（sno）；</span><br><span class="line">表明此索引的每一个索引值只对应唯一的数据记录，对于单列惟一性索引，这保证单列不包含重复的值。对于多列惟一性索引，保证多个值的组合不重复。</span><br><span class="line"></span><br><span class="line">主键索引：   primary key</span><br><span class="line">数据库表经常有一列或列组合，其值唯一标识表中的每一行。该列称为表的主键。   在数据库关系图中为表定义主键将自动创建主键索引，主键索引是唯一索引的特定类型。该索引要求主键中的每个值都唯一。当在查询中使用主键索引时，它还允许对数据的快速访问。 </span><br><span class="line"></span><br><span class="line">聚集索引（也叫聚簇索引）：cluster  </span><br><span class="line">在聚集索引中，表中行的物理顺序与键值的逻辑（索引）顺序相同。一个表只能包含一个聚集索引。   如果某索引不是聚集索引，则表中行的物理顺序与键值的逻辑顺序不匹配。与非聚集索引相比，聚集索引通常提供更快的数据访问速度。</span><br><span class="line"></span><br><span class="line">非聚集索引：</span><br><span class="line">一种索引，该索引中索引的逻辑顺序与磁盘上行的物理存储顺序不同。</span><br></pre></td></tr></table></figure>

<p>索引的实现方式（原理）</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">B+树</span><br><span class="line">	我们经常听到B+树就是这个概念，用这个树的目的和红黑树差不多，也是为了尽量保持树的平衡，当然红黑树是二叉树，但B+树就不是二叉树了，节点下面可以有多个子节点，数据库开发商会设置子节点数的一个最大值，这个值不会太小，所以B+树一般来说比较矮胖，而红黑树就比较瘦高了。</span><br><span class="line">	关于B+树的插入，删除，会涉及到一些算法以保持树的平衡，这里就不详述了。ORACLE的默认索引就是这种结构的。</span><br><span class="line">	如果经常需要同时对两个字段进行AND查询,那么使用两个单独索引不如建立一个复合索引，因为两个单独索引通常数据库只能使用其中一个，而使用复合索引因为索引本身就对应到两个字段上的，效率会有很大提高。</span><br></pre></td></tr></table></figure>

<h3 id="了解MVCC吗？"><a href="#了解MVCC吗？" class="headerlink" title="了解MVCC吗？"></a>了解MVCC吗？</h3><p>链接：<a href="https://blog.csdn.net/kyle_wu_/article/details/113287187" target="_blank" rel="noopener">https://blog.csdn.net/kyle_wu_/article/details/113287187</a></p>
<ol>
<li>什么是MVCC</li>
</ol>
<p><strong>MVCC</strong>(Multi Version Concurrency Control的简称)，代表<strong>多版本并发控制</strong>。</p>
<p>MVCC最大的优势：<strong>读不加锁，读写不冲突</strong>。读写不冲突是非常重要的，极大的增加了系统的并发性能。<strong>MVCC机制也是乐观锁的一种体现。</strong></p>
<p>InnoDB的MVCC是通过在每行记录后面保存两个隐藏的列来实现的，分别保存这条行的<strong>创建时间</strong>和<strong>行的删除时间</strong>。这里的时间并不是实际的时间值，而是<strong>系统版本号（事务的ID）</strong>。事务开始时刻的系统版本号会作为事务的ID，每次执行一个新事务，系统版本号就会自动递增。随之引出了<strong>当前读</strong>和<strong>快照读</strong>。</p>
<ol start="2">
<li>MVCC解决了什么问题</li>
</ol>
<p>MVCC在MySQL InnoDB中的实现主要是为了提高数据库并发性能，用更好的方式去处理读-写冲突，做到即使有读写冲突时，也能做到不加锁，非阻塞并发读</p>
<p>在并发读写数据库时，可以做到在读操作时不用阻塞写操作，写操作也不用阻塞读操作，提高了数据库并发读写的性能</p>
<p>同时还可以解决脏读，幻读，不可重复读等事务隔离问题，但不能解决更新丢失问题。</p>
<ol start="3">
<li>什么是当前读和快照读</li>
</ol>
<ul>
<li>当前读</li>
</ul>
<p>像select lock in share mode(共享锁), select for update ; update, insert ,delete(排他锁)这些操作都是一种当前读。</p>
<p>当前读就是它读取的是记录的最新版本，读取时还要保证其他并发事务不能修改当前记录，会对读取的记录进行加锁</p>
<ul>
<li>快照读</li>
</ul>
<p>快照读的前提是隔离级别不是串行级别，串行级别下的快照读会退化成当前读；</p>
<p>之所以出现快照读的情况，是基于提高并发性能的考虑，快照读的实现是基于多版本并发控制，即MVCC,可以认为MVCC是行锁的一个变种，但它在很多情况下，避免了加锁操作，降低了开销；既然是基于多版本，即快照读可能读到的并不一定是数据的最新版本，而有可能是之前的历史版本</p>
<p>不加锁的简单的 SELECT 都属于快照读，例如：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> t <span class="keyword">WHERE</span> <span class="keyword">id</span>=<span class="number">1</span></span><br></pre></td></tr></table></figure>

<p>与 快照读 相对应的则是 当前读，当前读就是读取最新数据，而不是历史版本的数据。加锁的 SELECT 就属于当前读，例如：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> t <span class="keyword">WHERE</span> <span class="keyword">id</span>=<span class="number">1</span> <span class="keyword">LOCK</span> <span class="keyword">IN</span> <span class="keyword">SHARE</span> <span class="keyword">MODE</span>;</span><br><span class="line"><span class="keyword">SELECT</span> * <span class="keyword">FROM</span> t <span class="keyword">WHERE</span> <span class="keyword">id</span>=<span class="number">1</span> <span class="keyword">FOR</span> <span class="keyword">UPDATE</span>;</span><br></pre></td></tr></table></figure>

<h3 id="写个SQL行列转换"><a href="#写个SQL行列转换" class="headerlink" title="写个SQL行列转换"></a>写个SQL行列转换</h3><p>链接：<a href="https://www.cnblogs.com/weibanggang/p/9679301.html" target="_blank" rel="noopener">https://www.cnblogs.com/weibanggang/p/9679301.html</a></p>
<ol>
<li>使用case when</li>
</ol>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">name</span> <span class="keyword">as</span> 姓名，</span><br><span class="line"><span class="keyword">max</span>(<span class="keyword">case</span> <span class="keyword">when</span> Subject <span class="keyword">when</span> <span class="string">'语文'</span> <span class="keyword">then</span> <span class="keyword">Result</span> <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">end</span>) <span class="keyword">as</span> 语文，</span><br><span class="line"><span class="keyword">max</span>(<span class="keyword">case</span> Subject <span class="keyword">when</span> <span class="string">'数学'</span> <span class="keyword">then</span> <span class="keyword">result</span> <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">end</span>) <span class="keyword">as</span> 数学,</span><br><span class="line"><span class="keyword">max</span>(<span class="keyword">case</span> Subject <span class="keyword">when</span> <span class="string">'英语'</span> <span class="keyword">then</span> <span class="keyword">Result</span> <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">end</span>) <span class="keyword">as</span> 英语,</span><br><span class="line"><span class="keyword">max</span>(<span class="keyword">case</span> Subject <span class="keyword">when</span> <span class="string">'政治'</span> <span class="keyword">then</span> <span class="keyword">Result</span> <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">end</span>) <span class="keyword">as</span> 政治,</span><br><span class="line"><span class="keyword">max</span>(<span class="keyword">case</span> Subject <span class="keyword">when</span> <span class="string">'物理'</span> <span class="keyword">then</span> <span class="keyword">Result</span> <span class="keyword">else</span> <span class="number">0</span> <span class="keyword">end</span>) <span class="keyword">as</span> 物理</span><br><span class="line"><span class="keyword">from</span> cj <span class="keyword">group</span> <span class="keyword">by</span> <span class="keyword">name</span></span><br></pre></td></tr></table></figure>

<ol start="2">
<li>使用with rollup</li>
</ol>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span></span><br><span class="line">	<span class="keyword">ifnull</span>(<span class="keyword">name</span>,<span class="string">'TOLL'</span>) <span class="keyword">name</span>,</span><br><span class="line">	<span class="keyword">sum</span>(<span class="keyword">if</span>(Subject=<span class="string">'语文'</span>,Fraction,<span class="number">0</span>) <span class="keyword">as</span> 语文),</span><br><span class="line">	<span class="keyword">sum</span>(<span class="keyword">if</span>(Subject=<span class="string">'英语'</span>,Fraction,<span class="number">0</span>)) <span class="keyword">as</span> 英语,</span><br><span class="line">	<span class="keyword">sum</span>(<span class="keyword">if</span>(Subject=<span class="string">'数学'</span>,Fraction,<span class="number">0</span>)) <span class="keyword">as</span> 英语,</span><br><span class="line">	<span class="keyword">sum</span>(Fraction) 总分</span><br><span class="line"><span class="keyword">from</span> t_score <span class="keyword">group</span> <span class="keyword">by</span> <span class="keyword">name</span> <span class="keyword">with</span> <span class="keyword">rollup</span></span><br></pre></td></tr></table></figure>

<h3 id="自动装箱和拆箱、引用传递"><a href="#自动装箱和拆箱、引用传递" class="headerlink" title="自动装箱和拆箱、引用传递"></a>自动装箱和拆箱、引用传递</h3><p>装箱就是自动将基本数据类型转换为包装器类型；拆箱就是自动将包装器类型转换为基本数据类型。</p>
<p> <strong>值传递</strong>：指在调用函数时将实际参数复制一份传递到函数中，这样在函数中如果对参数进行修改，将不会影响到实际参数。<br>  <strong>引用传递</strong>：是指在调用函数时将实际参数的地址直接传递到函数中(的形参)，那么在函数中对参数所进行的修改，将影响到实际参数。</p>
<p>Java 中的基本类型，属于值传递。</p>
<p>Java 中的引用类型，属于引用传递。</p>
<p>Java 中的 String 及包装类，属于特殊群体，作为形参时，由于每次赋值都相当于重新创建了对象，因此看起来像值传递，但是其特性已经破坏了，值传递、引用传递的定义。因此他们属于引用传递的定义，却表现为值传递。</p>
<h3 id="垃圾回收器"><a href="#垃圾回收器" class="headerlink" title="垃圾回收器"></a>垃圾回收器</h3><p><strong>垃圾收集器的分代</strong></p>
<p>年轻代：Serial, ParNew, Parallel Scavenge</p>
<p>老年代：CMS, Serial Old, Parallel Old</p>
<p>通杀：G1</p>
<ol>
<li><strong>Serial收集器</strong></li>
</ol>
<p>垃圾收集器最基本，发展历史最悠久的收集器。jdk1.3之前是HotSpot新生代收集器唯一的选择。</p>
<ul>
<li>特点</li>
</ul>
<p>采用复制算法</p>
<p>单线程收集</p>
<p>stop the world 进行垃圾收集时， 必须暂停所有工作线程， 直到完成。</p>
<ol start="2">
<li><strong>Serial Old收集器</strong></li>
</ol>
<p>serial Old是Serial收集器的老年代版本</p>
<ul>
<li>特点：</li>
</ul>
<p>针对老年代的收集器</p>
<p>采用的是标记整理算法， （还有压缩， mark-sweep-compact）</p>
<p>单线程收集</p>
<ol start="3">
<li><strong>ParNew收集器</strong></li>
</ol>
<ul>
<li>特点：</li>
</ul>
<p>除了多线程外， 其余的行为， 特点和Serial收集器一样</p>
<p>和Serial收集器公用了不少的代码</p>
<ul>
<li>为什么只有ParNew能与CMS收集器配合</li>
</ul>
<p>因为Parallel Scavenge(以及G1) 都没有使用传统的GC收集器代码框架， 而是另外独立实现；而其余几种收集器则公用了部分的框架代码。</p>
<ol start="4">
<li><strong>Parallel Scavenge收集器</strong></li>
</ol>
<p>因为与吞吐量关系密切， 也称为吞吐量收集器</p>
<ul>
<li>特点：</li>
</ul>
<p>a. 和ParNew收集器相似</p>
<p>新生代收集器</p>
<p>采用复制算法</p>
<p>多线程收集</p>
<ol start="5">
<li><strong>Parallel Old收集器</strong></li>
</ol>
<p>Parallel Old垃圾收集器是Parallel Scavenge收集器的老年代版本；</p>
<p>JDK1.6中才开始提供</p>
<ul>
<li>特点：</li>
</ul>
<p>针对老年代</p>
<p>采用标记整理算法</p>
<p>多线程收集；</p>
<ol start="6">
<li><strong>CMS收集器</strong></li>
</ol>
<p>并发标记清理收集器也称为并发低停顿收集器或低延迟垃圾收集器</p>
<ul>
<li>特点：</li>
</ul>
<p>针对老年代</p>
<p>基于标记清除算法</p>
<p>以获取最短回收停顿时间为目标；</p>
<p>并发收集， 低停顿</p>
<p>需要更多的内存， 是HotSpot在JDK1.5推出的第一款真正意义上的并发收集器</p>
<p>第一次实现了让垃圾收集线程与用户线程（基本上）同时工作。</p>
<h3 id="volatile在单例模式中的作用"><a href="#volatile在单例模式中的作用" class="headerlink" title="volatile在单例模式中的作用"></a>volatile在单例模式中的作用</h3><p>链接：<a href="https://blog.csdn.net/llllllkkkkkooooo/article/details/115360630" target="_blank" rel="noopener">https://blog.csdn.net/llllllkkkkkooooo/article/details/115360630</a></p>
<p>singleton 用了 volatile 修饰，而我们都知道 volatile 的作用</p>
<ol>
<li><strong>不能保证原子性</strong></li>
<li><strong>保证共享变量的可见性</strong></li>
<li><strong>禁止指令重排序</strong></li>
</ol>
<p><strong>在单例模式中，volatile的作用主要是 禁止指令重排序</strong></p>
<p><strong>singleton = new Singleton(); 可分解为以下步骤：</strong></p>
<ol>
<li><strong>分配对象内存空间</strong></li>
<li><strong>初始化对象</strong></li>
<li><strong>设置 singleton 指向分配的内存地址</strong></li>
</ol>
<p>但是步骤2，3是可能交换的，也就是发生重排序，但是根据 Java语言规范 intra-thread semantics，它是允许那些在单线程内，不会改变单线程程序执行结果的重排序，也就是说，虽然步骤2，3发生重排序，但是对于单线程来说，初次访问对象时，其结果都是正确的，所以即使重排序也无所谓了。</p>
<p>但是<strong>在多线程环境中，这就会出现问题</strong>，在发生重排序的情况下，会导致线程B在 t3 时间下，判断出 singleton 不为null，那么线程B就会拿到这个 singleton 去做别的事，那么此时这个 singleton 没有初始化，那么就会报错，这就出了问题。</p>
<h3 id="java序列化"><a href="#java序列化" class="headerlink" title="java序列化"></a>java序列化</h3><p>链接：<a href="https://www.cnblogs.com/wxgblogs/p/5849951.html" target="_blank" rel="noopener">https://www.cnblogs.com/wxgblogs/p/5849951.html</a></p>
<p>如果你只知道实现 Serializable 接口的对象，可以序列化为本地文件。</p>
<ol>
<li>序列化 ID 问题</li>
</ol>
<p>　　<strong>情境</strong>：两个客户端 A 和 B 试图通过网络传递对象数据，A 端将对象 C 序列化为二进制数据再传给 B，B 反序列化得到 C。</p>
<p>　　<strong>问题</strong>：C 对象的全类路径假设为 com.inout.Test，在 A 和 B 端都有这么一个类文件，功能代码完全一致。也都实现了 Serializable 接口，但是反序列化时总是提示不成功。</p>
<p>　　<strong>解决</strong>：虚拟机是否允许反序列化，不仅取决于类路径和功能代码是否一致，一个非常重要的一点是两个类的序列化 ID 是否一致（就是 private static final long serialVersionUID = 1L）。清单 1 中，虽然两个类的功能代码完全一致，但是序列化 ID 不同，他们无法相互序列化和反序列化。</p>
<p>​    序列化 ID 在 Eclipse 下提供了两种生成策略，一个是固定的 1L，一个是随机生成一个不重复的 long 类型数据（实际上是使用 JDK 工具生成），在这里有一个建议，如果没有特殊需求，就是用默认的 1L 就可以，这样可以确保代码一致时反序列化成功。那么随机生成的序列化 ID 有什么作用呢，有些时候，通过改变序列化 ID 可以用来限制某些用户的使用。</p>
<ol start="2">
<li>静态变量序列化</li>
</ol>
<p>序列化保存的是对象的状态，静态变量属于类的状态，因此 <strong><em>序列化并不保存静态变量\</em></strong>。</p>
<ol start="3">
<li>父类的序列化与 Transient 关键字</li>
</ol>
<p>　　<strong>情境</strong>：一个子类实现了 Serializable 接口，它的父类都没有实现 Serializable 接口，序列化该子类对象，然后反序列化后输出父类定义的某变量的数值，该变量数值与序列化时的数值不同。</p>
<p>　　解决：要想将父类对象也序列化，就需要让父类也实现Serializable 接口。如果父类不实现的话的，就需要有默认的无参的构造函数。 在父类没有实现 Serializable 接口时，虚拟机是不会序列化父对象的，而一个 Java 对象的构造必须先有父对象，才有子对象，反序列化也不例外。所以反序列化时，为了构造父对象，只能调用父类的无参构造函数作为默认的父对象。因此当我们取 父对象的变量值时，它的值是调用父类无参构造函数后的值。如果你考虑到这种序列化的情况，在父类无参构造函数中对变量进行初始化，否则的话，父类变量值都 是默认声明的值，如 int 型的默认是 0，string 型的默认是 null。</p>
<p>　　Transient 关键字的作用是控制变量的序列化，在变量声明前加上该关键字，可以阻止该变量被序列化到文件中，在被反序列化后，transient 变量的值被设为初始值，如 int 型的是 0，对象型的是 null。</p>
<ol start="4">
<li>对敏感字段加密</li>
</ol>
<p>　　<strong>情境</strong>：服务器端给客户端发送序列化对象数据，对象中有一些数据是敏感的，比如密码字符串等，希望对该密码字段在序列化时，进行加密，而客户端如果拥有解密的密钥，只有在客户端进行反序列化时，才可以对密码进行读取，这样可以一定程度保证序列化对象的数据安全。</p>
<p>　　<strong>解决</strong>： 在序列化过程中，虚拟机会试图调用对象类里的 writeObject 和 readObject 方法，进行用户自定义的序列化和反序列化，如果没有这样的方法，则默认调用是 ObjectOutputStream 的 defaultWriteObject 方法以及 ObjectInputStream 的 defaultReadObject 方法。用户自定义的 writeObject 和 readObject 方法可以允许用户控制序列化的过程，比如可以在序列化的过程中动态改变序列化的数值。</p>
<ol start="5">
<li>序列化存储规则</li>
</ol>
<p>Java 序列化机制为了节省磁盘空间，具有特定的存储规则，当写入文件的为同一对象时，并不会再将对象的内容进行存储，而只是再次存储一份引用，上面增加的 5 字节的存储空间就是新增引用和一些控制信息的空间。反序列化时，恢复引用关系，使得清单 3 中的 t1 和 t2 指向唯一的对象，二者相等，输出 true。该存储规则极大的节省了存储空间。</p>
<h3 id="1-9填九宫格，行列对角线和都相等"><a href="#1-9填九宫格，行列对角线和都相等" class="headerlink" title="1-9填九宫格，行列对角线和都相等"></a>1-9填九宫格，行列对角线和都相等</h3><p>二四为肩，六八为足</p>
<p>左三右七，戴九履一</p>
<p>4 9 2</p>
<p>3 5 7 </p>
<p>8 1 6</p>
<h3 id="给定一句话，只考虑数字和字母（不考虑大小写），判断是否为回文"><a href="#给定一句话，只考虑数字和字母（不考虑大小写），判断是否为回文" class="headerlink" title="给定一句话，只考虑数字和字母（不考虑大小写），判断是否为回文"></a>给定一句话，只考虑数字和字母（不考虑大小写），判断是否为回文</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 回文串</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">isPalindrome</span><span class="params">(String s)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">int</span> n = s.length();</span><br><span class="line">  <span class="keyword">int</span> left = <span class="number">0</span>, right = n - <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">while</span> (left &lt; right) &#123;</span><br><span class="line">    <span class="keyword">while</span> (left &lt; right &amp;&amp; !Character.isLetterOrDigit(s.charAt(left))) &#123;</span><br><span class="line">      ++left;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">while</span> (left &lt; right &amp;&amp; !Character.isLetterOrDigit(s.charAt(right))) &#123;</span><br><span class="line">      --right;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (left &lt; right) &#123;</span><br><span class="line">      <span class="keyword">if</span> (Character.isLowerCase(s.charAt(left)) != Character.isLowerCase(s.charAt(right))) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">      &#125;</span><br><span class="line">      ++left;</span><br><span class="line">      --right;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="优缺点"><a href="#优缺点" class="headerlink" title="优缺点"></a>优缺点</h3><p>优点：愿意钻研技术，执行能力比较强。</p>
<p>缺点：前面提到执行能力比较强，可能相反我的领导能力有欠缺。</p>
<h3 id="任务完不成，怎么办？"><a href="#任务完不成，怎么办？" class="headerlink" title="任务完不成，怎么办？"></a>任务完不成，怎么办？</h3><p>​    遇到任务完不成，第一时间就是分析任务无法完成的原因，任务完不成造成的结果，如果有不良影响应该怎么解决。如果是由于个人原因造成，这可能是最开始工作不熟，能力不够，一定加倍努力，下次不再犯。而如果是客观原因，比如公司决策，大环境影响，竞争对手的下作手段等，那就只有尽可能想办法弥补造成的损失，随时跟踪任务进度，遇到问题及时预警及处理，尽最大的努力完成任务！</p>
]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>刷题</tag>
      </tags>
  </entry>
  <entry>
    <title>面试题随笔-21/4/10</title>
    <url>/2021/04/10/%E9%9D%A2%E8%AF%95%E9%A2%98%E9%9A%8F%E7%AC%94-21-4-10/</url>
    <content><![CDATA[<h2 id="今日面经"><a href="#今日面经" class="headerlink" title="今日面经"></a>今日面经</h2><p>链接：<a href="https://www.nowcoder.com/discuss/630480?source_id=profile_create_nctrack&amp;channel=-1" target="_blank" rel="noopener">https://www.nowcoder.com/discuss/630480?source_id=profile_create_nctrack&amp;channel=-1</a></p>
<h3 id="HBase是根据rowkey查询，当数据量相当大的时候，是怎么读的很快的，缓存、索引、布隆过滤器、有序"><a href="#HBase是根据rowkey查询，当数据量相当大的时候，是怎么读的很快的，缓存、索引、布隆过滤器、有序" class="headerlink" title="HBase是根据rowkey查询，当数据量相当大的时候，是怎么读的很快的，缓存、索引、布隆过滤器、有序"></a>HBase是根据rowkey查询，当数据量相当大的时候，是怎么读的很快的，缓存、索引、布隆过滤器、有序</h3><p>如果快速查询（从磁盘读数据），hbase是根据rowkey查询的，只要能快速的定位rowkey, 就能实现快速的查询，主要是以下因素：<br>   1、hbase是可划分成多个region，你可以简单的理解为关系型数据库的多个分区。<br>   2、键是排好序了的<br>   3、按列存储的</p>
<ol>
<li><p>首先，能快速找到行所在的region(分区)，假设表有10亿条记录，占空间1TB,  分列成了500个region, 1个region占2个G. 最多读取2G的记录，就能找到对应记录； </p>
</li>
<li><p>其次，是按列存储的，其实是列族，假设分为3个列族，每个列族就是666M， 如果要查询的东西在其中1个列族上，1个列族包含1个或者多个 HStoreFile，假设一个HStoreFile是128M， 该列族包含5个HStoreFile在磁盘上. 剩下的在内存中。</p>
</li>
<li><p>再次，是排好序了的，你要的记录有可能在最前面，也有可能在最后面，假设在中间，我们只需遍历2.5个HStoreFile共300M</p>
</li>
<li><p>最后，每个HStoreFile(HFile的封装)，是以键值对（key-value）方式存储，只要遍历一个个数据块中的key的位置，并判断符合条 件可以了。 一般key是有限的长度，假设跟value是1:19（忽略HFile上其它块），最终只需要15M就可获取的对应的记录，按照磁盘的访问 100M/S，只需0.15秒。 加上块缓存机制（LRU原则），会取得更高的效率。</p>
</li>
<li><p><strong>hbase数据存储及访问</strong></p>
</li>
</ol>
<p>hbase的实际存储结构是HFile，它是位于hdfs系统中的，也就是在磁盘中。而加载到内存中的数据存储在MemStore中，当MemStore中的数据达到一定数量时，它会将数据存入HFile中。</p>
<p>​    HFIle是由一个个数据块与索引块组成，他们通常默认为64KB。hbase是通过块索引来访问这些数据块的。而索引是由每个数据块的第一行数据的rowkey组成的。当hbase打开一个HFile时，块索引信息会优先加载到内存当中。然后hbase会通过这些块索引来查询数据。</p>
<ol start="2">
<li><p><strong>布隆过滤器在hbase中的应用</strong></p>
<p>当我们随机读get数据时，如果采用hbase的块索引机制，hbase会加载很多块文件。</p>
</li>
</ol>
<p>​    采用布隆过滤器后，它能够准确判断该HFile的所有数据块中是否含有我们查询的数据，从而大大减少不必要的块加载，增加吞吐，降低内存消耗，提高性能</p>
<p>​     在读取数据时，hbase会首先在布隆过滤器中查询，根据布隆过滤器的结果，再在MemStore中查询，最后再在对应的HFile中查询。</p>
<h3 id="zookeeper的作用"><a href="#zookeeper的作用" class="headerlink" title="zookeeper的作用"></a>zookeeper的作用</h3><ol>
<li>数据发布/订阅</li>
</ol>
<p>数据发布/订阅（Publish/Subscribe）系统，即所谓的配置中心，顾明思义就是发布者将数据发布到zookeeper的一个或一系列的节点上，供订阅者进行数据订阅，进而达到动态获取数据的目的，实现配置信息的集中式管理和数据的动态更新。</p>
<ol start="2">
<li>负载均衡</li>
</ol>
<p>每台服务端在启动时都会去zookeeper的servers节点下注册临时节点（注册临时节点是因为，当服务不可用时，这个临时节点会消失，客户端也就不会请求这个服务端），每台客户端在启动时都会去servers节点下取得所有可用的工作服务器列表，并通过一定的负载均衡算法计算得出应该将请求发到哪个服务器上</p>
<ol start="3">
<li>命名服务</li>
</ol>
<p>命名服务是分布式系统中比较常见的一类场景。在分布式系统中，被命名的实体通常可以是集群中的机器，提供的服务地址或远程对象等-这些我们都可以统称他们为名字，其中较为常见的就是一些分布式服务框架（如RPC，RMI）中的服务地址列表，通过命名服务，客户端应用能够根据指定名字来获取资源的实体，服务地址和提供者的信息等。</p>
<ol start="4">
<li>Master选举</li>
</ol>
<p>Master选举是一个在分布式系统中非常常见的应用场景。在分布式系统中，Master往往用来协调系统中的其他系统单元，具有对分布式系统状态变更的决定权。例如，在一些读写分离的应用场景用，客户端的写请求往往是由Master来处理的，而在另一些场景中， Master则常常负负责处理一下复杂的逻辑，并将处理结果同步给集群中其他系统单元。</p>
<p>客户端集群往zookeeper上创建一个/master临时节点。在这个过程中，只有一个客户端能够成功创建这个节点，那么这个客户端就成了master。同时其他没有在zookeeper上成功创建节点的客户端，都会在节点/master上注册一个变更的watcher，用于监控当前的master机器是否存活，一旦发现当前的master挂了，那么其余的客户端将会重新进行master选举。</p>
<h3 id="一个文件有上亿url，内存很小，找Top10"><a href="#一个文件有上亿url，内存很小，找Top10" class="headerlink" title="一个文件有上亿url，内存很小，找Top10"></a>一个文件有上亿url，内存很小，找Top10</h3><p>利用hash函数，划分成n个文件，再利用小根堆，求每个文件的to p10</p>
<h3 id="每个店铺访问次数top3的访客信息，输出店铺名称，访客id，访问次数"><a href="#每个店铺访问次数top3的访客信息，输出店铺名称，访客id，访问次数" class="headerlink" title="每个店铺访问次数top3的访客信息，输出店铺名称，访客id，访问次数"></a>每个店铺访问次数top3的访客信息，输出店铺名称，访客id，访问次数</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span></span><br><span class="line">	t2.user_id,</span><br><span class="line">	t2.shop,</span><br><span class="line">	t2.num</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">(</span><br><span class="line">  <span class="keyword">select</span></span><br><span class="line">    t1.user_id user_id,</span><br><span class="line">    t1.shop shop,</span><br><span class="line">    t1.num,</span><br><span class="line">    <span class="keyword">rank</span>() <span class="keyword">over</span>(<span class="keyword">partition</span> <span class="keyword">by</span> t1.user_id <span class="keyword">order</span> <span class="keyword">by</span> <span class="keyword">num</span>) con</span><br><span class="line">  <span class="keyword">from</span> </span><br><span class="line">  (</span><br><span class="line">    <span class="keyword">select</span></span><br><span class="line">      user_id,</span><br><span class="line">      shop,</span><br><span class="line">      <span class="keyword">count</span>(*) <span class="keyword">num</span></span><br><span class="line">    <span class="keyword">from</span> second_visit</span><br><span class="line">    <span class="keyword">group</span> <span class="keyword">by</span> user_id,shop</span><br><span class="line">  ) t1</span><br><span class="line">) t2</span><br><span class="line"><span class="keyword">where</span> con &lt;= <span class="number">3</span>;</span><br></pre></td></tr></table></figure>

<h2 id="今日面经2"><a href="#今日面经2" class="headerlink" title="今日面经2"></a>今日面经2</h2><p>链接：<a href="https://www.nowcoder.com/discuss/636761?type=post&amp;order=time&amp;pos=&amp;page=0&amp;channel=-1&amp;source_id=search_post_nctrack&amp;subType=2" target="_blank" rel="noopener">https://www.nowcoder.com/discuss/636761?type=post&amp;order=time&amp;pos=&amp;page=0&amp;channel=-1&amp;source_id=search_post_nctrack&amp;subType=2</a></p>
<h3 id="spark怎样连接MySQL"><a href="#spark怎样连接MySQL" class="headerlink" title="spark怎样连接MySQL"></a>spark怎样连接MySQL</h3><p>第一种方式</p>
<p>spark.read.jdbc(“jdbc:mysql://192.168.126.111:3306/spark”,”person”,properties)</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">  <span class="keyword">val</span> spark: <span class="type">SparkSession</span> = <span class="type">SparkSession</span>.builder().master(<span class="string">"local"</span>).appName(<span class="string">"createdataframefrommysql"</span>)</span><br><span class="line">  .config(<span class="string">"spark.sql.shuffle.partitions"</span>, <span class="number">1</span>).getOrCreate()</span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * 读取mysql的第一中方式</span></span><br><span class="line"><span class="comment">    *</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">  <span class="keyword">val</span> properties = <span class="keyword">new</span> <span class="type">Properties</span>()</span><br><span class="line">  properties.setProperty(<span class="string">"user"</span>,<span class="string">"root"</span>)</span><br><span class="line">  properties.setProperty(<span class="string">"password"</span>,<span class="string">"123"</span>)</span><br><span class="line">  <span class="keyword">val</span> person: <span class="type">DataFrame</span> = spark.read.jdbc(<span class="string">"jdbc:mysql://192.168.126.111:3306/spark"</span>,<span class="string">"person"</span>,properties)</span><br><span class="line"></span><br><span class="line">  person.show()</span><br><span class="line">  spark.read.jdbc(<span class="string">"jdbc:mysql://192.168.126.111:3306/spark"</span>,<span class="string">"(select person.id,person.name,person.age,score.score from person,score where person.id = score.id) T"</span>,properties).show()</span><br></pre></td></tr></table></figure>

<p>第二种方式</p>
<p>spark.read.format(“jdbc”) + Map</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> map: <span class="type">Map</span>[<span class="type">String</span>, <span class="type">String</span>] = <span class="type">Map</span>[<span class="type">String</span>, <span class="type">String</span>](</span><br><span class="line">  elems = <span class="string">"url"</span> -&gt; <span class="string">"jdbc:mysql://192.168.126.111:3306/spark"</span>,</span><br><span class="line">  <span class="string">"driver"</span> -&gt; <span class="string">"com.mysql.jdbc.Driver"</span>,</span><br><span class="line">  <span class="string">"user"</span> -&gt; <span class="string">"root"</span>,</span><br><span class="line">  <span class="string">"password"</span> -&gt; <span class="string">"123"</span>,</span><br><span class="line">  <span class="string">"dbtable"</span> -&gt; <span class="string">"score"</span></span><br><span class="line"></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> score: <span class="type">DataFrame</span> = spark.read.format(<span class="string">"jdbc"</span>).options(map).load</span><br><span class="line"></span><br><span class="line">score.show()</span><br></pre></td></tr></table></figure>

<p>第三种方式</p>
<p>DataFrameReader</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> reader: <span class="type">DataFrameReader</span> = spark.read.format(<span class="string">"jdbc"</span>)</span><br><span class="line">.option(<span class="string">"url"</span>, <span class="string">"jdbc:mysql://192.168.126.111:3306/spark"</span>)</span><br><span class="line">.option(<span class="string">"driver"</span>, <span class="string">"com.mysql.jdbc.Driver"</span>)</span><br><span class="line">.option(<span class="string">"user"</span>, <span class="string">"root"</span>)</span><br><span class="line">.option(<span class="string">"password"</span>, <span class="string">"123"</span>)</span><br><span class="line">.option(<span class="string">"dbtable"</span>, <span class="string">"score"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> source2: <span class="type">DataFrame</span> = reader.load()</span><br><span class="line"></span><br><span class="line">source2.show()</span><br></pre></td></tr></table></figure>

<p>第四种方式</p>
<p>Sparksql</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">//将以上两张表注册为临时表，进行关联查询</span></span><br><span class="line">person.createOrReplaceTempView(<span class="string">"person"</span>)</span><br><span class="line">score2.createOrReplaceTempView(<span class="string">"score"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">val</span> result = spark.sql(<span class="string">"select person.id,person.name,person.age,score.score from person,score  where person.id=score.id "</span>)</span><br><span class="line"></span><br><span class="line">result.show()</span><br><span class="line"></span><br><span class="line"><span class="comment">//将查询出的结果保存到mysql表之中</span></span><br><span class="line"></span><br><span class="line">result.write.mode(<span class="type">SaveMode</span>.<span class="type">Append</span>).jdbc(<span class="string">"jdbc:mysql://192.168.126.111:3306/spark"</span>,<span class="string">"result"</span>,properties)</span><br></pre></td></tr></table></figure>

<h3 id="HashMap和hashtable区别，concurrenthashmap底层"><a href="#HashMap和hashtable区别，concurrenthashmap底层" class="headerlink" title="HashMap和hashtable区别，concurrenthashmap底层"></a>HashMap和hashtable区别，concurrenthashmap底层</h3><p>①hashMap实现了Map接口，实现了将唯一隐射到特定值上。允许一个null键和多个null值。非线程安全。</p>
<p>②hashTable：类似于HashMap，但不允许null键和null值，比HashMap慢，因为它是同步的。它是线程安全的类，它使用sysnchronized来锁住整张hash表来实现线程安全，每次锁住整张表让线程独占。</p>
<p>③ConcurrentHashMap：允许多个修改操作并发进行，关键在于使用了锁分离技术。使用多个锁控制对哈希表的不同部分进行的修改。它内部使用段（Segment）来表示这些不同的部分，每个段其实就是一个小的Hashtable，它们有自己的锁。多个修改操作发生在不同的时段上，它们就可以并发进行。</p>
<h3 id="为什么java类加载要使用双亲委派模型"><a href="#为什么java类加载要使用双亲委派模型" class="headerlink" title="为什么java类加载要使用双亲委派模型"></a>为什么java类加载要使用双亲委派模型</h3><p>双亲委派模型的工作过程如下：<br>（1）当前类加载器从自己已经加载的类中查询是否此类已经加载，如果已经加载则直接返回原来已经加载的类。<br>（2）如果没有找到，就去委托父类加载器去加载（如代码c = parent.loadClass(name, false)所示）。父类加载器也会采用同样的策略，查看自己已经加载过的类中是否包含这个类，有就返回，没有就委托父类的父类去加载，一直到启动类加载器。因为如果父加载器为空了，就代表使用启动类加载器作为父加载器去加载。<br>（3）如果启动类加载器加载失败（例如在$JAVA_HOME/jre/lib里未查找到该class），则会抛出一个异常ClassNotFoundException，然后再调用当前加载器的findClass()方法进行加载。 </p>
<p>双亲委派的原因：<br>（1）主要是为了安全性，避免用户自己编写的类动态替换 Java的一些核心类，比如 String。<br>（2）同时也避免了类的重复加载，因为 JVM中区分不同类，不仅仅是根据类名，相同的 class文件被不同的 ClassLoader加载就是不同的两个类。</p>
<h3 id="数据库四大特性"><a href="#数据库四大特性" class="headerlink" title="数据库四大特性"></a>数据库四大特性</h3><p>原子性：事务内包含的所有操作要么全部成功，要么全部失败回滚；实现：日志，将所有的更新操作全部写入日志当中，若因为一些系统奔溃/断电等原因导致事务中的部分更新操作已经执行，部分操作未执行，则通过回溯日志，将操作回滚，使系统保证原子性以及一致性；</p>
<p>一致性：不管任何时间有少个并发的事务，系统也必须保持一致；</p>
<p>隔离性：多个并发的事务的操作，在同一时间只能有一个事务执行（及串行的执行）；</p>
<p>持久性：事务正确执行后，事务中对数据的操作不会回滚；</p>
<h3 id="数据库最左匹配原则"><a href="#数据库最左匹配原则" class="headerlink" title="数据库最左匹配原则"></a>数据库最左匹配原则</h3><p>写在前面：我在上大学的时候就听说过数据库的最左匹配原则，当时是通过各大博客论坛了解的，但是这些博客的局限性在于它们对最左匹配原则的描述就像一些数学定义一样，往往都是列出123点，满足这123点就能匹配上索引，否则就不能。但是我觉得编程不是死记硬背，这个所谓最左匹配原则肯定是有他背后的原理的。所以我尝试说明一下这个原理，这样以后用上优化索引的时候就不需要去记这些像数学定理一样的东西。了解原理比记住某些表面特点，我觉得是更聪明的方式。</p>
<ol>
<li>简单说下什么是最左匹配原则<br>顾名思义：最左优先，以最左边的为起点任何连续的索引都能匹配上。同时遇到范围查询(&gt;、&lt;、between、like)就会停止匹配。<br>例如：b = 2 如果建立(a,b)顺序的索引，是匹配不到(a,b)索引的；但是如果查询条件是a = 1 and b = 2或者a=1(又或者是b = 2 and b = 1)就可以，因为优化器会自动调整a,b的顺序。再比如a = 1 and b = 2 and c &gt; 3 and d = 4 如果建立(a,b,c,d)顺序的索引，d是用不到索引的，因为c字段是一个范围查询，它之后的字段会停止匹配。</li>
<li>最左匹配原则的原理<br>最左匹配原则都是针对联合索引来说的，所以我们有必要了解一下联合索引的原理。了解了联合索引，那么为什么会有最左匹配原则这种说法也就理解了。</li>
</ol>
<p>我们都知道索引的底层是一颗B+树，那么联合索引当然还是一颗B+树，只不过联合索引的健值数量不是一个，而是多个。构建一颗B+树只能根据一个值来构建，因此数据库依据联合索引最左的字段来构建B+树。<br>例子：假如创建一个（a,b)的联合索引，那么它的索引树是这样的</p>
<p><img src="https://lixiangbetter.github.io/2021/04/10/%E9%9D%A2%E8%AF%95%E9%A2%98%E9%9A%8F%E7%AC%94-21-04-10/1281680-20190117145740508-758737271.png" alt></p>
<p>可以看到a的值是有顺序的，1，1，2，2，3，3，而b的值是没有顺序的1，2，1，4，1，2。所以b = 2这种查询条件没有办法利用索引，因为联合索引首先是按a排序的，b是无序的。</p>
<p>同时我们还可以发现在a值相等的情况下，b值又是按顺序排列的，但是这种顺序是相对的。所以最左匹配原则遇上范围查询就会停止，剩下的字段都无法使用索引。例如a = 1 and b = 2 a,b字段都可以使用索引，因为在a值确定的情况下b是相对有序的，而a&gt;1and b=2，a字段可以匹配上索引，但b值不可以，因为a的值是一个范围，在这个范围中b是无序的。</p>
<p>链接:<a href="https://www.cnblogs.com/lanqi/p/10282279.html" target="_blank" rel="noopener">https://www.cnblogs.com/lanqi/p/10282279.html</a></p>
<h3 id="Java里你自己怎么实现栈、队列、HashMap"><a href="#Java里你自己怎么实现栈、队列、HashMap" class="headerlink" title="Java里你自己怎么实现栈、队列、HashMap"></a>Java里你自己怎么实现栈、队列、HashMap</h3><p>（1）两个栈实现一个队列</p>
<p>（2）两个队列实现一个栈</p>
<h3 id="GC算法有哪些？"><a href="#GC算法有哪些？" class="headerlink" title="GC算法有哪些？"></a>GC算法有哪些？</h3><p>1：标记—清除 Mark-Sweep<br>过程：标记可回收对象，进行清除<br>缺点：标记和清除效率低，清除后会产生内存碎片</p>
<p>2：复制算法<br>过程：将内存划分为相等的两块，将存活的对象复制到另一块内存，把已经使用的内存清理掉<br>缺点：使用的内存变为了原来的一半<br>进化：将一块内存按8:1的比例分为一块Eden区（80%）和两块Survivor区（10%）<br>每次使用Eden和一块Survivor，回收时，将存活的对象一次性复制到另一块Survivor上，如果另一块Survivor空间不足，则使用分配担保机制存入老年代</p>
<p>3：标记—整理 Mark—Compact<br>过程：所有存活的对象向一端移动，然后清除掉端边界以外的内存</p>
<p>4：分代收集算法<br>过程：将堆分为新生代和老年代，根据区域特点选用不同的收集算法，如果新生代朝生夕死，则采用复制算法，老年代采用标记清除，或标记整理</p>
<ul>
<li>对象怎样有新生代转到年老代</li>
</ul>
<p>大对象直接进入老年代中。-XX:+PretenuerSizeThreshold 控制”大对象的“的大小。即当创建的对象大于这个临界值时，则该对象直接进入老年代。</p>
<p>长期存活的对象将进入老年代。虚拟机对每个对象定义了一个对象年龄（Age）计数器。当年龄增加到一定的临界值时，就会晋升到老年代中，该临界值由参数：-XX:MaxTenuringThreshold来设置。如果对象在Eden出生并在第一次发生Minor GC时仍然存活，并且能够被Survivor中所容纳的话，则该对象会被移动到Survivor中，并且设Age=1；以后每经历一次Minor GC，该对象还存活的话会被移动到另一个Survivor区中，并且Age=Age+1。</p>
<h3 id="进程并发和线程并发"><a href="#进程并发和线程并发" class="headerlink" title="进程并发和线程并发"></a>进程并发和线程并发</h3><p>首先：</p>
<p>​    并行是指两个或者多个事件在同一时刻发生；而并发是指两个或多个事件在同一时间间隔发生。</p>
<p>多进程其实是将所有进程按时间分为一个一个的时间片，每一个时刻只执行该运行的片，时间片过期后转而执行下一个进程。</p>
<p>线程并发，是一个进程中多个线程并发执行。</p>
<h3 id="进程间有哪些通信方式？进程怎样共享内存？"><a href="#进程间有哪些通信方式？进程怎样共享内存？" class="headerlink" title="进程间有哪些通信方式？进程怎样共享内存？"></a>进程间有哪些通信方式？进程怎样共享内存？</h3><p>共享内存是进程间通信中最简单的方式之一。共享内存允许两个或更多进程访问同一块内存，就如同 malloc() 函数向不同进程返回了指向同一个物理内存区域的指针。当一个进程改变了这块地址中的内容的时候，其它进程都会察觉到这个更改。</p>
<ul>
<li>共享内存的特点：</li>
</ul>
<ol>
<li>共享内存是进程间共享数据的一种最快的方法。一个进程向共享的内存区域写入了数据，共享这个内存区域的所有进程就可以立刻看到其中的内容。</li>
<li>使用共享内存要注意的是多个进程之间对一个给定存储区访问的互斥。若一个进程正在向共享内存区写数据，则在它做完这一步操作前，别的进程不应当去读、写这些数据。</li>
</ol>
<h3 id="僵尸进程是怎么出现的？"><a href="#僵尸进程是怎么出现的？" class="headerlink" title="僵尸进程是怎么出现的？"></a>僵尸进程是怎么出现的？</h3><p>僵尸进程是当子进程比父进程先结束，而父进程又没有回收子进程，释放子进程占用的资源，此时子进程将成为一个僵尸进程</p>
<p>一个进程结束了，但是他的父进程没有等待(调用wait / waitpid)他， 那么他将变成一个僵尸进程。 但是如果该进程的父进程已经先结束了，那么该进程就不会变成僵尸进程， 因为每个进程结束的时候，系统都会扫描当前系统中所运行的所有进程， 看有没有哪个进程是刚刚结束的这个进程的子进程，如果是的话，就由Init 来接管他，成为他的父进程</p>
<h3 id="TCP-和-UDP-的区别"><a href="#TCP-和-UDP-的区别" class="headerlink" title="TCP 和 UDP 的区别"></a>TCP 和 UDP 的区别</h3><ul>
<li>TCP 是面向连接的，UDP 是面向无连接的</li>
<li>UDP程序结构较简单</li>
<li>TCP 是面向字节流的，UDP 是基于数据报的</li>
<li>TCP 保证数据正确性，UDP 可能丢包</li>
<li>TCP 保证数据顺序，UDP 不保证</li>
</ul>
<p>udp的好处：</p>
<p> 快。</p>
<p>比TCP稍安全 UDP没有TCP的握手、确认、窗口、重传、拥塞控制等机制，UDP是一个无状态的传输协议，所以它在传递数据时非常快。没有TCP的这些机制，UDP较TCP被攻击者利用的漏洞就要少一些。</p>
<h3 id="HTTP常见状态码有哪些"><a href="#HTTP常见状态码有哪些" class="headerlink" title="HTTP常见状态码有哪些?"></a>HTTP常见状态码有哪些?</h3><ul>
<li>2开头状态码；2xx (成功)表示成功处理了请求的状态代码；如：200 (成功) 服务器已成功处理了请求。</li>
<li>3开头状态码；3xx (重定向) 表示要完成请求，需要进一步操作。 通常，这些状态代码用来重定向。如：304 (未修改) 自从上次请求后，请求的网页未修改过。 服务器返回此响应时，不会返回网页内容</li>
<li>4开头状态码；4xx(请求错误) 这些状态代码表示请求可能出错，妨碍了服务器的处理；如：400 (错误请求) 服务器不理解请求的语法；403 (禁止) 服务器拒绝请求。404 (未找到) 服务器找不到请求的网页。</li>
<li>5开头状态码；5xx(服务器错误)这些状态代码表示服务器在尝试处理请求时发生内部错误。 这些错误可能是服务器本身的错误，而不是请求出错；如：500 (服务器内部错误) 服务器遇到错误，无法完成请求</li>
</ul>
<h3 id="甲乙两人轮流拿100枚硬币，一次可拿1-5枚，拿到最后一枚的人获胜，如果甲先拿，怎样可以保证甲获胜？"><a href="#甲乙两人轮流拿100枚硬币，一次可拿1-5枚，拿到最后一枚的人获胜，如果甲先拿，怎样可以保证甲获胜？" class="headerlink" title="甲乙两人轮流拿100枚硬币，一次可拿1-5枚，拿到最后一枚的人获胜，如果甲先拿，怎样可以保证甲获胜？"></a>甲乙两人轮流拿100枚硬币，一次可拿1-5枚，拿到最后一枚的人获胜，如果甲先拿，怎样可以保证甲获胜？</h3><p>甲先拿4个,以后的拿法为：乙拿x个,1≤x≤5,则甲拿6-x个<br>乙拿一次甲拿一次算一轮的话,每轮都会减少6个球.<br>所以最后一轮会剩下6个,无论乙拿几个甲都会拿到最后1个</p>
<h3 id="还了解什么其他的数据库吗？"><a href="#还了解什么其他的数据库吗？" class="headerlink" title="还了解什么其他的数据库吗？"></a>还了解什么其他的数据库吗？</h3><p>redis：</p>
<p>缓存穿透，缓存雪崩，缓存击穿</p>
<p>哨兵模式</p>
<p>数据类型</p>
<p>持久化（rdb,aof)</p>
<p>悲观锁</p>
<p>乐观锁</p>
<h3 id="将一条线段任意分成三段-求这三条线段可以组成一个三角形的概率"><a href="#将一条线段任意分成三段-求这三条线段可以组成一个三角形的概率" class="headerlink" title="将一条线段任意分成三段,求这三条线段可以组成一个三角形的概率."></a>将一条线段任意分成三段,求这三条线段可以组成一个三角形的概率.</h3><p>设线段长为a,任意分成三段的长度分别是x 、y 和z=a-(x+y) ,<br>x +y＜a<br>三段能构成三角形,则<br>x+y＞z,即 x+y&gt;(a-x-y),x +y&gt;a/2<br>y+z＞x,即 y+(a-x-y)&gt;x,x＜a/2<br>z+x＞y,即 (a-x-y)+x&gt;y,y＜a/2<br>所求概率等于x+y=a/2、x=a/2、y=a/2三条直线所包围图形的面积除以直线(x+y)=a与x轴、y轴所包围图形的面积.<br>故将一条线段任意分成三段,这三条线段可以组成一个三角形的概率是<br>（a/2<em>a/2</em>1/2）÷（a<em>a</em>1/2）=a²/8÷a²/2=1/4</p>
]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>刷题</tag>
      </tags>
  </entry>
  <entry>
    <title>面试题随笔-21/4/9</title>
    <url>/2021/04/09/%E9%9D%A2%E8%AF%95%E9%A2%98%E9%9A%8F%E7%AC%94-21-4-9/</url>
    <content><![CDATA[<h3 id="红黑树理解"><a href="#红黑树理解" class="headerlink" title="红黑树理解"></a>红黑树理解</h3><p>红黑树的性质（重点）：</p>
<ol>
<li>每个结点不是红色就是黑色</li>
<li>不可能有连在一起的红色结点</li>
<li>根结点都是黑色</li>
<li>每个红色结点的两个子结点都是黑色。叶子结点都是黑色：出度为0</li>
</ol>
<p>变换规则</p>
<p>旋转和颜色的变换规则：所有插入的点默认为红色</p>
<ol>
<li>变颜色的情况：当前结点的父亲是红色，且它的祖父结点的另一个子结点也是红色。（叔叔结点）<ol>
<li>把父结点设为黑色</li>
<li>把叔叔结点设为黑色</li>
<li>把祖父也就是父亲的父亲设为红色（爷爷）</li>
<li>把指针定义到祖父结点设为当前要操作的。（爷爷）分析的点变换规则</li>
</ol>
</li>
<li>左旋：当前父结点是红色，叔叔是黑色的时候，且当前的结点是右子树。左旋以父结点作为左旋。</li>
<li>右旋：当前父结点是红色，叔叔是黑色的时候，且当前的结点是左子树。右旋<ol>
<li>把父结点设为黑色</li>
<li>把祖父结点变为红色（爷爷）</li>
<li>以祖父结点旋转（爷爷）</li>
</ol>
</li>
</ol>
<p><img src="https://lixiangbetter.github.io/2021/04/09/%E9%9D%A2%E8%AF%95%E9%A2%98%E9%9A%8F%E7%AC%94-21-4-9/1617936441936.jpg" alt></p>
<p><img src="https://lixiangbetter.github.io/2021/04/09/%E9%9D%A2%E8%AF%95%E9%A2%98%E9%9A%8F%E7%AC%94-21-4-9/1617936527318.jpg" alt></p>
<p>b站链接：<a href="https://www.bilibili.com/video/BV1tE411f7tP?p=4" target="_blank" rel="noopener">https://www.bilibili.com/video/BV1tE411f7tP?p=4</a></p>
<h3 id="B-树"><a href="#B-树" class="headerlink" title="B+树"></a>B+树</h3><p>记录一片博客<a href="http://www.liuzk.com/410.html" target="_blank" rel="noopener">http://www.liuzk.com/410.html</a></p>
<p>写的比较好</p>
<h2 id="面试模拟"><a href="#面试模拟" class="headerlink" title="面试模拟"></a>面试模拟</h2><h3 id="请你解释一下TCP为什么可靠一些"><a href="#请你解释一下TCP为什么可靠一些" class="headerlink" title="请你解释一下TCP为什么可靠一些"></a>请你解释一下TCP为什么可靠一些</h3><p>主要是tcp的三次握手、超时重传、滑动窗口、拥塞控制等决定的。</p>
<p>TCP 连接的每一端都必须设有两个窗口——一个发送窗口和一个接收窗口。TCP 的可靠传输机制用字节的序号进行</p>
<p>制。TCP 所有的确认都是基于序号而不是基于报文段。 发送过的数据未收到确认之前必须保留，以便超时重传时使用。</p>
<p>发送窗口没收到确认不动，和收到新的确认后前移。 </p>
<p>拥塞控制：当网络拥塞时，减少数据的发送。</p>
<h3 id="请你说明一下，TCP协议的4次挥手。"><a href="#请你说明一下，TCP协议的4次挥手。" class="headerlink" title="请你说明一下，TCP协议的4次挥手。"></a>请你说明一下，TCP协议的4次挥手。</h3><p>由于TCP连接是全双工的，因此每个方向都必须单独进行关闭。这个原则是当一方完成它的数据发送任务后就能发送一个FIN来终止这个方向的连接。收到一个 FIN只意味着这一方向上没有数据流动，一个TCP连接在收到一个FIN后仍能发送数据。首先进行关闭的一方将执行主动关闭，而另一方执行被动关闭。 TCP的连接的拆除需要发送四个包，因此称为四次挥手(four-way handshake)。客户端或服务器均可主动发起挥手动作，在socket编程中，任何一方执行close()操作即可产生挥手操作。 （1）客户端A发送一个FIN，用来关闭客户A到服务器B的数据传送。 （2）服务器B收到这个FIN，它发回一个ACK，确认序号为收到的序号加1。和SYN一样，一个FIN将占用一个序号。 （3）服务器B关闭与客户端A的连接，发送一个FIN给客户端A。 （4）客户端A发回ACK报文确认，并将确认序号设置为收到序号加1。</p>
<h3 id="请你简单讲解一下，负载均衡-反向代理模式的优点、缺点"><a href="#请你简单讲解一下，负载均衡-反向代理模式的优点、缺点" class="headerlink" title="请你简单讲解一下，负载均衡 反向代理模式的优点、缺点"></a>请你简单讲解一下，负载均衡 反向代理模式的优点、缺点</h3><p>（1）反向代理（Reverse Proxy）方式是指以代理服务器来接受internet上的连接请求，然后将请求转发给内部网络上的服务器，并将从服务器上得到的结果返回给internet上请求连接的客户端，此时代理服务器对外就表现为一个服务器。</p>
<p>（2）反向代理负载均衡技术是把将来自internet上的连接请求以反向代理的方式动态地转发给内部网络上的多台服务器进行处理，从而达到负载均衡的目的。</p>
<p>（3）反向代理负载均衡能以软件方式来实现，如apache mod_proxy、netscape proxy等，也可以在高速缓存器、负载均衡器等硬件设备上实现。反向代理负载均衡可以将优化的负载均衡策略和代理服务器的高速缓存技术结合在一起，提升静态网页的访问速度，提供有益的性能；由于网络外部用户不能直接访问真实的服务器，具备额外的安全性（同理，NAT负载均衡技术也有此优点）。 </p>
<p>（4）其缺点主要表现在以下两个方面 反向代理是处于OSI参考模型第七层应用的，所以就必须为每一种应用服务专门开发一个反向代理服务器，这样就限制了反向代理负载均衡技术的应用范围，现在一般都用于对web服务器的负载均衡。 针对每一次代理，代理服务器就必须打开两个连接，一个对外，一个对内，因此在并发连接请求数量非常大的时候，代理服务器的负载也就非常大了，在最后代理服务器本身会成为服务的瓶颈。 一般来讲，可以用它来对连接数量不是特别大，但每次连接都需要消耗大量处理资源的站点进行负载均衡，如search等。</p>
<h3 id="请你谈谈DNS的寻址过程。"><a href="#请你谈谈DNS的寻址过程。" class="headerlink" title="请你谈谈DNS的寻址过程。"></a>请你谈谈DNS的寻址过程。</h3><p>1、客户机发出查询请求，在本地计算机缓存查找，若没有找到，就会将请求发送给dns服务器</p>
<p>2、先发送给本地dns服务器，本地的就会在自己的区域里面查找，若找到，根据此记录进行解析，若没有找到，就会在本地的缓存里面查找</p>
<p>3、本地服务器没有找到客户机查询的信息，就会将此请求发送到根域名dns服务器</p>
<p>4、根域名服务器解析客户机请求的根域部分，它把包含的下一级的dns服务器的地址返回到客户机的dns服务器地址</p>
<p>5、客户机的dns服务器根据返回的信息接着访问下一级的dns服务器</p>
<p>6、这样递归的方法一级一级接近查询的目标，最后在有目标域名的服务器上面得到相应的IP信息</p>
<p>7、客户机的本地的dns服务器会将查询结果返回给我们的客户机</p>
<p>8、客户机根据得到的ip信息访问目标主机，完成解析过程</p>
<h3 id="什么是icmp协议，它的作用是什么？"><a href="#什么是icmp协议，它的作用是什么？" class="headerlink" title="什么是icmp协议，它的作用是什么？"></a>什么是icmp协议，它的作用是什么？</h3><p>ICMP协议（Internet Control Message Protocol 网际控制报文协议）是ip层协议；作用是 提供主机或者路由器差错报告和异常情况报告，</p>
<p>我们经常使用的ping命令就是一种icmp报文，它用来探测两台主机之间的连通性</p>
<h3 id="请你说明一下，SSL四次握手的过程"><a href="#请你说明一下，SSL四次握手的过程" class="headerlink" title="请你说明一下，SSL四次握手的过程"></a>请你说明一下，SSL四次握手的过程</h3><p>1.客户端请求建立SSL链接，并向服务端发送一个随机数（client random）和客户端支持的加密方法（比如RSA），此时是明文传输的。</p>
<p>2.服务端选择客户端支持的一种加密算法并生成另一个随机数（server random），并将授信的服务端证书和公钥下发给客户端。</p>
<p>3.客户端收到服务端的回复，会校验服务端证书的合法性，若合法，则生成一个新的随机数premaster secret并通过服务端下发的公钥及加密方法进行加密，然后发送给服务端。</p>
<p>4.服务端收到客户端的回复，利用已知的加解密方式进行解密，同时利用client random、server random和premater secret通过一定算法生成对称加密key - session key。</p>
<p>此后，数据传输即通过对称加密方式进行加密传输。</p>
<p>从以上过程可以看看https实际上是用了对称加密技术和非对称加密技术，非对称加密解密速度慢，但安全性高，用来加密对称加密的密钥；而对称加密虽然安全性低，但解密速度快，可以用于传输数据的加密。</p>
<h2 id="今日面经"><a href="#今日面经" class="headerlink" title="今日面经"></a>今日面经</h2><p>链接：<a href="https://www.nowcoder.com/discuss/624109?source_id=profile_create_nctrack&amp;channel=-1" target="_blank" rel="noopener">https://www.nowcoder.com/discuss/624109?source_id=profile_create_nctrack&amp;channel=-1</a></p>
<h3 id="自我介绍"><a href="#自我介绍" class="headerlink" title="自我介绍"></a>自我介绍</h3><p>版本一（不提项目）：你好，我叫李想，来自湖南工商大学计算机学院，**公司是我一直尊敬的公司，这一次我应聘的是大数据开发工程师，大数据开发是我一直想从事的工作。为此我在研究生阶段，很早就结合岗位的要求进行了准备，包括对各种大数据的相关知识的熟悉、大数据组件的实践等。曾经在学校自己写了三个项目，在研究生阶段我也重点选择了大数据相关的课题进行了研究，希望能够加入公司从事大数据开发的相关工作。谢谢</p>
<p>版本二：你好，我叫李想，来自湖南工商大学计算机学院，**公司是我一直尊敬的公司，这一次我应聘的是大数据开发工程师，大数据开发是我一直想从事的工作。为此我在研究生阶段，很早就结合岗位的要求进行了准备，包括对各种大数据的相关知识的熟悉、大数据组件的实践等。曾经在学校自己写了两个项目，分别是离线电商数据仓库项目、移动应用统计分析平台项目，使用的技术大多是Springboot、Hadoop、Hive、Spark Streaming、Kafka、HBase等。做项目过程中，我也遇到很多的技术问题，基本是我自行查询解决的。在研究生阶段我也重点选择了大数据相关的课题进行了研究，希望能够加入公司从事大数据开发的相关工作。谢谢</p>
<h3 id="哪个项目有技术含量，简单介绍下"><a href="#哪个项目有技术含量，简单介绍下" class="headerlink" title="哪个项目有技术含量，简单介绍下"></a>哪个<a href="https://www.nowcoder.com/jump/super-jump/word?word=项目" target="_blank" rel="noopener">项目</a>有技术含量，简单介绍下</h3><p>离线电商数据仓库项目</p>
<h3 id="围绕项目，遇到的困难和挑战，数据量之类"><a href="#围绕项目，遇到的困难和挑战，数据量之类" class="headerlink" title="围绕项目，遇到的困难和挑战，数据量之类"></a>围绕项目，遇到的困难和挑战，数据量之类</h3><ol>
<li>自定义UDF和UDTF解析和调试复杂字段</li>
<li>Sqoop中导入导出Null存储一致性问题</li>
<li>当Sqoop导出数据到MySql时，使用4个map怎么保证数据的一致性</li>
<li>如何优雅的关闭SparkStreaming任务</li>
</ol>
<h3 id="HDFS架构流程原理（面试开发重点）"><a href="#HDFS架构流程原理（面试开发重点）" class="headerlink" title="HDFS架构流程原理（面试开发重点）"></a>HDFS架构流程原理（面试开发重点）</h3><ol>
<li><p>HDFS定义及优缺点<br> HDFS是分布式文件管理系统，这种系统可以管理多台机器的文件，是可以分布式的存储文件，适合一次写入，多次读出的场景，但不支持文件的修改。<br> 优点：多副本提高容错，可以处理大数据，使用机器廉价。<br> 缺点：不适合实时数据，害怕小文件，只能追加不能随机修改。</p>
</li>
<li><p>HDFS成员及工作原理（面试开发重点）</p>
<ul>
<li>NameNode(nn):主管，管理数据块（block）和与客户端通信。<br> 工作机制：nn的元数据存在内存中但是会在磁盘上产生备份元数据的Fsimage，为防止断电丢数据，引入Edits文件，只记录对元数据产生的操作。断电后只需要合并Fsimage和Edits即可恢复原来操作。nn只负责第一次开启时将两个文件加载到内存中合并，后面合并任务交给2nn</li>
<li>DataNode(dn):小弟，存储数据块，执行读写操作。<br> 工作机制：数据块在dn上以文件的形式存储在磁盘上，包括两个文件，一个是数据本身，一个是元数据包括数据块的长度，校验和，时间戳。dn启动后先向nn注册，周期性汇报块信息，心跳是每3秒一次，由dn向nn发送，返回结果带有nn命令，10分钟nn未收到心跳，该节点不可用。</li>
<li>SecondaryNameNode(2nn):秘书，分担nn工作量，定期合并Fsimage和Edits，紧急情况下可恢复nn，2nn在合并了两个文件后将结果返回给nn。</li>
<li>client：客户端，将文件切分为block，输入命令管理HDFS。</li>
</ul>
</li>
<li><p>HDFS文件块大小（面试重点）<br> 文件在HDFS上是按块（block）存储的，在hadoop2.0后为128M，块的大小不能太大也不能太小。</p>
</li>
<li><p>HDFS写数据流程（面试重点）<br>1、客户端通过Distributed FileSystem向nn请求上传文件，nn检查后返回是否可以上传。<br>2、客户端请求第一个block上传到哪个dn上，nn返回dn1，dn2，dn3<br>3、客户端通过FSDataOutputStream请求dn1上传，dn1回将2和3调起，通信管道完成<br>4、客户端上传第一个block到dn1上，以packet为单位，dn1传给2和3<br>5、第一个block传完后，重复上述步骤，传第二个block</p>
</li>
<li><p>HDFS读数据流程<br>1、客户端通过DistributedFileSystem向nn请求下载文件，nn找到文件所在dn地址<br>2、就近挑选一台dn，请求读取数据<br>3、dn开始传输数据到客户端，以packet为单位，客户端接收后现在本地缓存，再写入目标文件</p>
</li>
<li><p>机架感知</p>
<p>通常，大型Hadoop集群会分布在很多机架上。在这种情况下，</p>
<p>  – 希望不同节点之间的通信能够尽量发生在同一个机架之内，而不是跨机架。</p>
<p>  – 为了提高容错能力，名称节点会尽可能把数据块的副本放到多个机架上。</p>
</li>
</ol>
<h3 id="MySQL统计分析的极限，数据量多少会变慢"><a href="#MySQL统计分析的极限，数据量多少会变慢" class="headerlink" title="MySQL统计分析的极限，数据量多少会变慢"></a>MySQL统计分析的极限，数据量多少会变慢</h3><p>首先是存储引擎的考虑，一般我们现在只讨论</p>
<ul>
<li>MyIsam</li>
<li>InnoDB</li>
</ul>
<p>其次和表的索引设置也息息相关.</p>
<p>默认采用B+ Tree.</p>
<p>还要考虑服务器的RAM和磁盘问题。</p>
<p>现在一般推荐一张表考虑到事物的要求，采用InnoDB, 不考虑服务器配置， 数据量保持在 1kw-2kw为最佳，主要支持</p>
<p>来自B+ Tree的索引考虑。</p>
<h3 id="索引有没有了解过，为什么用B-树存储，建立一个索引的时候是新建了一个b-树吗，表里有多少索引就"><a href="#索引有没有了解过，为什么用B-树存储，建立一个索引的时候是新建了一个b-树吗，表里有多少索引就" class="headerlink" title="索引有没有了解过，为什么用B+树存储，建立一个索引的时候是新建了一个b+树吗，表里有多少索引就"></a>索引有没有了解过，为什么用B+树存储，建立一个索引的时候是新建了一个b+树吗，表里有多少索引就</h3><h3 id="有多少b-树吗？"><a href="#有多少b-树吗？" class="headerlink" title="有多少b+树吗？"></a>有多少b+树吗？</h3><ul>
<li>为什么用b树？</li>
</ul>
<p>文件系统和数据库的索引都是存在硬盘上的，并且如果数据量大的话，不一定能一次性加载到内存中。</p>
<ul>
<li>为什么用b+树？</li>
</ul>
<p>数据库中select数据，不一定只选一条，很多时候会选多条，比如按照id排序后选10条。</p>
<p>如果是多条的话，B树需要做局部的中序遍历，可能要跨层访问。而B+树由于所有数据都在叶子结点，不用跨层，同时</p>
<p>由于有链表结构，只需要找到首尾，通过链表就能把所有数据取出来了。</p>
<p>而且数据库中的索引一般是在磁盘上，数据量大的情况可能无法一次装入内存，B+树的设计可以允许数据分批加载，同</p>
<p>时树的高度较低，提高查找效率。</p>
<p>链接：<a href="https://www.sohu.com/a/280609547_818692" target="_blank" rel="noopener">https://www.sohu.com/a/280609547_818692</a></p>
<h3 id="MySQL不好的地方，应用的问题，提到了上面7说的，表大会比较慢（给自己挖坑了，不是特别了解的不要提），扯到了大表优化，水平拆分的原则，id怎么拆分比较好"><a href="#MySQL不好的地方，应用的问题，提到了上面7说的，表大会比较慢（给自己挖坑了，不是特别了解的不要提），扯到了大表优化，水平拆分的原则，id怎么拆分比较好" class="headerlink" title="MySQL不好的地方，应用的问题，提到了上面7说的，表大会比较慢（给自己挖坑了，不是特别了解的不要提），扯到了大表优化，水平拆分的原则，id怎么拆分比较好"></a>MySQL不好的地方，应用的问题，提到了上面7说的，表大会比较慢（给自己挖坑了，不是特别了解的不要提），扯到了大表优化，水平拆分的原则，id怎么拆分比较好</h3><p>在数据库表中，对字段建立索引可以大大提高查询速度。虽说使用索引有很多好处，但过多的使用索引将会造成滥用。因此索引也会有它的缺点：</p>
<p>1.虽然索引大大提高了查询速度，同时却会降低更新表的速度，如对表进行INSERT、UPDATE和DELETE。因为更新表时，MySQL不仅要保存数据，还要保存一下索引文件。</p>
<p>2.建立索引会占用磁盘空间的索引文件。一般情况这个问题不太严重，但如果你在一个大表上创建了多种组合索引，索引文件的会膨胀很快。</p>
<p>索引只是提高效率的一个因素，如果你的MySQL有大数据量的表，就需要花时间研究建立最优秀的索引，或优化查询语句。</p>
<h3 id="hash什么情况下造成冲突，hash冲突的原理是什么，算法是怎样的"><a href="#hash什么情况下造成冲突，hash冲突的原理是什么，算法是怎样的" class="headerlink" title="hash什么情况下造成冲突，hash冲突的原理是什么，算法是怎样的"></a>hash什么情况下造成冲突，hash冲突的原理是什么，算法是怎样的</h3><p>Hash也称散列、哈希，对应的英文都是Hash。基本原理就是把任意长度的输入，通过Hash算法变成固定长度的输出。这个映射的规则就是对应的Hash算法，而原始数据映射后的二进制串就是哈希值。</p>
<p>什么情况下造成冲突？</p>
<p>hash算法的冲突概率要小 由于hash的原理是将输入空间的值映射成hash空间内，而hash值的空间远小于输入的空间。根据抽屉原理，一定会存在不同的输入被映射成相同输出的情况。</p>
<p>哈希是通过对数据进行再压缩，提高效率的一种解决方法。但由于通过哈希函数产生的哈希值是有限的，而数据可能比较多，导致经过哈希函数处理后仍然有不同的数据对应相同的值。这时候就产生了哈希冲突。</p>
<p>hash冲突解决方法</p>
<p>1.开放地址方法</p>
<p>　　（1）线性探测</p>
<p>　　　按顺序决定值时，如果某数据的值已经存在，则在原来值的基础上往后加一个单位，直至不发生哈希冲突。　</p>
<p>2.链式地址法（HashMap的哈希冲突解决方法）</p>
<p>　　对于相同的值，使用链表进行连接。使用数组存储每一个链表。</p>
<p>优点：</p>
<p>　　（1）拉链法处理冲突简单，且无堆积现象，即非同义词决不会发生冲突，因此平均查找长度较短；</p>
<p>　　（2）由于拉链法中各链表上的结点空间是动态申请的，故它更适合于造表前无法确定表长的情况；</p>
<p>　　（3）开放定址法为减少冲突，要求装填因子α较小，故当结点规模较大时会浪费很多空间。而拉链法中可取α≥1，且结点较大时，拉链法中增加的指针域可忽略不计，因此节省空间；</p>
<p>　　（4）在用拉链法构造的散列表中，删除结点的操作易于实现。只要简单地删去链表上相应的结点即可。<br>　　缺点：</p>
<p>　　指针占用较大空间时，会造成空间浪费，若空间用于增大散列表规模进而提高开放地址法的效率。</p>
<p>3.建立公共溢出区</p>
<p>　　建立公共溢出区存储所有哈希冲突的数据。</p>
<p>4.再哈希法</p>
<p>　　对于冲突的哈希值再次进行哈希处理，直至没有哈希冲突。</p>
<h3 id="kafka如何保证顺序性"><a href="#kafka如何保证顺序性" class="headerlink" title="kafka如何保证顺序性"></a>kafka如何保证顺序性</h3><ul>
<li>一个 topic，一个 partition，一个 consumer，内部单线程消费，单线程吞吐量太低，一般不会用这个。</li>
<li>写 N 个内存 queue，具有相同 key 的数据都到同一个内存 queue；然后对于 N 个线程，每个线程分别消费一个内存 queue 即可，这样就能保证顺序性。</li>
</ul>
<h3 id="如何保证消息不被重复消费？（消息的幂等性）"><a href="#如何保证消息不被重复消费？（消息的幂等性）" class="headerlink" title="如何保证消息不被重复消费？（消息的幂等性）"></a>如何保证消息不被重复消费？（消息的幂等性）</h3><p>对于更新操作，天然具有幂等性。<br>对于新增操作，可以给每条消息一个唯一的id，处理前判断是否被处理过。这个id可以存储在 Redis 中，如果是写数据库可以用主键约束。</p>
<h3 id="如何保证消息的可靠性传输？（消息丢失的问题）"><a href="#如何保证消息的可靠性传输？（消息丢失的问题）" class="headerlink" title="如何保证消息的可靠性传输？（消息丢失的问题）"></a>如何保证消息的可靠性传输？（消息丢失的问题）</h3><p>根据kafka架构，有三个地方可能丢失消息：Consumer，Producer和 Server</p>
<ol>
<li>消费端弄丢了数据</li>
</ol>
<p>当 server.properties/enable.auto.commit 设置为 true 的时候，kafka 会先 commit offset 再处理消息，如果这时候出现异常，这条消息就丢失了。</p>
<p>因此可以关闭自动提交 offset，在处理完成后手动提交 offset，这样可以保证消息不丢失；但是如果提交 offset 失败，可能导致重复消费的问题， 这时保证幂等性即可。</p>
<ol start="2">
<li>Kafka弄丢了消息</li>
</ol>
<p>如果某个 broker 不小心挂了，此时若 replica 只有一个，broker 上的消息就丢失了；若 replica &gt; 1 ,给 leader 重新选一个 follower 作为新的 leader, 如果 follower 还有些消息没有同步，这部分消息便丢失了。</p>
<p>可以进行如下配置，避免上面的问题：</p>
<p>给 topic 设置 replication.factor 参数：这个值必须大于 1，要求每个 partition 必须有至少 2 个副本。<br>在 Kafka 服务端设置 min.insync.replicas 参数：这个值必须大于 1，这个是要求一个 leader 至少感知到有至少一个 follower 还跟自己保持联系，没掉队，这样才能确保 leader 挂了还有一个 follower 吧。<br>在 producer 端设置 acks=all：这个是要求每条数据，必须是写入所有 replica 之后，才能认为是写成功了。<br>在 producer 端设置 retries=MAX（很大很大很大的一个值，无限次重试的意思）：这个是要求一旦写入失败，就无限重试，卡在这里了。</p>
<ol start="3">
<li>Producer弄丢了消息</li>
</ol>
<p>在 producer 端设置 acks=all，保证所有的ISR都同步了消息才认为写入成功。</p>
<ol start="4">
<li>如何保证消息的顺序性？</li>
</ol>
<p>kafka 中 partition 上的消息是顺序的，可以将需要顺序消费的消息发送到同一个 partition 上，用单个 consumer 消费。</p>
<h3 id="kafka消息持久化"><a href="#kafka消息持久化" class="headerlink" title="kafka消息持久化"></a>kafka消息持久化</h3><p><a href="https://www.jianshu.com/p/8a4154780204" target="_blank" rel="noopener">https://www.jianshu.com/p/8a4154780204</a></p>
<p>一个 Topic 被分成多 Partition，每个 Partition 在存储层面是一个 append-only 日志文件，属于一个 Partition 的消息都会被直接追加到日志文件的尾部，每条消息在文件中的位置称为 offset（偏移量）。</p>
<p><strong>写</strong></p>
<p>日志文件允许串行附加，并且总是附加到最后一个文件。当文件达到配置指定的大小（<code>log.segment.bytes</code> = 1073741824 (bytes)）时，就会被滚动到一个新文件中（每个文件称为一个 segment file）。日志有两个配置参数：M，强制操作系统将文件刷新到磁盘之前写入的消息数；S，强制操作系统将文件刷新到磁盘之前的时间（秒）。在系统崩溃的情况下，最多会丢失M条消息或S秒的数据。</p>
<p><strong>读</strong></p>
<p>通过给出消息的偏移量（offset）和最大块大小（S）来读取数据。返回一个缓冲区为S大小的消息迭代器，S应该大于任何单个消息的大小，如果消息异常大，则可以多次重试读取，每次都将缓冲区大小加倍，直到成功读取消息为止。可以指定最大消息大小和缓冲区大小，以使服务器拒绝大于某个大小的消息。读取缓冲区可能以部分消息结束，这很容易被大小分隔检测到。</p>
<p>读取指定偏移量的数据时，需要首先找到存储数据的 segment file，由全局偏移量计算 segment file 中的偏移量，然后从此位置开始读取。</p>
<p>有了索引文件，消费者可以从 Kafka 的任意可用偏移量位置开始读取消息。索引也被分成片段，所以在删除消息时，也可以删除相应的索引。Kafka 不维护索引的校验和，如果索引出现损坏，Kafka 会通过重新读取消息来重新生成索引。<br>链接：<a href="https://www.jianshu.com/p/8a4154780204" target="_blank" rel="noopener">https://www.jianshu.com/p/8a4154780204</a><br>来源：简书</p>
<h3 id="G1了解过吗，full-gc的过程，对系统的影响，jvm所有线程都会停顿吗"><a href="#G1了解过吗，full-gc的过程，对系统的影响，jvm所有线程都会停顿吗" class="headerlink" title="G1了解过吗，full gc的过程，对系统的影响，jvm所有线程都会停顿吗"></a>G1了解过吗，full gc的过程，对系统的影响，jvm所有线程都会停顿吗</h3><p>所有的用户线程会挂起。</p>
<h4 id="G1收集器"><a href="#G1收集器" class="headerlink" title="G1收集器"></a>G1收集器</h4><p><code>G1</code>收集器与之前垃圾收集器的一个显著区别就是——之前收集器都有三个区域，新、老两代和元空间。而G1收集器只有G1区和元空间。而G1区，不像之前的收集器，分为新、老两代，而是一个一个Region，每个Region既可能包含新生代，也可能包含老年代。</p>
<p><code>G1</code>收集器既可以提高吞吐量，又可以减少GC时间。最重要的是<strong>STW可控</strong>，增加了预测机制，让用户指定停顿时间。</p>
<p>使用<code>-XX:+UseG1GC</code>开启，还有<code>-XX:G1HeapRegionSize=n</code>、<code>-XX:MaxGCPauseMillis=n</code>等参数可调。</p>
<h4 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h4><ol>
<li><strong>并行和并发</strong>：充分利用多核、多线程CPU，尽量缩短STW。</li>
<li><strong>分代收集</strong>：虽然还保留着新、老两代的概念，但物理上不再隔离，而是融合在Region中。</li>
<li><strong>空间整合</strong>：<code>G1</code>整体上看是<strong>标整</strong>算法，在局部看又是<strong>复制算法</strong>，不会产生内存碎片。</li>
<li><strong>可预测停顿</strong>：用户可以指定一个GC停顿时间，<code>G1</code>收集器会尽量满足。</li>
</ol>
<h4 id="过程"><a href="#过程" class="headerlink" title="过程"></a>过程</h4><p>与<code>CMS</code>类似。</p>
<ol>
<li>初始标记。</li>
<li>并发标记。</li>
<li>最终标记。</li>
<li>筛选回收。</li>
</ol>
<p>G1算法将堆划分为若干个区域(Region），它仍然属于分代收集器。</p>
<ol>
<li><p>这些Region的一部分包含新生代，新生代的垃圾收集依然采用暂停所有应用线程的方式，将存活对象拷贝到老年代或者Survivor空间。</p>
<p>这些Region的一部分包含老年代，G1收集器通过将对象从一个区域复制到另外一个区域，完成了清理工作。这就意味着，在正常的处理过程中，G1完成了堆的压缩（至少是部分堆的压缩），这样也就不会有CMS内存碎片问题的存在了。</p>
</li>
<li><p>在G1中，还有一种特殊的区域，叫Humongous区域。</p>
<p>如果一个对象占用的空间超过了分区容量50%以上，G1收集器就认为这是一个巨型对象。这些巨型对象默认直接会被分配在年老代，但是如果它是一个短期存在的巨型对象，就会对垃圾收集器造成负面影响。</p>
<p>为了解决这个问题，G1划分了一个Humongous区，它用来专门存放巨型对象。如果一个H区装不下一个巨型对象，那么G1会寻找连续的H分区来存储。为了能找到连续的H区，有时候不得不启动Full GC。</p>
</li>
</ol>
<h3 id="jdk1-8新特性"><a href="#jdk1-8新特性" class="headerlink" title="jdk1.8新特性"></a>jdk1.8新特性</h3><p>Lambda 表达式</p>
<p>函数式编程就是一种抽象程度很高的编程范式，纯粹的函数式编程语言编写的函数没有变量，因此，任意一个函数，只要输入是确定的，输出就是确定的，这种纯函数我们称之为没有副作用。</p>
<p>函数式接口</p>
<p>定义：“函数式接口”是指仅仅只包含一个抽象方法的接口，每一个该类型的lambda表达式都会被匹配到这个抽象方法。jdk1.8提供了一个@FunctionalInterface注解来定义函数式接口，如果我们定义的接口不符合函数式的规范便会报错。</p>
<h3 id="7的2007次方最后一位是多少"><a href="#7的2007次方最后一位是多少" class="headerlink" title="7的2007次方最后一位是多少"></a>7的2007次方最后一位是多少</h3>]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>刷题</tag>
      </tags>
  </entry>
  <entry>
    <title>面试题随笔-21/4/6</title>
    <url>/2021/04/06/%E9%9D%A2%E8%AF%95%E9%A2%98%E9%9A%8F%E7%AC%94-21-4-6/</url>
    <content><![CDATA[<h3 id="滑动窗口最大值"><a href="#滑动窗口最大值" class="headerlink" title="滑动窗口最大值"></a>滑动窗口最大值</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 滑动窗口的最大值 (暴力破解）</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">int</span>[] maxSlidingWindow(<span class="keyword">int</span>[] nums, <span class="keyword">int</span> k) &#123;</span><br><span class="line">  <span class="keyword">int</span> len = nums.length;</span><br><span class="line">  <span class="keyword">if</span> (len * k == <span class="number">0</span>) &#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> <span class="keyword">int</span>[<span class="number">0</span>];</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">int</span>[] win = <span class="keyword">new</span> <span class="keyword">int</span>[len - k + <span class="number">1</span>];</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 遍历所有的滑动窗口</span></span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; len - k + <span class="number">1</span>; i++) &#123;</span><br><span class="line">    <span class="keyword">int</span> max = Integer.MAX_VALUE;</span><br><span class="line">    <span class="comment">// 找到每一个滑动窗口的最大值</span></span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> j = i; j &lt; i + k; j++) &#123;</span><br><span class="line">      max = Math.max(max, nums[j]);</span><br><span class="line">    &#125;</span><br><span class="line">    win[i] = max;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> win;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 双向队列法</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">int</span>[] maxSlidingWindow2(<span class="keyword">int</span>[] nums, <span class="keyword">int</span> k) &#123;</span><br><span class="line">  <span class="keyword">if</span> (nums.length == <span class="number">0</span> || k == <span class="number">0</span>) &#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> <span class="keyword">int</span>[<span class="number">0</span>];</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  Deque&lt;Integer&gt; deque = <span class="keyword">new</span> LinkedList&lt;&gt;();</span><br><span class="line">  <span class="keyword">int</span>[] res = <span class="keyword">new</span> <span class="keyword">int</span>[nums.length - k + <span class="number">1</span>];</span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>, i = <span class="number">1</span> - k; j &lt; nums.length; i++, j++) &#123;</span><br><span class="line">    <span class="comment">// 删除 deque 中对应的 nums[i-1]</span></span><br><span class="line">    <span class="keyword">if</span> (i &gt; <span class="number">0</span> &amp;&amp; deque.peekFirst() == nums[i - <span class="number">1</span>]) &#123;</span><br><span class="line">      deque.removeFirst();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="comment">// 保持 deque 递减</span></span><br><span class="line">    <span class="keyword">while</span> (!deque.isEmpty() &amp;&amp; deque.peekLast() &lt; nums[j]) &#123;</span><br><span class="line">      deque.removeLast();</span><br><span class="line">    &#125;</span><br><span class="line">    deque.addLast(nums[j]);</span><br><span class="line">    <span class="comment">// 记录窗口最大值</span></span><br><span class="line">    <span class="keyword">if</span> (i &gt;= <span class="number">0</span>) &#123;</span><br><span class="line">      res[i] = deque.peekFirst();</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> res;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="无重复字符的最长子串"><a href="#无重复字符的最长子串" class="headerlink" title="无重复字符的最长子串"></a>无重复字符的最长子串</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">int</span> <span class="title">lengthOfLongestSubstring</span><span class="params">(String s)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">int</span> n = s.length();</span><br><span class="line">  Set&lt;Character&gt; set = <span class="keyword">new</span> HashSet&lt;&gt;();</span><br><span class="line"></span><br><span class="line">  <span class="keyword">int</span> result = <span class="number">0</span>, i = <span class="number">0</span>, j = <span class="number">0</span>;</span><br><span class="line">  <span class="keyword">while</span> (i &lt; n &amp;&amp; j &lt; n) &#123;</span><br><span class="line">    <span class="comment">// charAt: 返回指定位置处的字符</span></span><br><span class="line">    <span class="keyword">if</span> (!set.contains(s.charAt(j))) &#123;</span><br><span class="line">      set.add(s.charAt(j));</span><br><span class="line">      j++;</span><br><span class="line">      result = Math.max(result, j - i);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      set.remove(s.charAt(i));</span><br><span class="line">      i++;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> result;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="字母异位词"><a href="#字母异位词" class="headerlink" title="字母异位词"></a>字母异位词</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> List&lt;Integer&gt; <span class="title">findAnagrams</span><span class="params">(String s, String p)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">if</span> (s == <span class="keyword">null</span> || p == <span class="keyword">null</span> || s.length() &lt; p.length()) &#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  List&lt;Integer&gt; list = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line"></span><br><span class="line">  <span class="keyword">int</span>[] pArr = <span class="keyword">new</span> <span class="keyword">int</span>[<span class="number">26</span>];</span><br><span class="line">  <span class="keyword">int</span>[] sArr = <span class="keyword">new</span> <span class="keyword">int</span>[<span class="number">26</span>];</span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; p.length(); i++) &#123;</span><br><span class="line">    sArr[s.charAt(i) - <span class="string">'a'</span>]++;</span><br><span class="line">    pArr[p.charAt(i) - <span class="string">'a'</span>]++;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">int</span> i = <span class="number">0</span>;</span><br><span class="line">  <span class="keyword">int</span> j = p.length();</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 窗口大小固定为p的长度</span></span><br><span class="line">  <span class="keyword">while</span> (j &lt; s.length()) &#123;</span><br><span class="line">    <span class="keyword">if</span> (isSame(pArr, sArr))</span><br><span class="line">      list.add(i);</span><br><span class="line">    sArr[s.charAt(i) - <span class="string">'a'</span>]--;</span><br><span class="line">    i++;</span><br><span class="line"></span><br><span class="line">    sArr[s.charAt(j) - <span class="string">'a'</span>]++;</span><br><span class="line">    j++;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (isSame(pArr, sArr)) &#123;</span><br><span class="line">    list.add(i);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> list;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">boolean</span> <span class="title">isSame</span><span class="params">(<span class="keyword">int</span>[] arr1, <span class="keyword">int</span>[] arr2)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; arr1.length; ++i) &#123;</span><br><span class="line">    <span class="keyword">if</span> (arr1[i] != arr2[i]) &#123;</span><br><span class="line">      <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="和为s的连续正数序列"><a href="#和为s的连续正数序列" class="headerlink" title="和为s的连续正数序列"></a>和为s的连续正数序列</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 和为s的连续正数序列</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">int</span>[][] findContinuousSequence(<span class="keyword">int</span> target) &#123;</span><br><span class="line">  ArrayList&lt;<span class="keyword">int</span>[]&gt; res = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line"></span><br><span class="line">  <span class="keyword">int</span> i = <span class="number">1</span>;</span><br><span class="line">  <span class="keyword">int</span> j = <span class="number">1</span>;</span><br><span class="line">  <span class="keyword">int</span> win = <span class="number">0</span>;</span><br><span class="line">  <span class="keyword">while</span> (i &lt;= target / <span class="number">2</span>) &#123;</span><br><span class="line">    <span class="keyword">if</span> (win &lt; target) &#123;</span><br><span class="line">      win += j;</span><br><span class="line">      j++;</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (win &gt; target) &#123;</span><br><span class="line">      win -= i;</span><br><span class="line">      i++;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="keyword">int</span>[] arr = <span class="keyword">new</span> <span class="keyword">int</span>[j - i];</span><br><span class="line">      <span class="keyword">for</span> (<span class="keyword">int</span> k = i; k &lt; j; k++) &#123;</span><br><span class="line">        arr[k - i] = k;</span><br><span class="line">      &#125;</span><br><span class="line">      res.add(arr);</span><br><span class="line">      win -= i;</span><br><span class="line">      i++;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> res.toArray(<span class="keyword">new</span> <span class="keyword">int</span>[res.size()][]);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>刷题</tag>
      </tags>
  </entry>
  <entry>
    <title>面试题随笔-21/4/5</title>
    <url>/2021/04/06/%E9%9D%A2%E8%AF%95%E9%A2%98%E9%9A%8F%E7%AC%94-21-4-5/</url>
    <content><![CDATA[<h3 id="二叉搜索树查找"><a href="#二叉搜索树查找" class="headerlink" title="二叉搜索树查找"></a>二叉搜索树查找</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 二叉搜索树的查找</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> TreeNode <span class="title">searchBST</span><span class="params">(TreeNode root, <span class="keyword">int</span> val)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">if</span> (root == <span class="keyword">null</span>)</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (root.value &gt; val) &#123;</span><br><span class="line">    <span class="keyword">return</span> searchBST(root.left, val);</span><br><span class="line">  &#125; <span class="keyword">else</span> <span class="keyword">if</span> (root.value &lt; val) &#123;</span><br><span class="line">    <span class="keyword">return</span> searchBST(root.right, val);</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="keyword">return</span> root;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 迭代法</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> TreeNode <span class="title">searchBST2</span><span class="params">(TreeNode root, <span class="keyword">int</span> val)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">while</span> (root != <span class="keyword">null</span>) &#123;</span><br><span class="line">    <span class="keyword">if</span> (root.value == val) &#123;</span><br><span class="line">      <span class="keyword">return</span> root;</span><br><span class="line">    &#125; <span class="keyword">else</span> <span class="keyword">if</span> (root.value &gt; val) &#123;</span><br><span class="line">      root = root.left;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      root = root.right;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="二叉搜索树删除某个节点"><a href="#二叉搜索树删除某个节点" class="headerlink" title="二叉搜索树删除某个节点"></a>二叉搜索树删除某个节点</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> TreeNode <span class="title">deleteNode</span><span class="params">(TreeNode root, <span class="keyword">int</span> key)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">if</span> (root == <span class="keyword">null</span>) &#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (key &lt; root.value) &#123;</span><br><span class="line">    root.left = deleteNode(root.left, key);</span><br><span class="line">    <span class="keyword">return</span> root;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (key &gt; root.value) &#123;</span><br><span class="line">    root.right = deleteNode(root.right, key);</span><br><span class="line">    <span class="keyword">return</span> root;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 到这里意味已经查找到目标</span></span><br><span class="line">  <span class="keyword">if</span> (root.right == <span class="keyword">null</span>) &#123;</span><br><span class="line">    <span class="comment">// 右子树为空</span></span><br><span class="line">    <span class="keyword">return</span> root.left;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">if</span> (root.left == <span class="keyword">null</span>) &#123;</span><br><span class="line">    <span class="comment">// 左子树为空</span></span><br><span class="line">    <span class="keyword">return</span> root.right;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  TreeNode minNode = root.right;</span><br><span class="line">  <span class="keyword">while</span> (minNode.left != <span class="keyword">null</span>) &#123;</span><br><span class="line">    <span class="comment">// 查找后继</span></span><br><span class="line">    minNode = minNode.left;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  root.value = minNode.value;</span><br><span class="line">  root.right = deleteMinNode(root.right);</span><br><span class="line">  <span class="keyword">return</span> root;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> TreeNode <span class="title">deleteMinNode</span><span class="params">(TreeNode root)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  TreeNode pRight = <span class="keyword">null</span>;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (root.left == <span class="keyword">null</span>) &#123;</span><br><span class="line">    pRight = root.right;</span><br><span class="line">    root.right = <span class="keyword">null</span>;</span><br><span class="line">    <span class="keyword">return</span> pRight;</span><br><span class="line">  &#125;</span><br><span class="line">  root.left = deleteMinNode(root.left);</span><br><span class="line">  <span class="keyword">return</span> root;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="平衡二叉树"><a href="#平衡二叉树" class="headerlink" title="平衡二叉树"></a>平衡二叉树</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//平衡二叉树</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">isBalanced</span><span class="params">(TreeNode root)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">if</span> (root == <span class="keyword">null</span>) &#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (!isBalanced(root.left) || !isBalanced(root.right)) &#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">int</span> leftH = maxDepth(root.left) + <span class="number">1</span>;</span><br><span class="line">  <span class="keyword">int</span> rightH = maxDepth(root.right) + <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (Math.abs(leftH - rightH) &gt; <span class="number">1</span>) &#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">int</span> <span class="title">maxDepth</span><span class="params">(TreeNode root)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">if</span> (root == <span class="keyword">null</span>) &#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> Math.max(maxDepth(root.left), maxDepth(root.right)) + <span class="number">1</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="完全二叉树，求节点个数。"><a href="#完全二叉树，求节点个数。" class="headerlink" title="完全二叉树，求节点个数。"></a>完全二叉树，求节点个数。</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 完全二叉树, 求该树的节点个数。</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">countNodes</span><span class="params">(TreeNode root)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">if</span> (root != <span class="keyword">null</span>) &#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span> + countNodes(root.right) + countNodes(root.left);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> <span class="number">1</span> + countNodes(root.right) + countNodes(root.left);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">countNodes2</span><span class="params">(TreeNode root)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">if</span> (root == <span class="keyword">null</span>) &#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">int</span> left = countLevel(root.left);</span><br><span class="line">  <span class="keyword">int</span> right = countLevel(root.right);</span><br><span class="line">  <span class="keyword">if</span> (left == right) &#123;</span><br><span class="line">    <span class="keyword">return</span> countNodes2(root.right) + (<span class="number">1</span> &lt;&lt; left);</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="keyword">return</span> countNodes2(root.left) + (<span class="number">1</span> &lt;&lt; right);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">int</span> <span class="title">countLevel</span><span class="params">(TreeNode root)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">int</span> level = <span class="number">0</span>;</span><br><span class="line">  <span class="keyword">while</span> (root != <span class="keyword">null</span>) &#123;</span><br><span class="line">    level++;</span><br><span class="line">    root = root.left;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> level;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>刷题</tag>
      </tags>
  </entry>
  <entry>
    <title>面试题随笔-21/4/4</title>
    <url>/2021/04/04/%E9%9D%A2%E8%AF%95%E9%A2%98%E9%9A%8F%E7%AC%94-21-4-4/</url>
    <content><![CDATA[<h3 id="大数打印"><a href="#大数打印" class="headerlink" title="大数打印"></a>大数打印</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 大数打印</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">int</span>[] printNumbers(<span class="keyword">int</span> n) &#123;</span><br><span class="line">  <span class="keyword">int</span> len = (<span class="keyword">int</span>) Math.pow(<span class="number">10</span>, n);</span><br><span class="line">  <span class="keyword">int</span>[] res = <span class="keyword">new</span> <span class="keyword">int</span>[len - <span class="number">1</span>];</span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; len; i++) &#123;</span><br><span class="line">    res[i - <span class="number">1</span>] = i;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> res;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">int</span>[] printNumbers2(<span class="keyword">int</span> n) &#123;</span><br><span class="line">  <span class="keyword">int</span> len = <span class="number">0</span>;</span><br><span class="line">  <span class="keyword">while</span> (n &gt; <span class="number">0</span>) &#123;</span><br><span class="line">    n--;</span><br><span class="line">    len = len * <span class="number">10</span> + <span class="number">9</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">int</span>[] res = <span class="keyword">new</span> <span class="keyword">int</span>[len];</span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt; len + <span class="number">1</span>; i++) &#123;</span><br><span class="line">    res[i - <span class="number">1</span>] = i;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> res;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">printNumbers3</span><span class="params">(<span class="keyword">int</span> n)</span></span>&#123;</span><br><span class="line">  <span class="comment">// 声明字符数组，用来存放一个大数</span></span><br><span class="line">  <span class="keyword">char</span>[] number = <span class="keyword">new</span> <span class="keyword">char</span>[n];</span><br><span class="line">  Arrays.fill(number, <span class="string">'0'</span>);</span><br><span class="line">  <span class="keyword">while</span> (!incrementNumber(number)) &#123;</span><br><span class="line">    saveNumber(number); <span class="comment">// 存储数值</span></span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">boolean</span> <span class="title">incrementNumber</span><span class="params">(<span class="keyword">char</span>[] number)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 循环体退出标识</span></span><br><span class="line">  <span class="keyword">boolean</span> isBreak = <span class="keyword">false</span>;</span><br><span class="line"></span><br><span class="line">  <span class="comment">//进位标识</span></span><br><span class="line">  <span class="keyword">int</span> carryFlag = <span class="number">0</span>;</span><br><span class="line">  <span class="keyword">int</span> l = number.length;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = l - <span class="number">1</span>; i &gt;= <span class="number">0</span>; i--) &#123;</span><br><span class="line">    <span class="comment">//取第i位的数字转化位int</span></span><br><span class="line">    <span class="keyword">int</span> nSum = number[i] - <span class="string">'0'</span> + carryFlag;</span><br><span class="line">    <span class="keyword">if</span> (i == l - <span class="number">1</span>) &#123;</span><br><span class="line">      <span class="comment">//最低位加1</span></span><br><span class="line">      ++nSum;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (nSum &gt;= <span class="number">10</span>) &#123;</span><br><span class="line">      <span class="keyword">if</span> (i == <span class="number">0</span>) &#123;</span><br><span class="line">        isBreak = <span class="keyword">true</span>;</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="comment">//进位之后减10，并把进位标识设置为1</span></span><br><span class="line">        nSum -= <span class="number">10</span>;</span><br><span class="line">        carryFlag = <span class="number">1</span>;</span><br><span class="line">        number[i] = (<span class="keyword">char</span>) (<span class="string">'0'</span> + nSum);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      number[i] = (<span class="keyword">char</span>) (nSum + <span class="string">'0'</span>);</span><br><span class="line">      <span class="keyword">break</span>;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> isBreak;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">saveNumber</span><span class="params">(<span class="keyword">char</span>[] number)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">boolean</span> isBegin0 = <span class="keyword">true</span>;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">char</span> c : number) &#123;</span><br><span class="line">    <span class="keyword">if</span> (isBegin0 &amp;&amp; c != <span class="string">'0'</span>) &#123;</span><br><span class="line">      isBegin0 = <span class="keyword">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (!isBegin0) &#123;</span><br><span class="line">      System.out.print(c);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  System.out.println();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="验证回文串"><a href="#验证回文串" class="headerlink" title="验证回文串"></a>验证回文串</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 验证是否是回文串</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">isPalindrome</span><span class="params">(String s)</span> </span>&#123;</span><br><span class="line">  s = s.toLowerCase().replaceAll(<span class="string">"[^0-9a-z]"</span>, <span class="string">""</span>);</span><br><span class="line"></span><br><span class="line">  <span class="keyword">char</span>[] c = s.toCharArray();</span><br><span class="line">  <span class="keyword">int</span> i = <span class="number">0</span>, j = c.length - <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">while</span> (i &lt; j) &#123;</span><br><span class="line">    <span class="keyword">if</span> (c[i] != c[j]) &#123;</span><br><span class="line">      <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    i++;</span><br><span class="line">    j--;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">isPalindrome2</span><span class="params">(String s)</span> </span>&#123;</span><br><span class="line">  s = s.toLowerCase();</span><br><span class="line">  <span class="keyword">char</span>[] c = s.toCharArray();</span><br><span class="line">  <span class="keyword">int</span> i = <span class="number">0</span>;</span><br><span class="line">  <span class="keyword">int</span> j = s.length() - <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">while</span> (i &lt; j) &#123;</span><br><span class="line">    <span class="keyword">if</span> (!((c[i] &gt;= <span class="string">'0'</span> &amp;&amp; c[i] &lt;= <span class="string">'9'</span>) || (c[i] &gt;= <span class="string">'a'</span> &amp;&amp; c[i] &lt;= <span class="string">'z'</span>))) &#123;</span><br><span class="line">      i++;</span><br><span class="line">      <span class="keyword">continue</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (!((c[j] &gt;= <span class="string">'0'</span> &amp;&amp; c[j] &lt;= <span class="string">'9'</span>) || (c[j] &gt;= <span class="string">'a'</span> &amp;&amp; c[j] &lt;= <span class="string">'z'</span>))) &#123;</span><br><span class="line">      j--;</span><br><span class="line">      <span class="keyword">continue</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (c[i] != c[j]) &#123;</span><br><span class="line">      <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    i++;</span><br><span class="line">    j--;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="kmp"><a href="#kmp" class="headerlink" title="kmp"></a>kmp</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 暴力匹配</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">BFSearch</span><span class="params">(String haystack, String needle)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">char</span>[] h1 = haystack.toCharArray();</span><br><span class="line">  <span class="keyword">char</span>[] n1 = needle.toCharArray();</span><br><span class="line"></span><br><span class="line">  <span class="keyword">int</span> l1 = h1.length;</span><br><span class="line">  <span class="keyword">int</span> l2 = n1.length;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">int</span> i = <span class="number">0</span>, j = <span class="number">0</span>;</span><br><span class="line">  <span class="keyword">while</span> (i &lt; l1 &amp;&amp; j &lt; l2) &#123;</span><br><span class="line">    <span class="keyword">if</span> (h1[i] == h1[j]) &#123;</span><br><span class="line">      i++;</span><br><span class="line">      j++;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      i -= j - <span class="number">1</span>;</span><br><span class="line">      j = <span class="number">0</span>;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (j == l2) &#123;</span><br><span class="line">    <span class="keyword">return</span> i - j;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> -<span class="number">1</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// kmp匹配</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">KmpSearch</span><span class="params">(String haystack, String needle, <span class="keyword">int</span>[] next)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">char</span>[] h1 = haystack.toCharArray();</span><br><span class="line">  <span class="keyword">char</span>[] n1 = needle.toCharArray();</span><br><span class="line"></span><br><span class="line">  <span class="keyword">int</span> l1 = h1.length;</span><br><span class="line">  <span class="keyword">int</span> l2 = n1.length;</span><br><span class="line">  <span class="keyword">int</span> i = <span class="number">0</span>, j = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">while</span> (i &lt; l1 &amp;&amp; j &lt; l2) &#123;</span><br><span class="line">    <span class="keyword">if</span> (j == -<span class="number">1</span> || h1[i] == n1[j]) &#123;</span><br><span class="line">      i++;</span><br><span class="line">      j++;</span><br><span class="line">    &#125;<span class="keyword">else</span> &#123;</span><br><span class="line">      j = next[j];</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (j == l2) &#123;</span><br><span class="line">    <span class="keyword">return</span> i - j;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> -<span class="number">1</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="旋转字符串"><a href="#旋转字符串" class="headerlink" title="旋转字符串"></a>旋转字符串</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 旋转字符串</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">rotateString</span><span class="params">(String A, String B)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">if</span> (A.equals(<span class="string">""</span>) &amp;&amp; B.equals(<span class="string">""</span>)) &#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">int</span> len = A.length();</span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; len; i++) &#123;</span><br><span class="line">    String first = A.substring(<span class="number">0</span>, <span class="number">1</span>);</span><br><span class="line">    String last = A.substring(<span class="number">1</span>, len);</span><br><span class="line">    A = last + first;</span><br><span class="line">    <span class="keyword">if</span> (A.equals(B)) &#123;</span><br><span class="line">      <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">rotateString2</span><span class="params">(String A, String B)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">return</span> A.length() == B.length() &amp;&amp; (A + A).contains(B);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">rotateString3</span><span class="params">(String a, String b)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">char</span>[] A = a.toCharArray();</span><br><span class="line">  <span class="keyword">char</span>[] B = b.toCharArray();</span><br><span class="line"></span><br><span class="line">  <span class="keyword">int</span> n = A.length;</span><br><span class="line">  <span class="keyword">if</span> (n != B.length) <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">  <span class="keyword">if</span> (n == <span class="number">0</span>) <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">int</span>[] next = <span class="keyword">new</span> <span class="keyword">int</span>[n];</span><br><span class="line">  next[<span class="number">0</span>] = -<span class="number">1</span>;</span><br><span class="line">  <span class="keyword">int</span> i = <span class="number">0</span>, j = -<span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">while</span> (i &lt; n - <span class="number">1</span>) &#123;</span><br><span class="line">    <span class="keyword">if</span> (j == -<span class="number">1</span> || B[i] == B[j]) &#123;</span><br><span class="line">      ++i;</span><br><span class="line">      ++j;</span><br><span class="line">      next[i] = j;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      j = next[j];</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  i = <span class="number">0</span>;</span><br><span class="line">  j = <span class="number">0</span>;</span><br><span class="line">  a = a + a;</span><br><span class="line">  A = a.toCharArray();</span><br><span class="line">  <span class="keyword">while</span>(i &lt; <span class="number">2</span> * n &amp;&amp; j &lt; n) &#123;</span><br><span class="line">    <span class="keyword">if</span> (j == -<span class="number">1</span> || A[i] == B[j]) &#123;</span><br><span class="line">      ++i;</span><br><span class="line">      ++j;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      j = next[j];</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span>(j &gt;= n) <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="最后一个单词的长度"><a href="#最后一个单词的长度" class="headerlink" title="最后一个单词的长度"></a>最后一个单词的长度</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 最后一个单词的长度</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">lengthOfLastWord</span><span class="params">(String s)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">if</span> (s == <span class="keyword">null</span> || s.length() == <span class="number">0</span>) &#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">int</span> count = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = s.length() - <span class="number">1</span>; i &gt;= <span class="number">0</span>; i--) &#123;</span><br><span class="line">    <span class="keyword">if</span> (s.charAt(i) == <span class="string">' '</span>) &#123;</span><br><span class="line">      <span class="keyword">if</span>(count==<span class="number">0</span>) <span class="keyword">continue</span>;</span><br><span class="line">      <span class="keyword">break</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    count++;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> count;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">lengthOfLastWord2</span><span class="params">(String s)</span> </span>&#123;</span><br><span class="line">  s = s.trim();</span><br><span class="line">  <span class="keyword">int</span> start = s.lastIndexOf(<span class="string">" "</span>) + <span class="number">1</span>;</span><br><span class="line">  <span class="keyword">return</span> s.substring(start).length();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">lengthOfLastWord3</span><span class="params">(String s)</span> </span>&#123;</span><br><span class="line">  String[] words = s.split(<span class="string">"  "</span>);</span><br><span class="line">  <span class="keyword">if</span> (words.length &lt; <span class="number">1</span>) &#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> words[words.length - <span class="number">1</span>].length();</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> String <span class="title">trim</span><span class="params">(String s)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">char</span>[] value = s.toCharArray();</span><br><span class="line"></span><br><span class="line">  <span class="keyword">int</span> len = value.length;</span><br><span class="line">  <span class="keyword">int</span> st = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">char</span>[] val = value;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">while</span> ((st &lt; len) &amp;&amp; val[st] &lt;= <span class="string">' '</span>) &#123;</span><br><span class="line">    st++;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">while</span> ((st &lt; len) &amp;&amp; (val[len - <span class="number">1</span>] &lt;= <span class="string">' '</span>)) &#123;</span><br><span class="line">    len--;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> ((st &gt; <span class="number">0</span>) || (len &lt; value.length)) ? s.substring(st, len) : s;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="最大深度与DFS"><a href="#最大深度与DFS" class="headerlink" title="最大深度与DFS"></a>最大深度与DFS</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 最大深度与dfs</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">maxDepth</span><span class="params">(TreeNode root)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">if</span> (root == <span class="keyword">null</span>) &#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> Math.max(maxDepth(root.left), maxDepth(root.right)) + <span class="number">1</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="comment">// 非递归DFS</span></span><br><span class="line"><span class="function"><span class="keyword">private</span> List&lt;TreeNode&gt; <span class="title">traversal</span><span class="params">(TreeNode root)</span> </span>&#123;</span><br><span class="line">  List&lt;TreeNode&gt; res = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">  Stack&lt;TreeNode&gt; stack = <span class="keyword">new</span> Stack&lt;&gt;();</span><br><span class="line">  stack.add(root);</span><br><span class="line"></span><br><span class="line">  <span class="keyword">while</span> (!stack.empty()) &#123;</span><br><span class="line">    TreeNode node = stack.peek();</span><br><span class="line">    res.add(node);</span><br><span class="line">    stack.pop();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (node.right != <span class="keyword">null</span>) &#123;</span><br><span class="line">      stack.push(node.right);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">if</span> (node.left != <span class="keyword">null</span>) &#123;</span><br><span class="line">      stack.push(node.left);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> res;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="层次遍历"><a href="#层次遍历" class="headerlink" title="层次遍历"></a>层次遍历</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">private</span> <span class="keyword">static</span> Map&lt;Integer, List&lt;Integer&gt;&gt; res = <span class="keyword">new</span> HashMap&lt;Integer, List&lt;Integer&gt;&gt;();</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">levelOrder</span><span class="params">(TreeNode root)</span> </span>&#123;</span><br><span class="line">  dfs(root, <span class="number">0</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">dfs</span><span class="params">(TreeNode root, <span class="keyword">int</span> level)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (root == <span class="keyword">null</span>) &#123;</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (res.size() == level) &#123;</span><br><span class="line">    List&lt;Integer&gt; tmp = <span class="keyword">new</span> ArrayList&lt;Integer&gt;();</span><br><span class="line">    tmp.add(root.value);</span><br><span class="line">    res.put(level, tmp);</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    res.get(level).add(root.value);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  dfs(root.left, level + <span class="number">1</span>);</span><br><span class="line">  dfs(root.right, level + <span class="number">1</span>);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> Map&lt;Integer, List&lt;Integer&gt;&gt; levelOrder2(TreeNode root) &#123;</span><br><span class="line"></span><br><span class="line">  Map&lt;Integer, List&lt;Integer&gt;&gt; result = <span class="keyword">new</span> HashMap&lt;Integer, List&lt;Integer&gt;&gt;();</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (root == <span class="keyword">null</span>) &#123;</span><br><span class="line">    <span class="keyword">return</span> result;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 定义一个队列</span></span><br><span class="line">  Queue&lt;TreeNode&gt; queue = <span class="keyword">new</span> LinkedList&lt;TreeNode&gt;();</span><br><span class="line">  queue.add(root);</span><br><span class="line">  <span class="keyword">int</span> level = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">while</span> (queue.size() &gt; <span class="number">0</span>) &#123;</span><br><span class="line"></span><br><span class="line">    ArrayList&lt;Integer&gt; tmp = <span class="keyword">new</span> ArrayList&lt;Integer&gt;();</span><br><span class="line">    result.put(level, tmp);</span><br><span class="line"></span><br><span class="line">    <span class="keyword">int</span> level_length = queue.size();</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; level_length; i++) &#123;</span><br><span class="line">      TreeNode node = queue.poll();</span><br><span class="line">      tmp.add(node.value);</span><br><span class="line"></span><br><span class="line">      <span class="keyword">if</span> (node.left != <span class="keyword">null</span>) &#123;</span><br><span class="line">        queue.add(node.left);</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="keyword">if</span> (node.right != <span class="keyword">null</span>) &#123;</span><br><span class="line">        queue.add(node.right);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    level++;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> result;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="二叉搜索树的验证"><a href="#二叉搜索树的验证" class="headerlink" title="二叉搜索树的验证"></a>二叉搜索树的验证</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 二叉搜索树的验证</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">isValidBST</span><span class="params">(TreeNode root)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">if</span> (root == <span class="keyword">null</span>) &#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> isBST(root, Integer.MIN_VALUE, Integer.MAX_VALUE);</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">private</span> <span class="keyword">boolean</span> <span class="title">isBST</span><span class="params">(TreeNode root, <span class="keyword">int</span> min, <span class="keyword">int</span> max)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">if</span> (root == <span class="keyword">null</span>) &#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (min &gt;= root.value || max &lt;= root.value) &#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> isBST(root.left, min, root.value) &amp;&amp; isBST(root.right, root.value, max);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>刷题</tag>
      </tags>
  </entry>
  <entry>
    <title>面试题随笔-21/4/3</title>
    <url>/2021/04/03/%E9%9D%A2%E8%AF%95%E9%A2%98%E9%9A%8F%E7%AC%94-21-4-3/</url>
    <content><![CDATA[<h3 id="环形链表"><a href="#环形链表" class="headerlink" title="环形链表"></a>环形链表</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 环形链表 (该方法还不是理解)</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">hashCycle</span><span class="params">(ListNode head)</span> </span>&#123;</span><br><span class="line">  HashMap&lt;ListNode,Integer&gt; map = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">  <span class="keyword">while</span> (head != <span class="keyword">null</span>) &#123;</span><br><span class="line">    <span class="keyword">if</span> (map.containsKey(head)) &#123;</span><br><span class="line">      <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      map.put(head, head.val);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    head = head.next;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">hashCycle2</span><span class="params">(ListNode head)</span> </span>&#123;</span><br><span class="line">  ListNode slow,fast;</span><br><span class="line">  slow = fast = head;</span><br><span class="line">  <span class="keyword">while</span> (fast != <span class="keyword">null</span> &amp;&amp; fast.next != <span class="keyword">null</span>) &#123;</span><br><span class="line">    fast = fast.next.next;</span><br><span class="line">    slow = slow.next;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (fast == slow)</span><br><span class="line">      <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="两数相加"><a href="#两数相加" class="headerlink" title="两数相加"></a>两数相加</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 两数相加</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> ListNode <span class="title">addTwoNumbers</span><span class="params">(ListNode l1,ListNode l2)</span> </span>&#123;</span><br><span class="line">  ListNode result = <span class="keyword">new</span> ListNode(<span class="number">0</span>);</span><br><span class="line">  <span class="keyword">int</span> tmp = <span class="number">0</span>;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">int</span> i = <span class="number">0</span>;</span><br><span class="line">  ListNode head = <span class="keyword">null</span>;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">while</span> (l1 != <span class="keyword">null</span> || l2 != <span class="keyword">null</span> || tmp != <span class="number">0</span>) &#123;</span><br><span class="line">    <span class="keyword">if</span> (l1 != <span class="keyword">null</span>) &#123;</span><br><span class="line">      tmp += l1.val;</span><br><span class="line">      l1 = l1.next;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (l2 != <span class="keyword">null</span>) &#123;</span><br><span class="line">      tmp += l2.val;</span><br><span class="line">      l2 = l2.next;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    result.next = <span class="keyword">new</span> ListNode(tmp % <span class="number">10</span>);</span><br><span class="line">    tmp = tmp / <span class="number">10</span>;</span><br><span class="line">    result = result.next;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (i == <span class="number">0</span>) &#123;</span><br><span class="line">      head = result;</span><br><span class="line">    &#125;</span><br><span class="line">    i++;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> head;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="爬楼梯"><a href="#爬楼梯" class="headerlink" title="爬楼梯"></a>爬楼梯</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 爬楼梯</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">climbStairs</span><span class="params">(<span class="keyword">int</span> n)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">if</span> (n == <span class="number">1</span>) &#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">1</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">int</span>[] dp = <span class="keyword">new</span> <span class="keyword">int</span>[n + <span class="number">1</span>];</span><br><span class="line">  dp[<span class="number">1</span>] = <span class="number">1</span>;</span><br><span class="line">  dp[<span class="number">2</span>] = <span class="number">2</span>;</span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">3</span>; i &lt;= n; i++) &#123;</span><br><span class="line">    dp[i] = dp[i - <span class="number">1</span>] + dp[i - <span class="number">2</span>];</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> dp[n];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="最大子序和"><a href="#最大子序和" class="headerlink" title="最大子序和"></a>最大子序和</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 最大子序和</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">maxSubArray</span><span class="params">(<span class="keyword">int</span>[] nums)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">if</span> (nums.length &lt; <span class="number">1</span>) &#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">int</span>[] dp = <span class="keyword">new</span> <span class="keyword">int</span>[nums.length];</span><br><span class="line">  <span class="keyword">int</span> result = nums[<span class="number">0</span>];</span><br><span class="line">  dp[<span class="number">0</span>] = nums[<span class="number">0</span>];</span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt; nums.length; i++) &#123;</span><br><span class="line">    dp[i] = Math.max(dp[i - <span class="number">1</span>] + nums[i], nums[i]);</span><br><span class="line">    result = Math.max(dp[i], result);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> result;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="最长上升子序列"><a href="#最长上升子序列" class="headerlink" title="最长上升子序列"></a>最长上升子序列</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 最长上升子序列</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">lengthOfLIS</span><span class="params">(<span class="keyword">int</span>[] nums)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">if</span> (nums.length &lt; <span class="number">1</span>) &#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">int</span>[] dp = <span class="keyword">new</span> <span class="keyword">int</span>[nums.length];</span><br><span class="line">  <span class="keyword">int</span> result = <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; nums.length; i++) &#123;</span><br><span class="line">    dp[i] = <span class="number">1</span>;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; i; j++) &#123;</span><br><span class="line">      <span class="keyword">if</span> (nums[j] &lt; nums[i]) &#123;</span><br><span class="line">        dp[i] = Math.max(dp[j] + <span class="number">1</span>, dp[i]);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    result = Math.max(result, dp[i]);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> result;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="三角形最小路径和"><a href="#三角形最小路径和" class="headerlink" title="三角形最小路径和"></a>三角形最小路径和</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 三角形最小路径和</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">minimumTotal</span><span class="params">(List&lt;List&lt;Integer&gt;&gt; triangle)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">int</span> n = triangle.size();</span><br><span class="line">  <span class="keyword">int</span>[][] f = <span class="keyword">new</span> <span class="keyword">int</span>[n][n];</span><br><span class="line">  f[<span class="number">0</span>][<span class="number">0</span>] = triangle.get(<span class="number">0</span>).get(<span class="number">0</span>);</span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt; n; ++i) &#123;</span><br><span class="line">    f[i][<span class="number">0</span>] = f[i - <span class="number">1</span>][<span class="number">0</span>] + triangle.get(i).get(<span class="number">0</span>);</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">1</span>; j &lt; i; j++) &#123;</span><br><span class="line">      f[i][j] = Math.min(f[i - <span class="number">1</span>][j - <span class="number">1</span>], f[i - <span class="number">1</span>][j]) + triangle.get(i).get(j);</span><br><span class="line">    &#125;</span><br><span class="line">    f[i][i] = f[i - <span class="number">1</span>][i - <span class="number">1</span>] + triangle.get(i).get(i);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">int</span> minTotal = f[n - <span class="number">1</span>][<span class="number">0</span>];</span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; n; ++i) &#123;</span><br><span class="line">    minTotal = Math.min(minTotal, f[n - <span class="number">1</span>][i]);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> minTotal;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="最小路径"><a href="#最小路径" class="headerlink" title="最小路径"></a>最小路径</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 最小路径</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">miniPathSum</span><span class="params">(<span class="keyword">int</span>[][] grid)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">if</span> (grid == <span class="keyword">null</span> || grid.length == <span class="number">0</span> || grid[<span class="number">0</span>].length == <span class="number">0</span>) &#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">int</span> rows = grid.length, columns = grid[<span class="number">0</span>].length;</span><br><span class="line">  <span class="keyword">int</span>[][] dp = <span class="keyword">new</span> <span class="keyword">int</span>[rows][columns];</span><br><span class="line"></span><br><span class="line">  dp[<span class="number">0</span>][<span class="number">0</span>] = grid[<span class="number">0</span>][<span class="number">0</span>];</span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt; rows; i++) &#123;</span><br><span class="line">    dp[i][<span class="number">0</span>] = dp[i - <span class="number">1</span>][<span class="number">0</span>] + grid[i][<span class="number">0</span>];</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">0</span>; j &lt; columns; j++) &#123;</span><br><span class="line">    dp[<span class="number">0</span>][j] = dp[<span class="number">0</span>][j - <span class="number">1</span>] + grid[<span class="number">0</span>][j];</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">1</span>; i &lt; rows; i++) &#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> j = <span class="number">1</span>; j &lt; columns; j++) &#123;</span><br><span class="line">      dp[i][j] = Math.min(dp[i - <span class="number">1</span>][j], dp[i][j - <span class="number">1</span>]) + grid[i][j];</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> dp[rows - <span class="number">1</span>][columns - <span class="number">1</span>];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="打家劫舍"><a href="#打家劫舍" class="headerlink" title="打家劫舍"></a>打家劫舍</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 打家劫舍</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">rob</span><span class="params">(<span class="keyword">int</span>[] nums)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (nums.length &lt; <span class="number">1</span>) &#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (nums.length == <span class="number">1</span>) &#123;</span><br><span class="line">    <span class="keyword">return</span> nums[<span class="number">0</span>];</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (nums.length == <span class="number">2</span>) &#123;</span><br><span class="line">    <span class="keyword">return</span> Math.max(nums[<span class="number">0</span>], nums[<span class="number">1</span>]);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">int</span>[] dp = <span class="keyword">new</span> <span class="keyword">int</span>[nums.length];</span><br><span class="line">  dp[<span class="number">0</span>] = nums[<span class="number">0</span>];</span><br><span class="line">  dp[<span class="number">1</span>] = Math.max(nums[<span class="number">0</span>], nums[<span class="number">1</span>]);</span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">2</span>; i &lt; nums.length; i++) &#123;</span><br><span class="line">    dp[i] = Math.max(dp[i - <span class="number">2</span>] + nums[i], dp[i - <span class="number">1</span>]);</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> dp[nums.length - <span class="number">1</span>];</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="反转字符串"><a href="#反转字符串" class="headerlink" title="反转字符串"></a>反转字符串</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 反转字符串</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">char</span>[] reverseString(<span class="keyword">char</span>[] s) &#123;</span><br><span class="line">  <span class="keyword">int</span> left = <span class="number">0</span>;</span><br><span class="line">  <span class="keyword">int</span> right = s.length - <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">char</span> tmp = <span class="string">'#'</span>;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">while</span> (left &lt; right) &#123;</span><br><span class="line">    tmp = s[left];</span><br><span class="line">    s[left] = s[right];</span><br><span class="line">    s[right] = tmp;</span><br><span class="line"></span><br><span class="line">    left++;</span><br><span class="line">    right--;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> s;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="第一唯一字符"><a href="#第一唯一字符" class="headerlink" title="第一唯一字符"></a>第一唯一字符</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 第一唯一字符</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">firstUniqueChar</span><span class="params">(String s)</span> </span>&#123;</span><br><span class="line">  Map&lt;Character, Integer&gt; frequency = <span class="keyword">new</span> HashMap&lt;Character, Integer&gt;();</span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; s.length(); i++) &#123;</span><br><span class="line">    <span class="keyword">char</span> ch = s.charAt(i);</span><br><span class="line">    frequency.put(ch, frequency.getOrDefault(ch, <span class="number">0</span>) + <span class="number">1</span>);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; s.length(); i++) &#123;</span><br><span class="line">    <span class="keyword">if</span> (frequency.get(s.charAt(i)) == <span class="number">1</span>) &#123;</span><br><span class="line">      <span class="keyword">return</span> i;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> -<span class="number">1</span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="sunday匹配"><a href="#sunday匹配" class="headerlink" title="sunday匹配"></a>sunday匹配</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// sunday 匹配</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">strStr</span><span class="params">(String origin, String aim)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">if</span> (origin == <span class="keyword">null</span> || aim == <span class="keyword">null</span>) &#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="number">0</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">if</span> (origin.length() &lt; aim.length()) &#123;</span><br><span class="line">    <span class="keyword">return</span> -<span class="number">1</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 目标串匹配索</span></span><br><span class="line">  <span class="keyword">int</span> originIndex = <span class="number">0</span>;</span><br><span class="line">  <span class="comment">// 模式串匹配索引</span></span><br><span class="line">  <span class="keyword">int</span> aimIndex = <span class="number">0</span>;</span><br><span class="line">  <span class="comment">// 成功匹配完终止条件： 所有aim均成功匹配</span></span><br><span class="line">  <span class="keyword">while</span> (aimIndex &lt; aim.length()) &#123;</span><br><span class="line">    <span class="comment">// 针对origin匹配完，但aim未匹配完情况处理 如 mississippi sippia</span></span><br><span class="line">    <span class="keyword">if</span> (originIndex &gt; origin.length() - <span class="number">1</span>) &#123;</span><br><span class="line">      <span class="keyword">return</span> -<span class="number">1</span>;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">if</span> (origin.charAt(originIndex) == aim.charAt(aimIndex)) &#123;</span><br><span class="line">      <span class="comment">// 匹配则index均加1</span></span><br><span class="line">      originIndex++;</span><br><span class="line">      aimIndex++;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      <span class="comment">// 在我们上面的样例中，第一次计算值为6，第二次值为13</span></span><br><span class="line">      <span class="keyword">int</span> nextCharIndex = originIndex - aimIndex + aim.length();</span><br><span class="line">      <span class="comment">// 判断下一个目标字符（上面图里的那个绿框框）是否存在。</span></span><br><span class="line">      <span class="keyword">if</span> (nextCharIndex &lt; origin.length()) &#123;</span><br><span class="line">        <span class="comment">// 判断目标字符在模式串中匹配到，返回最后一个匹配的index</span></span><br><span class="line">        <span class="keyword">int</span> step = aim.lastIndexOf(origin.charAt(nextCharIndex));</span><br><span class="line">        <span class="keyword">if</span> (step != -<span class="number">1</span>) &#123;</span><br><span class="line">          <span class="comment">// 不存在的话，设置到目标字符的下一个元素</span></span><br><span class="line">          originIndex = nextCharIndex + <span class="number">1</span>;</span><br><span class="line">        &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">          <span class="comment">// 存在的话，移动对应的数字（参考上文中的存在公式）</span></span><br><span class="line">          originIndex = nextCharIndex - step;</span><br><span class="line">        &#125;</span><br><span class="line">        <span class="comment">// 模式串总是从第一个开始匹配</span></span><br><span class="line">        aimIndex = <span class="number">0</span>;</span><br><span class="line">      &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">        <span class="keyword">return</span> -<span class="number">1</span>;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> originIndex - aimIndex;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>刷题</tag>
      </tags>
  </entry>
  <entry>
    <title>面试题随笔-21/4/2</title>
    <url>/2021/04/02/%E9%9D%A2%E8%AF%95%E9%A2%98%E9%9A%8F%E7%AC%94-21-4-2/</url>
    <content><![CDATA[<h2 id="面试题随笔-21-4-2"><a href="#面试题随笔-21-4-2" class="headerlink" title="面试题随笔-21/4/2"></a>面试题随笔-21/4/2</h2><h3 id="进程和线程共享资源？"><a href="#进程和线程共享资源？" class="headerlink" title="进程和线程共享资源？"></a>进程和线程共享资源？</h3><p>共享：</p>
<ul>
<li>进程代码段</li>
<li>进程的公有数据（利用这些共享的数据，线程很容易的实现相互之间的通讯）</li>
<li>进程打开的文件描述符</li>
<li>进程用户id和进程组id</li>
</ul>
<p>线程独有：</p>
<ul>
<li>线程id</li>
<li>寄存器组的值</li>
<li>线程的栈</li>
<li>错误返回码</li>
</ul>
<h3 id="单链表长度"><a href="#单链表长度" class="headerlink" title="单链表长度"></a>单链表长度</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Node</span> </span>&#123;</span><br><span class="line">    <span class="comment">//链表节点的数据</span></span><br><span class="line">    <span class="keyword">int</span> data;</span><br><span class="line">    <span class="comment">//链表指向的下一个节点的指针</span></span><br><span class="line">    Node next = <span class="keyword">null</span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Node</span><span class="params">(<span class="keyword">int</span> data)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.data = data;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">length</span><span class="params">(Node head)</span> </span>&#123;</span><br><span class="line">    Node curNode = head;</span><br><span class="line">    <span class="keyword">int</span> length = <span class="number">0</span>;</span><br><span class="line">    <span class="keyword">while</span> (curNode != <span class="keyword">null</span>) &#123;</span><br><span class="line">      curNode = curNode.next;</span><br><span class="line">      length ++;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> length;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="最大子串"><a href="#最大子串" class="headerlink" title="最大子串"></a>最大子串</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">int</span> <span class="title">lengthOfLongestSubstring</span><span class="params">(String s)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">int</span> length = s.length();</span><br><span class="line">    <span class="keyword">int</span> ans = <span class="number">0</span>;</span><br><span class="line">    Map&lt;Character, Integer&gt; m = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> start = <span class="number">0</span>, end = <span class="number">0</span>; end &lt; length; end++) &#123;</span><br><span class="line">      <span class="keyword">char</span> c = s.charAt(end);</span><br><span class="line">      <span class="keyword">if</span> (m.containsKey(c)) &#123;</span><br><span class="line">        start = Math.max(start, m.get(c));</span><br><span class="line">      &#125;</span><br><span class="line">      ans = Math.max(ans, end - start + <span class="number">1</span>);</span><br><span class="line">      m.put(c, end + <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> ans;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="数组求交集"><a href="#数组求交集" class="headerlink" title="数组求交集"></a>数组求交集</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 数组求交集</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> HashSet <span class="title">intersection</span><span class="params">(Integer[] strOne, Integer[] strTwo)</span> </span>&#123;</span><br><span class="line">    <span class="keyword">if</span> (ArrayUtils.isEmpty(strOne) || ArrayUtils.isEmpty(strTwo)) &#123;</span><br><span class="line">      <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">    &#125;</span><br><span class="line">    HashSet&lt;Integer&gt; set = <span class="keyword">new</span> HashSet&lt;&gt;(Arrays.asList(strOne));</span><br><span class="line">    set.retainAll(Arrays.asList(strTwo));</span><br><span class="line">    <span class="keyword">return</span> set;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="词联想问题"><a href="#词联想问题" class="headerlink" title="词联想问题"></a>词联想问题</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> List&lt;String&gt; <span class="title">search</span><span class="params">(String in, List&lt;String&gt; data)</span></span>&#123;</span><br><span class="line">    in=in.trim();</span><br><span class="line">    List&lt;String&gt; result = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line">    <span class="keyword">int</span> flag;</span><br><span class="line">    <span class="keyword">for</span>(String s:data)&#123;</span><br><span class="line">      flag=s.indexOf(in);</span><br><span class="line">      <span class="keyword">if</span> (flag != -<span class="number">1</span>) &#123;</span><br><span class="line">        result.add(s);</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">    <span class="keyword">return</span> result;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="狼羊草问题"><a href="#狼羊草问题" class="headerlink" title="狼羊草问题"></a>狼羊草问题</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 狼羊草问题</span></span><br><span class="line"><span class="keyword">private</span> List&lt;String&gt; listThis = <span class="keyword">new</span> ArrayList&lt;String&gt;();</span><br><span class="line"><span class="keyword">private</span> List&lt;String&gt; listThat = <span class="keyword">new</span> ArrayList&lt;String&gt;();</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">boolean</span> <span class="title">isSafe</span><span class="params">(List&lt;String&gt; list)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">if</span> (list.contains(<span class="string">"狼"</span>) &amp;&amp; list.contains(<span class="string">"羊"</span>) || list.contains(<span class="string">"羊"</span>) &amp;&amp; list.contains(<span class="string">"草"</span>)) &#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">false</span>;</span><br><span class="line">  &#125;<span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="keyword">true</span>;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">thisToThat</span><span class="params">()</span> </span>&#123;</span><br><span class="line">  String str = listThis.get(<span class="number">0</span>);</span><br><span class="line">  listThis.remove(str);</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (<span class="keyword">this</span>.isSafe(listThis)) &#123;</span><br><span class="line">    System.out.println(<span class="string">"农夫带着 "</span> + str + <span class="string">"从此岸到彼岸"</span>);</span><br><span class="line">    System.out.println(<span class="string">"此岸"</span> + listThis + <span class="string">","</span> + <span class="string">"彼岸"</span> + listThat);</span><br><span class="line">    System.out.println();</span><br><span class="line">    listThat.add(str);</span><br><span class="line">    thatToThis();</span><br><span class="line">  &#125;<span class="keyword">else</span>&#123;</span><br><span class="line">    listThis.add(str);</span><br><span class="line">    thisToThat();</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">thatToThis</span><span class="params">()</span></span>&#123;</span><br><span class="line">  <span class="keyword">if</span> (listThis.isEmpty()) &#123;</span><br><span class="line">    System.out.println(<span class="string">"此岸"</span> + listThis + <span class="string">","</span> + <span class="string">"彼岸"</span> + listThat);</span><br><span class="line">    <span class="keyword">return</span>;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (isSafe(listThat)) &#123;</span><br><span class="line">    System.out.println(<span class="string">"农夫从彼岸到此岸"</span>);</span><br><span class="line">    System.out.println(<span class="string">"此岸"</span> + listThis + <span class="string">","</span> + <span class="string">"彼岸"</span> + listThat);</span><br><span class="line">    System.out.println();</span><br><span class="line">    thisToThat();</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    String str = listThat.get(<span class="number">0</span>);</span><br><span class="line">    listThat.remove(<span class="number">0</span>);</span><br><span class="line">    <span class="keyword">if</span> (isSafe(listThat)) &#123;</span><br><span class="line">      System.out.println(<span class="string">"农夫带着 "</span> + str + <span class="string">"从彼岸到此岸"</span>);</span><br><span class="line">      System.out.println(<span class="string">"此岸"</span> + listThis + <span class="string">","</span> + <span class="string">"彼岸"</span> + listThat);</span><br><span class="line">      System.out.println();</span><br><span class="line">      listThis.add(str);</span><br><span class="line">      thisToThat();</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      listThat.add(str);</span><br><span class="line">      thatToThis();</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="数组的交集"><a href="#数组的交集" class="headerlink" title="数组的交集"></a>数组的交集</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">int</span>[] intersect(<span class="keyword">int</span>[] num1, <span class="keyword">int</span>[] num2) &#123;</span><br><span class="line">  HashMap&lt;Integer, Integer&gt; map = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; num1.length; i++) &#123;</span><br><span class="line">    <span class="keyword">if</span> (map.get(num1[i]) == <span class="keyword">null</span>) &#123;</span><br><span class="line">      map.put(num1[i], <span class="number">1</span>);</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      map.put(num1[i], map.get(num1[i]) + <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">int</span> k = <span class="number">0</span>;</span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; num2.length; i++) &#123;</span><br><span class="line">    <span class="keyword">if</span> (map.get(num2[i]) != <span class="keyword">null</span> &amp;&amp; map.get(num2[i]) &gt; <span class="number">0</span>)&#123;</span><br><span class="line">      map.put(num2[i], map.get(num2[i]) - <span class="number">1</span>);</span><br><span class="line">      num2[k] = num2[i];</span><br><span class="line">      k++;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> Arrays.copyOfRange(num2, <span class="number">0</span>, k);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="最长公共前缀"><a href="#最长公共前缀" class="headerlink" title="最长公共前缀"></a>最长公共前缀</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 最长公共前缀</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> String <span class="title">longestCommonPrefix</span><span class="params">(String[] strs)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">if</span> (strs.length &lt; <span class="number">1</span>) &#123;</span><br><span class="line">    <span class="keyword">return</span> <span class="string">""</span>;</span><br><span class="line">  &#125;</span><br><span class="line">  String prefix = strs[<span class="number">0</span>];</span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; strs.length; i++) &#123;</span><br><span class="line">    <span class="keyword">while</span> (strs[i].indexOf(prefix) != <span class="number">0</span>) &#123;</span><br><span class="line">      <span class="keyword">if</span> (prefix.length() == <span class="number">0</span>) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="string">""</span>;</span><br><span class="line">      &#125;</span><br><span class="line">      prefix = prefix.substring(<span class="number">0</span>, prefix.length() - <span class="number">1</span>);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> prefix;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="旋转数组"><a href="#旋转数组" class="headerlink" title="旋转数组"></a>旋转数组</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 旋转数组</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">int</span>[] rotate(<span class="keyword">int</span>[] nums, <span class="keyword">int</span> k) &#123;</span><br><span class="line">  <span class="keyword">int</span>[] reverse_1 = reverse(nums);</span><br><span class="line">  <span class="keyword">int</span>[] reverse_2 = reverse(reverse_1);</span><br><span class="line">  <span class="keyword">int</span>[] reverse_3 = reverse(reverse_2);</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> reverse_3;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">private</span> <span class="keyword">int</span>[] reverse(<span class="keyword">int</span>[] arrs) &#123;</span><br><span class="line">  <span class="keyword">int</span> tmp = <span class="number">0</span>;</span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; arrs.length / <span class="number">2</span>; i++) &#123;</span><br><span class="line">    tmp = arrs[i];</span><br><span class="line">    arrs[i] = arrs[arrs.length - i - <span class="number">1</span>];</span><br><span class="line">    arrs[arrs.length - i - <span class="number">1</span>] = tmp;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> arrs;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="加一"><a href="#加一" class="headerlink" title="加一"></a>加一</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">int</span>[] plusOne(<span class="keyword">int</span>[] digits) &#123;</span><br><span class="line">  <span class="keyword">int</span> length = digits.length;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = length - <span class="number">1</span>; i &gt; <span class="number">0</span>; i--) &#123;</span><br><span class="line">    digits[i]++;</span><br><span class="line">    digits[i] %= <span class="number">10</span>;</span><br><span class="line">    <span class="keyword">if</span> (digits[i] != <span class="number">0</span>) &#123;</span><br><span class="line">      <span class="keyword">return</span> digits;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="comment">// 针对第三种情况单独讨论</span></span><br><span class="line">  digits = <span class="keyword">new</span> <span class="keyword">int</span>[length + <span class="number">1</span>];</span><br><span class="line">  digits[<span class="number">0</span>] = <span class="number">1</span>;</span><br><span class="line">  <span class="keyword">return</span> digits;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="两数之和"><a href="#两数之和" class="headerlink" title="两数之和"></a>两数之和</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 两数之和 （ 暴力破解）</span></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">int</span>[] twoSum(<span class="keyword">int</span>[] nums, <span class="keyword">int</span> target) &#123;</span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; nums.length; i++) &#123;</span><br><span class="line">    <span class="keyword">for</span> (<span class="keyword">int</span> j = i+<span class="number">1</span>; j &lt; nums.length; j++) &#123;</span><br><span class="line">      <span class="keyword">if</span> (target - nums[j] == nums[i]) &#123;</span><br><span class="line">        <span class="keyword">return</span> <span class="keyword">new</span> <span class="keyword">int</span>[]&#123;i, j&#125;;</span><br><span class="line">      &#125;</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="keyword">int</span>[] twoSum2(<span class="keyword">int</span>[] nums, <span class="keyword">int</span> target) &#123;</span><br><span class="line">  Map&lt;Integer, Integer&gt; map = <span class="keyword">new</span> HashMap&lt;&gt;();</span><br><span class="line"></span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; nums.length; i++) &#123;</span><br><span class="line">    <span class="keyword">if</span> (map.containsKey(target - nums[i])) &#123;</span><br><span class="line">      <span class="keyword">return</span> <span class="keyword">new</span> <span class="keyword">int</span>[]&#123;map.get(target - nums[i]), i&#125;;</span><br><span class="line">    &#125;</span><br><span class="line">    map.put(nums[i], i);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> <span class="keyword">new</span> <span class="keyword">int</span>[<span class="number">0</span>];</span><br></pre></td></tr></table></figure>

<h3 id="z字形变换"><a href="#z字形变换" class="headerlink" title="z字形变换"></a>z字形变换</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// z字形变换</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> <span class="keyword">static</span> String <span class="title">convert</span><span class="params">(String s, <span class="keyword">int</span> numRows)</span> </span>&#123;</span><br><span class="line">  <span class="keyword">if</span>(numRows == <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">return</span> s;</span><br><span class="line"></span><br><span class="line">  String[] arr = <span class="keyword">new</span> String[numRows];</span><br><span class="line">  Arrays.fill(arr, <span class="string">""</span>);</span><br><span class="line">  <span class="keyword">char</span>[] chars = s.toCharArray();</span><br><span class="line">  <span class="keyword">int</span> len = chars.length;</span><br><span class="line">  <span class="keyword">int</span> period = numRows * <span class="number">2</span> - <span class="number">2</span>;</span><br><span class="line">  <span class="keyword">for</span> (<span class="keyword">int</span> i = <span class="number">0</span>; i &lt; len; i++) &#123;</span><br><span class="line">    <span class="keyword">int</span> mod = i % period;</span><br><span class="line">    <span class="keyword">if</span> (mod &lt; numRows) &#123;</span><br><span class="line">      arr[mod] += chars[i];</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      arr[period - mod] += String.valueOf(chars[i]);</span><br><span class="line">    &#125;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  StringBuilder res = <span class="keyword">new</span> StringBuilder();</span><br><span class="line">  <span class="keyword">for</span> (String ch : arr) &#123;</span><br><span class="line">    res.append(ch);</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> res.toString();</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="删除链表倒数第N个节点"><a href="#删除链表倒数第N个节点" class="headerlink" title="删除链表倒数第N个节点"></a>删除链表倒数第N个节点</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">// 删除链表倒数第N个节点</span></span><br><span class="line"><span class="function"><span class="keyword">public</span> ListNode <span class="title">removeNthFromEnd</span><span class="params">(ListNode head, <span class="keyword">int</span> n)</span> </span>&#123;</span><br><span class="line">  ListNode result = <span class="keyword">new</span> ListNode(<span class="number">0</span>);</span><br><span class="line">  result.next = head;</span><br><span class="line"></span><br><span class="line">  ListNode pre = <span class="keyword">null</span>;</span><br><span class="line">  ListNode cur = result;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">int</span> i = <span class="number">1</span>;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">while</span> (head != <span class="keyword">null</span>) &#123;</span><br><span class="line">    <span class="keyword">if</span> (i &gt;= n) &#123;</span><br><span class="line">      pre = cur;</span><br><span class="line">      cur = cur.next;</span><br><span class="line">    &#125;</span><br><span class="line">    head = head.next;</span><br><span class="line">    i++;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  pre.next = pre.next.next;</span><br><span class="line">  <span class="keyword">return</span> result.next;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="合并两个有序链表"><a href="#合并两个有序链表" class="headerlink" title="合并两个有序链表"></a>合并两个有序链表</h3><figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="function"><span class="keyword">public</span> ListNode <span class="title">mergeTwoLists</span><span class="params">(ListNode l1, ListNode l2)</span> </span>&#123;</span><br><span class="line">  ListNode prehead = <span class="keyword">new</span> ListNode(-<span class="number">1</span>);</span><br><span class="line">  ListNode result = prehead;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">while</span> (l1 != <span class="keyword">null</span> &amp;&amp; l2 != <span class="keyword">null</span>) &#123;</span><br><span class="line">    <span class="keyword">if</span> (l1.val &lt; l2.val) &#123;</span><br><span class="line">      prehead.next = l1;</span><br><span class="line">      l1 = l1.next;</span><br><span class="line">    &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">      prehead.next = l2;</span><br><span class="line">      l2 = l2.next;</span><br><span class="line">    &#125;</span><br><span class="line">    prehead = prehead.next;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">if</span> (l1 != <span class="keyword">null</span>) &#123;</span><br><span class="line">    prehead.next = l1;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="keyword">if</span> (l2 != <span class="keyword">null</span>) &#123;</span><br><span class="line">    prehead.next = l2;</span><br><span class="line">  &#125;</span><br><span class="line"></span><br><span class="line">  <span class="keyword">return</span> result.next;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>算法</category>
      </categories>
      <tags>
        <tag>刷题</tag>
      </tags>
  </entry>
  <entry>
    <title>面试题随笔-21/3/28</title>
    <url>/2021/03/28/%E9%9D%A2%E8%AF%95%E9%A2%98%E9%9A%8F%E7%AC%94-21-3-28/</url>
    <content><![CDATA[<h2 id="面试题随笔-21-3-28"><a href="#面试题随笔-21-3-28" class="headerlink" title="面试题随笔-21/3/28"></a>面试题随笔-21/3/28</h2><h3 id="Spark-机器学习和-Spark-图计算接触过没有，举例说明你用它做过什么？"><a href="#Spark-机器学习和-Spark-图计算接触过没有，举例说明你用它做过什么？" class="headerlink" title="Spark 机器学习和 Spark 图计算接触过没有，举例说明你用它做过什么？"></a>Spark 机器学习和 Spark 图计算接触过没有，举例说明你用它做过什么？</h3><p>Spark 提供了很多机器学习库，我们只需要填入数据，设置参数就可以用了。使用起来非常方便。另外一方面，由于它把所有的东西都写到了内部，我们无法修改其实现过程。要想修改里面的某个环节，还的修改源码，重新编译。比如 kmeans 算法，如果没有特殊需求，很方便。但是spark内部使用的两个向量间的距离是欧式距离。如果你想改为余弦或者马氏距离，就的重新编译源码了。Spark 里面的机器学习库都是一些经典的算法，这些代码网上也好找。这些代码使用起来叫麻烦，但是很灵活。Spark 有一个很大的优势，那就是 RDD。模型的训练完全是并行的。</p>
<p><strong>Spark 的 ML 和 MLLib 两个包区别和联系</strong></p>
<p>技术角度上，面向的数据集类型不一样：ML 的 API 是面向 Dataset 的（Dataframe 是 Dataset 的子集，也就是 Dataset[Row]）， mllib 是面对 RDD 的。Dataset 和 RDD 有啥不一样呢？Dataset 的底端是 RDD。Dataset 对 RDD 进行了更深一层的优化，比如说有 sql 语言类似的黑魔法，Dataset 支持静态类型分析所以在 compile time 就能报错，各种 combinators（map，foreach 等）性能会更好，等等。</p>
<p>编程过程上，构建机器学习算法的过程不一样：ML 提倡使用 pipelines，把数据想成水，水从管道的一段流入，从另一端流出。ML 是1.4比 Mllib 更高抽象的库，它解决如何简洁的设计一个机器学习工作流的问题，而不是具体的某种机器学习算法。未来这两个库会并行发展。</p>
<h3 id="Spark-RDD是怎么容错的，基本原理是什么？"><a href="#Spark-RDD是怎么容错的，基本原理是什么？" class="headerlink" title="Spark RDD是怎么容错的，基本原理是什么？"></a>Spark RDD是怎么容错的，基本原理是什么？</h3><p>一般来说，分布式数据集的容错性有两种方式：数据检查点和记录数据的更新。 </p>
<p>面向大规模数据分析，数据检查点操作成本很高，需要通过数据中心的网络连接在机器之间复制庞大的数据集，而网络带宽往往比内存带宽低得多，同时还需要消耗更多的存储资源。因此，Spark选择记录更新的方式。但是，如果更新粒度太细太多，那么记录更新成本也不低。因此，RDD只支持粗粒度转换，即只记录单个块上执行的单个操作，然后将创建RDD的一系列变换序列（每个RDD都包含了他是如何由其他RDD变换过来的以及如何重建某一块数据的信息。因此RDD的容错机制又称“血统(Lineage)”容错）记录下来，以便恢复丢失的分区。 </p>
<p>Lineage本质上很类似于数据库中的重做日志（Redo Log），只不过这个重做日志粒度很大，是对全局数据做同样的重做进而恢复数据。</p>
<h3 id="为什么要用Yarn来部署Spark"><a href="#为什么要用Yarn来部署Spark" class="headerlink" title="为什么要用Yarn来部署Spark?"></a>为什么要用Yarn来部署Spark?</h3><p>因为 Yarn 支持动态资源配置。Standalone 模式只支持简单的固定资源分配策略，每个任务固定数量的 core，各 Job 按顺序依次分配资源，资源不够的时候就排队。这种模式比较适合单用户的情况，多用户的情境下，会有可能有些用户的任务得不到资源。</p>
<p>Yarn 作为通用的资源调度平台，除了 Spark 提供调度服务之外，还可以为其他系统提供调度，如 Hadoop MapReduce, Hive 等。</p>
<h3 id="说说yarn-cluster和yarn-client的异同点。"><a href="#说说yarn-cluster和yarn-client的异同点。" class="headerlink" title="说说yarn-cluster和yarn-client的异同点。"></a>说说yarn-cluster和yarn-client的异同点。</h3><p>cluster 模式会在集群的某个节点上为 Spark 程序启动一个称为 Master 的进程，然后 Driver 程序会运行正在这个 Master 进程内部，由这种进程来启动 Driver 程序，客户端完成提交的步骤后就可以退出，不需要等待 Spark 程序运行结束，这是适合生产环境的运行方式</p>
<p>client 模式也有一个 Master 进程，但是 Driver 程序不会运行在这个 Master 进程内部，而是运行在本地，只是通过 Master 来申请资源，直到运行结束，这种模式非常适合需要交互的计算。显然 Driver 在 client 模式下会对本地资源造成一定的压力。</p>
<h3 id="解释一下-groupByKey-reduceByKey-还有-reduceByKeyLocally"><a href="#解释一下-groupByKey-reduceByKey-还有-reduceByKeyLocally" class="headerlink" title="解释一下 groupByKey, reduceByKey 还有 reduceByKeyLocally"></a>解释一下 groupByKey, reduceByKey 还有 reduceByKeyLocally</h3><p>Groupbykey: 当调用一个(K, V)对的数据集时，返回一个(K，可迭代<v>)对的数据集。<br>Note: 如果分组是为了对每个键执行聚合(比如求和或平均值)，那么使用 reduceByKey 或 aggregateByKey 将产生更好的性能。<br>Note: 默认情况下，输出中的并行度取决于父 RDD 的分区数。您可以传递一个可选的 numPartitions 参数来设置不同数量的任务。</v></p>
<p>Reducebykey: 当对一个(K, V)对的数据集调用时，返回一个(K, V)对的数据集，其中每个键的值使用给定的 reduce 函数 func 进行聚合，与 groupByKey 类似，reduce 任务的数量可以通过第二个可选参数进行配置。</p>
<p>ReducebykeyLocally: 该函数将RDD[K,V]中每个K对应的V值根据映射函数来运算，运算结果映射到一个Map[K,V]中，而不是RDD[K,V]。</p>
<h3 id="说说-persist-和-cache-的异同"><a href="#说说-persist-和-cache-的异同" class="headerlink" title="说说 persist() 和 cache() 的异同"></a>说说 persist() 和 cache() 的异同</h3><p><strong>RDD的cache和persist的区别</strong></p>
<p>cache()是persist()的简化方式，调用persist的无参版本，也就是调用persist(StorageLevel.MEMORY_ONLY)，cache只有一个默认的缓存级别MEMORY_ONLY，即将数据持久化到内存中，而persist可以通过传递一个 StorageLevel 对象来设置缓存的存储级别。</p>
<p><strong>DataFrame的cache和persist的区别</strong></p>
<p>cache()依然调用的persist()，但是persist调用cacheQuery，而cacheQuery的默认存储级别为MEMORY_AND_DISK，这点和rdd是不一样的。</p>
<h3 id="可以解释一下这两段程序的异同吗"><a href="#可以解释一下这两段程序的异同吗" class="headerlink" title="可以解释一下这两段程序的异同吗"></a>可以解释一下这两段程序的异同吗</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> counter = <span class="number">0</span></span><br><span class="line"><span class="keyword">val</span> data = <span class="type">Seq</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line">data.foreach(x =&gt; counter += x)</span><br><span class="line">println(<span class="string">"Counter value: "</span> + counter)</span><br><span class="line"><span class="keyword">val</span> counter = <span class="number">0</span></span><br><span class="line"><span class="keyword">val</span> data = <span class="type">Seq</span>(<span class="number">1</span>, <span class="number">2</span>, <span class="number">3</span>)</span><br><span class="line"><span class="keyword">var</span> rdd = sc.parallelizze(data)</span><br><span class="line">rdd.foreach(x =&gt; counter += x)</span><br><span class="line">println(<span class="string">"Counter value: "</span> + counter)</span><br></pre></td></tr></table></figure>

<p>所有在 Driver 程序追踪的代码看上去好像在 Driver 上计算，实际上都不在本地，每个 RDD 操作都被转换成 Job 分发至集群的执行器 Executor 进程中运行，即便是单机本地运行模式，也是在单独的执行器进程上运行，与 Driver 进程属于不用的进程。所以每个 Job 的执行，都会经历序列化、网络传输、反序列化和运行的过程。</p>
<p>再具体一点解释是 foreach 中的匿名函数 x =&gt; counter += x 首先会被序列化然后被传入计算节点，反序列化之后再运行，因为 foreach 是 Action 操作，结果会返回到 Driver 进程中。</p>
<p>在序列化的时候，Spark 会将 Job 运行所依赖的变量、方法全部打包在一起序列化，相当于它们的副本，所以 counter 会一起被序列化，然后传输到计算节点，是计算节点上的 counter 会自增，而 Driver 程序追踪的 counter 则不会发生变化。执行完成之后，结果会返回到 Driver 程序中。而 Driver 中的 counter 依然是当初的那个 Driver 的值为0。</p>
<p>因此说，RDD 操作不能嵌套调用，即在 RDD 操作传入的函数参数的函数体中，不可以出现 RDD 调用。</p>
<h3 id="说说map和mapPartitions的区别"><a href="#说说map和mapPartitions的区别" class="headerlink" title="说说map和mapPartitions的区别"></a>说说map和mapPartitions的区别</h3><p>map 中的 func 作用的是 RDD 中每一个元素，而 mapPartitioons 中的 func 作用的对象是 RDD 的一整个分区。所以 func 的类型是 Iterator<t> =&gt; Iterator<t>，其中 T 是输入 RDD 的元素类型。</t></t></p>
<h3 id="groupByKey和reduceByKey是属于Transformation还是-Action？"><a href="#groupByKey和reduceByKey是属于Transformation还是-Action？" class="headerlink" title="groupByKey和reduceByKey是属于Transformation还是 Action？"></a>groupByKey和reduceByKey是属于Transformation还是 Action？</h3><p>前者，因为 Action 输出的不再是 RDD 了，也就意味着输出不是分布式的，而是回送到 Driver 程序。以上两种操作都是返回 RDD，所以应该属于 Transformation。</p>
<h3 id="说说Spark支持的3种集群管理器"><a href="#说说Spark支持的3种集群管理器" class="headerlink" title="说说Spark支持的3种集群管理器"></a>说说Spark支持的3种集群管理器</h3><p>Standalone 模式：资源管理器是 Master 节点，调度策略相对单一，只支持先进先出模式。</p>
<p>Hadoop Yarn 模式：资源管理器是 Yarn 集群，主要用来管理资源。Yarn 支持动态资源的管理，还可以调度其他实现了 Yarn 调度接口的集群计算，非常适用于多个集群同时部署的场景，是目前最流行的一种资源管理系统。</p>
<p>Apache Mesos：Mesos 是专门用于分布式系统资源管理的开源系统，与 Yarn 一样是 C++ 开发，可以对集群中的资源做弹性管理。</p>
<h3 id="说说Worker和Excutor的异同"><a href="#说说Worker和Excutor的异同" class="headerlink" title="说说Worker和Excutor的异同"></a>说说Worker和Excutor的异同</h3><p>Worker 是指每个及节点上启动的一个进程，负责管理本节点，jps 可以看到 Worker 进程在运行。Excutor 每个Spark 程序在每个节点上启动的一个进程，专属于一个 Spark 程序，与 Spark 程序有相同的生命周期，负责 Spark 在节点上启动的 Task，管理内存和磁盘。如果一个节点上有多个 Spark 程序，那么相应就会启动多个Excutor。</p>
<h3 id="说说Spark提供的两种共享变量"><a href="#说说Spark提供的两种共享变量" class="headerlink" title="说说Spark提供的两种共享变量"></a>说说Spark提供的两种共享变量</h3><p>Spark 程序的大部分操作都是 RDD 操作，通过传入函数给 RDD 操作函数来计算，这些函数在不同的节点上并发执行，内部的变量有不同的作用域，不能相互访问，有些情况下不太方便。</p>
<p>广播变量，是一个只读对象，在所有节点上都有一份缓存，创建方法是 SparkContext.broadcast()。创建之后再更新它的值是没有意义的，一般用 val 来修改定义。</p>
<p>计数器，只能增加，可以用计数或求和，支持自定义类型。创建方法是 SparkContext.accumulator(V, name)。只有 Driver 程序可以读这个计算器的变量，RDD 操作中读取计数器变量是无意义的。</p>
<p>以上两种类型都是 Spark 的共享变量。</p>
<h3 id="说说检查点的意义"><a href="#说说检查点的意义" class="headerlink" title="说说检查点的意义"></a>说说检查点的意义</h3><p>在容错机制中，如果一个节点死机了，而且运算窄依赖，则只要把丢失的父RDD分区重算即可，不依赖于其他节点。</p>
<p>而宽依赖需要父RDD的所有分区都存在，重算就很昂贵了。可以这样理解开销的经济与否：在窄依赖中，在子RDD的分区丢失、重算父RDD分区时，父RDD相应分区的所有数据都是子RDD分区的数据，并不存在冗余计算。</p>
<p>在宽依赖情况下，丢失一个子RDD分区重算的每个父RDD的每个分区的所有数据并不是都给丢失的子RDD分区用的，会有一部分数据相当于对应的是未丢失的子RDD分区中需要的数据，这样就会产生冗余计算开销，这也是宽依赖开销更大的原因。因此如果使用Checkpoint算子来做检查点，不仅要考虑Lineage是否足够长，也要考虑是否有宽依赖，对宽依赖加Checkpoint是最物有所值的。</p>
<h3 id="说说Spark的高可用和容错"><a href="#说说Spark的高可用和容错" class="headerlink" title="说说Spark的高可用和容错"></a>说说Spark的高可用和容错</h3><p>Spark 应用程序的高可用性主要包含两个部分：集群环境的高可用以及应用程序的容错特性；集群环境的高可用，主要由集群框架来控制，比如 spark on yarn 模式下的 ResourceManager 的 HA、Spark Standalone 模式下的 Master HA 等特性的设置保障集群的高可用性；至于应用程序的容错需要考虑应用的各个组成部分的容错。</p>
<p>spark 应用程序执行过程中，一般存在以下失败的情况：</p>
<ul>
<li>Driver 集成宕机：Driver 运行机器宕机、Driver 程序运行过程中异常导致宕机</li>
<li>Executor 进程宕机：Executor 所在的work 宕机，Exector 和 Driver 通信超时</li>
<li>Task 执行失败：task 执行过程发生异常导致失败</li>
</ul>
<p>Driver 进程宕机解决方案：</p>
<ul>
<li>监控机器机器是否存活，如果机器宕机，重启服务机器和 spark 集群</li>
<li>通过 spark job 的 history 服务监控应用是否执行成功，如果执行失败，通过开发人员重启服务即可</li>
<li>SparkStreaming 中，重启spark应用后，可通过 checkpoint 进行job数据恢复</li>
</ul>
<p>Executor 宕机解决方案：选择一个work 节点重启Executor 进程，Driver 重新分配任务</p>
<p>Task 执行失败解决方案：</p>
<p>Spark 会自动进行 task 重试机制，如果某个 task 失败重试次数超过3次（spark.task.maxFailures）后，当前job 执行失败；local 模式默认不启用 task 重试机制。<br>Task 数据恢复/重新运行的机制实际上是 RDD 容错机制，即 Lineage 机制，RDD的 Lineage 机制记录的是粗粒度的特定数据的 Transformation 操作行为。当这个 RDD 的部分数据丢失时，它可以通过 lineage 获取足够的信息来重新运算和恢复丢失的数据分区；该机制体现在RDD上就是RDD依赖特性。<br>如果 rdd 的 lineage 的生命线特别长，此时某些 task 执行失败的恢复成本就会比较高，那么可以采用检查点或缓存的方式将数据冗余下来，当检查点/缓存点之后的rdd的task出现异常的时候，可以直接从检查点重新构建lineage，可以减少执行开销。</p>
<h3 id="解释一下Spark-Master的选举过程"><a href="#解释一下Spark-Master的选举过程" class="headerlink" title="解释一下Spark Master的选举过程"></a>解释一下Spark Master的选举过程</h3><p>Master作为Spark standalone模式的核心，如果Master出现异常，那么集群就不能正常工作。所以Spark会从Standby中选择一个节点作为Master.</p>
<p> Spark支持以下几种策略，这种策略可以通过配置文件spark-env.sh配置spark.deploy.recoveryMode</p>
<p>ZOOKEEPER: 集群元数据持久化到zookeeper,当master出现异常的时候，zookeeper会通过选举机制选举出新的Master,新的Master接管集群时需要从zookeeper获取持久化信息，并根据这些信息恢复集群状态。</p>
<p>FILESYSTEM: 集群的元数据持久化到文件系统，当Master出现异常的时候，只要在该机器上重启Master，启动后的Master获取持久化信息并根据持久化信息恢复集群状态。</p>
<p>CUSTOM: 自定义恢复模式，实现StandaloneRecoveryModeFactory抽象类进行实现，并把该类配置到配置文件，当Master出现异常，会根据用户自定义的方式进行恢复集群状况。</p>
<p>NONE: 不持久化集群元数据，当Master出现异常时，新启动的Master不进行恢复集群状态。</p>
<h3 id="说说Spark如何实现序列化组件的"><a href="#说说Spark如何实现序列化组件的" class="headerlink" title="说说Spark如何实现序列化组件的"></a>说说Spark如何实现序列化组件的</h3><p>Spark通过两种方式来创建序列化器</p>
<p>Java序列化</p>
<p>在默认情况下，Spark 采用 Java的 ObjectOutputStream 序列化一个对象。该方式适用于所有实现了 java.io.Serializable 的类。通过继承 java.io.Externalizable，你能进一步控制序列化的性能。Java序列化非常灵活，但是速度较慢，在某些情况下序列化的结果也比较大。</p>
<p>Kryo序列化</p>
<p>Spark 也能使用 Kryo（版本2）序列化对象。Kryo 不但速度极快，而且产生的结果更为紧凑（通常能提高10倍）。Kryo 的缺点是不支持所有类型，为了更好的性能，你需要提前注册程序中所使用的类（class）。</p>
<p>Java 的序列化比较简单，就和前面的一样，下面主要介绍Kryo序列化的使用。</p>
<p>Kryo序列化怎么用？</p>
<p>可以在创建 SparkContext 之前，通过调用 System.setProperty(“spark.serializer”, “spark.KryoSerializer”)，将序列化方式切换成Kryo。</p>
<p>但是 Kryo 需要用户进行注册，这也是为什么 Kryo 不能成为 Spark 序列化默认方式的唯一原因，但是建议对于任何“网络密集型”（network-intensive)的应用，都采用这种方式进行序列化方式。</p>
<p>Kryo文档描述了很多便于注册的高级选项，例如添加用户自定义的序列化代码。</p>
<p>如果对象非常大，你还需要增加属性 spark.kryoserializer.buffer.mb 的值。该属性的默认值是32，但是该属性需要足够大以便能够容纳需要序列化的最大对象。</p>
<p>最后，如果你不注册你的类，Kryo仍然可以工作，但是需要为了每一个对象保存其对应的全类名（full class name),这是非常浪费的。</p>
<h3 id="说说对Master的理解"><a href="#说说对Master的理解" class="headerlink" title="说说对Master的理解"></a>说说对Master的理解</h3><p>Master 是local-cluster 部署模式和 Standalone 部署模式中，整个 Spark 集群最为重要的组件之一，分担了对整个集群资源的管理和分配的工作。</p>
<p>local-cluster 下，Master 作为 JVM 进程的对象启动，而在 Standalone 模式下，就是单独的进程启动。 </p>
<h3 id="说说什么是窗口间隔和滑动间隔"><a href="#说说什么是窗口间隔和滑动间隔" class="headerlink" title="说说什么是窗口间隔和滑动间隔"></a>说说什么是窗口间隔和滑动间隔</h3><p>对于窗口操作，在其窗口内部会有 N 个批处理数据，批处理数据的个数由窗口间隔决定，其为窗口持续的时间，在窗口操作中只有窗口间隔满足了才会触发批数据的处理（指一开始的阶段）。</p>
<p>滑动间隔是指经过多长时间窗口滑动一次形成新的窗口，滑动传功库默认情况下和批次间隔的相同，而窗口间隔一般设置得要比它们都大。</p>
<h3 id="说说Spark的WAL（预写日志）机制？"><a href="#说说Spark的WAL（预写日志）机制？" class="headerlink" title="说说Spark的WAL（预写日志）机制？"></a>说说Spark的WAL（预写日志）机制？</h3><p>也叫 WriteAheadLogs，通常被用于数据库和文件系统中，保证数据操作的持久性。预写日志通常是先将操作写入到一个持久可靠的日志文件中，然后才对数据施加该操作，当加入施加该操作中出现异常，可以通过读取日志文件并重新施加该操作，从而恢复系统。</p>
<p>当 WAL 开启后，所有收到的数据同时保存到了容错文件系统的日志文件中，当 Spark Streaming 失败，这些接受到的数据也不会丢失。另外，接收数据的正确性只在数据被预写到日志以后接收器才会确认。已经缓存但还没有保存的数据可以在 Driver 重新启动之后由数据源再发送一次（经常问）。</p>
<p>这两个机制保证了数据的零丢失，即所有的数据要么从日志中恢复，要么由数据源重发。</p>
<h3 id="Spark-Streaming小文件问题"><a href="#Spark-Streaming小文件问题" class="headerlink" title="Spark Streaming小文件问题"></a>Spark Streaming小文件问题</h3><p>使用 Spark Streaming 时，如果实时计算结果要写入到 HDFS，那么不可避免的会遇到一个问题，那就是在默认情况下会产生非常多的小文件，这是由 Spark Streaming 的微批处理模式和 DStream(RDD) 的分布式(partition)特性导致的，Spark Streaming 为每个 Partition 启动一个独立的线程（一个 task/partition 一个线程）来处理数据，一旦文件输出到 HDFS，那么这个文件流就关闭了，再来一个 batch 的 parttition 任务，就再使用一个新的文件流，那么假设，一个 batch 为10s，每个输出的 DStream 有32个 partition，那么一个小时产生的文件数将会达到(3600/10)*32=11520个之多。</p>
<p>众多小文件带来的结果是有大量的文件元信息，比如文件的 location、文件大小、block number 等需要 NameNode 来维护，NameNode 会因此鸭梨山大。不管是什么格式的文件，parquet、text、JSON 或者 Avro，都会遇到这种小文件问题，这里讨论几种处理 Spark Streaming 小文件的典型方法。</p>
<ul>
<li>增加 batch 大小：这种方法很容易理解，batch 越大，从外部接收的 event 就越多，内存积累的数据也就越多，那么输出的文件数也就会变少，比如上边的时间从10s增加为100s，那么一个小时的文件数量就会减少到1152个。但别高兴太早，实时业务能等那么久吗，本来人家10s看到结果更新一次，现在要等快两分钟，是人都会骂娘。所以这种方法适用的场景是消息实时到达，但不想挤压在一起处理，因为挤压在一起处理的话，批处理任务在干等，这时就可以采用这种方法。</li>
<li>Coalesce：文章开头讲了，小文件的基数是 batch_number * partition_number，而第一种方法是减少 batch_number，那么这种方法就是减少 partition_number 了，这个 api 不细说，就是减少初始的分区个数。看过 spark 源码的童鞋都知道，对于窄依赖，一个子 RDD 的 partition 规则继承父 RDD，对于宽依赖(就是那些个xxxByKey操作)，如果没有特殊指定分区个数，也继承自父 rdd。那么初始的 SourceDstream 是几个 partition，最终的输出就是几个 partition。所以 Coalesce 大法的好处就是，可以在最终要输出的时候，来减少一把 partition 个数。但是这个方法的缺点也很明显，本来是32个线程在写256M数据，现在可能变成了4个线程在写256M数据，而没有写完成这256M数据，这个 batch 是不算结束的。那么一个 batch 的处理时延必定增长，batch 挤压会逐渐增大。</li>
<li>Spark Streaming 外部来处理：我们既然把数据输出到 hdfs，那么说明肯定是要用 Hive 或者 Spark Sql 这样的“sql on hadoop”系统类进一步进行数据分析，而这些表一般都是按照半小时或者一小时、一天，这样来分区的(注意不要和 Spark Streaming 的分区混淆，这里的分区，是用来做分区裁剪优化的)，那么我们可以考虑在 Spark Streaming 外再启动定时的批处理任务来合并 Spark Streaming 产生的小文件。这种方法不是很直接，但是却比较有用，“性价比”较高，唯一要注意的是，批处理的合并任务在时间切割上要把握好，搞不好就可能会去合并一个还在写入的 Spark Streaming 小文件。</li>
<li>自己调用 foreach 去 append：Spark Streaming 提供的 foreach 这个api （一种 Action 操作），可以让我们自定义输出计算结果的方法。那么我们其实也可以利用这个特性，那就是每个 batch 在要写文件时，并不是去生成一个新的文件流，而是把之前的文件打开。考虑这种方法的可行性，首先，HDFS 上的文件不支持修改，但是很多都支持追加，那么每个 batch 的每个 partition 就对应一个输出文件，每次都去追加这个 partition 对应的输出文件，这样也可以实现减少文件数量的目的。这种方法要注意的就是不能无限制的追加，当判断一个文件已经达到某一个阈值时，就要产生一个新的文件进行追加了。所以大概就是一直32个文件。</li>
</ul>
<h3 id="Spark的UDF"><a href="#Spark的UDF" class="headerlink" title="Spark的UDF?"></a>Spark的UDF?</h3><p>因为目前 Spark SQL 本身支持的函数有限，一些常用的函数都没有，比如 len, concat…etc 但是使用 UDF 来自己实现根据业务需要的功能是非常方便的。Spark SQL UDF 其实是一个 Scala 函数，被 catalyst 封装成一个 Expression 结点，最后通过 eval 方法计根据当前 Row 计算 UDF 的结果。UDF 对表中的单行进行转换，以便为每行生成单个对应的输出值。例如，大多数 SQL 环境提供 UPPER 函数返回作为输入提供的字符串的大写版本。</p>
<p>用户自定义函数可以在 Spark SQL 中定义和注册为 UDF，并且可以关联别名，这个别名可以在后面的 SQL 查询中使用。作为一个简单的示例，我们将定义一个 UDF 来将以下 JSON 数据中的温度从摄氏度（degrees Celsius）转换为华氏度（degrees Fahrenheit）。</p>
<h3 id="Mesos下粗粒度和细粒度对比"><a href="#Mesos下粗粒度和细粒度对比" class="headerlink" title="Mesos下粗粒度和细粒度对比?"></a>Mesos下粗粒度和细粒度对比?</h3><p>粗粒度运行模式：Spark 应用程序在注册到 Mesos 时会分配对应系统资源，在执行过程中由 SparkContext 和 Executor 直接交互，该模式优点是由于资源长期持有减少了资源调度的时间开销，缺点是该模式下 Mesos 无法感知资源使用的变化，容易造成系统资源的闲置，无法被 Mesos 其他框架使用，造成资源浪费。</p>
<p>细粒度的运行模式：Spark 应用程序是以单个任务的粒度发送到 Mesos 中执行，在执行过程中 SparkContext 并不能和 Executor 直接交互，而是由 Mesos Master 进行统一的调度管理，这样能够根据整个 Mesos 集群资源使用的情况动态调整。该模式的优点是系统资源能够得到充分利用，缺点是该模式中每个人物都需要从 Mesos 获取资源，调度延迟较大，对于 Mesos Master 开销较大。</p>
<h3 id="Spark-Local和Standalone有什么区别"><a href="#Spark-Local和Standalone有什么区别" class="headerlink" title="Spark Local和Standalone有什么区别"></a>Spark Local和Standalone有什么区别</h3><p>Spark一共有5种运行模式：Local，Standalone，Yarn-Cluster，Yarn-Client 和 Mesos。</p>
<p>Local：Local 模式即单机模式，如果在命令语句中不加任何配置，则默认是 Local 模式，在本地运行。这也是部署、设置最简单的一种模式，所有的 Spark 进程都运行在一台机器或一个虚拟机上面。</p>
<p>Standalone：Standalone是 Spark 自身实现的资源调度框架。如果我们只使用 Spark 进行大数据计算，不使用其他的计算框架（如MapReduce或者Storm）时，就采用 Standalone 模式就够了，尤其是单用户的情况下。Standalone 模式是 Spark 实现的资源调度框架，其主要的节点有 Client 节点、Master 节点和 Worker 节点。其中 Driver 既可以运行在 Master 节点上中，也可以运行在本地 Client 端。当用 spark-shell 交互式工具提交 Spark 的 Job 时，Driver 在 Master 节点上运行；当使用 spark-submit 工具提交 Job 或者在 Eclipse、IDEA 等开发平台上使用 new SparkConf.setManager(“spark://master:7077”) 方式运行 Spark 任务时，Driver 是运行在本地 Client 端上的。</p>
<p>Standalone 模式的部署比较繁琐，需要把 Spark 的部署包安装到每一台节点机器上，并且部署的目录也必须相同，而且需要 Master 节点和其他节点实现 SSH 无密码登录。启动时，需要先启动 Spark 的 Master 和 Slave 节点。提交命令类似于:</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line">./bin/spark-submit \</span><br><span class="line">  <span class="comment">--class org.apache.spark.examples.SparkPi \</span></span><br><span class="line">  <span class="comment">--master spark://Oscar-2.local:7077 \</span></span><br><span class="line">  /tmp/spark-2.2.0-bin-hadoop2.7/examples/jars/spark-examples_2.11-2.2.0.jar \</span><br><span class="line">  100</span><br></pre></td></tr></table></figure>

<h3 id="说说SparkContext和SparkSession有什么区别"><a href="#说说SparkContext和SparkSession有什么区别" class="headerlink" title="说说SparkContext和SparkSession有什么区别?"></a>说说SparkContext和SparkSession有什么区别?</h3><p>Application: 用户编写的 Spark 应用程序，Driver 即运行上述 Application 的 main() 函数并且创建 SparkContext。Application 也叫应用。</p>
<p>SparkContext: 整个应用的上下文，控制应用的生命周期。</p>
<p>RDD: 不可变的数据集合，可由 SparkContext 创建，是 Spark 的基本计算单元。</p>
<p>SparkSession: 可以由上节图中看出，Application、SparkSession、SparkContext、RDD之间具有包含关系，并且前三者是1对1的关系。SparkSession 是 Spark 2.0 版本引入的新入口，在这之前，创建一个 Application 对应的上下文是这样的：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">//set up the spark configuration and create contexts</span></span><br><span class="line"><span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"SparkSessionZipsExample"</span>).setMaster(<span class="string">"local"</span>)</span><br><span class="line"><span class="comment">// your handle to SparkContext to access other context like SQLContext</span></span><br><span class="line"><span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf).set(<span class="string">"spark.some.config.option"</span>, <span class="string">"some-value"</span>)</span><br><span class="line"><span class="keyword">val</span> sqlContext = <span class="keyword">new</span> org.apache.spark.sql.<span class="type">SQLContext</span>(sc)</span><br></pre></td></tr></table></figure>

<p>现在 SparkConf、SparkContext 和 SQLContext 都已经被封装在 SparkSession 当中，并且可以通过 builder 的方式创建：</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">// Create a SparkSession. No need to create SparkContext</span></span><br><span class="line"><span class="comment">// You automatically get it as part of the SparkSession</span></span><br><span class="line"><span class="keyword">val</span> warehouseLocation = <span class="string">"file:$&#123;system:user.dir&#125;/spark-warehouse"</span></span><br><span class="line"><span class="keyword">val</span> spark = <span class="type">SparkSession</span></span><br><span class="line">   .builder()</span><br><span class="line">   .appName(<span class="string">"SparkSessionZipsExample"</span>)</span><br><span class="line">   .config(<span class="string">"spark.sql.warehouse.dir"</span>, warehouseLocation)</span><br><span class="line">   .enableHiveSupport()</span><br><span class="line">   .getOrCreate()</span><br></pre></td></tr></table></figure>

<h3 id="如果Spark-Streaming停掉了，如何保证Kafka的重新运作是合理的呢"><a href="#如果Spark-Streaming停掉了，如何保证Kafka的重新运作是合理的呢" class="headerlink" title="如果Spark Streaming停掉了，如何保证Kafka的重新运作是合理的呢"></a>如果Spark Streaming停掉了，如何保证Kafka的重新运作是合理的呢</h3><p>首先要说一下 Spark 的快速故障恢复机制，在节点出现故障的情况下，传统流处理系统会在其他节点上重启失败的连续算子，并可能重新运行先前数据流处理操作获取部分丢失数据。在此过程中只有该节点重新处理失败的过程。只有在新节点完成故障前所有计算后，整个系统才能够处理其他任务。</p>
<p>在 Spark 中，计算将会分成许多小的任务，保证能在任何节点运行后能够正确合并，因此，就算某个节点出现故障，这个节点的任务将均匀地分散到集群中的节点进行计算，相对于传递故障恢复机制能够更快地恢复。</p>
<h3 id="列举Spark中-Transformation-和-Action算子"><a href="#列举Spark中-Transformation-和-Action算子" class="headerlink" title="列举Spark中 Transformation 和 Action算子"></a>列举Spark中 Transformation 和 Action算子</h3><p>Transformantion：Map, Filter, FlatMap, Sample, GroupByKey, ReduceByKey, Union, Join, Cogroup, MapValues, Sort, PartionBy</p>
<p>Action：Collect, Reduce, Lookup, Save （主要记住，结果不是 RDD 的就是 Action）</p>
<h3 id="Spark经常说的Repartition是个什么玩意"><a href="#Spark经常说的Repartition是个什么玩意" class="headerlink" title="Spark经常说的Repartition是个什么玩意"></a>Spark经常说的Repartition是个什么玩意</h3><p>简单的说：返回一个恰好有numPartitions个分区的RDD，可以增加或者减少此RDD的并行度。内部，这将使用shuffle重新分布数据，如果你减少分区数，考虑使用coalesce，这样可以避免执行shuffle。</p>
<p>目的：</p>
<ul>
<li>避免小文件</li>
<li>减少 Task 个数</li>
<li>但是会增加每个 Task 处理的数据量</li>
</ul>
<h3 id="Spark-Streaming-Duration的概念"><a href="#Spark-Streaming-Duration的概念" class="headerlink" title="Spark Streaming Duration的概念"></a>Spark Streaming Duration的概念</h3><p>Spark Streaming 是微批处理。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="type">SparkConf</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setAppName(<span class="string">"SparkStreaming"</span>).setMaster(<span class="string">"local[*]"</span>); </span><br><span class="line"><span class="type">JavaStreamingContext</span> javaStreamingContext = <span class="keyword">new</span> <span class="type">JavaStreamingContext</span>(sparkConf, <span class="type">Durations</span>.seconds(<span class="number">1000</span>));</span><br></pre></td></tr></table></figure>

<p>Durations.seconds(1000)设置的是 sparkstreaming 批处理的时间间隔，每隔 Batch Duration 时间去提交一次 job，如果 job 的处理时间超过 Batch Duration，会使得 job 无法按时提交，随着时间推移，越来越多的作业被拖延，最后导致整个 Streaming作业被阻塞，无法做到实时处理数据。</p>
<h3 id="简单写一个WordCount程序"><a href="#简单写一个WordCount程序" class="headerlink" title="简单写一个WordCount程序"></a>简单写一个WordCount程序</h3><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">sc.textFile(<span class="string">"/Users/runzhliu/workspace/spark-2.2.1-bin-hadoop2.7/README.md"</span>)</span><br><span class="line">.flatMap(_.split(<span class="string">" "</span>))</span><br><span class="line">.map(x =&gt; (x, <span class="number">1</span>))</span><br><span class="line">.reduceByKey(_ + _)</span><br><span class="line">.map(x =&gt; (x._2, x._1))</span><br><span class="line">.sortByKey(<span class="literal">false</span>)</span><br><span class="line">.map(x =&gt; (x._2, x._1))</span><br><span class="line">.take(<span class="number">10</span>)</span><br></pre></td></tr></table></figure>

<h3 id="说说Yarn-cluster的运行阶段"><a href="#说说Yarn-cluster的运行阶段" class="headerlink" title="说说Yarn-cluster的运行阶段"></a>说说Yarn-cluster的运行阶段</h3><p>在 Yarn-cluset 模式下，当用户向 Yarn 提交一个应用程序后，Yarn 将两个阶段运行该应用程序:</p>
<ul>
<li>第一阶段是把 Spark 的 Driver 作为一个 Application Master 在 Yarn 集群中先启动。</li>
<li>第二阶段是由 Application Master 创建应用程序，然后为它向 Resource Manager 申请资源，并启动 Executor 来运行任务集，同时监控它的整个过程，直到运行介绍结束。</li>
</ul>
<h3 id="说说Standalone模式下运行Spark程序的大概流程"><a href="#说说Standalone模式下运行Spark程序的大概流程" class="headerlink" title="说说Standalone模式下运行Spark程序的大概流程"></a>说说Standalone模式下运行Spark程序的大概流程</h3><p>Standalone 模式分别由客户端、Master 节点和 Worker 节点组成。在 Spark Shell 提交计算代码的时候，所在机器作为客户端启动应用程序，然后向 Master 注册应用程序，由 Master 通知 Worker 节点启动 Executor，Executor 启动之后向客户端的 Driver 注册，最后由 Driver 发送执行任务给 Executor 并监控任务执行情况。该程序代码中，在触发计算行数动作之前，需要设置缓存代码，这样在执行计算行数行为的时候进行缓存数据，缓存后再运行计算行数。</p>
<h3 id="如何区分-Appliction-应用程序-还有-Driver-驱动程序"><a href="#如何区分-Appliction-应用程序-还有-Driver-驱动程序" class="headerlink" title="如何区分 Appliction(应用程序)还有 Driver(驱动程序)"></a>如何区分 Appliction(应用程序)还有 Driver(驱动程序)</h3><p>Application 是指用户编写的 Spark 应用程序，包含驱动程序 Driver 和分布在集群中多个节点上运行的 Executor 代码，在执行过程之中由一个或多个做作业组成。</p>
<p>Driver 是 Spark 中的 Driver 即运行上述 Application 的 main 函数并且创建 SparkContext，其中创建 SparkContext 的目的是为了准备 Spark 应用程序的运行环境。在 Spark 中由 sc 负责与 ClusterManager 通信，进行资源的申请，任务的分配和监控等。当 Executor 部分运行完毕后，Driver 负责把 sc 关闭，通常 Driver 会拿 SparkContext 来代表。</p>
<h3 id="介绍一下-Spark-通信的启动方式"><a href="#介绍一下-Spark-通信的启动方式" class="headerlink" title="介绍一下 Spark 通信的启动方式"></a>介绍一下 Spark 通信的启动方式</h3><p>Spark 启动过程主要是 Master 与 Worker 之间的通信，首先由 Worker 节点向 Master 发送注册消息，然后 Master 处理完毕后，返回注册成功消息或失败消息，如果成功注册，那么 Worker 就会定时发送心跳消息给 Master。</p>
<h3 id="介绍一下-Spark-运行时候的消息通信"><a href="#介绍一下-Spark-运行时候的消息通信" class="headerlink" title="介绍一下 Spark 运行时候的消息通信"></a>介绍一下 Spark 运行时候的消息通信</h3><p>用户提交应用程序时，应用程序的 SparkContext 会向 Master 发送应用注册消息，并由 Master 给该应用分配 Executor，Excecutor 启动之后，Executor 会向 SparkContext 发送注册成功消息。</p>
<p>当 SparkContext 的 RDD 触发行动操作之后，将创建 RDD 的 DAG。通过 DAGScheduler 进行划分 Stage 并把 Stage 转化为 TaskSet，接着 TaskScheduler 向注册的 Executor 发送执行消息，Executor 接收到任务消息后启动并运行。最后当所有任务运行时候，由 Driver 处理结果并回收资源。</p>
<h3 id="解释一下Stage"><a href="#解释一下Stage" class="headerlink" title="解释一下Stage"></a>解释一下Stage</h3><p>每个作业会因为 RDD 之间的依赖关系拆分成多组任务集合，称为调度阶段，也叫做任务集。调度阶段的划分由 DAGScheduler 划分，调度阶段有 Shuffle Map Stage 和 Result Stage 两种。</p>
<h3 id="描述一下Worker异常的情况"><a href="#描述一下Worker异常的情况" class="headerlink" title="描述一下Worker异常的情况"></a>描述一下Worker异常的情况</h3><p>Spark 独立运行模式 Standalone 采用的是 Master/Slave 的结构，其中 Slave 是由 Worker 来担任的，在运行的时候会发送心跳给 Master，让 Master 知道 Worker 的实时状态;</p>
<p>另一方面，Master 也会检测注册的 Worker 是否超时，因为在集群运行的过程中，可能由于机器宕机或者进程被杀死等原因造成 Worker 异常退出。</p>
<h3 id="描述一下Master异常的情况"><a href="#描述一下Master异常的情况" class="headerlink" title="描述一下Master异常的情况"></a>描述一下Master异常的情况</h3><p>Master 出现异常的时候，会有几种情况，而在独立运行模式 Standalone 中，Spark 支持几种策略，来让 Standby Master 来接管集群。主要配置的地方在于 spark-env.sh 文件中。配置项是 spark.deploy.recoveryMode 进行设置，默认是 None。</p>
<ul>
<li>ZOOKEEPER：集群元数据持久化到 Zookeeper 中，当 Master 出现异常，ZK 通过选举机制选举新的 Master，新的 Master 接管的时候只要从 ZK 获取持久化信息并根据这些信息恢复集群状态。StandBy 的 Master 随时候命的。</li>
<li>FILESYSTEM：集群元数据持久化到本地文件系统中，当 Master 出现异常的时候，只要在该机器上重新启动 Master，启动后新的 Master 获取持久化信息并根据这些信息恢复集群的状态。</li>
<li>CUSTOM：自定义恢复方式，对 StandaloneRecoveryModeFactory 抽象类进行实现并把该类配置到系统中，当 Master 出现异常的时候，会根据用户自定义的方式进行恢复集群状态。</li>
<li>NONE：不持久化集群的元数据，当出现异常的是，新启动 Master 不进行信息恢复集群状态，而是直接接管集群。</li>
</ul>
<h3 id="Spark的存储体系"><a href="#Spark的存储体系" class="headerlink" title="Spark的存储体系"></a>Spark的存储体系</h3><p>简单来讲，Spark存储体系是各个Driver与Executor实例中的BlockManager所组成的；</p>
<p>但是从一个整体来看，把各个节点的BlockManager看成存储体系的一部分，那存储体系就有了更多衍生的内容，比如块传输服务、map任务输出跟踪器、Shuffle管理器等。</p>
<h3 id="简述Spark-Streaming"><a href="#简述Spark-Streaming" class="headerlink" title="简述Spark Streaming"></a>简述Spark Streaming</h3><p>具有高吞吐量和容错能力强的特点，输入源有很多，如 Kafka, Flume, Twitter 等待。</p>
<p>关于流式计算的做法，如果按照传统工具的做法把数据存储到数据库中再进行计算，这样是无法做到实时的，而完全把数据放到内存中计算，万一宕机、断电了，数据也就丢失了。</p>
<p>因此 Spark 流式计算引入了检查点 CheckPoint 和日志，以便能够从中恢复计算结果。而本质上 Spark Streaming 是接收实时输入数据流并把他们按批次划分，然后交给 Spark 计算引擎处理生成按照批次划分的结果流。</p>
<h3 id="知道-Hadoop-MRv1-的局限吗"><a href="#知道-Hadoop-MRv1-的局限吗" class="headerlink" title="知道 Hadoop MRv1 的局限吗"></a>知道 Hadoop MRv1 的局限吗</h3><p>可扩展性查，在运行的时候，JobTracker 既负责资源管理，又负责任务调度，当集群繁忙的时候，JobTracker 很容易成为瓶颈，最终导致可扩展性的问题。</p>
<p>可用性差，采用单节点的 Master 没有备用 Master 以及选举操作，这导致一旦 Master 出现故障，整个集群将不可用。</p>
<p>资源利用率低，TaskTracker 使用 slot 等量划分本节点上的资源量，slot 代表计算资源将各个 TaskTracker 上的空闲 slot 分配给 Task 使用，一些 Task 并不能充分利用 slot，而其他 Task 无法使用这些空闲的资源。有时会因为作业刚刚启动等原因导致 MapTask 很多，而 Reduce Task 任务还没调度的情况，这时 Reduce slot 也会被闲置。</p>
<p>不能支持多种 MapReduce 框架，无法通过可插拔方式将自身的 MapReduce 框架替换为其他实现，例如 Spark，Storm。</p>
<h3 id="说说Spark的特点，相对于MR来说"><a href="#说说Spark的特点，相对于MR来说" class="headerlink" title="说说Spark的特点，相对于MR来说"></a>说说Spark的特点，相对于MR来说</h3><ol>
<li>减少磁盘 I/O，MR 会把 map 端将中间输出和结果存储在磁盘中，reduce 端又需要从磁盘读写中间结果，势必造成磁盘 I/O 称为瓶颈。Spark 允许将 map 端的中间结果输出和结果存储在内存中，reduce 端在拉取中间结果的时候避免了大量的磁盘 I/O。</li>
<li>增加并行度，由于把中间结果写到磁盘与从磁盘读取中间结果属于不同的环节，Hadoop 将他们简单地通过串行执行衔接起来，Spark 则把不同的环节抽象成为 Stage，允许多个 Stage 既可以串行又可以并行执行。</li>
<li>避免重新计算，当 Stage 中某个分区的 Task 执行失败后，会重新对此 Stage 调度，但在重新调度的时候会过滤已经执行成功的分区任务，所以不会造成重复计算和资源浪费。</li>
<li>可选的 Shuffle 排序，MR 在 Shuffle 之前有着固定的排序操作，而 Spark 则可以根据不同场景选择在 map 端排序还是 reduce 排序。</li>
<li>灵活的内存管理策略，Spark 将内存分为堆上的存储内存、堆外的存储内存，堆上的执行内存，堆外的执行内存4个部分。</li>
</ol>
<h3 id="说说Spark-Narrow-Dependency的分类"><a href="#说说Spark-Narrow-Dependency的分类" class="headerlink" title="说说Spark Narrow Dependency的分类"></a>说说Spark Narrow Dependency的分类</h3><p>OneToOneDependency</p>
<p>RangeDependency</p>
<h3 id="Task的分类"><a href="#Task的分类" class="headerlink" title="Task的分类"></a>Task的分类</h3><p>Task 指具体的执行任务，一个 Job 在每个 Stage 内都会按照 RDD 的 Partition 数量，创建多个 Task，Task 分为 ShuffleMapTask 和 ResultTask 两种。ShuffleMapStage 中的 Task 为 ShuffleMapTask，而 ResultStage 中的 Task 为 ResultTask。ShuffleMapTask 和 ResultTask 类似于 Hadoop 中的 Map 任务和 Reduce 任务。</p>
<h3 id="Spark的编程模型"><a href="#Spark的编程模型" class="headerlink" title="Spark的编程模型"></a>Spark的编程模型</h3><p>1.创建应用程序 SparkContext</p>
<p>2.创建RDD，有两种方式，</p>
<p>方式一：输入算子，即读取外部存储创建RDD，Spark与Hadoop完全兼容，所以对Hadoop所支持的文件类型或者数据库类型，Spark同样支持。</p>
<p>方式二：从集合创建RDD</p>
<p>3.Transformation算子，这种变换并不触发提交作业，完成作业中间过程处理。也就是说从一个RDD 转换生成另一个 RDD 的转换操作不是马上执行，需要等到有 Action 操作的时候才会真正触发运算。</p>
<p>4.Action 算子，这类算子会触发 SparkContext 提交 Job 作业。并将数据输出 Spark系统。</p>
<p>5.保存结果</p>
<p>6.关闭应用程序</p>
<h3 id="Spark的计算模型"><a href="#Spark的计算模型" class="headerlink" title="Spark的计算模型"></a>Spark的计算模型</h3><p>用户程序对 RDD 通过多个函数进行操作，将 RDD 进行转换。</p>
<p>Block-Manager 管理 RDD 的物理分区，每个 Block 就是节点上对应的一个数据块，可以存储在内存或者磁盘。</p>
<p>而 RDD 中的 partition 是一个逻辑数据块，对应相应的物理块 Block。</p>
<p>本质上一个 RDD 在代码中相当于是数据的一个元数据结构，存储着数据分区及其逻辑结构映射关系，存储着 RDD 之前的依赖转换关系。</p>
<h3 id="总述Spark的架构"><a href="#总述Spark的架构" class="headerlink" title="总述Spark的架构"></a>总述Spark的架构</h3><p>从集群部署的角度来看，Spark 集群由集群管理器 Cluster Manager、工作节点 Worker、执行器 Executor、驱动器 Driver、应用程序 Application 等部分组成。</p>
<ul>
<li>Cluster Manager：主要负责对集群资源的分配和管理，Cluster Manager在YARN 部署模式下为 RM，在 Mesos 下为 Mesos Master，Standalone 模式下为 Master。CM 分配的资源属于一级分配，它将各个 Worker 上的内存、CPU 等资源分配给 Application，但是不负责对 Executor 的资源分类。Standalone 模式下的 Master 会直接给 Application 分配内存、CPU 及 Executor 等资源。</li>
<li>Worker：Spark 的工作节点。在 YARN 部署模式下实际由 NodeManager 替代。Worker 节点主要负责，把自己的内存、CPU 等资源通过注册机制告知 CM，创建 Executor，把资源和任务进一步分配给 Executor，同步资源信息，Executor 状态信息给 CM 等等。Standalone 部署模式下，Master 将 Worker 上的内存、CPU 以及 Executor 等资源分配给 Application 后，将命令 Worker 启动 CoarseGrainedExecutorBackend 进程（此进程会创建 Executor 实例）。</li>
<li>Executor：执行计算任务的一线组件，主要负责任务的执行及与 Worker Driver 信息同步。</li>
<li>Driver：Application 的驱动程序，Application 通过 Driver 与 CM、Executor 进行通信。Driver 可以运行在 Application 中，也可以由 Application 提交给 CM 并由 CM 安排 Worker 运行。</li>
<li>Application：用户使用 Spark 提供的 API 编写的应用程序，Application 通过 Spark API 将进行 RDD 的转换和 DAG 的创建，并通过 Driver 将 Application 注册到 CM，CM 将会根据 Application 的资源需求，通过一级资源分配将 Excutor、内存、CPU 等资源分配给 Application。Drvier 通过二级资源分配将 Executor 等资源分配给每一个任务，Application 最后通过 Driver 告诉 Executor 运行任务。</li>
</ul>
<h3 id="一句话说说-Spark-Streaming-是如何收集和处理数据的"><a href="#一句话说说-Spark-Streaming-是如何收集和处理数据的" class="headerlink" title="一句话说说 Spark Streaming 是如何收集和处理数据的"></a>一句话说说 Spark Streaming 是如何收集和处理数据的</h3><p>在 Spark Streaming 中，数据采集是逐条进行的，而数据处理是按批 mini batch进行的，因此 Spark Streaming 会先设置好批处理间隔 batch duration，当超过批处理间隔就会把采集到的数据汇总起来成为一批数据交给系统去处理。</p>
<h3 id="解释一下窗口间隔window-duration和滑动间隔slide-duration"><a href="#解释一下窗口间隔window-duration和滑动间隔slide-duration" class="headerlink" title="解释一下窗口间隔window duration和滑动间隔slide duration"></a>解释一下窗口间隔window duration和滑动间隔slide duration</h3><ol>
<li><p>红色的矩形就是一个窗口，窗口 hold 的是一段时间内的数据流。</p>
</li>
<li><p>这里面每一个 time 都是时间单元，在官方的例子中，每隔 window size 是3 time unit， 而且每隔2个单位时间，窗口会 slide 一次。</p>
</li>
<li><p>所以基于窗口的操作，需要指定2个参数:</p>
</li>
<li><p>window length - The duration of the window (3 in the figure)<br>slide interval - The interval at which the window-based operation is performed (2 in the figure).<br>窗口大小，个人感觉是一段时间内数据的容器。</p>
<p>滑动间隔，就是我们可以理解的 cron 表达式吧。</p>
</li>
</ol>
<p>窗口间隔一般大于（批处理间隔、滑动间隔）。这都是理解窗口操作的关键。</p>
<h3 id="介绍一下Spark-Streaming的foreachRDD-func-方法"><a href="#介绍一下Spark-Streaming的foreachRDD-func-方法" class="headerlink" title="介绍一下Spark Streaming的foreachRDD(func)方法"></a>介绍一下Spark Streaming的foreachRDD(func)方法</h3><p>将函数应用于 DStream 的 RDD 上，这个操作会输出数据到外部系统，比如保存 RDD 到文件或者网络数据库等。需要注意的是 func 函数是运行该 Streaming 应用的 Driver 进程里执行的。</p>
<h3 id="简单描述一下Spark-Streaming的容错原理"><a href="#简单描述一下Spark-Streaming的容错原理" class="headerlink" title="简单描述一下Spark Streaming的容错原理"></a>简单描述一下Spark Streaming的容错原理</h3><p>Spark Streaming 的一个特点就是高容错。</p>
<p>首先 Spark RDD 就有容错机制，每一个 RDD 都是不可变的分布式可重算的数据集，其记录这确定性的操作血统，所以只要输入数据是可容错的，那么任意一个 RDD 的分区出错或不可用，都是可以利用原始输入数据通过转换操作而重新计算出来的。</p>
<p>预写日志通常被用于数据库和文件系统中，保证数据操作的持久性。预写日志通常是先将操作写入到一个持久可靠的日志文件中，然后才对数据施加该操作，当加入施加操作中出现了异常，可以通过读取日志文件并重新施加该操作。</p>
<p>另外接收数据的正确性只在数据被预写到日志以后接收器才会确认，已经缓存但还没保存的数据可以在 Driver 重新启动之后由数据源再发送一次，这两个机制确保了零数据丢失，所有数据或者从日志中恢复，或者由数据源重发。</p>
<h3 id="DStream-有几种转换操作"><a href="#DStream-有几种转换操作" class="headerlink" title="DStream 有几种转换操作"></a>DStream 有几种转换操作</h3><p>Transform Operation、Window Operations、Join Operations</p>
<h3 id="聊聊Spark-Streaming的运行架构"><a href="#聊聊Spark-Streaming的运行架构" class="headerlink" title="聊聊Spark Streaming的运行架构"></a>聊聊Spark Streaming的运行架构</h3><p>Spark Streaming相对其他流处理系统最大的优势在于流处理引擎和数据处理在同一软件栈，其中Spark Streaming功能主要包括流处理引擎的流数据接收与存储以及批处理作业的生成与管理，而Spark Core负责处理Spark Streaming发送过来的作业。</p>
<p>Spark Streaming分为Driver端和Client端，运行在Driver端为StreamingContext实例，该实例包括DStreamGraph和JobScheduler(包括ReceiverTracker和JobGenerator)等，而Client包括ReceiverSupervisor和Receiver等。</p>
<p>Spark Streaming进行流数据处理大致可以分为：启动流数据引擎、接收及存储流数据、处理流数据和输出处理结果等4个步骤。</p>
<ol>
<li><p>初始化StreamingContext对象，在该对象启动过程中实例化DStreamGraph和JobScheduler</p>
<p>DStreamGraph用于存放DStream以及DStream之间的依赖关系等信息。</p>
<p>JobScheduler中包括ReceiverTracker和JobGenerator。ReceiverTracker为Driver端流数据接收器(Receiver)的管理者；</p>
<p>JobGenerator为批处理作业生成器。</p>
<p>在ReceiverTracker启动过程中，根据流数据接收器分发策略通知对应的Executor中的流数据接收管理器</p>
<p>(ReciverSupervisor)启动，再由ReciverSupervisor启动流数据接收器。</p>
</li>
<li><p>当流数据接收器Receiver启动后，持续不断地接收实时流数据，根据传过来数据的大小进行判断，如果数据量很小，</p>
<p>则攒多条数据成一块，然后再进行块存储；如果数据量大，则直接进行块存储。</p>
<p>对于这些数据Receiver直接交到ReciverSupervisor，由其进行数据转储操作。块存储根据设置是否预写日志分为两种：</p>
<ul>
<li>一种是使用非预写日志BlockManagerBasedBlockHandler方法直接写到Worker的内存或磁盘中</li>
<li>另一种是进行预写日志WriteAheadLogBasedBlockHandler方法，即在预写日志同时把数据写入到Worker的内存或磁盘中</li>
</ul>
<p>数据存储完毕后，ReciverSupervisor会把数据存储的云信息上报给ReceiverTracker，ReceiverTracker再把这些信息转发给ReceivedBlockTracker，由它负责管理收到数据块的元信息。</p>
</li>
<li><p>在StreamingContext的JobGenerator中维护一个定时器，该定时器在批处理时间到来时会进行生成作业的操作。在该操作中会进行如下操作：</p>
<p>a.通知ReceiverTracker将接收到的数据进行提交，在提交时采用synchronized关键字进行处理，保证每条数据被划入一个且只被划入一个批次中。</p>
<p>b.要求DStreamGraph根据DStream依赖关系生成作业序列Seq[Job]</p>
<p>c.从第一步中ReceiverTracker获取本批次数据的元数据。</p>
<p>d.把批处理时间time、作业序列Seq[Job]和本批次数据的元数据包装为JobSet，调用JobScheduler.submitJobSet(JobSet)提交给JobScheduler,JobScheduler将把这些作业发送给Spark核心进行处理，由于该执行为异步，因此本步执行速度将非常快。</p>
<p>e.只要提交结束（不管作业是否被执行）,SparkStreaming对整个系统做一个检查点（checkpoint）</p>
</li>
<li><p>Spark核心的作业对数据进行处理，处理完毕后输出到外部系统，如数据库或文件系统，输出的数据可以被外部系统所使用。</p>
<p>由于实时流数据的数据源源不断地流入，Spark会周而复始地进行数据处理，响应也会持续不断地输出结果。</p>
</li>
</ol>
<h3 id="说说DStreamGraph"><a href="#说说DStreamGraph" class="headerlink" title="说说DStreamGraph"></a>说说DStreamGraph</h3><p>Spark Streaming 中作业生成与 Spark 核心类似，对 DStream 进行的各种操作让它们之间的操作会被记录到名为DStream 使用输出操作时，这些依赖关系以及它们之间的操作会被记录到名为 DStreamGraph 的对象中表示一个作业。这些作业注册到 DStreamGraph 并不会立即运行，而是等到 Spark Streaming 启动之后，达到批处理时间，才根据 DG 生成作业处理该批处理时间内接收的数据。</p>
<h3 id="创建RDD的方式以及如何继承创建RDD"><a href="#创建RDD的方式以及如何继承创建RDD" class="headerlink" title="创建RDD的方式以及如何继承创建RDD"></a>创建RDD的方式以及如何继承创建RDD</h3><p>Spark 可以从 Hadoop 支持的任何存储源创建分布式数据集，包括本地文件系统、HDFS、Cassandra、HBase、Amazon S3等。Spark 支持文本文件、SequenceFile 和任何其他 Hadoop InputFormat。可以使用SparkContext的textFile方法创建文本文件RDDs。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">val</span> distFile = sc.textFile(<span class="string">"data.txt"</span>)</span><br></pre></td></tr></table></figure>

<h3 id="分析一下Spark-Streaming的transform-和updateStateByKey-两个操作"><a href="#分析一下Spark-Streaming的transform-和updateStateByKey-两个操作" class="headerlink" title="分析一下Spark Streaming的transform()和updateStateByKey()两个操作"></a>分析一下Spark Streaming的transform()和updateStateByKey()两个操作</h3><p>transform(func) 操作：允许 DStream 任意的 RDD-to-RDD 函数。</p>
<p>updateStateByKey 操作：可以保持任意状态，同时进行信息更新，先定义状态，后定义状态更新函数。</p>
<h3 id="说说Spark-Streaming的输出操作"><a href="#说说Spark-Streaming的输出操作" class="headerlink" title="说说Spark Streaming的输出操作"></a>说说Spark Streaming的输出操作</h3><p>print()、</p>
<p>saveAsTextFiles(prefix, [suffix])、</p>
<p>saveAsObjectFiles(prefix, [suffix])、</p>
<p>saveAsHadoopFiles(prefix, [suffix])、</p>
<p>foreachRDD(func)</p>
<h3 id="谈谈Spark-Streaming-Driver端重启会发生什么"><a href="#谈谈Spark-Streaming-Driver端重启会发生什么" class="headerlink" title="谈谈Spark Streaming Driver端重启会发生什么"></a>谈谈Spark Streaming Driver端重启会发生什么</h3><p>恢复计算：使用检查点信息重启 Driver 端，重构上下文并重启接收器</p>
<p>恢复元数据块：为了保证能够继续下去所必备的全部元数据块都被恢复</p>
<p>未完成作业的重新形成：由于失败而没有处理完成的批处理，将使用恢复的元数据再次产生 RDD 和对应的作业</p>
<p>读取保存在日志中的块数据：在这些作业执行的时候，块数据直接从预写日志中读出，这将恢复在日志中可靠地保存所有必要的数据</p>
<p>重发尚未确认的数据：失败时没有保存到日志中的缓存数据将由数据源再次发送</p>
<h3 id="再谈Spark-Streaming的容错性"><a href="#再谈Spark-Streaming的容错性" class="headerlink" title="再谈Spark Streaming的容错性"></a>再谈Spark Streaming的容错性</h3><p>实时流处理系统需要长时间接收并处理数据，这个过程中出现异常是难以避免的，需要流程系统具备高容错性。Spark Streaming 一开始就考虑了两个方面。</p>
<ol>
<li>利用 Spark 自身的容错设计、存储级别和 RDD 抽象设计能够处理集群中任何 Worker 节点的故障</li>
<li>Spark 运行多种运行模式，其 Driver 端可能运行在 Master 节点或者集群中的任意节点，这样让 Driver 端具备容错能力是很大的挑战，但是由于其接收的数据是按照批进行存储和处理，这些批次数据的元数据可以通过执行检查点的方式定期写入到可靠的存储中，在 Driver 端重新启动中恢复这些状态</li>
</ol>
<p>当接收到的数据缓存在 Executor 内存中的丢失风险要怎么处理呢？</p>
<p>如果是独立运行模式/Yarn/Mesos 模式，当 Driver 端失败的时候，该 Driver 端所管理的 Executor 以及内存中数据将终止，即时 Driver 端重新启动这些缓存的数据也不能被恢复。为了避免这种数据损失，就需要预写日志功能了。</p>
<p>当 Spark Streaming 应用开始的时候，也就是 Driver 开始的时候，接收器成为长驻运行任务，这些接收器接收并保存流数据到 Spark 内存以供处理。</p>
<ol>
<li>接收器将数据分成一系列小块，存储到 Executor 内存或磁盘中，如果启动预写日志，数据同时还写入到容错文件系统的预写日志文件。</li>
<li>通知 StreamingContext，接收块中的元数据被发送到 Driver 的 StreamingContext，这个元数据包括两种，一是定位其 Executor 内存或磁盘中数据位置的块编号，二是块数据在日志中的偏移信息（如果启用 WAL 的话）。</li>
</ol>
<h3 id="流数据如何存储"><a href="#流数据如何存储" class="headerlink" title="流数据如何存储"></a>流数据如何存储</h3><p>作为流数据接收器调用 Receiver.store 方式进行数据存储，该方法有多个重载方法，如果数据量很小，则攒多条数据成数据块再进行块存储，如果数据量大，则直接进行块存储。</p>
<h3 id="StreamingContext启动时序图吗"><a href="#StreamingContext启动时序图吗" class="headerlink" title="StreamingContext启动时序图吗"></a>StreamingContext启动时序图吗</h3><ol>
<li>初始化 StreamingContext 中的 DStreamGraph 和 JobScheduler，进而启动 JobScheduler 的 ReceiveTracker 和 JobGenerator。</li>
<li>初始化阶段会进行成员变量的初始化，重要的包括 DStreamGraph（包含 DStream 之间相互依赖的有向无环图），JobScheduler（定时查看 DStreamGraph，然后根据流入的数据生成运行作业），StreamingTab（在 Spark Streaming 运行的时候对流数据处理的监控）。</li>
<li>然后就是创建 InputDStream，接着就是对 InputDStream 进行 flatMap, map, reduceByKey, print 等操作，类似于RDD 的转换操作。</li>
<li>启动 JobScheduler，实例化并启动 ReceiveTracker 和 JobGenerator。</li>
<li>启动 JobGenerator</li>
<li>启动 ReceiverTracker</li>
</ol>
]]></content>
      <categories>
        <category>bigdata</category>
      </categories>
      <tags>
        <tag>面试题</tag>
      </tags>
  </entry>
  <entry>
    <title>面试题随笔-21/3/27</title>
    <url>/2021/03/27/%E9%9D%A2%E8%AF%95%E9%A2%98%E9%9A%8F%E7%AC%94-21-3-27/</url>
    <content><![CDATA[<h2 id="面试题随笔-21-3-27"><a href="#面试题随笔-21-3-27" class="headerlink" title="面试题随笔-21/3/27"></a>面试题随笔-21/3/27</h2><h3 id="应用层协议"><a href="#应用层协议" class="headerlink" title="应用层协议"></a>应用层协议</h3><p>域名系统DNS协议、FTP文件传输协议、、telnet远程终端协议、HTTP超文本传送协议、SMTP电子邮件协议、POP3邮件读取协议、Telnet远程登录协议、SNMP简单网络管理协议等。</p>
<p>1、DNS：域名系统DNS是因特网使用的命名系统，用来把便于人们使用的机器名字转换为IP地址。</p>
<p>现在顶级域名TLD分为三大类：国家顶级域名nTLD；通用顶级域名gTLD;基础结构域名</p>
<p>域名服务器分为四种类型：根域名服务器；顶级域名服务器；本地域名服务器；权限域名服务器。</p>
<p>2、FTP：文件传输协议FTP是因特网上使用得最广泛的文件传送协议。FTP提供交互式的访问，允许客户指明文件类型与格式，并允许文件具有存取权限。FTP其于TCP。</p>
<p>3、telnet远程终端协议：telnet是一个简单的远程终端协议，它也是因特网的正式标准。又称为终端仿真协议。</p>
<p>4、HTTP：超文本传送协议，是面向事务的应用层协议，它是万维网上能够可靠地交换文件的重要基础。http使用面向连接的TCP作为运输层协议，保证了数据的可靠传输。</p>
<h3 id="什么场景用TCP，什么场景用UDP"><a href="#什么场景用TCP，什么场景用UDP" class="headerlink" title="什么场景用TCP，什么场景用UDP"></a>什么场景用TCP，什么场景用UDP</h3><p>TCP应用场景：<br>效率要求相对低，但对准确性要求相对高的场景。因为传输中需要对数据确认、重发、排序等操作，相比之下效率没有UDP高。举几个例子：文件传输（准确高要求高、但是速度可以相对慢）、接受邮件、远程登录。</p>
<p>UDP应用场景：<br>效率要求相对高，对准确性要求相对低的场景。举几个例子：QQ聊天、在线视频、网络语音电话（即时通讯，速度要求高，但是出现偶尔断续不是太大问题，并且此处完全不可以使用重发机制）、广播通信（广播、多播）。</p>
<h3 id="HTTP状态码都有哪些，具体说一下"><a href="#HTTP状态码都有哪些，具体说一下" class="headerlink" title="HTTP状态码都有哪些，具体说一下"></a>HTTP状态码都有哪些，具体说一下</h3><p>下面是常见的HTTP状态码：</p>
<p>200 - 请求成功<br>301 - 资源（网页等）被永久转移到其它URL<br>404 - 请求的资源（网页等）不存在<br>500 - 内部服务器错误</p>
<h3 id="HTTP长连接和短连接"><a href="#HTTP长连接和短连接" class="headerlink" title="HTTP长连接和短连接"></a>HTTP长连接和短连接</h3><p>长连接：client方与server方先建立连接，连接建立后不断开，然后再进行报文发送和接收。这种方式下由于通讯连接一直存在。此种方式常用于P2P通信。</p>
<p>短连接：Client方与server每进行一次报文收发交易时才进行通讯连接，交易完毕后立即断开连接。此方式常用于一点对多点通讯。C/S通信。</p>
<p><strong>长连接与短连接的操作过程</strong></p>
<p>短连接的操作步骤是：</p>
<p>建立连接——数据传输——关闭连接…建立连接——数据传输——关闭连接</p>
<p>长连接的操作步骤是：</p>
<p>建立连接——数据传输…（保持连接）…数据传输——关闭连接</p>
<h3 id="url和uri的区别"><a href="#url和uri的区别" class="headerlink" title="url和uri的区别"></a>url和uri的区别</h3><p>URI(Uniform Resource Identifier) = URL(Uniform Resource Locator)+URN(Uniform Resource Name)</p>
<p>1、作用的区别</p>
<p>URL（统一资源定位符）主要用于链接网页，网页组件或网页上的程序，借助访问方法（http，ftp，mailto等协议）来检索位置资源。</p>
<p>URI（统一资源标识符）用于定义项目的标识，此处单词标识符表示无论使用的方法是什么（URL或URN），都要将一个资源与其他资源区分开来。</p>
<h3 id="http请求头的一些字段内容"><a href="#http请求头的一些字段内容" class="headerlink" title="http请求头的一些字段内容"></a>http请求头的一些字段内容</h3><table>
<thead>
<tr>
<th>字段</th>
<th>意思</th>
</tr>
</thead>
<tbody><tr>
<td>Accept</td>
<td>这个头信息指定浏览器或其他客户端可以处理的 MIME 类型。有text/html,image/，/等几种常用类型。/*可以简单的概括为告诉服务器，客户端什么数据类型都支持</td>
</tr>
<tr>
<td>Accept-Charset</td>
<td>这个头信息指定浏览器可以用来显示信息的字符集。例如 ISO-8859-1</td>
</tr>
<tr>
<td>Accept-Encoding</td>
<td>这个头信息指定浏览器知道如何处理的编码类型。值 gzip 或 compress 是最常见的两种可能值</td>
</tr>
<tr>
<td>Accept-Language</td>
<td>这个头信息指定客户端的首选语言，在这种情况下，Servlet 会产生多种语言的结果。例如，en、en-us、ru 等。</td>
</tr>
<tr>
<td>Authorization</td>
<td>这个头信息用于客户端在访问受密码保护的网页时识别自己的身份。</td>
</tr>
<tr>
<td>Connection</td>
<td>这个头信息指示客户端是否可以处理持久 HTTP 连接。持久连接允许客户端或其他浏览器通过单个请求来检索多个文件。值 Keep-Alive 意味着使用了持续连接。</td>
</tr>
<tr>
<td>Content-Length</td>
<td>这个头信息只适用于 POST 请求，并给出 POST 数据的大小（以字节为单位）。</td>
</tr>
<tr>
<td>Cookie</td>
<td>这个头信息把之前发送到浏览器的 cookies 返回到服务器。</td>
</tr>
<tr>
<td>Host</td>
<td>这个头信息指定原始的 URL 中的主机和端口。</td>
</tr>
<tr>
<td>If-Modified-Since</td>
<td>这个头信息表示只有当页面在指定的日期后已更改时，客户端想要的页面。如果没有新的结果可以使用，服务器会发送一个 304 代码，表示 Not Modified 头信息。 Last-Modified 与If-Modified-Since都是用来记录页面的最后修改时间。当客户端访问页面时，服务器会将页面最后修改时间通过 Last-Modified 标识由服务器发往客户端，客户端记录修改时间，再次请求本地存在的cache页面时，客户端会通过 If-Modified-Since 头将先前服务器端发过来的最后修改时间戳发送回去，服务器端通过这个时间戳判断客户端的页面是否是最新的，如果不是最新的，则返回新的内容，如果是最新的，则 返回 304 告诉客户端其本地 cache 的页面是最新的，于是客户端就可以直接从本地加载页面了，这样在网络上传输的数据就会大大减少，同时也减轻了服务器的负担</td>
</tr>
<tr>
<td>If-Unmodified-Since</td>
<td>这个头信息是 If-Modified-Since 的对立面，它指定只有当文档早于指定日期时，操作才会成功。</td>
</tr>
<tr>
<td>Referer</td>
<td>这个头信息指示所指向的 Web 页的 URL。例如，如果您在网页 1，点击一个链接到网页 2，当浏览器请求网页 2 时，网页 1 的 URL 就会包含在 Referer 头信息中。</td>
</tr>
<tr>
<td>User-Agent</td>
<td>这个头信息识别发出请求的浏览器或其他客户端，并可以向不同类型的浏览器返回不同的内容。</td>
</tr>
</tbody></table>
<h3 id="hashmap的实现"><a href="#hashmap的实现" class="headerlink" title="hashmap的实现"></a>hashmap的实现</h3><p>HashMap基于Map接口实现，元素以键值对的方式存储，并且允许使用null键和null值，因为key不允许重复，因此只能有一个键为null,另外HashMap不能保证放入元素的顺序，它是无序的，和放入的顺序并不能相同。HashMap是线程不安全的。</p>
<p>1.HashMap由数组和链表来实现对数据的存储</p>
<p>HashMap采用Entry数组来存储key-value对，每一个键值对组成了一个Entry实体，Entry类实际上是一个单向的链表结构，它具有Next指针，可以连接下一个Entry实体，以此来解决Hash冲突的问题。</p>
<p>数组存储区间是连续的，占用内存严重，故空间复杂的很大。但数组的二分查找时间复杂度小，为O(1)；数组的特点是：寻址容易，插入和删除困难；</p>
<p>链表存储区间离散，占用内存比较宽松，故空间复杂度很小，但时间复杂度很大，达O（N）。链表的特点是：寻址困难，插入和删除容易。</p>
<p>2.HashMap采用数组+链表+红黑树实现。</p>
<p>在Jdk1.8中HashMap的实现方式做了一些改变，但是基本思想还是没有变得，只是在一些地方做了优化，下面来看一下这些改变的地方,数据结构的存储由数组+链表的方式，变化为数组+链表+红黑树的存储方式，当链表长度超过阈值（8）时，将链表转换为红黑树。在性能上进一步得到提升。</p>
<h3 id="Hashmap扩容"><a href="#Hashmap扩容" class="headerlink" title="Hashmap扩容"></a>Hashmap扩容</h3><h4 id="hashmap的loadfactory是干嘛的"><a href="#hashmap的loadfactory是干嘛的" class="headerlink" title="hashmap的loadfactory是干嘛的"></a>hashmap的loadfactory是干嘛的</h4><ul>
<li>capacity 即容量，默认16。</li>
<li>loadFactor 加载因子，默认是0.75</li>
<li>threshold 阈值。阈值=容量*加载因子。默认12。当元素数量超过阈值时便会触发扩容。</li>
</ul>
<h4 id="hashmap的扩容"><a href="#hashmap的扩容" class="headerlink" title="hashmap的扩容"></a>hashmap的扩容</h4><p>一般情况下，当元素数量超过阈值时便会触发扩容。每次扩容的容量都是之前容量的2倍。</p>
<p>HashMap的容量是有上限的，必须小于1&lt;&lt;30，即1073741824。如果容量超出了这个数，则不再增长，且阈值会被设置为Integer.MAX_VALUE（ [公式] ，即永远不会超出阈值了）。</p>
<h4 id="为什么每次扩容都是翻倍，增加1-5倍不行吗；扩容的时候为啥不多扩一点，比如4倍"><a href="#为什么每次扩容都是翻倍，增加1-5倍不行吗；扩容的时候为啥不多扩一点，比如4倍" class="headerlink" title="为什么每次扩容都是翻倍，增加1.5倍不行吗；扩容的时候为啥不多扩一点，比如4倍"></a>为什么每次扩容都是翻倍，增加1.5倍不行吗；扩容的时候为啥不多扩一点，比如4倍</h4><p>理论上，扩容倍数用多少都行，1.5， 2.5 ，3.5都可以的，都能实现HashMap。</p>
<p>实际上，HashMap选用了2倍，是为了做一个优化。回头看下HashMap的原理，它是用Key的哈希值，对应存放到数组的“指定位置”。就是在计算这个指定位置的时候，为了使得节点能够尽可能均匀地分布在数组上，会用Key的哈希值对数组长度取模来作为这个“目标位置”。比如一个Key的 hashCode() 是 1029991，而这个时候HashMap内部的数组长度是16，那么这个Key 会被存放在数组的第 1029991 % 16 = 7 位上。而取模运算本身是有一定的优化空间的，它可以在桶是2^n 的时候用位运算来代替，而位运算的性能则要好很多</p>
<p>（详情可以看下：【Java】使用位运算(&amp;)代替取模运算(%) - 后端）。</p>
<p>所以，HashMap的开发者想要优化下这个取模运算的速度，那么他就需要把HashMap内部的数组长度固定为 2^n 的长度了，也就是说HashMap里面的数组的长度，始终都是2的n次幂。为了实现这个效果，它的扩容因子很自然就是2倍了。</p>
<p>当然，把扩容因子设置成4倍、8倍、16倍等等也是一样可以实现的。或者假如不想要这个优化，就想自己搞一个按照  1.5 倍速度扩容的HashMap的话，完全重写一个HashMap也是OK的，这个只是为了性能优化，而不是一定要这么做的。以上。</p>
<h3 id="进程的用户栈和内核栈"><a href="#进程的用户栈和内核栈" class="headerlink" title="进程的用户栈和内核栈"></a>进程的用户栈和内核栈</h3><p>linux下的cpu有两个状态：内核态和用户态，内核态的cpu的权限高于用户态下的cpu。</p>
<p>linux下的内存分为用户态内存和内核态内存，一般4个G内存，3个G的分给用户态，1个G分给内核态。<br>linux进程有时需要调用内核资源时，如读写文件，io读写等，这时候是通过系统调用实现对内核资源的访问的，在访问内核资源前是用户栈，经过系统调用进入到内核态时，cpu的状态也由用户态变为内核态，访问的内存就是内核态下管理的内存了-内核栈，对内核里的资源访问完返回，内存又回到了用户栈，cpu也回到用户态。</p>
<h3 id="同步IO和异步IO"><a href="#同步IO和异步IO" class="headerlink" title="同步IO和异步IO"></a>同步IO和异步IO</h3><p>同步<br>发送一个请求，等待返回，再发送下一个请求，同步可以避免出现死锁，脏读的发生。</p>
<p>异步<br>发送一个请求，不等待返回，随时可以再发送下一个请求，可以提高效率，保证并发。</p>
<h3 id="先进先出分页置换算法"><a href="#先进先出分页置换算法" class="headerlink" title="先进先出分页置换算法"></a>先进先出分页置换算法</h3><p>先进先出算法是最简单的分页替换算法，是指每次有新的<a href="https://baike.baidu.com/item/分页/2888444" target="_blank" rel="noopener">分页</a>需要调入时，会选择调入内存时间最久的分页换出。它简单，容易实现，但这种绝对的公平方式容易导致效率的降低。</p>
<p>特点：</p>
<p><strong>优点</strong></p>
<p>简单，且容易实现。</p>
<p><strong>缺点</strong></p>
<p>这种绝对的公平方式容易导致效率的降低。例如，如果最先加载进来的页面是经常被访问的页面，这样做很可能造成常被访问的页面替换到磁盘上，导致很快就需要再次发生缺页中断，从而降低效率。</p>
<h3 id="同步-异步-谈谈你的理解"><a href="#同步-异步-谈谈你的理解" class="headerlink" title="同步 异步 谈谈你的理解"></a>同步 异步 谈谈你的理解</h3><p>在Java语言中，一共提供了三种IO模型，分别是阻塞IO（BIO）、非阻塞IO（NIO）、异步IO（AIO）。</p>
<p>这里面的BIO和NIO都是同步的IO模型，即同步阻塞IO和同步非阻塞IO，异步IO指的是异步非阻塞IO。</p>
<p>BIO （Blocking I/O）：同步阻塞I/O模式，数据的读取写入必须阻塞在一个线程内等待其完成。</p>
<p>NIO （New I/O）：同时支持阻塞与非阻塞模式，但主要是使用同步非阻塞IO。</p>
<p>AIO （Asynchronous I/O）：异步非阻塞I/O模型。</p>
<p>可以拿烧水的例子来解释这三种I/O模型</p>
<p>BIO （Blocking I/O）：有一排水壶在烧开水，BIO的工作模式就是，叫一个线程停留在一个水壶那，直到这个水壶烧开，才去处理下一个水壶。但是实际上线程在等待水壶烧开的时间段什么都没有做。</p>
<p>NIO （New I/O）：NIO的做法是叫一个线程不断的轮询每个水壶的状态，看看是否有水壶的状态发生了改变，从而进行下一步的操作。</p>
<p>AIO （ Asynchronous I/O）：为每个水壶上面装了一个开关，水烧开之后，水壶会自动通知我水烧开了。</p>
<h3 id="java锁的理解"><a href="#java锁的理解" class="headerlink" title="java锁的理解"></a>java锁的理解</h3><p>一段synchronized的代码被一个线程执行之前，他要先拿到执行这段代码的权限，在Java里边就是拿到某个同步对象的锁（一个对象只有一把锁）； </p>
<p>如果这个时候同步对象的锁被其他线程拿走了，他（这个线程）就只能等了（线程阻塞在锁池等待队列中）。</p>
<p>取到锁后，他就开始执行同步代码(被synchronized修饰的代码）；</p>
<p>线程执行完同步代码后马上就把锁还给同步对象，其他在锁池中等待的某个线程就可以拿到锁执行同步代码了。</p>
<p>这样就保证了同步代码在统一时刻只有一个线程在执行。</p>
<p>Java 中的锁有很多，可以按照不同的功能、种类进行分类，下面是我对 Java 中一些常用锁的分类，包括一些基本的概述。</p>
<ul>
<li>从线程是否需要对资源加锁可以分为 悲观锁 和 乐观锁</li>
<li>从资源已被锁定，线程是否阻塞可以分为 自旋锁</li>
<li>从多个线程并发访问资源，也就是 Synchronized 可以分为 无锁、偏向锁、 轻量级锁 和 重量级锁</li>
<li>从锁的公平性进行区分，可以分为公平锁 和 非公平锁</li>
<li>从根据锁是否重复获取可以分为 可重入锁 和 不可重入锁</li>
<li>从那个多个线程能否获取同一把锁分为 共享锁 和 排他锁</li>
</ul>
<p>乐观锁：</p>
<p>CAS 即 compare and swap（比较与交换），是一种有名的无锁算法。</p>
<p>CAS 中涉及三个要素：</p>
<ul>
<li>需要读写的内存值 V</li>
<li>进行比较的值 A</li>
<li>拟写入的新值 B</li>
</ul>
<p><strong>循环开销大</strong><br>我们知道乐观锁在进行写操作的时候会判断是否能够写入成功，如果写入不成功将触发等待 -&gt; 重试机制，这种情况是一个自旋锁，简单来说就是适用于短期内获取不到，进行等待重试的锁，它不适用于长期获取不到锁的情况，另外，自旋循环对于性能开销比较大。</p>
<p><strong>CAS与synchronized的使用情景</strong><br>简单的来说 CAS 适用于写比较少的情况下（多读场景，冲突一般较少），synchronized 适用于写比较多的情况下（多写场景，冲突一般较多）</p>
<p>对于资源竞争较少（线程冲突较轻）的情况，使用 Synchronized 同步锁进行线程阻塞和唤醒切换以及用户态内核态间的切换操作额外浪费消耗 cpu 资源；</p>
<p>而 CAS 基于硬件实现，不需要进入内核，不需要切换线程，操作自旋几率较少，因此可以获得更高的性能。</p>
<p>对于资源竞争严重（线程冲突严重）的情况，CAS自旋的概率会比较大，从而浪费更多的 CPU 资源，效率低于 synchronized。</p>
<p><strong>什么是自旋锁</strong></p>
<p>自旋锁的定义：当一个线程尝试去获取某一把锁的时候，如果这个锁此时已经被别人获取(占用)，那么此线程就无法获取到这把锁，该线程将会等待，间隔一段时间后会再次尝试获取。这种采用循环加锁 -&gt; 等待的机制被称为自旋锁(spinlock)。</p>
<p><strong>自旋锁的原理</strong></p>
<p>自旋锁的原理比较简单，如果持有锁的线程能在短时间内释放锁资源，那么那些等待竞争锁的线程就不需要做内核态和用户态之间的切换进入阻塞状态，它们只需要等一等(自旋)，等到持有锁的线程释放锁之后即可获取，这样就避免了用户进程和内核切换的消耗。</p>
<p><strong>多个线程并发访问资源    锁状态的分类</strong><br>Java 语言专门针对 synchronized 关键字设置了四种状态，它们分别是：无锁、偏向锁、轻量级锁和重量级锁，但是在了解这些锁之前还需要先了解一下 Java 对象头和 Monitor。</p>
<p><strong>无锁</strong><br>无锁状态，无锁即没有对资源进行锁定，所有的线程都可以对同一个资源进行访问，但是只有一个线程能够成功修改资源。</p>
<p>无锁的特点就是在循环内进行修改操作，线程会不断的尝试修改共享资源，直到能够成功修改资源并退出，在此过程中没有出现冲突的发生，这很像我们在之前文章中介绍的 CAS 实现，CAS 的原理和应用就是无锁的实现。无锁无法全面代替有锁，但无锁在某些场合下的性能是非常高的。</p>
<p><strong>偏向锁</strong><br>HotSpot 的作者经过研究发现，大多数情况下，锁不仅不存在多线程竞争，还存在锁由同一线程多次获得的情况，偏向锁就是在这种情况下出现的，它的出现是为了解决只有在一个线程执行同步时提高性能。</p>
<p>可以从对象头的分配中看到，偏向锁要比无锁多了线程ID 和 epoch，下面我们就来描述一下偏向锁的获取过程</p>
<p><strong>轻量级锁</strong><br>轻量级锁是指当前锁是偏向锁的时候，资源被另外的线程所访问，那么偏向锁就会升级为轻量级锁，其他线程会通过自旋的形式尝试获取锁，不会阻塞，从而提高性能，下面是详细的获取过程。</p>
<p><strong>重量级锁</strong></p>
<p>当前线程没有使用 CAS 成功获取锁，就会自旋一会儿，再次尝试获取，如果在多次自旋到达上限后还没有获取到锁，那么轻量级锁就会升级为 重量级锁。</p>
<h3 id="进程通信和线程通信"><a href="#进程通信和线程通信" class="headerlink" title="进程通信和线程通信"></a>进程通信和线程通信</h3><p>对于进程来说，子进程是父进程的复制品，从父进程那里获得父进程的数据空间，堆和栈的复制品。</p>
<p>而线程，相对于进程而言，是一个更加接近于执行体的概念，可以和同进程的其他线程之间直接共享数据，而且拥有自己的栈空间，拥有独立序列。</p>
<p>共同点： 它们都能提高程序的并发度，提高程序运行效率和响应时间。线程和进程在使用上各有优缺点。 线程执行开销比较小，但不利于资源的管理和保护，而进程相反。同时，线程适合在SMP机器上运行，而进程可以跨机器迁移。</p>
<p>他们之间根本区别在于：多进程中每个进程有自己的地址空间，线程则共享地址空间。所有其他区别都是因为这个区别产生的。比如说：</p>
<p>　　1、速度。线程产生的速度快，通讯快，切换快，因为他们处于同一地址空间。</p>
<p>　　2、线程的资源利用率好。</p>
<p>　　3、线程使用公共变量或者内存的时候需要同步机制，但进程不用。</p>
<p>　　而他们通信方式的差异也仍然是由于这个根本原因造成的。</p>
<h3 id="hadoop生态圈"><a href="#hadoop生态圈" class="headerlink" title="hadoop生态圈"></a>hadoop生态圈</h3><p>1，HDFS（hadoop分布式文件系统）</p>
<p>是hadoop体系中数据存储管理的基础。他是一个高度容错的系统，能检测和应对硬件故障。</p>
<p>client：切分文件，访问HDFS，与namenode交互，获取文件位置信息，与DataNode交互，读取和写入数据。</p>
<p> namenode：master节点，在hadoop1.x中只有一个，管理HDFS的名称空间和数据块映射信息，配置副本策略，处理客户端请求。</p>
<p> DataNode：slave节点，存储实际的数据，汇报存储信息给namenode。</p>
<p> secondary namenode：辅助namenode，分担其工作量：定期合并fsimage和fsedits，推送给namenode；紧急情况下和辅助恢复namenode，但其并非namenode的热备。</p>
<p>2，mapreduce（分布式计算框架）</p>
<p>mapreduce是一种计算模型，用于处理大数据量的计算。其中map对应数据集上的独立元素进行指定的操作，生成键-值对形式中间，reduce则对中间结果中相同的键的所有值进行规约，以得到最终结果。</p>
<p>jobtracker：master节点，只有一个，管理所有作业，任务/作业的监控，错误处理等，将任务分解成一系列任务，并分派给tasktracker。</p>
<p>tacktracker：slave节点，运行 map task和reducetask；并与jobtracker交互，汇报任务状态。</p>
<p>map task：解析每条数据记录，传递给用户编写的map（）并执行，将输出结果写入到本地磁盘（如果为map—only作业，则直接写入HDFS）。</p>
<p>reduce task：从map执行结果中，远程读取输入数据，对数据进行排序，将数据分组传递给用户编写的reduce函数执行。</p>
<p>3， hive（基于hadoop的数据仓库）</p>
<p>由Facebook开源，最初用于解决海量结构化的日志数据统计问题。</p>
<p>hive定于了一种类似sql的查询语言（hql）将sql转化为mapreduce任务在hadoop上执行。</p>
<p>4，hbase（分布式列存数据库）</p>
<p>hbase是一个针对结构化数据的可伸缩，高可靠，高性能，分布式和面向列的动态模式数据库。和传统关系型数据库不同，hbase采用了bigtable的数据模型：增强了稀疏排序映射表（key/value）。其中，键由行关键字，列关键字和时间戳构成，hbase提供了对大规模数据的随机，实时读写访问，同时，hbase中保存的数据可以使用mapreduce来处理，它将数据存储和并行计算完美结合在一起。</p>
<p>5，zookeeper（分布式协作服务）</p>
<p>解决分布式环境下的数据管理问题：统一命名，状态同步，集群管理，配置同步等。</p>
<p>6，sqoop（数据同步工具）</p>
<p>sqoop是sql-to-hadoop的缩写，主要用于传统数据库和hadoop之间传输数据。</p>
<p>数据的导入和导出本质上是mapreduce程序，充分利用了MR的并行化和容错性。</p>
<p>7，pig（基于hadoop的数据流系统）</p>
<p>定义了一种数据流语言-pig latin，将脚本转换为mapreduce任务在hadoop上执行。</p>
<p>通常用于离线分析。</p>
<p>8，mahout（数据挖掘算法库）</p>
<p>mahout的主要目标是创建一些可扩展的机器学习领域经典算法的实现，旨在帮助开发人员更加方便快捷地创建只能应用程序。mahout现在已经包含了聚类，分类，推荐引擎（协同过滤）和频繁集挖掘等广泛使用的数据挖掘方法。除了算法是，mahout还包含了数据的输入/输出工具，与其他存储系统（如数据库，mongoDB或Cassandra）集成等数据挖掘支持架构。</p>
<p>9，flume（日志收集工具）</p>
<p>cloudera开源的日志收集系统，具有分布式，高可靠，高容错，易于定制和扩展的特点。他将数据从产生，传输，处理并写入目标的路径的过程抽象为数据流，在具体的数据流中，数据源支持在flume中定制数据发送方，从而支持收集各种不同协议数据。</p>
<p>10，资源管理器的简单介绍（YARN和mesos）</p>
<h3 id="你觉得spark-可以完全替代hadoop-么"><a href="#你觉得spark-可以完全替代hadoop-么" class="headerlink" title="你觉得spark 可以完全替代hadoop 么?"></a>你觉得spark 可以完全替代hadoop 么?</h3><p>Spark 会替代 MR，Spark 存储依赖 HDFS，资源调度依赖 YARN，集群管理依赖 Zookeeper。</p>
<h3 id="Spark消费-Kafka，分布式的情况下，如何保证消息的顺序"><a href="#Spark消费-Kafka，分布式的情况下，如何保证消息的顺序" class="headerlink" title="Spark消费 Kafka，分布式的情况下，如何保证消息的顺序?"></a>Spark消费 Kafka，分布式的情况下，如何保证消息的顺序?</h3><p>Kafka 分布式的单位是 Partition。如何保证消息有序，需要分几个情况讨论。</p>
<ul>
<li>同一个 Partition 用一个 write ahead log 组织，所以可以保证 FIFO 的顺序。</li>
<li>不同 Partition 之间不能保证顺序。但是绝大多数用户都可以通过 message key 来定义，因为同一个 key 的 message 可以保证只发送到同一个 Partition。比如说 key 是 user id，table row id 等等，所以同一个 user 或者同一个 record 的消息永远只会发送到同一个 Partition上，保证了同一个 user 或 record 的顺序。</li>
<li>当然，如果你有 key skewness 就有些麻烦，需要特殊处理。</li>
</ul>
<p>实际情况中: （1）不关注顺序的业务大量存在；（2）队列无序不代表消息无序。</p>
<p>第（2）条的意思是说:：我们不保证队列的全局有序，但可以保证消息的局部有序。举个例子: 保证来自同1个 order id 的消息，是有序的！</p>
<p>Kafka 中发送1条消息的时候，可以指定(topic, partition, key) 3个参数。partiton 和 key 是可选的。如果你指定了 partition，那就是所有消息发往同1个 partition，就是有序的。并且在消费端，Kafka 保证，1个 partition 只能被1个 consumer 消费。</p>
<p>或者你指定 key（比如 order id），具有同1个 key 的所有消息，会发往同1个 partition。也是有序的。</p>
<h3 id="对于-Spark-中的数据倾斜问题你有什么好的方案？"><a href="#对于-Spark-中的数据倾斜问题你有什么好的方案？" class="headerlink" title="对于 Spark 中的数据倾斜问题你有什么好的方案？"></a>对于 Spark 中的数据倾斜问题你有什么好的方案？</h3><p>简单一句：Spark 数据倾斜的几种场景以及对应的解决方案，包括避免数据源倾斜，调整并行度，使用自定义 Partitioner，使用 Map 侧 Join 代替 Reduce 侧 Join（内存表合并），给倾斜 Key 加上随机前缀等。</p>
<p>什么是数据倾斜 对 Spark/Hadoop 这样的大数据系统来讲，数据量大并不可怕，可怕的是数据倾斜。数据倾斜指的是，并行处理的数据集中，某一部分（如 Spark 或 Kafka 的一个 Partition）的数据显著多于其它部分，从而使得该部分的处理速度成为整个数据集处理的瓶颈（木桶效应）。</p>
<p>数据倾斜是如何造成的 在 Spark 中，同一个 Stage 的不同 Partition 可以并行处理，而具有依赖关系的不同 Stage 之间是串行处理的。假设某个 Spark Job 分为 Stage 0和 Stage 1两个 Stage，且 Stage 1依赖于 Stage 0，那 Stage 0完全处理结束之前不会处理Stage 1。而 Stage 0可能包含 N 个 Task，这 N 个 Task 可以并行进行。如果其中 N-1个 Task 都在10秒内完成，而另外一个 Task 却耗时1分钟，那该 Stage 的总时间至少为1分钟。换句话说，一个 Stage 所耗费的时间，主要由最慢的那个 Task 决定。由于同一个 Stage 内的所有 Task 执行相同的计算，在排除不同计算节点计算能力差异的前提下，不同 Task 之间耗时的差异主要由该 Task 所处理的数据量决定。</p>
<p>具体解决方案 ：</p>
<ol>
<li>调整并行度分散同一个 Task 的不同 Key：Spark 在做 Shuffle 时，默认使用 HashPartitioner对数据进行分区。如果并行度设置的不合适，可能造成大量不相同的 Key 对应的数据被分配到了同一个 Task 上，造成该 Task 所处理的数据远大于其它 Task，从而造成数据倾斜。如果调整 Shuffle 时的并行度，使得原本被分配到同一 Task 的不同 Key 发配到不同 Task 上处理，则可降低原 Task 所需处理的数据量，从而缓解数据倾斜问题造成的短板效应。</li>
<li>自定义Partitioner：使用自定义的 Partitioner（默认为 HashPartitioner），将原本被分配到同一个 Task 的不同 Key 分配到不同 Task，可以拿上图继续想象一下，通过自定义 Partitioner 可以把原本分到 Task0 的 Key 分到 Task1，那么 Task0 的要处理的数据量就少了。 </li>
<li>将 Reduce side（侧） Join 转变为 Map side（侧） Join：通过 Spark 的 Broadcast 机制，将 Reduce 侧 Join 转化为 Map 侧 Join，避免 Shuffle 从而完全消除 Shuffle 带来的数据倾斜。可以看到 RDD2 被加载到内存中了。</li>
<li>为 skew 的 key 增加随机前/后缀：为数据量特别大的 Key 增加随机前/后缀，使得原来 Key 相同的数据变为 Key 不相同的数据，从而使倾斜的数据集分散到不同的 Task 中，彻底解决数据倾斜问题。Join 另一则的数据中，与倾斜 Key 对应的部分数据，与随机前缀集作笛卡尔乘积，从而保证无论数据倾斜侧倾斜 Key 如何加前缀，都能与之正常 Join。</li>
<li>大表随机添加 N 种随机前缀，小表扩大 N 倍：如果出现数据倾斜的 Key 比较多，上一种方法将这些大量的倾斜 Key 分拆出来，意义不大（很难一个 Key 一个 Key 都加上后缀）。此时更适合直接对存在数据倾斜的数据集全部加上随机前缀，然后对另外一个不存在严重数据倾斜的数据集整体与随机前缀集作笛卡尔乘积（即将数据量扩大 N 倍），可以看到 RDD2 扩大了 N 倍了，再和加完前缀的大数据做笛卡尔积。</li>
</ol>
<h3 id="你所理解的-Spark-的-shuffle-过程？"><a href="#你所理解的-Spark-的-shuffle-过程？" class="headerlink" title="你所理解的 Spark 的 shuffle 过程？"></a>你所理解的 Spark 的 shuffle 过程？</h3><p>Spark shuffle 处于一个宽依赖，可以实现类似混洗的功能，将相同的 Key 分发至同一个 Reducer上进行处理。</p>
<h3 id="Spark有哪些聚合类的算子-我们应该尽量避免什么类型的算子？"><a href="#Spark有哪些聚合类的算子-我们应该尽量避免什么类型的算子？" class="headerlink" title="Spark有哪些聚合类的算子,我们应该尽量避免什么类型的算子？"></a>Spark有哪些聚合类的算子,我们应该尽量避免什么类型的算子？</h3><p>在我们的开发过程中，能避免则尽可能避免使用 reduceByKey、join、distinct、repartition 等会进行 shuffle 的算子，尽量使用 map 类的非 shuffle 算子。这样的话，没有 shuffle 操作或者仅有较少 shuffle 操作的 Spark 作业，可以大大减少性能开销。</p>
<h1 id="spark-on-yarn-作业执行流程，yarn-client-和-yarn-cluster-有什么区别"><a href="#spark-on-yarn-作业执行流程，yarn-client-和-yarn-cluster-有什么区别" class="headerlink" title="spark on yarn 作业执行流程，yarn-client 和 yarn cluster 有什么区别"></a><strong>spark on yarn 作业执行流程，yarn-client 和 yarn cluster 有什么区别</strong></h1><p>Spark On Yarn 的优势 </p>
<p>\1.Spark 支持资源动态共享，运行于 Yarn 的框架都共享一个集中配置好的资源池 </p>
<p>\2.可以很方便的利用 Yarn 的资源调度特性来做分类·，隔离以及优先级控制负载，拥有更灵活的调度策略 </p>
<p>\3.Yarn 可以自由地选择 executor 数量 </p>
<p>\4.Yarn 是唯一支持 Spark 安全的集群管理器，使用 Yarn，Spark 可以运行于 Kerberos Hadoop 之上，在它们进程之间进行安全认证</p>
<p>yarn-client 和 yarn cluster 的异同 ：</p>
<p>\1. 从广义上讲，yarn-cluster 适用于生产环境。而 yarn-client 适用于交互和调试，也就是希望快速地看到 application 的输出。</p>
<p>\2.从深层次的含义讲，yarn-cluster 和 yarn-client 模式的区别其实就是 Application Master 进程的区别，yarn-cluster 模式下，driver 运行在 AM(Application Master)中，它负责向 YARN 申请资源，并监督作业的运行状况。当用户提交了作业之后，就可以关掉 Client，作业会继续在 YARN 上运行。然而 yarn-cluster 模式不适合运行交互类型的作业。而 yarn-client 模式下，Application Master 仅仅向 YARN 请求 executor，Client 会和请求的 container 通信来调度他们工作，也就是说 Client 不能离开。</p>
<h3 id="Spark为什么快，Spark-SQL-一定比-Hive-快吗"><a href="#Spark为什么快，Spark-SQL-一定比-Hive-快吗" class="headerlink" title="Spark为什么快，Spark SQL 一定比 Hive 快吗"></a>Spark为什么快，Spark SQL 一定比 Hive 快吗</h3><p>Spark SQL 比 Hadoop Hive 快，是有一定条件的，而且不是 Spark SQL 的引擎比 Hive 的引擎快，相反，Hive 的 HQL 引擎还比 Spark SQL 的引擎更快。其实，关键还是在于 Spark 本身快。</p>
<ol>
<li><p>消除了冗余的 HDFS 读写：Hadoop 每次 shuffle 操作后，必须写到磁盘，而 Spark 在 shuffle 后不一定落盘，可以 cache 到内存中，以便迭代时使用。如果操作复杂，很多的 shufle 操作，那么 Hadoop 的读写 IO 时间会大大增加，也是 Hive 更慢的主要原因了。</p>
</li>
<li><p>消除了冗余的 MapReduce 阶段：Hadoop 的 shuffle 操作一定连着完整的 MapReduce 操作，冗余繁琐。而 Spark 基于 RDD 提供了丰富的算子操作，且 reduce 操作产生 shuffle 数据，可以缓存在内存中。</p>
</li>
<li><p>JVM 的优化：Hadoop 每次 MapReduce 操作，启动一个 Task 便会启动一次 JVM，基于进程的操作。而 Spark 每次 MapReduce 操作是基于线程的，只在启动 Executor 是启动一次 JVM，内存的 Task 操作是在线程复用的。每次启动 JVM 的时间可能就需要几秒甚至十几秒，那么当 Task 多了，这个时间 Hadoop 不知道比 Spark 慢了多少。</p>
</li>
</ol>
<p>记住一种反例 考虑一种极端查询:</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">Select</span> month_id, <span class="keyword">sum</span>(sales) <span class="keyword">from</span> T <span class="keyword">group</span> <span class="keyword">by</span> month_id;</span><br></pre></td></tr></table></figure>

<p>这个查询只有一次 shuffle 操作，此时，也许 Hive HQL 的运行时间也许比 Spark 还快，反正 shuffle 完了都会落一次盘，或者都不落盘。</p>
<p>结论 Spark 快不是绝对的，但是绝大多数，Spark 都比 Hadoop 计算要快。这主要得益于其对 mapreduce 操作的优化以及对 JVM 使用的优化。</p>
<h3 id="RDD-DAG-Stage怎么理解？"><a href="#RDD-DAG-Stage怎么理解？" class="headerlink" title="RDD, DAG, Stage怎么理解？"></a>RDD, DAG, Stage怎么理解？</h3><p>DAG：Spark 中使用 DAG 对 RDD 的关系进行建模，描述了 RDD 的依赖关系，这种关系也被称之为 lineage（血缘），RDD 的依赖关系使用 Dependency 维护。DAG 在 Spark 中的对应的实现为 DAGScheduler。</p>
<p>RDD：RDD 是 Spark 的灵魂，也称为弹性分布式数据集。一个 RDD 代表一个可以被分区的只读数据集。RDD 内部可以有许多分区(partitions)，每个分区又拥有大量的记录(records)。</p>
<p>Rdd的五个特征：</p>
<ol>
<li><p>dependencies：建立 RDD 的依赖关系，主要 RDD 之间是宽窄依赖的关系，具有窄依赖关系的 RDD 可以在同一个 stage 中进行计算。</p>
</li>
<li><p>partition：一个 RDD 会有若干个分区，分区的大小决定了对这个 RDD 计算的粒度，每个 RDD 的分区的计算都在一个单独的任务中进行。</p>
</li>
<li><p>preferedlocations：按照“移动数据不如移动计算”原则，在 Spark 进行任务调度的时候，优先将任务分配到数据块存储的位置。</p>
</li>
<li><p>compute：Spark 中的计算都是以分区为基本单位的，compute 函数只是对迭代器进行复合，并不保存单次计算的结果。</p>
</li>
<li><p>partitioner: 只存在于（K,V）类型的 RDD 中，非（K,V）类型的 partitioner 的值就是 None。</p>
</li>
</ol>
<p>RDD 的算子主要分成2类，action 和 transformation。这里的算子概念，可以理解成就是对数据集的变换。action 会触发真正的作业提交，而 transformation 算子是不会立即触发作业提交的。每一个 transformation 方法返回一个新的 RDD。只是某些 transformation 比较复杂，会包含多个子 transformation，因而会生成多个 RDD。这就是实际 RDD 个数比我们想象的多一些 的原因。通常是，当遇到 action 算子时会触发一个job的提交，然后反推回去看前面的 transformation 算子，进而形成一张有向无环图。</p>
<p>Stage 在 DAG 中又进行 stage 的划分，划分的依据是依赖是否是 shuffle 的，每个 stage 又可以划分成若干 task。接下来的事情就是 driver 发送 task 到 executor，executor 自己的线程池去执行这些 task，完成之后将结果返回给 driver。action 算子是划分不同 job 的依据。</p>
<h3 id="RDD-如何通过记录更新的方式容错"><a href="#RDD-如何通过记录更新的方式容错" class="headerlink" title="RDD 如何通过记录更新的方式容错"></a><strong>RDD 如何通过记录更新的方式容错</strong></h3><p>RDD 的容错机制实现分布式数据集容错方法有两种：1. 数据检查点 2. 记录更新。</p>
<p>RDD 采用记录更新的方式：记录所有更新点的成本很高。所以，RDD只支持粗颗粒变换，即只记录单个块（分区）上执行的单个操作，然后创建某个 RDD 的变换序列（血统 lineage）存储下来；变换序列指，每个 RDD 都包含了它是如何由其他 RDD 变换过来的以及如何重建某一块数据的信息。因此 RDD 的容错机制又称“血统”容错。</p>
<h3 id="宽依赖、窄依赖怎么理解？"><a href="#宽依赖、窄依赖怎么理解？" class="headerlink" title="宽依赖、窄依赖怎么理解？"></a>宽依赖、窄依赖怎么理解？</h3><p>窄依赖指的是每一个 parent RDD 的 partition 最多被子 RDD 的一个 partition 使用（一子一亲）。</p>
<p>宽依赖指的是多个子 RDD 的 partition 会依赖同一个 parent RDD的 partition（多子一亲）。</p>
<p>RDD 作为数据结构，本质上是一个只读的分区记录集合。一个 RDD 可以包含多个分区，每个分区就是一个 dataset 片段。RDD 可以相互依赖。</p>
<p>首先，窄依赖可以支持在同一个 cluster node上，以 pipeline 形式执行多条命令（也叫同一个 stage 的操作），例如在执行了 map 后，紧接着执行 filter。相反，宽依赖需要所有的父分区都是可用的，可能还需要调用类似 MapReduce 之类的操作进行跨节点传递。</p>
<p>其次，则是从失败恢复的角度考虑。窄依赖的失败恢复更有效，因为它只需要重新计算丢失的 parent partition 即可，而且可以并行地在不同节点进行重计算（一台机器太慢就会分配到多个节点进行），相反，宽依赖牵涉 RDD 各级的多个 parent partition。</p>
<h3 id="Job-和-Task-怎么理解"><a href="#Job-和-Task-怎么理解" class="headerlink" title="Job 和 Task 怎么理解"></a>Job 和 Task 怎么理解</h3><p>Job：Spark 的 Job 来源于用户执行 action 操作（这是 Spark 中实际意义的 Job），就是从 RDD 中获取结果的操作，而不是将一个 RDD 转换成另一个 RDD 的 transformation 操作。</p>
<p>Task：一个 Stage 内，最终的 RDD 有多少个 partition，就会产生多少个 task。看一看图就明白了，可以数一数每个 Stage 有多少个 Task。</p>
<h3 id="Spark-血统的概念"><a href="#Spark-血统的概念" class="headerlink" title="Spark 血统的概念"></a>Spark 血统的概念</h3><p>RDD 的 lineage 记录的是粗颗粒度的特定数据转换（transformation）操作（filter, map, join etc.)行为。当这个 RDD 的部分分区数据丢失时，它可以通过 lineage 获取足够的信息来重新运算和恢复丢失的数据分区。这种粗颗粒的数据模型，限制了 Spark 的运用场合，但同时相比细颗粒度的数据模型，也带来了性能的提升。</p>
<h3 id="任务的概念"><a href="#任务的概念" class="headerlink" title="任务的概念"></a>任务的概念</h3><p>包含很多 task 的并行计算，可以认为是 Spark RDD 里面的 action，每个 action 的计算会生成一个 job。用户提交的 job 会提交给 DAGScheduler，job 会被分解成 Stage 和 Task。</p>
<h3 id="容错方法"><a href="#容错方法" class="headerlink" title="容错方法"></a>容错方法</h3><p>Spark 选择记录更新的方式。但是，如果更新粒度太细太多，那么记录更新成本也不低。因此，RDD只支持粗粒度转换，即只记录单个块上执行的单个操作，然后将创建 RDD 的一系列变换序列（每个 RDD 都包含了他是如何由其他 RDD 变换过来的以及如何重建某一块数据的信息。因此 RDD 的容错机制又称血统容错）记录下来，以便恢复丢失的分区。lineage本质上很类似于数据库中的重做日志（Redo Log），只不过这个重做日志粒度很大，是对全局数据做同样的重做进而恢复数据。</p>
<p>相比其他系统的细颗粒度的内存数据更新级别的备份或者 LOG 机制，RDD 的 lineage 记录的是粗颗粒度的特定数据 transformation 操作行为。当这个 RDD 的部分分区数据丢失时，它可以通过 lineage 获取足够的信息来重新运算和恢复丢失的数据分区。</p>
<h3 id="Spark-粗粒度和细粒度调度"><a href="#Spark-粗粒度和细粒度调度" class="headerlink" title="Spark 粗粒度和细粒度调度"></a>Spark 粗粒度和细粒度调度</h3><p>Spark 中，每个 application 对应一个 SparkContext。对于 SparkContext 之间的调度关系，取决于 Spark 的运行模式。对 Standalone 模式而言，Spark Master 节点先计算集群内的计算资源能否满足等待队列中的应用对内存和 CPU 资源的需求，如果可以，则 Master 创建 Spark Driver，启动应用的执行。宏观上来讲，这种对应用的调度类似于 FIFO 策略。在 Mesos 和 Yarn 模式下，底层的资源调度系统的调度策略都是由 Mesos 和 Yarn 决定的。具体分类描述如下：</p>
<ol>
<li>Standalone 模式：默认以用户提交 Applicaiton 的顺序来调度，即 FIFO 策略。每个应用执行时独占所有资源。如果有多个用户要共享集群资源，则可以使用参数 <code>spark.cores.max</code> 来配置应用在集群中可以使用的最大 CPU 核的数量。如果不配置，则采用默认参数 <code>spark.deploy.defaultCore</code> 的值来确定。</li>
<li>Mesos 模式：如果在 Mesos 上运行 Spark，用户想要静态配置资源的话，可以设置 <code>spark.mesos.coarse</code> 为 true，这样 Mesos 变为粗粒度调度模式。然后可以设置 <code>spark.cores.max</code> 指定集群中可以使用的最大核数，与上面 Standalone 模式类似。同时，在 Mesos 模式下，用户还可以设置参数 <code>spark.executor.memory</code> 来配置每个 executor 的内存使用量。如果想使 Mesos 在细粒度模式下运行，可以通过 <code>mesos://</code> 设置动态共享 CPU core 的执行模式。在这种模式下，应用不执行时的空闲 CPU 资源得以被其他用户使用，提升了 CPU 使用率。</li>
</ol>
<h3 id="Spark优越性"><a href="#Spark优越性" class="headerlink" title="Spark优越性"></a>Spark优越性</h3><p>一、Spark 的5大优势：</p>
<ol>
<li><p>更高的性能。因为数据被加载到集群主机的分布式内存中。数据可以被快速的转换迭代，并缓存用以后续的频繁访问需求。在数据全部加载到内存的情况下，Spark可以比Hadoop快100倍，在内存不够存放所有数据的情况下快hadoop10倍。</p>
</li>
<li><p>通过建立在Java,Scala,Python,SQL（应对交互式查询）的标准API以方便各行各业使用，同时还含有大量开箱即用的机器学习库。</p>
</li>
<li><p>与现有Hadoop 1和2.x(YARN)生态兼容，因此机构可以无缝迁移。</p>
</li>
<li><p>方便下载和安装。方便的shell（REPL: Read-Eval-Print-Loop）可以对API进行交互式的学习。</p>
</li>
<li><p>借助高等级的架构提高生产力，从而可以讲精力放到计算上。</p>
</li>
</ol>
<p>二、MapReduce与Spark相比，有哪些异同点：</p>
<p>1、基本原理上：（1） MapReduce：基于磁盘的大数据批量处理系统 （2）Spark：基于RDD(弹性分布式数据集)数据处理，显示将RDD数据存储到磁盘和内存中。</p>
<p>2、模型上：（1） MapReduce可以处理超大规模的数据，适合日志分析挖掘等较少的迭代的长任务需求，结合了数据的分布式的计算。（2） Spark：适合数据的挖掘，机器学习等多轮迭代式计算任务。</p>
<h3 id="Transformation和action是什么？区别？举几个常用方法"><a href="#Transformation和action是什么？区别？举几个常用方法" class="headerlink" title="Transformation和action是什么？区别？举几个常用方法"></a><strong>Transformation和action是什么？区别？举几个常用方法</strong></h3><p>RDD 创建后就可以在 RDD 上进行数据处理。RDD 支持两种操作：1. 转换（transformation）: 即从现有的数据集创建一个新的数据集 2. 动作（action）: 即在数据集上进行计算后，返回一个值给 Driver 程序</p>
<p>RDD 的转化操作是返回一个新的 RDD 的操作，比如 map() 和 filter() ，而行动操作则是向驱动器程序返回结果或把结果写入外部系统的操作，会触发实际的计算，比如 count() 和 first() 。Spark 对待转化操作和行动操作的方式很不一样，因此理解你正在进行的操作的类型是很重要的。如果对于一个特定的函数是属于转化操作还是行动操作感到困惑，你可以看看它的返回值类型：转化操作返回的是 RDD，而行动操作返回的是其他的数据类型。</p>
<p>RDD 中所有的 Transformation 都是惰性的，也就是说，它们并不会直接计算结果。相反的它们只是记住了这些应用到基础数据集（例如一个文件）上的转换动作。只有当发生一个要求返回结果给 Driver 的 Action 时，这些 Transformation 才会真正运行。</p>
<p>这个设计让 Spark 更加有效的运行。</p>
<h3 id="Spark作业提交流程是怎么样的"><a href="#Spark作业提交流程是怎么样的" class="headerlink" title="Spark作业提交流程是怎么样的"></a><strong>Spark作业提交流程是怎么样的</strong></h3><ul>
<li><code>spark-submit</code> 提交代码，执行 <code>new SparkContext()</code>，在 SparkContext 里构造 <code>DAGScheduler</code> 和 <code>TaskScheduler</code>。</li>
<li>TaskScheduler 会通过后台的一个进程，连接 Master，向 Master 注册 Application。</li>
<li>Master 接收到 Application 请求后，会使用相应的资源调度算法，在 Worker 上为这个 Application 启动多个 Executer。</li>
<li>Executor 启动后，会自己反向注册到 TaskScheduler 中。所有 Executor 都注册到 Driver 上之后，SparkContext 结束初始化，接下来往下执行我们自己的代码。</li>
<li>每执行到一个 Action，就会创建一个 Job。Job 会提交给 DAGScheduler。</li>
<li>DAGScheduler 会将 Job划分为多个 stage，然后每个 stage 创建一个 TaskSet。</li>
<li>TaskScheduler 会把每一个 TaskSet 里的 Task，提交到 Executor 上执行。</li>
<li>Executor 上有线程池，每接收到一个 Task，就用 TaskRunner 封装，然后从线程池里取出一个线程执行这个 task。(TaskRunner 将我们编写的代码，拷贝，反序列化，执行 Task，每个 Task 执行 RDD 里的一个 partition)</li>
</ul>
<h3 id="Spark-streamning工作流程是怎么样的，和Storm比有什么区别"><a href="#Spark-streamning工作流程是怎么样的，和Storm比有什么区别" class="headerlink" title="Spark streamning工作流程是怎么样的，和Storm比有什么区别"></a>Spark streamning工作流程是怎么样的，和Storm比有什么区别</h3><p>Spark Streaming 与 Storm 都可以用于进行实时流计算。但是他们两者的区别是非常大的。其中区别之一，就是，Spark Streaming 和 Storm 的计算模型完全不一样，Spark Streaming 是基于 RDD 的，因此需要将一小段时间内的，比如1秒内的数据，收集起来，作为一个 RDD，然后再针对这个 batch 的数据进行处理。</p>
<p>而 Storm 却可以做到每来一条数据，都可以立即进行处理和计算。因此，Spark Streaming 实际上严格意义上来说，只能称作准实时的流计算框架；而 Storm 是真正意义上的实时计算框架。</p>
<p>此外，Storm 支持的一项高级特性，是 Spark Streaming 暂时不具备的，即 Storm 支持在分布式流式计算程序（Topology）在运行过程中，可以动态地调整并行度，从而动态提高并发处理能力。而 Spark Streaming 是无法动态调整并行度的。</p>
<p>但是 Spark Streaming 也有其优点，首先 Spark Streaming 由于是基于 batch 进行处理的，因此相较于 Storm 基于单条数据进行处理，具有数倍甚至数十倍的吞吐量。此外，Spark Streaming 由于也身处于 Spark 生态圈内，因此Spark Streaming可以与Spark Core、Spark SQL，甚至是Spark MLlib、Spark GraphX进行无缝整合。</p>
<p>流式处理完的数据，可以立即进行各种map、reduce转换操作，可以立即使用sql进行查询，甚至可以立即使用machine learning或者图计算算法进行处理。这种一站式的大数据处理功能和优势，是 Storm 无法匹敌的。因此，综合上述来看，通常在对实时性要求特别高，而且实时数据量不稳定，比如在白天有高峰期的情况下，可以选择使用 Storm。但是如果是对实时性要求一般，允许1秒的准实时处理，而且不要求动态调整并行度的话，选择Spark Streaming是更好的选择。</p>
]]></content>
      <categories>
        <category>bigdata</category>
      </categories>
      <tags>
        <tag>面试题</tag>
      </tags>
  </entry>
  <entry>
    <title>面试题随笔-21/3/26</title>
    <url>/2021/03/26/%E9%9D%A2%E8%AF%95%E9%A2%98%E9%9A%8F%E7%AC%94-21-3-26/</url>
    <content><![CDATA[<h2 id="面试题随笔-21-3-26"><a href="#面试题随笔-21-3-26" class="headerlink" title="面试题随笔-21/3/26"></a>面试题随笔-21/3/26</h2><h3 id="寄存器和存储器区别"><a href="#寄存器和存储器区别" class="headerlink" title="寄存器和存储器区别"></a>寄存器和存储器区别</h3><p>寄存器是中央处理器内的组成部份。它跟CPU有关。寄存器是有限存贮容量的高速存贮部件，它们可用来暂存指令、数据和位址。在中央处理器的控制部件中，包含的寄存器有指令寄存器(IR)和程序计数器(PC)。在中央处理器的算术及逻辑部件中，包含的寄存器有累加器(ACC)。</p>
<p>存储器范围最大，它几乎涵盖了所有关于存储的范畴。你所说的寄存器，内存，都是存储器里面的一种。凡是有存储能力的硬件，都可以称之为存储器，这是自然，硬盘更加明显了，它归入外存储器行列，由此可见。</p>
<p><strong>主要区别</strong></p>
<p>1、寄存器存在于CPU中，速度很快，数目有限；<br>存储器就是内存，速度稍慢，但数量很大；<br>计算机做运算时，必须将数据读入寄存器才能运算。<br>2、存储器包括寄存器,<br>存储器有ROM和RAM</p>
<h3 id="kafka在什么地方需要用到zookeeper"><a href="#kafka在什么地方需要用到zookeeper" class="headerlink" title="kafka在什么地方需要用到zookeeper"></a>kafka在什么地方需要用到zookeeper</h3><p>kafka使用ZooKeeper用于管理、协调代理。每个Kafka代理通过Zookeeper协调其他Kafka代理。</p>
<p>当Kafka系统中新增了代理或某个代理失效时，Zookeeper服务将通知生产者和消费者。生产者与消费者据此开始与其他代理协调工作。</p>
<p>Zookeeper在Kakfa中扮演的角色：Kafka将元数据信息保存在Zookeeper中，但是发送给Topic本身的数据是不会发到Zk上的。</p>
<h3 id="了解HBase吗？"><a href="#了解HBase吗？" class="headerlink" title="了解HBase吗？"></a>了解HBase吗？</h3><h4 id="HBase-概述"><a href="#HBase-概述" class="headerlink" title="HBase 概述"></a>HBase 概述</h4><p>HBase是Hadoop的生态系统，是建立在Hadoop文件系统（HDFS）之上的分布式、面向列的数据库，通过利用Hadoop的文件系统提供容错能力。如果你需要进行实时读写或者随机访问大规模的数据集的时候，请考虑使用HBase！</p>
<p>HBase作为Google Bigtable的开源实现，Google Bigtable利用GFS作为其文件存储系统类似，则HBase利用Hadoop HDFS作为其文件存储系统；Google通过运行MapReduce来处理Bigtable中的海量数据，同样，HBase利用Hadoop MapReduce来处理HBase中的海量数据；Google Bigtable利用Chubby作为协同服务，HBase利用 Zookeeper 作为对应。</p>
<h4 id="HBase处理数据"><a href="#HBase处理数据" class="headerlink" title="HBase处理数据"></a>HBase处理数据</h4><p>虽然Hadoop是一个高容错、高延时的分布式文件系统和高并发的批处理系统，但是它不适用于提供实时计算；HBase是可以提供实时计算的分布式数据库，数据被保存在HDFS分布式文件系统上，由HDFS保证期高容错性，但是再生产环境中，HBase是如何基于hadoop提供实时性呢？ HBase上的数据是以StoreFile(HFile)二进制流的形式存储在HDFS上block块儿中；但是HDFS并不知道的HBase用于存储什么，它只把存储文件认为是二进制文件，也就是说，HBase的存储数据对于HDFS文件系统是透明的。</p>
<p>HBase与HDFS<br>在下面的表格中，我们对HDFS与HBase进行比较：</p>
<table>
<thead>
<tr>
<th>HDFS</th>
<th>HBase</th>
</tr>
</thead>
<tbody><tr>
<td>HDFS适于存储大容量文件的分布式文件系统。</td>
<td>HBase是建立在HDFS之上的数据库。</td>
</tr>
<tr>
<td>HDFS不支持快速单独记录查找。</td>
<td>HBase提供在较大的表快速查找</td>
</tr>
<tr>
<td>HDFS提供了高延迟批量处理;没有批处理概念。</td>
<td>HBase提供了数十亿条记录低延迟访问单个行记录（随机存取）。</td>
</tr>
<tr>
<td>HDFS提供的数据只能顺序访问。</td>
<td>HBase内部使用哈希表和提供随机接入，并且其存储索引，可将在HDFS文件中的数据进行快速查找。</td>
</tr>
</tbody></table>
<h4 id="HBase-数据模型"><a href="#HBase-数据模型" class="headerlink" title="HBase 数据模型"></a>HBase 数据模型</h4><p>HBase通过表格的模式存储数据，每个表格由列和行组成，其中，每个列又被划分为若干个列族（row family），请参考下面的图：</p>
<p><img src="https://lixiangbetter.github.io/2021/03/26/%E9%9D%A2%E8%AF%95%E9%A2%98%E9%9A%8F%E7%AC%94-21-3-26/7875120-5801f9c53bee98e3.png" alt></p>
<p>现在我们来看看HBase的逻辑数据模型与物理数据模型（实际存储的数据模型）：</p>
<p>逻辑数据模型：</p>
<p><img src="https://lixiangbetter.github.io/2021/03/26/%E9%9D%A2%E8%AF%95%E9%A2%98%E9%9A%8F%E7%AC%94-21-3-26/3.png" alt></p>
<p>物理数据模型：</p>
<p><img src="https://lixiangbetter.github.io/2021/03/26/%E9%9D%A2%E8%AF%95%E9%A2%98%E9%9A%8F%E7%AC%94-21-3-26/7875120-1f82e76d451038f7.png" alt></p>
<p><img src="https://lixiangbetter.github.io/2021/03/26/%E9%9D%A2%E8%AF%95%E9%A2%98%E9%9A%8F%E7%AC%94-21-3-26/1.png" alt></p>
<h4 id="HBase-架构"><a href="#HBase-架构" class="headerlink" title="HBase 架构"></a>HBase 架构</h4><p>下图显示了HBase的组成结构：</p>
<p><img src="https://lixiangbetter.github.io/2021/03/26/%E9%9D%A2%E8%AF%95%E9%A2%98%E9%9A%8F%E7%AC%94-21-3-26/4.jpeg" alt></p>
<p>HBase<br>通过上图我们可以得出Hbase中的每张表都按照一定的范围被分割成多个子表（HRegion），默认一个HRegion超过 256M 就要被分割成两个，由 HRegionServer管理，管理哪些HRegion由HMaster分配。</p>
<p>现在我们来介绍一下HBase中的一些组成部件以及它们起到的作用：</p>
<p>Client：包含访问HBase的接口，并维护cache来加快对HBase的访问。</p>
<p>Zookeeper：HBase依赖Zookeeper，默认情况下HBase管理Zookeeper实例（启动或关闭Zookeeper），Master与RegionServers启动时会向Zookeeper注册。Zookeeper的作用如下：</p>
<ul>
<li>保证任何时候，集群中只有一个master</li>
<li>存储所有Region的寻址入口</li>
<li>实时监控Region server的上线和下线信息。并实时通知给master</li>
<li>存储HBase的schema和table元数据</li>
</ul>
<p>HRegionServer：用来维护master分配给他的region，处理对这些region的io请求；负责切分正在运行过程中变的过大的region。</p>
<p>HRegion：HBase表在行的方向上分隔为多个Region。Region是HBase中分布式存储和负载均衡的最小单元，即不同的region可以分别在不同的Region Server上，但同一个Region是不会拆分到多个server上。Region按大小分隔，每个表一般是只有一个region，当region的某个列族达到一个阈值（默认256M）时就会分成两个新的region。</p>
<p>Store：每一个Region由一个或多个Store组成，至少是一个Store，HBase会把一起访问的数据放在一个Store里面，即为每个ColumnFamily建一个Store，如果有几个ColumnFamily，也就有几个Store。一个Store由一个memStore和0或者多个StoreFile组成。Store的大小被HBase用来判断是否需要切分Region。</p>
<p>StoreFile：memStore内存中的数据写到文件后就是StoreFile，StoreFile底层是以HFile的格式保存。</p>
<p>HLog：HLog记录数据的所有变更，可以用来恢复文件，一旦region server 宕机，就可以从log中进行恢复。</p>
<p>LogFlusher：一个LogFlusher的类是用来调用HLog.optionalSync()的。</p>
<h4 id="HBase-的应用"><a href="#HBase-的应用" class="headerlink" title="HBase 的应用"></a>HBase 的应用</h4><ul>
<li>HBase是用来当有需要写重的应用程序。</li>
<li>HBase可以帮助快速随机访问数据。</li>
<li>HBase被许多公司所采纳，例如，Facebook、Twitter、Yahoo!、Adobe、OpenPlaces、WorldLingo等等。</li>
</ul>
<h3 id="说下spark中的transform和action"><a href="#说下spark中的transform和action" class="headerlink" title="说下spark中的transform和action"></a>说下spark中的transform和action</h3><p><img src="https://lixiangbetter.github.io/2021/03/26/%E9%9D%A2%E8%AF%95%E9%A2%98%E9%9A%8F%E7%AC%94-21-3-26/5.png" alt></p>
<h3 id="spark中有了RDD，为什么还要有Dataframe和DataSet？"><a href="#spark中有了RDD，为什么还要有Dataframe和DataSet？" class="headerlink" title="spark中有了RDD，为什么还要有Dataframe和DataSet？"></a>spark中有了RDD，为什么还要有Dataframe和DataSet？</h3><p><strong>RDD</strong><br>RDD(Resilient Distributed Datasets)叫做弹性分布式数据集，是Spark中最基本的数据抽象，源码中是一个抽象类，代表一个不可变、可分区、里面的元素可并行计算的集合。编译时类型安全，但是无论是集群间的通信，还是IO操作都需要对对象的结构和数据进行序列化和反序列化，还存在较大的GC的性能开销，会频繁的创建和销毁对象。RDD也不支持SparkSQL操作。</p>
<p><strong>DataFrame</strong><br>与RDD类似，DataFrame是一个分布式数据容器，不过它更像数据库中的二维表格，除了数据之外，还记录这数据的结构信息（即schema）。DataFrame也是懒执行的，性能上要比RDD高（主要因为执行计划得到了优化）。由于RDD每一行的数据结构一样，且存在schema中，Spark通过schema就能读懂数据，因此在通信和IO时只需要序列化和反序列化数据，而结构部分不用。Spark能够以二进制的形式序列化数据到JVM堆以外（off-heap：非堆）的内存，这些内存直接受操作系统管理，也就不再受JVM的限制和GC的困扰了。但是DataFrame不是类型安全的。</p>
<p><strong>DataSet</strong><br>DataSet是DataFrame API的一个扩展，是Spark最新的数据抽象，结合了RDD和DataFrame的优点。DataFrame=DataSet[Row]（Row表示表结构信息的类型），DataFrame只知道字段，但是不知道字段类型，而DataSet是强类型的，不仅仅知道字段，而且知道字段类型。样例类被用来在DataSet中定义数据的结构信息，样例类中的每个属性名称直接对应到DataSet中的字段名称。DataSet具有类型安全检查，也具有DataFrame的查询优化特性，还支持编解码器，当需要访问非堆上的数据时可以避免反序列化整个对象，提高了效率。</p>
<h3 id="了解函数式编程吗？说下c-c-和scala这种函数式编程语言的区别"><a href="#了解函数式编程吗？说下c-c-和scala这种函数式编程语言的区别" class="headerlink" title="了解函数式编程吗？说下c/c++和scala这种函数式编程语言的区别"></a>了解函数式编程吗？说下c/<a href="https://www.nowcoder.com/jump/super-jump/word?word=c%2B%2B" target="_blank" rel="noopener">c++</a>和scala这种函数式编程语言的区别</h3><p>简单说，”函数式编程”是一种”编程范式”（programming paradigm），也就是如何编写程序的方法论。</p>
<p>它属于”结构化编程”的一种，主要思想是把运算过程尽量写成一系列嵌套的函数调用。</p>
<p>特点</p>
<ol>
<li>函数是”第一等公民”</li>
</ol>
<p>所谓”第一等公民”（first class），指的是函数与其他数据类型一样，处于平等地位，可以赋值给其他变量，也可以作为参数，传入另一个函数，或者作为别的函数的返回值。</p>
<p>举例来说，下面代码中的print变量就是一个函数，可以作为另一个函数的参数。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="keyword">var</span> print = function(i)&#123; console.log(i);&#125;;</span><br><span class="line">[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>].forEach(print);</span><br></pre></td></tr></table></figure>

<ol start="2">
<li>只用”表达式”，不用”语句”</li>
</ol>
<p>“表达式”（expression）是一个单纯的运算过程，总是有返回值；”语句”（statement）是执行某种操作，没有返回值。函数式编程要求，只使用表达式，不使用语句。也就是说，每一步都是单纯的运算，而且都有返回值。</p>
<p>原因是函数式编程的开发动机，一开始就是为了处理运算（computation），不考虑系统的读写（I/O）。”语句”属于对系统的读写操作，所以就被排斥在外。</p>
<p>当然，实际应用中，不做I/O是不可能的。因此，编程过程中，函数式编程只要求把I/O限制到最小，不要有不必要的读写行为，保持计算过程的单纯性。</p>
<ol start="3">
<li>没有”副作用”</li>
</ol>
<p>所谓”副作用”（side effect），指的是函数内部与外部互动（最典型的情况，就是修改全局变量的值），产生运算以外的其他结果。</p>
<p>函数式编程强调没有”副作用”，意味着函数要保持独立，所有功能就是返回一个新的值，没有其他行为，尤其是不得修改外部变量的值。</p>
<ol start="4">
<li>不修改状态</li>
</ol>
<p>上一点已经提到，函数式编程只是返回新的值，不修改系统变量。因此，不修改变量，也是它的一个重要特点。</p>
<p>在其他类型的语言中，变量往往用来保存”状态”（state）。不修改变量，意味着状态不能保存在变量中。函数式编程使用参数保存状态，最好的例子就是递归。下面的代码是一个将字符串逆序排列的函数，它演示了不同的参数如何决定了运算所处的”状态”。</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line">function reverse(string) &#123;</span><br><span class="line">  <span class="keyword">if</span>(string.length == <span class="number">0</span>) &#123;</span><br><span class="line">    <span class="keyword">return</span> string;</span><br><span class="line">  &#125; <span class="keyword">else</span> &#123;</span><br><span class="line">    <span class="keyword">return</span> reverse(string.substring(<span class="number">1</span>, string.length)) + string.substring(<span class="number">0</span>, <span class="number">1</span>);</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>由于使用了递归，函数式语言的运行速度比较慢，这是它长期不能在业界推广的主要原因。</p>
<ol start="5">
<li>引用透明</li>
</ol>
<p>引用透明（Referential transparency），指的是函数的运行不依赖于外部变量或”状态”，只依赖于输入的参数，任何时候只要参数相同，引用函数所得到的返回值总是相同的。</p>
<p>有了前面的第三点和第四点，这点是很显然的。其他类型的语言，函数的返回值往往与系统状态有关，不同的状态之下，返回值是不一样的。这就叫”引用不透明”，很不利于观察和理解程序的行为。</p>
<ul>
<li>面向过程：根据业务逻辑从上到下写垒代码</li>
<li>函数式：将某功能代码封装到函数中，日后便无需重复编写，仅调用函数即可</li>
<li>面向对象：以对象为核心，该方法认为程序由一系列对象组成。类是对现实世界的抽象，包括表示静态属性的数据和对数据的操作，对象是类的实例化。对象间通过消息传递相互通信，来模拟现实世界中不同实体间的联系。在面向对象的程序设计中，对象是组成程序的基本模块。</li>
</ul>
<h3 id="了解GC吗？说说"><a href="#了解GC吗？说说" class="headerlink" title="了解GC吗？说说"></a>了解GC吗？说说</h3><p>自动垃圾回收机制，简单来说就是寻觅 Java堆中的无用对象，GC是JVM对内存（实际上就是对象）进行管理的方式。</p>
<p>JVM 的区域分为两类 堆、非堆（Perm Gen 永久代 / Java 8 改为 元空间 Metaspace）。</p>
<p>堆区：Eden Space（伊甸园）、Survivor Space(幸存者区 0/1)、Tenured Gen（养老区）</p>
<p>新生代：Eden Space（伊甸园）+ Survivor Space 0 + Survivor Space 1 </p>
<p>养老代：Tenured Gen</p>
<p>a) 一个对象被创建后，就会出现在 Eden 区，每过一段时间 GC 就会过来进行回收，当对象没有引会被回收，有引用就会进入 Survivor Space 幸存者区</p>
<p>b) 在幸存者区里面有两个区域，一个是有对象的区域，另外一个是没对象的区域，有且仅有一个区域有对象。当 GC 到来时，有引用的对象都会转移到另外一个幸存者区，剩下没有引用的全部回收，当达对象到 16 次的没回收时候进入养老区</p>
<p>c) 在养老区内，GC 来的次数比较少，但是当 GC 来的时候，没有引用的对象一样会被清除。</p>
<p>d) 总结：新生代空间比较小，GC 频率高，养老代空间比较大，GC 频率低</p>
<h3 id="项目中kafka怎么使用的"><a href="#项目中kafka怎么使用的" class="headerlink" title="项目中kafka怎么使用的"></a>项目中kafka怎么使用的</h3><p><img src="https://lixiangbetter.github.io/2021/03/26/%E9%9D%A2%E8%AF%95%E9%A2%98%E9%9A%8F%E7%AC%94-21-3-26/%E5%AE%9E%E6%97%B6%E6%95%B0%E4%BB%93%E6%9E%B6%E6%9E%84.png" alt="实时数仓架构"></p>
<p>日志数据和业务数据，采集发送至kafka。再供离线和实时进行消费，进行统计分析。</p>
<p>该过程中：</p>
<p>Kafka的高吞吐能力、缓存机制能有效的解决高峰流量冲击问题。实践表明，在未将kafka引入系统前，当互联网关发送的数据量较大时，往往会挂起关系数据库，数据常常丢失。在引入kafka后，更新程序能够结合能力自主处理消息，不会引起数据丢失，关系型数据库的压力波动不会发生过于显著的变化，不会出现数据库挂起锁死现象。</p>
<p>依靠kafka的订阅分发机制，实现了一次发布，各分支依据需求自主订阅的功能。避免了各分支机构直接向数据中心请求数据，或者数据中心依次批量向分支机构传输数据以致实时性不足的情况。kafka提高了实时性，减轻了数据中心的压力，提高了效率。</p>
<h3 id="双层Flume的好处："><a href="#双层Flume的好处：" class="headerlink" title="双层Flume的好处："></a>双层Flume的好处：</h3><p>解耦，hdfs或者kafka需要升级时，第二层flume可以进行缓冲，不会影响第一层。<br>安全，hdfs或者kafka直接暴露给第一层不安全（第一层很多flume来自其他部门，第二层在本地）。<br>利于业务的分组管理，将第一组的繁杂业务在第二层可以进行分组。<br>小文件的数量会大大减少。<br>外部某个类型的业务日志数据节点需要扩容，直接在L1层将数据流指向数据平台内部与之相对应的L2层Flume Agent节点组即可。</p>
<h3 id="了解nginx"><a href="#了解nginx" class="headerlink" title="了解nginx"></a>了解nginx</h3><p>Nginx 是一款自由的、开源的、高性能的 HTTP 服务器和反向代理服务器；同时也是一个 IMAP、POP3、SMTP 代理服务器。</p>
<p>Nginx 可以作为一个 HTTP 服务器进行网站的发布处理，另外 Nginx 可以作为反向代理进行负载均衡的实现。</p>
<p>由于以下这几点，所以，Nginx 火了：</p>
<ul>
<li>Nginx 使用基于事件驱动架构，使得其可以支持数以百万级别的 TCP 连接。</li>
<li>高度的模块化和自由软件许可证使得第三方模块层出不穷（这是个开源的时代啊）。</li>
<li>Nginx 是一个跨平台服务器，可以运行在 Linux、Windows、FreeBSD、Solaris、AIX、Mac OS 等操作系统上。</li>
</ul>
<p><strong>nginx事件驱动架构</strong></p>
<p>由一些事件发生源来产生事件，由一个或多个事件收集器来收集、分发事件，然后许多事件处理器会注册自己感兴趣的事件，同时会消费这些事件。对于Nginx Web服务器而言，一般会由网卡、磁盘产生事件，事件模块将负责事件的收集、分发操作，而所有的模块都可能是事件消费者，它们首先需要向事件模块注册感兴趣的事件类型，在有事件产生时，事件模块会把事件分发到相应的模块中进行处理。</p>
<p>传统Web服务器，采用的事件驱动往往局限在TCP连接建立、关闭事件上，一个连接建立后，在其关闭之前所有的操作都不再是事件驱动，这时就会退化成按序执行每个操作的批处理模式，这样每个请求在连接建立后都将始终占用着系统资源，直到连接关闭才会释放资源。如果整个事件消费者进程只是等待某个条件而已，则会造成服务器资源的极大浪费，影响了系统可以处理的并发连接数。</p>
<p>Nginx不会使用进程或线程来作为事件消费者，所谓的事件消费者只能是某个模块。只有事件收集、分发器才会占用进程资源，在分发某个事件时调用事件消费者模块使用当前占用的进程资源。</p>
<p>传统的Web服务器与Nginx间的重要差别：前者是每个事件的消费者独占一个进程资源，后者的事件消费者只是被事件分发者进程短期调用而已。Nginx的处理事件的架构设计使得网络性能、用户感知的请求时延都得到了提升。但是这要求每个事件的消费者都不能有阻塞行为，否则会长事件占用事件分发者进程而导致其他事件得不到及时响应。</p>
<p><strong>Nginx事件驱动处理库：多路IO复用</strong></p>
<p><strong>select库</strong></p>
<p>各个版本的linux和windows都支持的基本事件驱动模型库。<br>使用select的一般步骤：</p>
<p>创建所关注事件的描述符集合。对于每个描述符，可以关注其上的读（Read）事件、写（Write）事件、异常发生（Exception）事件，所以要创建三类描述符集合。分别用来收集读事件的描述符、写事件的描述符、异常发生事件的描述符。<br>调用底层提供的select()函数，等待事件发生。<br>轮询所有事件描述符集合中的每一个描述符，检查是否有相应的事件发生，然后处理事件。<br>nginx默认会在编译阶段将select库编译到基本http模块中去，可以使用–without-select_module来选择不编译该库。</p>
<p><strong>poll库</strong><br>仅linux支持，windows不支持。<br>其与select的基本实现方式相同，只不过创建关注的描述符集合时，不分成三个集合，而是一个集合包括所有描述符。<br>也会被默认编译进基本HTTP模块中。</p>
<p><strong>epoll库</strong><br>是poll库的一个变种，仅linux支持，效率比select高很多。</p>
<p>当描述符比较多时，遍历描述符集合、然后查找每个描述符是否有相应事件发生这一过程会效率较低。</p>
<p>epoll选择将描述符列表的管理交给内核复制，一旦有事件发生，内核会将有事件发生的描述符列表通知给进程，这样避免了应用程序轮询整个描述符列表。</p>
<p>epoll会通过相关调用，通知内核创建一个N个描述符的事件列表，然后给这些描述符设置所关注的事件，并把他添加到内核的事件列表中，在编码过程中也可以通过相关调用对事件列表中的描述符进行动态地删除和修改。</p>
<h3 id="数据库的锁了解哪些，说说"><a href="#数据库的锁了解哪些，说说" class="headerlink" title="数据库的锁了解哪些，说说"></a>数据库的锁了解哪些，说说</h3><p>MySQL大致可归纳为以下3种锁：</p>
<p>表级锁：开销小，加锁快；不会出现死锁；锁定粒度大，发生锁冲突的概率最高，并发度最低。<br>行级锁：开销大，加锁慢；会出现死锁；锁定粒度最小，发生锁冲突的概率最低，并发度也最高。<br>页面锁：开销和加锁时间界于表锁和行锁之间；会出现死锁；锁定粒度界于表锁和行锁之间，并发度一般</p>
<p>在使用MyIsam时，我们只可以使用表级锁，而MySQL的表级锁有两种模式：</p>
<p>表共享锁（Table Read Lock）和表独占写锁（Table Write Lock），他们在工作时表现如下：</p>
<ul>
<li>对某一个表的读操作，不会阻塞其他用户对同一表请求，但会阻塞对同一表的写请求；</li>
<li>对MyISAM的写操作，则会阻塞其他用户对同一表的读和写操作；</li>
<li>MyISAM表的读操作和写操作之间，以及写操作之间是串行的。</li>
</ul>
<p>当一个线程获得对一个表的写锁后，只有持有锁的线程可以对表进行更新操作。其他线程的读、写操作都会等待，直到锁被释放为止。</p>
<p>InnoDB实现了以下两种类型的行锁。</p>
<ul>
<li>共享锁（s）：允许一个事务去读一行，阻止其他事务获得相同数据集的排他锁。</li>
<li>排他锁（Ｘ）：允许获取排他锁的事务更新数据，阻止其他事务取得相同的数据集共享读锁和排他写锁。</li>
</ul>
<p>另外，为了允许行锁和表锁共存，实现多粒度锁机制，InnoDB还有两种内部使用的意向锁（Intention Locks），这两种意向锁都是表锁。</p>
<p>意向共享锁（IS）：事务打算给数据行共享锁，事务在给一个数据行加共享锁前必须先取得该表的IS锁。</p>
<p>意向排他锁（IX）：事务打算给数据行加排他锁，事务在给一个数据行加排他锁前必须先取得该表的IX锁。</p>
]]></content>
      <categories>
        <category>bigdata</category>
      </categories>
      <tags>
        <tag>面试题</tag>
      </tags>
  </entry>
  <entry>
    <title>面试题随笔-21/3/25</title>
    <url>/2021/03/24/%E9%9D%A2%E8%AF%95%E9%A2%98%E9%9A%8F%E7%AC%94-21-3-25/</url>
    <content><![CDATA[<h2 id="面试题随笔-21-3-25"><a href="#面试题随笔-21-3-25" class="headerlink" title="面试题随笔-21-3-25"></a>面试题随笔-21-3-25</h2><h3 id="DAGscheduler干了什么活"><a href="#DAGscheduler干了什么活" class="headerlink" title="DAGscheduler干了什么活"></a>DAGscheduler干了什么活</h3><ul>
<li>对每个job划分stage</li>
<li>决定任务运行的首选位置（preferred locations）</li>
<li>面向stage进行任务调度</li>
<li>跟踪stage的输出</li>
<li>需要时重新提交stage</li>
</ul>
<h3 id="tcp四次挥手介绍一下-为什么会有第二次、第三次、第四次？"><a href="#tcp四次挥手介绍一下-为什么会有第二次、第三次、第四次？" class="headerlink" title="tcp四次挥手介绍一下 为什么会有第二次、第三次、第四次？"></a>tcp四次挥手介绍一下 为什么会有第二次、第三次、第四次？</h3><ul>
<li>请画出三次握手和四次挥手的示意图</li>
<li>为什么连接的时候是三次握手？</li>
<li>什么是半连接队列？</li>
<li>ISN(Initial Sequence Number)是固定的吗？</li>
<li>三次握手过程中可以携带数据吗？</li>
<li>如果第三次握手丢失了，客户端服务端会如何处理？</li>
<li>SYN攻击是什么？</li>
<li>挥手为什么需要四次？</li>
<li>四次挥手释放连接时，等待2MSL的意义?</li>
</ul>
<h3 id="三次握手"><a href="#三次握手" class="headerlink" title="三次握手"></a>三次握手</h3><p>三次握手（Three-way Handshake）其实就是指建立一个TCP连接时，需要客户端和服务器总共发送3个包。进行三次握手的主要作用就是为了确认双方的接收能力和发送能力是否正常、指定自己的初始化序列号为后面的可靠性传送做准备。实质上其实就是连接服务器指定端口，建立TCP连接，并同步连接双方的序列号和确认号，交换TCP窗口大小信息。</p>
<p>刚开始客户端处于 Closed 的状态，服务端处于 Listen 状态。<br>进行三次握手：</p>
<p>第一次握手：客户端给服务端发一个 SYN 报文，并指明客户端的初始化序列号 ISN。此时客户端处于 SYN_SENT 状态。</p>
<p>首部的同步位SYN=1，初始序号seq=x，SYN=1的报文段不能携带数据，但要消耗掉一个序号。</p>
<p>第二次握手：服务器收到客户端的 SYN 报文之后，会以自己的 SYN 报文作为应答，并且也是指定了自己的初始化序列号 ISN(s)。同时会把客户端的 ISN + 1 作为ACK 的值，表示自己已经收到了客户端的 SYN，此时服务器处于 SYN_RCVD 的状态。</p>
<p>在确认报文段中SYN=1，ACK=1，确认号ack=x+1，初始序号seq=y。</p>
<p>第三次握手：客户端收到 SYN 报文之后，会发送一个 ACK 报文，当然，也是一样把服务器的 ISN + 1 作为 ACK 的值，表示已经收到了服务端的 SYN 报文，此时客户端处于 ESTABLISHED 状态。服务器收到 ACK 报文之后，也处于 ESTABLISHED 状态，此时，双方已建立起了连接。</p>
<p>确认报文段ACK=1，确认号ack=y+1，序号seq=x+1（初始为seq=x，第二个报文段所以要+1），ACK报文段可以携带数据，不携带数据则不消耗序号。</p>
<p>发送第一个SYN的一端将执行主动打开（active open），接收这个SYN并发回下一个SYN的另一端执行被动打开（passive open）。</p>
<p>在socket编程中，客户端执行connect()时，将触发三次握手。</p>
<h3 id="为什么需要三次握手，两次不行吗？"><a href="#为什么需要三次握手，两次不行吗？" class="headerlink" title="为什么需要三次握手，两次不行吗？"></a>为什么需要三次握手，两次不行吗？</h3><p>弄清这个问题，我们需要先弄明白三次握手的目的是什么，能不能只用两次握手来达到同样的目的。</p>
<p>第一次握手：客户端发送网络包，服务端收到了。<br>这样服务端就能得出结论：客户端的发送能力、服务端的接收能力是正常的。<br>第二次握手：服务端发包，客户端收到了。<br>这样客户端就能得出结论：服务端的接收、发送能力，客户端的接收、发送能力是正常的。不过此时服务器并不能确认客户端的接收能力是否正常。<br>第三次握手：客户端发包，服务端收到了。<br>这样服务端就能得出结论：客户端的接收、发送能力正常，服务器自己的发送、接收能力也正常。<br>因此，需要三次握手才能确认双方的接收与发送能力是否正常。</p>
<h3 id="什么是半连接队列？"><a href="#什么是半连接队列？" class="headerlink" title="什么是半连接队列？"></a>什么是半连接队列？</h3><p>服务器第一次收到客户端的 SYN 之后，就会处于 SYN_RCVD 状态，此时双方还没有完全建立其连接，服务器会把此种状态下请求连接放在一个队列里，我们把这种队列称之为半连接队列。</p>
<p>当然还有一个全连接队列，就是已经完成三次握手，建立起连接的就会放在全连接队列中。如果队列满了就有可能会出现丢包现象。</p>
<p>这里在补充一点关于SYN-ACK 重传次数的问题：<br>服务器发送完SYN-ACK包，如果未收到客户确认包，服务器进行首次重传，等待一段时间仍未收到客户确认包，进行第二次重传。如果重传次数超过系统规定的最大重传次数，系统将该连接信息从半连接队列中删除。<br>注意，每次重传等待的时间不一定相同，一般会是指数增长，例如间隔时间为 1s，2s，4s，8s…</p>
<h3 id="ISN-Initial-Sequence-Number-是固定的吗？"><a href="#ISN-Initial-Sequence-Number-是固定的吗？" class="headerlink" title="ISN(Initial Sequence Number)是固定的吗？"></a>ISN(Initial Sequence Number)是固定的吗？</h3><p>当一端为建立连接而发送它的SYN时，它为连接选择一个初始序号。<strong>ISN随时间而变化</strong>，因此每个连接都将具有不同的ISN。ISN可以看作是一个32比特的计数器，每4ms加1 。这样选择序号的目的在于防止在网络中被延迟的分组在以后又被传送，而导致某个连接的一方对它做错误的解释。</p>
<p>三次握手的其中一个重要功能是客户端和服务端交换 ISN(Initial Sequence Number)，以便让对方知道接下来接收数据的时候如何按序列号组装数据。如果 ISN 是固定的，攻击者很容易猜出后续的确认号，因此 ISN 是动态生成的。</p>
<h3 id="三次握手过程中可以携带数据吗？"><a href="#三次握手过程中可以携带数据吗？" class="headerlink" title="三次握手过程中可以携带数据吗？"></a>三次握手过程中可以携带数据吗？</h3><p>其实第三次握手的时候，是可以携带数据的。但是，第一次、第二次握手不可以携带数据</p>
<p>为什么这样呢?大家可以想一个问题，假如第一次握手可以携带数据的话，如果有人要恶意攻击服务器，那他每次都在第一次握手中的 SYN 报文中放入大量的数据。因为攻击者根本就不理服务器的接收、发送能力是否正常，然后疯狂着重复发 SYN 报文的话，这会让服务器花费很多时间、内存空间来接收这些报文。</p>
<p>也就是说，第一次握手不可以放数据，其中一个简单的原因就是会让服务器更加容易受到攻击了。而对于第三次的话，此时客户端已经处于 ESTABLISHED 状态。对于客户端来说，他已经建立起连接了，并且也已经知道服务器的接收、发送能力是正常的了，所以能携带数据也没啥毛病。</p>
<h3 id="SYN攻击是什么？"><a href="#SYN攻击是什么？" class="headerlink" title="SYN攻击是什么？"></a>SYN攻击是什么？</h3><p>服务器端的资源分配是在二次握手时分配的，而客户端的资源是在完成三次握手时分配的，所以服务器容易受到SYN洪泛攻击。SYN攻击就是Client在短时间内伪造大量不存在的IP地址，并向Server不断地发送SYN包，Server则回复确认包，并等待Client确认，由于源地址不存在，因此Server需要不断重发直至超时，这些伪造的SYN包将长时间占用未连接队列，导致正常的SYN请求因为队列满而被丢弃，从而引起网络拥塞甚至系统瘫痪。SYN 攻击是一种典型的 DoS/DDoS 攻击。</p>
<p>检测 SYN 攻击非常的方便，当你在服务器上看到大量的半连接状态时，特别是源IP地址是随机的，基本上可以断定这是一次SYN攻击。在 Linux/Unix 上可以使用系统自带的 netstat 命令来检测 SYN 攻击。</p>
<h3 id="四次挥手"><a href="#四次挥手" class="headerlink" title="四次挥手"></a>四次挥手</h3><p>建立一个连接需要三次握手，而终止一个连接要经过四次挥手（也有将四次挥手叫做四次握手的）。这由TCP的半关闭（half-close）造成的。所谓的半关闭，其实就是TCP提供了连接的一端在结束它的发送后还能接收来自另一端数据的能力。</p>
<p>TCP 连接的拆除需要发送四个包，因此称为四次挥手(Four-way handshake)，客户端或服务端均可主动发起挥手动作。</p>
<p>刚开始双方都处于ESTABLISHED 状态，假如是客户端先发起关闭请求。四次挥手的过程如下：</p>
<p>第一次挥手：客户端发送一个 FIN 报文，报文中会指定一个序列号。此时客户端处于 FIN_WAIT1 状态。<br>即发出连接释放报文段（FIN=1，序号seq=u），并停止再发送数据，主动关闭TCP连接，进入FIN_WAIT1（终止等待1）状态，等待服务端的确认。<br>第二次挥手：服务端收到 FIN 之后，会发送 ACK 报文，且把客户端的序列号值 +1 作为 ACK 报文的序列号值，表明已经收到客户端的报文了，此时服务端处于 CLOSE_WAIT 状态。<br>即服务端收到连接释放报文段后即发出确认报文段（ACK=1，确认号ack=u+1，序号seq=v），服务端进入CLOSE_WAIT（关闭等待）状态，此时的TCP处于半关闭状态，客户端到服务端的连接释放。客户端收到服务端的确认后，进入FIN_WAIT2（终止等待2）状态，等待服务端发出的连接释放报文段。<br>第三次挥手：如果服务端也想断开连接了，和客户端的第一次挥手一样，发给 FIN 报文，且指定一个序列号。此时服务端处于 LAST_ACK 的状态。<br>即服务端没有要向客户端发出的数据，服务端发出连接释放报文段（FIN=1，ACK=1，序号seq=w，确认号ack=u+1），服务端进入LAST_ACK（最后确认）状态，等待客户端的确认。<br>第四次挥手：客户端收到 FIN 之后，一样发送一个 ACK 报文作为应答，且把服务端的序列号值 +1 作为自己 ACK 报文的序列号值，此时客户端处于 TIME_WAIT 状态。需要过一阵子以确保服务端收到自己的 ACK 报文之后才会进入 CLOSED 状态，服务端收到 ACK 报文之后，就处于关闭连接了，处于 CLOSED 状态。<br>即客户端收到服务端的连接释放报文段后，对此发出确认报文段（ACK=1，seq=u+1，ack=w+1），客户端进入TIME_WAIT（时间等待）状态。此时TCP未释放掉，需要经过时间等待计时器设置的时间2MSL后，客户端才进入CLOSED状态。<br>收到一个FIN只意味着在这一方向上没有数据流动。客户端执行主动关闭并进入TIME_WAIT是正常的，服务端通常执行被动关闭，不会进入TIME_WAIT状态。</p>
<p>在socket编程中，任何一方执行close()操作即可产生挥手操作。</p>
<h3 id="挥手为什么需要四次？"><a href="#挥手为什么需要四次？" class="headerlink" title="挥手为什么需要四次？"></a>挥手为什么需要四次？</h3><p>因为当服务端收到客户端的SYN连接请求报文后，可以直接发送SYN+ACK报文。其中ACK报文是用来应答的，SYN报文是用来同步的。但是关闭连接时，当服务端收到FIN报文时，很可能并不会立即关闭SOCKET，所以只能先回复一个ACK报文，告诉客户端，“你发的FIN报文我收到了”。只有等到我服务端所有的报文都发送完了，我才能发送FIN报文，因此不能一起发送。故需要四次挥手。</p>
<h3 id="四次挥手释放连接时，等待2MSL的意义"><a href="#四次挥手释放连接时，等待2MSL的意义" class="headerlink" title="四次挥手释放连接时，等待2MSL的意义?"></a>四次挥手释放连接时，等待2MSL的意义?</h3><p>MSL是Maximum Segment Lifetime的英文缩写，可译为“最长报文段寿命”，它是任何报文在网络上存在的最长时间，超过这个时间报文将被丢弃。</p>
<p>为了保证客户端发送的最后一个ACK报文段能够到达服务器。因为这个ACK有可能丢失，从而导致处在LAST-ACK状态的服务器收不到对FIN-ACK的确认报文。服务器会超时重传这个FIN-ACK，接着客户端再重传一次确认，重新启动时间等待计时器。最后客户端和服务器都能正常的关闭。假设客户端不等待2MSL，而是在发送完ACK之后直接释放关闭，一但这个ACK丢失的话，服务器就无法正常的进入关闭连接状态。</p>
<h3 id="为什么TIME-WAIT状态需要经过2MSL才能返回到CLOSE状态？"><a href="#为什么TIME-WAIT状态需要经过2MSL才能返回到CLOSE状态？" class="headerlink" title="为什么TIME_WAIT状态需要经过2MSL才能返回到CLOSE状态？"></a>为什么TIME_WAIT状态需要经过2MSL才能返回到CLOSE状态？</h3><p>理论上，四个报文都发送完毕，就可以直接进入CLOSE状态了，但是可能网络是不可靠的，有可能最后一个ACK丢失。所以<strong>TIME_WAIT状态就是用来重发可能丢失的ACK报文</strong>。</p>
<h3 id="为什么mysql底层用b-tree"><a href="#为什么mysql底层用b-tree" class="headerlink" title="为什么mysql底层用b+tree"></a>为什么mysql底层用b+tree</h3><p>1.文件很大，不可能全部存储在内存中，故要存储到磁盘上</p>
<p>2.索引的结构组织要尽量减少查找过程中磁盘I/O的存取次数（为什么使用B-/+Tree，还跟磁盘存取原理有关。）</p>
<p>3.局部性原理与磁盘预读，预读的长度一般为页（page）的整倍数，（在许多操作系统中，页得大小通常为4k）</p>
<p>4.数据库系统巧妙利用了磁盘预读原理，将一个节点的大小设为等于一个页，这样每个节点只需要一次I/O就可以完全载入，(由于节点中有两个数组，所以地址连续)。而红黑树这种结构，h明显要深的多。由于逻辑上很近的节点（父子）物理上可能很远，无法利用局部性</p>
<p>二叉查找树进化品种的红黑树等数据结构也可以用来实现索引，但是文件系统及数据库系统普遍采用B-/+Tree作为索引结构。</p>
<p>一般来说，索引本身也很大，不可能全部存储在内存中，因此索引往往以索引文件的形式存储的磁盘上。这样的话，索引查找过程中就要产生磁盘I/O消耗，相对于内存存取，I/O存取的消耗要高几个数量级，所以评价一个数据结构作为索引的优劣最重要的指标就是在查找过程中磁盘I/O操作次数的渐进复杂度。换句话说，索引的结构组织要尽量减少查找过程中磁盘I/O的存取次数。为什么使用B-/+Tree，还跟磁盘存取原理有关。</p>
<p><strong>局部性原理与磁盘预读</strong></p>
<p>由于存储介质的特性，磁盘本身存取就比主存慢很多，再加上机械运动耗费，磁盘的存取速度往往是主存的几百分分之一，因此为了提高效率，要尽量减少磁盘I/O。为了达到这个目的，磁盘往往不是严格按需读取，而是每次都会预读，即使只需要一个字节，磁盘也会从这个位置开始，顺序向后读取一定长度的数据放入内存。这样做的理论依据是计算机科学中著名的局部性原理：</p>
<p>当一个数据被用到时，其附近的数据也通常会马上被使用。</p>
<p>程序运行期间所需要的数据通常比较集中。</p>
<p>由于磁盘顺序读取的效率很高（不需要寻道时间，只需很少的旋转时间），因此对于具有局部性的程序来说，预读可以提高I/O效率。</p>
<p>预读的长度一般为页（page）的整倍数。页是计算机管理存储器的逻辑块，硬件及操作系统往往将主存和磁盘存储区分割为连续的大小相等的块，每个存储块称为一页（在许多操作系统中，页得大小通常为4k），主存和磁盘以页为单位交换数据。当程序要读取的数据不在主存中时，会触发一个缺页异常，此时系统会向磁盘发出读盘信号，磁盘会找到数据的起始位置并向后连续读取一页或几页载入内存中，然后异常返回，程序继续运行。</p>
<p>我们上面分析B-/+Tree检索一次最多需要访问节点：</p>
<p><strong>h =</strong></p>
<p><img src="https://lixiangbetter.github.io/2021/03/24/%E9%9D%A2%E8%AF%95%E9%A2%98%E9%9A%8F%E7%AC%94-21-3-25/v2-2d184b41c05347fda86c5fc52ab8b9b3_1440w.jpeg" alt="img"></p>
<p>一个为m阶的B-Tree，设其索引N个key。</p>
<p>数据库系统巧妙利用了磁盘预读原理，将一个节点的大小设为等于一个页，这样每个节点只需要一次I/O就可以完全载入。为了达到这个目的，在实际实现B- Tree还需要使用如下技巧：</p>
<p>每次新建节点时，直接申请一个页的空间，这样就保证一个节点物理上也存储在一个页里，加之计算机存储分配都是按页对齐的，就实现了一个node只需一次I/O。</p>
<p>B-Tree中一次检索最多需要h-1次I/O（根节点常驻内存），渐进复杂度为O（h）=O（logmN）。一般实际应用中，m是非常大的数字，通常超过100，因此h非常小（通常不超过3）。</p>
<p>综上所述，用B-Tree作为索引结构效率是非常高的。</p>
<p>而红黑树这种结构，<strong>h</strong>明显要深的多。由于逻辑上很近的节点（父子）物理上可能很远，无法利用局部性，所以红黑树的I/O渐进复杂度也为O（h），效率明显比B-Tree差很多。</p>
<h3 id="用spark求一下dau"><a href="#用spark求一下dau" class="headerlink" title="用spark求一下dau"></a>用spark求一下dau</h3><figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">substr</span>(dt, <span class="number">1</span>, <span class="number">10</span>) <span class="keyword">as</span> dtt, <span class="keyword">count</span>(<span class="keyword">distinct</span> uid ) <span class="keyword">as</span> DAU <span class="keyword">from</span> SC <span class="keyword">group</span> <span class="keyword">by</span> <span class="keyword">substr</span>(dt, <span class="number">1</span>, <span class="number">10</span>);</span><br></pre></td></tr></table></figure>

<p>类似，group by + count</p>
<h3 id="说一下Hadoop吧"><a href="#说一下Hadoop吧" class="headerlink" title="说一下Hadoop吧"></a>说一下Hadoop吧</h3><h4 id="hadoop是什么？"><a href="#hadoop是什么？" class="headerlink" title="hadoop是什么？"></a>hadoop是什么？</h4><p>(1)Hadoop是一个开源的框架，可编写和运行分布式应用处理大规模数据，是专为离线和大规模数据分析而设计的，并不适合那种对几个记录随机读写的在线事务处理模式。Hadoop=HDFS（文件系统，数据存储技术相关）+ Mapreduce（数据处理），Hadoop的数据来源可以是任何形式，在处理半结构化和非结构化数据上与关系型数据库相比有更好的性能，具有更灵活的处理能力，不管任何数据形式最终会转化为key/value，key/value是基本数据单元。用函数式变成Mapreduce代替SQL，SQL是查询语句，而Mapreduce则是使用脚本和代码，而对于适用于关系型数据库，习惯SQL的Hadoop有开源工具hive代替。</p>
<p>(2)Hadoop就是一个分布式计算的解决方案.</p>
<h4 id="hadoop能做什么？"><a href="#hadoop能做什么？" class="headerlink" title="hadoop能做什么？"></a>hadoop能做什么？</h4><p> hadoop擅长日志分析，facebook就用Hive来进行日志分析，2009年时facebook就有非编程人员的30%的人使用HiveQL进行数据分析；淘宝搜索中的自定义筛选也使用的Hive；利用Pig还可以做高级的数据处理，包括Twitter、LinkedIn 上用于发现您可能认识的人，可以实现类似Amazon.com的协同过滤的推荐效果。淘宝的商品推荐也是！在Yahoo！的40%的Hadoop作业是用pig运行的，包括垃圾邮件的识别和过滤，还有用户特征建模。（2012年8月25新更新，天猫的推荐系统是hive，少量尝试mahout！）</p>
<h4 id="hadoop能为我公司做什么？"><a href="#hadoop能为我公司做什么？" class="headerlink" title="hadoop能为我公司做什么？"></a>hadoop能为我公司做什么？</h4><ul>
<li>零数据基础，零数据平台，一切起点都是0</li>
<li>日志处理</li>
<li>用户细分特征建模</li>
<li>个性化广告推荐</li>
<li>智能仪器推荐</li>
<li>一切以增加企业的商业价值为核心目的、最终目的</li>
</ul>
<h4 id="怎么用hadoop？"><a href="#怎么用hadoop？" class="headerlink" title="怎么用hadoop？"></a>怎么用hadoop？</h4><p>hadoop的应用的在我司还属于研发型项目，拟用日志的分析来走通一次流程，因为此阶段目前来说还不需要数据挖掘的专业人员，在数据分析阶段即可，而系统有数据库工程师，Mapreduce有java开发工程师，而分析由我本人介入，而可视化暂时可由前端JS实现，本来我的调研方案，针对大数据的解决方案是hadoop+R的，但是对于R我们是完全不懂，在公司还没有大量投入人员的情况下，只有日志分析目前看来是最容易出成果的，也是可以通过较少人员能出一定成果的，所以选取了这个方向作为试点。</p>
<h3 id="Spark任务执行流程"><a href="#Spark任务执行流程" class="headerlink" title="Spark任务执行流程"></a>Spark任务执行流程</h3><p>Yarn-client:</p>
<p>1.client向ResouceManager申请启动ApplicationMaster，同时在SparkContext初始化中创建DAGScheduler和TaskScheduler</p>
<p>2.ResouceManager收到请求后，在一台NodeManager中启动第一个Container运行ApplicationMaster</p>
<p>3.Dirver中的SparkContext初始化完成后与ApplicationMaster建立通讯，ApplicationMaster向ResourceManager申请Application的资源</p>
<p>4.一旦ApplicationMaster申请到资源，便与之对应的NodeManager通讯，启动Executor，并把Executor信息反向注册给Dirver</p>
<p>5.Dirver分发task，并监控Executor的运行状态，负责重试失败的task</p>
<p>6.运行完成后，Client的SparkContext向ResourceManager申请注销并关闭自己</p>
<p>Yarn-cluster:</p>
<p>yarn-cluster模式中，当用户向yarn提交应用程序后，yarn将分为两阶段运行该应用程序：</p>
<p>第一个阶段是把Spark的Dirver作为一个ApplicationMaster在yarn中启动；</p>
<p>第二个阶段是ApplicationMaster向ResourceManager申请资源，并启动Executor来运行task，同时监控task整个运行流程并重试失败的task；</p>
<h3 id="了解yarn吗？"><a href="#了解yarn吗？" class="headerlink" title="了解yarn吗？"></a>了解yarn吗？</h3><p>YARN 是hadoop 的集群资源管理器。Yarn 在Hadoop2 中被引入，是为了要改善MapReduce1 的实现，但它具有足够的通用性，同样也支持其他的分布式计算模式。</p>
<p><strong>MapReduce1 的实现机制</strong></p>
<p>MapReduce 包括几个高级组件。主进程JobTracker是所有MapReduce 作业的信息交流中心。每个节点都有一个TaskTracker进程，管理相应节点的任务。TaskTracker 与JobTracker 通信，并受其控制。</p>
<p><strong>yarn 应用运行机制</strong></p>
<p>YARN 的基本思想就是将JobTracker 的两大主要职能：资源管理、作业的调度监控分为两个独立的进程。一个是全局的ResourceManager,另一个是每一个应用对应的ApplicationMaster。</p>
<p>ResourceManager 是一个纯粹的调度器，它根据应用程序的资源请求严格限制系统的可用资源。在保证容量、公平性及服务器等级的前提下，优化集群资源利用率，即让所有的资源都能被充分利用。</p>
<p>ApplicationMaster 负责与ResourceManager 协商资源，并和NodeManager 进行协同工作来执行容器和监控容器的状态。</p>
<p>NodeManager 是YARN 节点上的工作进程，管理集群中独立的计算节点。其职责包括启动应用程序的容器，监控它们的资源使用情况，并且报告给ResourceManager。</p>
<h3 id="了解zookeeper"><a href="#了解zookeeper" class="headerlink" title="了解zookeeper"></a>了解zookeeper</h3><h4 id="zk是什么："><a href="#zk是什么：" class="headerlink" title="zk是什么："></a>zk是什么：</h4><p>1、个人理解zk=文件系统+通知机制。</p>
<p>2、zk是一个分布式的应用程序协调服务，我理解的就是有两台集器A、B，A对一个数据进行了操作，B是如何知道的，这个就需要zk的支持。</p>
<p>3、 分布式应用程序可以基于 ZooKeeper 实现诸如数据发布/订阅、负载均衡、命名服务、分布式协调/通知、集群管理、Master 选举、配置维护，名字服务、分布式同步、分布式锁和分布式队列 等功能。</p>
<h4 id="zk的初始化选举机制："><a href="#zk的初始化选举机制：" class="headerlink" title="zk的初始化选举机制："></a>zk的初始化选举机制：</h4><p>1、首先一般选举里面都有2N+1台集器，如果是三台机器的A、B、C，A和B都会选举自己，每次投票会包括推举的服务器的myid和zxid，使用（myid，zxid）来表示。</p>
<p>2、集群中的服务器都接受到投票时，首先判断投票的有效性，如检查是不是本轮的投票，是否是looking状态的机器。</p>
<p>3、处理投票，比较zxid大小，越大权重越大，如果相同再比较myid。</p>
<p>4、统计投票。</p>
<p>5、改变服务器的状态。</p>
<p>ps：注意：在这个过程中，有个重要的数据结构，electionEpoch即逻辑时钟，用来判断是否在同一轮选举周期中，每次进入新的一轮投票都会自增，还有一个state，表示当前服务器的状态。</p>
<h4 id="zk的运行过程中leader崩溃选举机制："><a href="#zk的运行过程中leader崩溃选举机制：" class="headerlink" title="zk的运行过程中leader崩溃选举机制："></a>zk的运行过程中leader崩溃选举机制：</h4><p>1、状态变更，余下所有的observing服务器都会将自己的服务器状态变成looking状态。</p>
<p>2、每个server会发起投票。</p>
<p>3、接受各个服务器的投票。</p>
<p>4、处理投票。</p>
<p>5、统计投票。</p>
<p>6、改变服务器的状态。</p>
<p>注意：其实崩溃选举机制和初始化差不多，但是值得注意一点是每个机器中的electionEpoch，也就是逻辑时钟，如果有机器宕机的话，这个数值是和正常的机器不一样的，所以需要判断这个值不是正常值的机器投票数据是否是正常的。</p>
<h4 id="zk中的znode节点："><a href="#zk中的znode节点：" class="headerlink" title="zk中的znode节点："></a>zk中的znode节点：</h4><p>1、四种类型：</p>
<p>（1）持久化目录节点。</p>
<p>（2）持久化顺序编号目录节点。</p>
<p>（3）临时目录节点。</p>
<p>（4）临时顺序编号目录节点。</p>
<h4 id="zk中的监控原理："><a href="#zk中的监控原理：" class="headerlink" title="zk中的监控原理："></a>zk中的监控原理：</h4><p>1、zk类似于linux中的目录节点树方式的数据存储，即分层命名空间，zk并不是专门存储数据的，它的作用是主要是维护和监控存储数据的状态变化，通过监控这些数据状态的变化，从而可以达到基于数据的集群管理，zk中的杰点的数据上限时1M。</p>
<p>2、zk中的wathc机制：client端会对某个znode建立一个watcher事件，当该znode发生变化时，这些client会收到zk的通知，然后client可以根据znode变化来做出业务上的改变等。</p>
<p>3、main方法会创建zkClient，在创建zkClient的时候，会创建出listener进程和connect进程。一个是监控进程一个是网络连接进程。当zkClient调用getChildren等方法注册监听器的时候，connect进程向zk注册监听器，注册后的缉监听器位于zk的监听器列表中，监听器列表中记录了zkClient的ip地址，端口号，要监控的目录，一旦目标文件发生了改变，zk就会把这条消息发送给对应的zkClient的listener进程，listener进程接受到后，就会执行process方法。在process方法中针对发生的事件进行处理。</p>
<h4 id="zk的职责："><a href="#zk的职责：" class="headerlink" title="zk的职责："></a>zk的职责：</h4><p>1、命名服务：命名服务是指通过指定的名字来获取资源或者服务的地址，利用zk创建一个全局的路径，即是唯一的路径，这个路径就可以作为一个名字，指向集群中的集群，提供的服务的地址，或者一个远程的对象等等。</p>
<p>2、配置管理（文件系统、通知机制）：程序分布式的部署在不同的机器上，将程序的配置信息放在zk的znode下，当有配置发生改变时，也就是znode发生变化时，可以通过改变zk中某个目录节点的内容，利用watcher通知给各个客户端，从而更改配置。</p>
<p>3、集群管理：是否有机器退出和加入、选举master。对于机器的退出，所有机器约定在父目录下创建临时目录，对于新机器的加入，所有机器创建临时顺序编号目录节点。</p>
<p>4、分布式锁：分为两类，一个是保持独占：客户端需要的时候，就去通过createznode的方式实现，所有客户端都去创建/distribute_lock节点，用完就删除节点就行了。一个是控制时序，/distribute_lock已经预先存在，所有客户端在它下面创建临时顺序编号目录节点。主要流程是：客户端在获取分布式锁的时候在locker节点下创建临时顺序节点，释放锁的时候就删除，客户端首先调用createZnode放在在locker创建临时顺序节点，然后调用getChildren来获取locker下面的所有子节点，此时不用设置watch，客户端获取了所有子节点的path之后，反正最后要找到最小序号的那个节点，调用exist方法，同时对其注册事件监听器</p>
<p>5、队列管理：两种类型的队列，一种是同步队列，一个是按照FIFO方式进行入队和出队，第二种保证了队列消息的不会丢失，因为会在特定的目录下创建一个persistent_sequential节点，创建成功时watcher通知等待的队列，队列删除序列号最小的节点，此场景下，zk中的znode用于消息存储，znode存储的数据就是消息队列中的消息内容，sequential序列号就是消息的编号，按序列取出即可。</p>
<h4 id="zk中的数据复制："><a href="#zk中的数据复制：" class="headerlink" title="zk中的数据复制："></a>zk中的数据复制：</h4><p>1、作用：（1）容错。（2）提高系统的扩展功能。（3）提高性能。</p>
<p>2、数据复制分为两种：</p>
<p>（1）、写主，对数据的修改提交给指定节点，读没有限制，可以在任意的节点读取数据。</p>
<p>（2）、写任意：对数据的修改提交给任意的节点，读也是任意节点。</p>
<h4 id="zk的工作原理："><a href="#zk的工作原理：" class="headerlink" title="zk的工作原理："></a>zk的工作原理：</h4><p>1、核心就是原子广播。在ZooKeeper中所有的事务请求都由一个主服务器也就是Leader来处理，其他服务器为Follower，Leader将客户端的事务请求转换为事务Proposal，并且将Proposal分发给集群中其他所有的Follower，然后Leader等待Follwer反馈，当有 过半数（&gt;=N/2+1） 的Follower反馈信息后，Leader将再次向集群内Follower广播Commit信息，Commit为将之前的Proposal提交。</p>
<p>2、保证数据的一致性：</p>
<p>（1）、顺序的一致性。</p>
<p>（2）、原子性。</p>
<p>（3）、单一的系统映像。</p>
<p>（4）、持久性。</p>
<p>3、ZAB协议的两种实现方式：</p>
<p>（1）、恢复模式：当服务启动或在leader崩溃后，ZAB就进入了恢复模式，在leader选举出来之前，且大多数server完成和leader的状态同步以后，恢复模式就结束了。在这个过程中，要保证被leader提交的proposal最终被所有的follower执行，确保那些只在leader提出的proposal被丢弃。</p>
<p>（2）、广播模式：即leader提出一个决议，由follower进行投票，leader对投票结果进行计算决定是否通过决议，如果通过执行该决议，否则什么都不做。</p>
<h4 id="zk的watch机制："><a href="#zk的watch机制：" class="headerlink" title="zk的watch机制："></a>zk的watch机制：</h4><p>1、一次性触发数据改变时，一个watch event会被发送到client，但是client只会接收到一次这样的消息。</p>
<p>2、watch event异步发送到watcher的通知事件从server发送到client是异步的，但是可能由于网络延迟原因，所有导致客户端在不同时刻监听到事件，zk只保证最终的一致性，而无法保证强一致性。</p>
<p>3、数据监视由getData和exists方法，getchildren设置了子节点监视。</p>
<p>4、注册watcher：getData、exists、getChildren。</p>
<p>5、触发wathcer：create、delete、setData。</p>
<h3 id="hive的作用"><a href="#hive的作用" class="headerlink" title="hive的作用"></a>hive的作用</h3><h4 id="Hive是什么"><a href="#Hive是什么" class="headerlink" title="Hive是什么"></a>Hive是什么</h4><p>1）Hive 是建立在Hadoop (HDFS/MR)上的用于管理和查询结果化/非结构化的数据仓库；<br>2）一种可以存储、查询和分析存储在Hadoop 中的大规模数据的机制；<br>3）Hive 定义了简单的类SQL 查询语言，称为HQL，它允许熟悉SQL 的用户查询数据；<br>4）允许用Java开发自定义的函数UDF来处理内置无法完成的复杂的分析工作；<br>5）Hive没有专门的数据格式（分隔符等可以自己灵活的设定）；<br>ETL的流程（Extraction-Transformate-Loading）：将关系型数据库的数据抽取到HDFS上，hive作为数据仓库，经过hive的计算分析后，将结果再导入到关系型数据库的过程。</p>
<h4 id="Hive是构建在Hadoop之上的数据仓库"><a href="#Hive是构建在Hadoop之上的数据仓库" class="headerlink" title="Hive是构建在Hadoop之上的数据仓库"></a>Hive是构建在Hadoop之上的数据仓库</h4><p>1）使用HQL作为查询接口；<br>2）使用HDFS作为存储；<br>3）使用MapReduce作为计算；</p>
<h5 id="Hive应用场景"><a href="#Hive应用场景" class="headerlink" title="Hive应用场景"></a>Hive应用场景</h5><p>数据源：<br>1）文件数据，如中国移动某设备每天产生大量固定格式的文件；<br>2）数据库</p>
<p>以上两种不同的数据源有个共同点：要使用hive，那么必须要将数据放到hive中；通常采用如下两种方式：<br>1）文件数据：load到hive<br>2）数据库: sqoop到hive<br>数据的离线处理；<br>hive的执行延迟比较高，因为hive常用于数据分析的，对实时性要求不高；<br>hive优势在于处理大数据，对于处理小数据没有优势，因为hive的执行延迟比较高。<br>处理数据存放在hive表中，那么前台系统怎么去访问hive的数据呢？<br>先将hive的处理结果数据转移到关系型数据库中才可以，sqoop就是执行导入导出的操作</p>
<h3 id="数据库你都了解哪些内容"><a href="#数据库你都了解哪些内容" class="headerlink" title="数据库你都了解哪些内容"></a>数据库你都了解哪些内容</h3><h4 id="什么是存储过程？有哪些优缺点？"><a href="#什么是存储过程？有哪些优缺点？" class="headerlink" title="什么是存储过程？有哪些优缺点？"></a>什么是存储过程？有哪些优缺点？</h4><p>存储过程是一些预编译的SQL语句。</p>
<p>更加直白的理解：存储过程可以说是一个记录集，它是由一些T-SQL语句组成的代码块，这些T-SQL语句代码像一个方法一样实现一些功能（对单表或多表的增删改查），然后再给这个代码块取一个名字，在用到这个功能的时候调用他就行了。</p>
<ul>
<li>存储过程是一个预编译的代码块，执行效率比较高</li>
<li>一个存储过程替代大量T_SQL语句 ，可以降低网络通信量，提高通信速率</li>
<li>可以一定程度上确保数据安全</li>
</ul>
<h4 id="索引是什么？有什么作用以及优缺点？"><a href="#索引是什么？有什么作用以及优缺点？" class="headerlink" title="索引是什么？有什么作用以及优缺点？"></a>索引是什么？有什么作用以及优缺点？</h4><p>索引是对数据库表中一或多个列的值进行排序的结构，是帮助MySQL高效获取数据的数据结构</p>
<p>你也可以这样理解：索引就是加快检索表中数据的方法。数据库的索引类似于书籍的索引。在书籍中，索引允许用户不必翻阅完整个书就能迅速地找到所需要的信息。在数据库中，索引也允许数据库程序迅速地找到表中的数据，而不必扫描整个数据库。</p>
<p>MySQL数据库几个基本的索引类型：普通索引、唯一索引、主键索引、全文索引</p>
<p>索引加快数据库的检索速度<br>索引降低了插入、删除、修改等维护任务的速度<br>唯一索引可以确保每一行数据的唯一性<br>通过使用索引，可以在查询的过程中使用优化隐藏器，提高系统的性能<br>索引需要占物理和数据空间</p>
<h4 id="什么是事务？"><a href="#什么是事务？" class="headerlink" title="什么是事务？"></a>什么是事务？</h4><p>事务（Transaction）是并发控制的基本单位。所谓的事务，它是一个操作序列，这些操作要么都执行，要么都不执行，它是一个不可分割的工作单位。事务是数据库维护数据一致性的单位，在每个事务结束时，都能保持数据一致性。</p>
<h4 id="数据库的乐观锁和悲观锁是什么？"><a href="#数据库的乐观锁和悲观锁是什么？" class="headerlink" title="数据库的乐观锁和悲观锁是什么？"></a>数据库的乐观锁和悲观锁是什么？</h4><p>数据库管理系统（DBMS）中的并发控制的任务是确保在多个事务同时存取数据库中同一数据时不破坏事务的隔离性和统一性以及数据库的统一性。</p>
<p>乐观并发控制(乐观锁)和悲观并发控制（悲观锁）是并发控制主要采用的技术手段。</p>
<ul>
<li>悲观锁：假定会发生并发冲突，屏蔽一切可能违反数据完整性的操作</li>
<li>乐观锁：假设不会发生并发冲突，只在提交操作时检查是否违反数据完整性。</li>
</ul>
<h4 id="使用索引查询一定能提高查询的性能吗？为什么"><a href="#使用索引查询一定能提高查询的性能吗？为什么" class="headerlink" title="使用索引查询一定能提高查询的性能吗？为什么"></a>使用索引查询一定能提高查询的性能吗？为什么</h4><p>通常,通过索引查询数据比全表扫描要快.但是我们也必须注意到它的代价.</p>
<p>索引需要空间来存储,也需要定期维护, 每当有记录在表中增减或索引列被修改时,索引本身也会被修改. 这意味着每条记录的INSERT,DELETE,UPDATE将为此多付出4,5 次的磁盘I/O. 因为索引需要额外的存储空间和处理,那些不必要的索引反而会使查询反应时间变慢.使用索引查询不一定能提高查询性能,索引范围查询(INDEX RANGE SCAN)适用于两种情况:</p>
<ul>
<li>基于一个范围的检索,一般查询返回结果集小于表中记录数的30%</li>
<li>基于非唯一性索引的检索</li>
</ul>
<h4 id="简单说一说drop、delete与truncate的区别"><a href="#简单说一说drop、delete与truncate的区别" class="headerlink" title="简单说一说drop、delete与truncate的区别"></a>简单说一说drop、delete与truncate的区别</h4><p>SQL中的drop、delete、truncate都表示删除，但是三者有一些差别</p>
<p>delete和truncate只删除表的数据不删除表的结构<br>速度,一般来说: drop&gt; truncate &gt;delete<br>delete语句是dml,这个操作会放到rollback segement中,事务提交之后才生效;<br>如果有相应的trigger,执行的时候将被触发. truncate,drop是ddl, 操作立即生效,原数据不放到rollback segment中,不能回滚. 操作不触发trigger.</p>
<h4 id="drop、delete与truncate分别在什么场景之下使用？"><a href="#drop、delete与truncate分别在什么场景之下使用？" class="headerlink" title="drop、delete与truncate分别在什么场景之下使用？"></a>drop、delete与truncate分别在什么场景之下使用？</h4><p>不再需要一张表的时候，用drop<br>想删除部分数据行时候，用delete，并且带上where子句<br>保留表而删除所有数据的时候用truncate</p>
<h4 id="超键、候选键、主键、外键分别是什么？"><a href="#超键、候选键、主键、外键分别是什么？" class="headerlink" title="超键、候选键、主键、外键分别是什么？"></a>超键、候选键、主键、外键分别是什么？</h4><p>超键：在关系中能唯一标识元组的属性集称为关系模式的超键。一个属性可以为作为一个超键，多个属性组合在一起也可以作为一个超键。超键包含候选键和主键。</p>
<p>候选键：是最小超键，即没有冗余元素的超键。</p>
<p>主键：数据库表中对储存数据对象予以唯一和完整标识的数据列或属性的组合。一个数据列只能有一个主键，且主键的取值不能缺失，即不能为空值（Null）。</p>
<p>外键：在一个表中存在的另一个表的主键称此表的外键。</p>
<h4 id="什么是视图？以及视图的使用场景有哪些？"><a href="#什么是视图？以及视图的使用场景有哪些？" class="headerlink" title="什么是视图？以及视图的使用场景有哪些？"></a>什么是视图？以及视图的使用场景有哪些？</h4><p>视图是一种虚拟的表，具有和物理表相同的功能。可以对视图进行增，改，查，操作，试图通常是有一个表或者多个表的行或列的子集。对视图的修改不影响基本表。它使得我们获取数据更容易，相比多表查询。</p>
<ul>
<li>只暴露部分字段给访问者，所以就建一个虚表，就是视图。</li>
<li>查询的数据来源于不同的表，而查询者希望以统一的方式查询，这样也可以建立一个视图，把多个表查询结果联合起来，查询者只需要直接从视图中获取数据，不必考虑数据来源于不同表所带来的差异</li>
</ul>
<p>说一说三个范式。</p>
<p>第一范式（1NF）：数据库表中的字段都是单一属性的，不可再分。这个单一属性由基本类型构成，包括整型、实数、字符型、逻辑型、日期型等。</p>
<p>第二范式（2NF）：数据库表中不存在非关键字段对任一候选关键字段的部分函数依赖（部分函数依赖指的是存在组合关键字中的某些字段决定非关键字段的情况），也即所有非关键字段都完全依赖于任意一组候选关键字。</p>
<p>第三范式（3NF）：在第二范式的基础上，数据表中如果不存在非关键字段对任一候选关键字段的传递函数依赖则符合第三范式。所谓传递函数依赖，指的是如 果存在”A → B → C”的决定关系，则C传递函数依赖于A。因此，满足第三范式的数据库表应该不存在如下依赖关系： 关键字段 → 非关键字段 x → 非关键字段y</p>
<h3 id="1G文件，每行是一个词，内存1M，求词频最大的前100个词"><a href="#1G文件，每行是一个词，内存1M，求词频最大的前100个词" class="headerlink" title="1G文件，每行是一个词，内存1M，求词频最大的前100个词"></a>1G文件，每行是一个词，内存1M，求词频最大的前100个词</h3><ul>
<li><p>分而治之 + hash统计 + 堆/快速排序</p>
<p>分而治之/hash映射：顺序读文件中，对于每个词x，取hash(x)%5000，然后按照该值存到5000个小文件（记为x0,x1,…x4999）中。这样每个文件大概是200k左右。如果其中的有的文件超过了1M大小，还可以按照类似的方法继续往下分，直到分解得到的小文件的大小都不超过1M。<br>hash_map统计：对每个小文件，采用trie树/hash_map等统计每个文件中出现的词以及相应的频率。<br>堆/归并排序：取出出现频率最大的100个词（可以用含100个结点的最小堆）后，再把100个词及相应的频率存入文件，这样又得到了5000个文件。最后就是把这5000个文件进行归并（类似于归并排序）的过程了。</p>
</li>
</ul>
<h3 id="求前K大的数"><a href="#求前K大的数" class="headerlink" title="求前K大的数"></a>求前K大的数</h3><p>解题思路：一般思路就是将N个数排序后，取前k个数就ok。但是如果N个数是几十亿个数，加载不到内存怎么办？这时候就需要利用堆来解决这个问题</p>
<p>具体的思路是：先建一个k个数的小堆，然后从k+1个数往后的值与堆顶元素比较，若此数比堆顶元素大，就将堆顶元素用这个数替换，然后重新调整堆，以此向后重复上述过程，直到将N个数比较完成，那么此时组成这个堆的k个元素就是前k个大的数。</p>
<h3 id="抽象类和接口的区别"><a href="#抽象类和接口的区别" class="headerlink" title="抽象类和接口的区别"></a>抽象类和接口的区别</h3><p>1、抽象类可以提供成员方法的实现细节，而接口中只能存在public abstract 方法；<br>2、抽象类中的成员变量可以是各种类型的，而接口中的成员变量只能是public static final类型的；<br>3、一个类只能继承一个抽象类，而一个类却可以实现多个接口。</p>
<h3 id="数据倾斜"><a href="#数据倾斜" class="headerlink" title="数据倾斜"></a>数据倾斜</h3><p>数据倾斜是大数据领域绕不开的拦路虎，当你所需处理的数据量到达了上亿甚至是千亿条的时候，数据倾斜将是横在你面前一道巨大的坎。很可能有几周甚至几月都要头疼于数据倾斜导致的各类诡异的问题。</p>
<p>数据倾斜是指：mapreduce程序执行时，reduce节点大部分执行完毕，但是有一个或者几个reduce节点运行很慢，导致整个程序的处理时间很长，这是因为某一个key的条数比其他key多很多（有时是百倍或者千倍之多），这条key所在的reduce节点所处理的数据量比其他节点就大很多，从而导致某几个节点迟迟运行不完。Hive的执行是分阶段的，map处理数据量的差异取决于上一个stage的reduce输出，所以如何将数据均匀的分配到各个reduce中，就是解决数据倾斜的根本所在。</p>
<p><img src="https://lixiangbetter.github.io/2021/03/24/%E9%9D%A2%E8%AF%95%E9%A2%98%E9%9A%8F%E7%AC%94-21-3-25/20200720171238287.jpeg" alt></p>
<h4 id="Group-by-倾斜"><a href="#Group-by-倾斜" class="headerlink" title="Group by 倾斜"></a>Group by 倾斜</h4><p>group by造成的倾斜相对来说比较容易解决。hive提供两个参数可以解决：</p>
<p>1.1 hive.map.aggr</p>
<p>一个是hive.map.aggr，默认值已经为true，他的意思是做map aggregation，也就是在mapper里面做聚合。这个方法不同于直接写mapreduce的时候可以实现的combiner，但是却实现了类似combiner的效果。事实上各种基于mr的框架如pig，cascading等等用的都是map aggregation（或者叫partial aggregation）而非combiner的策略，也就是在mapper里面直接做聚合操作而不是输出到buffer给combiner做聚合。对于map aggregation，hive还会做检查，如果aggregation的效果不好，那么hive会自动放弃map aggregation。判断效果的依据就是经过一小批数据的处理之后，检查聚合后的数据量是否减小到一定的比例，默认是0.5，由hive.map.aggr.hash.min.reduction这个参数控制。所以如果确认数据里面确实有个别取值倾斜，但是大部分值是比较稀疏的，这个时候可以把比例强制设为1，避免极端情况下map aggr失效。hive.map.aggr还有一些相关参数，比如map aggr的内存占用等，具体可以参考<a href="http://dev.bizo.com/2013/02/map-side-aggregations-in-apache-hive.html" target="_blank" rel="noopener">这篇文章</a>。</p>
<p>1.2 hive.groupby.skewindata</p>
<p>另一个参数是hive.groupby.skewindata。这个参数的意思是做reduce操作的时候，拿到的key并不是所有相同值给同一个reduce，而是随机分发，然后reduce做聚合，做完之后再做一轮MR，拿前面聚合过的数据再算结果。所以这个参数其实跟hive.map.aggr做的是类似的事情，只是拿到reduce端来做，而且要额外启动一轮job，所以其实不怎么推荐用，效果不明显。</p>
<p>1.3 count distinct 改写</p>
<p>另外需要注意的是count distinct操作往往需要改写SQL，可以按照下面这么做：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">/*改写前*/</span></span><br><span class="line"><span class="keyword">select</span> a, <span class="keyword">count</span>(<span class="keyword">distinct</span> b) <span class="keyword">as</span> c <span class="keyword">from</span> tbl <span class="keyword">group</span> <span class="keyword">by</span> a;</span><br><span class="line"> </span><br><span class="line"><span class="comment">/*改写后*/</span></span><br><span class="line"><span class="keyword">select</span> a, <span class="keyword">count</span>(*) <span class="keyword">as</span> c <span class="keyword">from</span> (<span class="keyword">select</span> a, b <span class="keyword">from</span> tbl <span class="keyword">group</span> <span class="keyword">by</span> a, b) <span class="keyword">group</span> <span class="keyword">by</span> a;</span><br></pre></td></tr></table></figure>

<h4 id="Join倾斜"><a href="#Join倾斜" class="headerlink" title="Join倾斜"></a>Join倾斜</h4><p>2.1 skew join<br>join造成的倾斜，常见情况是不能做map join的两个表(能做map join的话基本上可以避免倾斜)，其中一个是行为表，另一个应该是属性表。比如我们有三个表，一个用户属性表users，一个商品属性表items，还有一个用户对商品的操作行为表日志表logs。假设现在需要将行为表关联用户表：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> <span class="keyword">logs</span> a <span class="keyword">join</span> <span class="keyword">users</span> b <span class="keyword">on</span> a.user_id = b.user_id;</span><br></pre></td></tr></table></figure>

<p>其中logs表里面会有一个特殊用户user_id = 0，代表未登录用户，假如这种用户占了相当的比例，那么个别reduce会收到比其他reduce多得多的数据，因为它要接收所有user_id = 0的记录进行处理，使得其处理效果会非常差，其他reduce都跑完很久了它还在运行。</p>
<p>hive给出的解决方案叫skew join，其原理把这种user_id = 0的特殊值先不在reduce端计算掉，而是先写入hdfs，然后启动一轮map join专门做这个特殊值的计算，期望能提高计算这部分值的处理速度。当然你要告诉hive这个join是个skew join，即：set</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">hive.optimize.skewjoin = true;</span><br></pre></td></tr></table></figure>

<p>还有要告诉hive如何判断特殊值，根据hive.skewjoin.key设置的数量hive可以知道，比如默认值是100000，那么超过100000条记录的值就是特殊值。</p>
<h4 id="特殊值分开处理法"><a href="#特殊值分开处理法" class="headerlink" title="特殊值分开处理法"></a>特殊值分开处理法</h4><p>不过，上述方法还要去考虑阈值之类的情况，其实也不够通用。所以针对join倾斜的问题，一般都是通过改写sql解决。对于上面这个问题，我们已经知道user_id = 0是一个特殊key，那么可以把特殊值隔离开来单独做join，这样特殊值肯定会转化成map join，非特殊值就是没有倾斜的普通join了：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span></span><br><span class="line">	*</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">	(</span><br><span class="line">		<span class="keyword">select</span> * <span class="keyword">from</span> <span class="keyword">logs</span> <span class="keyword">where</span> user_id = <span class="number">0</span></span><br><span class="line">	)</span><br><span class="line">	a</span><br><span class="line"><span class="keyword">join</span></span><br><span class="line">	(</span><br><span class="line">		<span class="keyword">select</span> * <span class="keyword">from</span> <span class="keyword">users</span> <span class="keyword">where</span> user_id = <span class="number">0</span></span><br><span class="line">	)</span><br><span class="line">	b</span><br><span class="line"><span class="keyword">on</span></span><br><span class="line">	a.user_id = b.user_id</span><br><span class="line"> </span><br><span class="line"><span class="keyword">union</span> <span class="keyword">all</span></span><br><span class="line"> </span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> <span class="keyword">logs</span> a <span class="keyword">join</span> <span class="keyword">users</span> b <span class="keyword">on</span> a.user_id &lt;&gt; <span class="number">0</span> <span class="keyword">and</span> a.user_id = b.user_id;</span><br></pre></td></tr></table></figure>

<h4 id="随机数分配法"><a href="#随机数分配法" class="headerlink" title="随机数分配法"></a>随机数分配法</h4><p>上面这种个别key倾斜的情况只是一种倾斜情况。最常见的倾斜是因为数据分布本身就具有长尾性质，比如我们将日志表和商品表关联：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> <span class="keyword">logs</span> a <span class="keyword">join</span> items b <span class="keyword">on</span> a.item_id = b.item_id;</span><br></pre></td></tr></table></figure>

<p>这个时候，分配到热门商品的reducer就会很慢，因为热门商品的行为日志肯定是最多的，而且我们也很难像上面处理特殊user那样去处理item。这个时候就会用到加随机数的方法，也就是在join的时候增加一个随机数，随机数的取值范围n相当于将item给分散到n个reducer：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span></span><br><span class="line">	a.*,</span><br><span class="line">	b.*</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">	(</span><br><span class="line">		<span class="keyword">select</span> *, <span class="keyword">cast</span>(<span class="keyword">rand</span>() * <span class="number">10</span> <span class="keyword">as</span> <span class="built_in">int</span>) <span class="keyword">as</span> r_id <span class="keyword">from</span> <span class="keyword">logs</span></span><br><span class="line">	)</span><br><span class="line">	a</span><br><span class="line"><span class="keyword">join</span></span><br><span class="line">	(</span><br><span class="line">		<span class="keyword">select</span> *, r_id <span class="keyword">from</span> items <span class="keyword">lateral</span> <span class="keyword">view</span> <span class="keyword">explode</span>(range_list(<span class="number">1</span>, <span class="number">10</span>)) rl <span class="keyword">as</span> r_id</span><br><span class="line">	)</span><br><span class="line">	b</span><br><span class="line"><span class="keyword">on</span></span><br><span class="line">	a.item_id = b.item_id</span><br><span class="line">	<span class="keyword">and</span> a.r_id = b.r_id</span><br></pre></td></tr></table></figure>

<p>上面的写法里，对行为表的每条记录生成一个1-10的随机整数，对于item属性表，每个item生成10条记录，随机key分别也是1-10，这样就能保证行为表关联上属性表。其中range_list(1,10)代表用udf实现的一个返回1-10整数序列的方法。这个做法是一个解决join倾斜比较根本性的通用思路，就是如何用随机数将key进行分散。当然，可以根据具体的业务场景做实现上的简化或变化。</p>
<h4 id="2-4-业务设计"><a href="#2-4-业务设计" class="headerlink" title="2.4 业务设计"></a>2.4 业务设计</h4><p>除了上面两类情况，还有一类情况是因为业务设计导致的问题，也就是说即使行为日志里面join key的数据分布本身并不明显倾斜，但是业务设计导致其倾斜。比如对于商品item_id的编码，除了本身的id序列，还人为的把item的类型也作为编码放在最后两位，这样如果类型1（电子产品）的编码是00，类型2（家居产品）的编码是01，并且类型1是主要商品类，将会造成以00为结尾的商品整体倾斜。这时，如果reduce的数量恰好是100的整数倍，会造成partitioner把00结尾的item_id都hash到同一个reducer，引爆问题。这种特殊情况可以简单的设置合适的reduce值来解决，但是这种坑对于不了解业务的情况下就会比较隐蔽。</p>
<h4 id="3-1-空值产生的数据倾斜"><a href="#3-1-空值产生的数据倾斜" class="headerlink" title="3.1 空值产生的数据倾斜"></a>3.1 空值产生的数据倾斜</h4><p>场景：如日志中，常会有信息丢失的问题，比如日志中的 user_id，如果取其中的 user_id 和 用户表中的user_id 关联，会碰到数据倾斜的问题。</p>
<ul>
<li>解决方法1： user_id为空的不参与关联</li>
</ul>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span></span><br><span class="line">	*</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">	<span class="keyword">log</span> a</span><br><span class="line"><span class="keyword">join</span> <span class="keyword">users</span> b</span><br><span class="line"><span class="keyword">on</span></span><br><span class="line">	a.user_id <span class="keyword">is</span> <span class="keyword">not</span> <span class="literal">null</span></span><br><span class="line">	<span class="keyword">and</span> a.user_id = b.user_id</span><br><span class="line"> </span><br><span class="line"><span class="keyword">union</span> <span class="keyword">all</span></span><br><span class="line"> </span><br><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> <span class="keyword">log</span> a <span class="keyword">where</span> a.user_id <span class="keyword">is</span> <span class="literal">null</span>;</span><br></pre></td></tr></table></figure>

<ul>
<li>解决方法2 ：赋与空值分新的key值</li>
</ul>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span></span><br><span class="line">	*</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">	<span class="keyword">log</span> a</span><br><span class="line"><span class="keyword">left</span> <span class="keyword">outer</span> <span class="keyword">join</span> <span class="keyword">users</span> b</span><br><span class="line"><span class="keyword">on</span></span><br><span class="line">	<span class="keyword">case</span></span><br><span class="line">		<span class="keyword">when</span> a.user_id <span class="keyword">is</span> <span class="literal">null</span></span><br><span class="line">		<span class="keyword">then</span> <span class="keyword">concat</span>(‘hive’, <span class="keyword">rand</span>())</span><br><span class="line">		<span class="keyword">else</span> a.user_id</span><br><span class="line">	<span class="keyword">end</span> = b.user_id;</span><br></pre></td></tr></table></figure>

<p>结论：方法2比方法1效率更好，不但io少了，而且作业数也少了。解决方法1中 log读取两次，jobs是2。解决方法2 job数是1 。这个优化适合无效 id (比如 -99 , ’’, null 等) 产生的倾斜问题。把空值的 key 变成一个字符串加上随机数，就能把倾斜的数据分到不同的reduce上 ,解决数据倾斜问题。</p>
<h4 id="3-2-不同数据类型关联产生数据倾斜"><a href="#3-2-不同数据类型关联产生数据倾斜" class="headerlink" title="3.2 不同数据类型关联产生数据倾斜"></a>3.2 不同数据类型关联产生数据倾斜</h4><p>场景：用户表中user_id字段为int，log表中user_id字段既有string类型也有int类型。当按照user_id进行两个表的Join操作时，默认的Hash操作会按int型的id来进行分配，这样会导致所有string类型id的记录都分配到一个Reducer中。</p>
<p>解决方法：把数字类型转换成字符串类型</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span></span><br><span class="line">	*</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">	<span class="keyword">users</span> a</span><br><span class="line"><span class="keyword">left</span> <span class="keyword">outer</span> <span class="keyword">join</span> <span class="keyword">logs</span> b</span><br><span class="line"><span class="keyword">on</span></span><br><span class="line">	a.usr_id = <span class="keyword">cast</span>(b.user_id <span class="keyword">as</span> <span class="keyword">string</span>)</span><br></pre></td></tr></table></figure>

<h4 id="3-3-小表不小不大，怎么用map-join解决倾斜问题"><a href="#3-3-小表不小不大，怎么用map-join解决倾斜问题" class="headerlink" title="3.3 小表不小不大，怎么用map join解决倾斜问题"></a>3.3 小表不小不大，怎么用map join解决倾斜问题</h4><p>使用 map join 解决小表(记录数少)关联大表的数据倾斜问题，这个方法使用的频率非常高，但如果小表很大，大到map join会出现bug或异常，这时就需要特别的处理。 以下例子:</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> * <span class="keyword">from</span> <span class="keyword">log</span> a <span class="keyword">left</span> <span class="keyword">outer</span> <span class="keyword">join</span> <span class="keyword">users</span> b <span class="keyword">on</span> a.user_id = b.user_id;</span><br></pre></td></tr></table></figure>

<p>users 表有 600w+ 的记录，把 users 分发到所有的 map 上也是个不小的开销，而且 map join 不支持这么大的小表。如果用普通的 join，又会碰到数据倾斜的问题。</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span></span><br><span class="line">	<span class="comment">/*+mapjoin(x)*/</span></span><br><span class="line">	*</span><br><span class="line"><span class="keyword">from</span></span><br><span class="line">	<span class="keyword">log</span> a</span><br><span class="line"><span class="keyword">left</span> <span class="keyword">outer</span> <span class="keyword">join</span></span><br><span class="line">	(</span><br><span class="line">		<span class="keyword">select</span></span><br><span class="line">			<span class="comment">/*+mapjoin(c)*/</span></span><br><span class="line">			d.*</span><br><span class="line">		<span class="keyword">from</span></span><br><span class="line">			(</span><br><span class="line">				<span class="keyword">select</span> <span class="keyword">distinct</span> user_id <span class="keyword">from</span> <span class="keyword">log</span></span><br><span class="line">			)</span><br><span class="line">			c</span><br><span class="line">		<span class="keyword">join</span> <span class="keyword">users</span> d</span><br><span class="line">		<span class="keyword">on</span></span><br><span class="line">			c.user_id = d.user_id</span><br><span class="line">	)</span><br><span class="line">	x <span class="keyword">on</span> a.user_id = b.user_id;</span><br></pre></td></tr></table></figure>

<h3 id="从输入URL到页面展示完整流程"><a href="#从输入URL到页面展示完整流程" class="headerlink" title="从输入URL到页面展示完整流程"></a>从输入URL到页面展示完整流程</h3><p>介绍完前面所需要的三个基本知识背景后（HTTP 请求流程包括了TCP连接），现在只需要将其串起来就能回答从输入 URL 到页面展示完整流程。<br>同样，先给出从输入 URL 到页面展示完整流程示意图：</p>
<p><img src="https://lixiangbetter.github.io/2021/03/24/%E9%9D%A2%E8%AF%95%E9%A2%98%E9%9A%8F%E7%AC%94-21-3-25/20200809162028289.png" alt></p>
<p>由图中不难发现，最外层是三种Chrome进程，而网络进程所执行的操作就是HTTP 请求流程并且渲染进程所执行的操作就是渲染流程。<br>下面，先给出此流程图精简版的描述：</p>
<ul>
<li>首先，浏览器进程接收到用户输入的 URL 请求，浏览器进程便将该 URL 转发给网络进程。</li>
<li>然后，在网络进程中发起真正的 URL 请求。</li>
<li>接着网络进程接收到了响应头数据，便解析响应头数据，并将数据转发给浏览器进程。</li>
<li>浏览器进程接收到网络进程的响应头数据之后，发送“提交导航 (CommitNavigation)”消息到渲染进程。</li>
<li>渲染进程接收到“提交导航”的消息之后，便开始准备接收 HTML 数据，接收数据的方式是直接和网络进程建立数据管道。</li>
<li>最后渲染进程会向浏览器进程“确认提交”，这是告诉浏览器进程：“已经准备好接受和解析页面数据了”。</li>
<li>浏览器进程接收到渲染进程“提交文档”的消息之后，便开始移除之前旧的文档，然后更新浏览器进程中的页面状态。</li>
</ul>
<h3 id="多核CPU和多CPU区别"><a href="#多核CPU和多CPU区别" class="headerlink" title="多核CPU和多CPU区别"></a>多核CPU和多CPU区别</h3><p>多核cpu是单颗cpu里边有多个核心，可以多线程工作。<br>多颗cpu是物理上就有多个。</p>
]]></content>
      <categories>
        <category>bigdata</category>
      </categories>
      <tags>
        <tag>面试题</tag>
      </tags>
  </entry>
  <entry>
    <title>面试题随笔-21/3/24</title>
    <url>/2021/03/24/%E9%9D%A2%E8%AF%95%E9%A2%98%E9%9A%8F%E7%AC%94-21-3-24/</url>
    <content><![CDATA[<h2 id="面试题随笔-21-3-24"><a href="#面试题随笔-21-3-24" class="headerlink" title="面试题随笔-21-3-24"></a>面试题随笔-21-3-24</h2><h3 id="spark-rdd介绍一下"><a href="#spark-rdd介绍一下" class="headerlink" title="spark rdd介绍一下"></a>spark rdd介绍一下</h3><p>RDD：Spark的核心概念是RDD (resilient distributed dataset)，指的是一个只读的，可分区的分布式数据集，这个数据集的全部或部分可以缓存在内存中，在多次计算间重用。</p>
<ul>
<li>为什么会产生RDD?<ul>
<li>传统的mapreduce虽然具有自动容错、平衡负载和可拓展性的优点，但是其最大的缺点是采用非循环式的数据流模型，使得在迭代计算时要进行大量的磁盘IO操作。RDD正是解决这一缺点的抽象方法。</li>
<li>RDD的具体描述RDD(弹性数据集)是Spark提供的最重要的抽象的概念，它是一种有容错机制的特殊集合，可以分布在集群的节点上，以函数式编操作集合的方式，进行各种并行操作。可以将RDD理解为一个具有容错机制的特殊集合，它提供了一种只读、只能有已存在的RDD变换而来的共享内存，然后将所有数据都加载到内存中，方便进行多次重用。a.他是分布式的，可以分布在多台机器上，进行计算。<ul>
<li>他是弹性的，计算过程中内存不够时它会和磁盘进行数据交换。</li>
<li>这些限制可以极大的降低自动容错开销</li>
<li>实质是一种更为通用的迭代并行计算框架，用户可以显示的控制计算的中间结果，然后将其自由运用于之后的计算。</li>
</ul>
</li>
<li>RDD的容错机制实现分布式数据集容错方法有两种：数据检查点和记录更新。</li>
<li>RDD采用记录更新的方式：记录所有更新点的成本很高。所以，RDD只支持粗颗粒变换，即只记录单个块上执行的单个操作，然后创建某个RDD的变换序列(血统)存储下来;变换序列指，每个RDD都包含了他是如何由其他RDD变换过来的以及如何重建某一块数据的信息。因此RDD的容错机制又称“血统”容错。</li>
<li>要实现这种“血统”容错机制，最大的难题就是如何表达父RDD和子RDD之间的依赖关系。实际上依赖关系可以分两种，窄依赖和宽依赖：窄依赖：子RDD中的每个数据块只依赖于父RDD中对应的有限个固定的数据块;宽依赖：子RDD中的一个数据块可以依赖于父RDD中的所有数据块。例如：map变换，子RDD中的数据块只依赖于父RDD中对应的一个数据块;groupByKey变换，子RDD中的数据块会依赖于多有父RDD中的数据块，因为一个key可能错在于父RDD的任何一个数据块中.</li>
<li>将依赖关系分类的两个特性：第一，窄依赖可以在某个计算节点上直接通过计算父RDD的某块数据计算得到子RDD对应的某块数据;宽依赖则要等到父RDD所有数据都计算完成之后，并且父RDD的计算结果进行hash并传到对应节点上之后才能计算子RDD。第二，数据丢失时，对于窄依赖只需要重新计算丢失的那一块数据来恢复;对于宽依赖则要将祖先RDD中的所有数据块全部重新计算来恢复。所以在长“血统”链特别是有宽依赖的时候，需要在适当的时机设置数据检查点。也是这两个特性要求对于不同依赖关系要采取不同的任务调度机制和容错恢复机制。</li>
</ul>
</li>
<li>RDD在Spark中的地位及作用<ul>
<li>为什么会有Spark?因为传统的并行计算模型无法有效的解决迭代计算(iterative)和交互式计算(interactive);而Spark的使命便是解决这两个问题，这也是他存在的价值和理由。</li>
<li>Spark如何解决迭代计算?其主要实现思想就是RDD，把所有计算的数据保存在分布式的内存中。迭代计算通常情况下都是对同一个数据集做反复的迭代计算，数据在内存中将大大提升IO操作。这也是Spark涉及的核心：内存计算。</li>
<li>Spark如何实现交互式计算?因为Spark是用scala语言实现的，Spark和scala能够紧密的集成，所以Spark可以完美的运用scala的解释器，使得其中的scala可以向操作本地集合对象一样轻松操作分布式数据集。</li>
<li>Spark和RDD的关系?可以理解为：RDD是一种具有容错性基于内存的集群计算抽象方法，Spark则是这个抽象方法的实现。</li>
</ul>
</li>
<li>如何操作RDD?<ul>
<li>如何获取RDD<br>a.从共享的文件系统获取，(如：HDFS)；<br>b.通过已存在的RDD转换；<br>c.将已存在scala集合(只要是Seq对象)并行化，通过调用SparkContext的parallelize方法实现；<br>d.改变现有RDD的持久性;RDD是懒散，短暂的。(RDD的固化：cache缓存至内存 save保存到分布式文件系统)</li>
<li>操作RDD的两个动作<br>a.Actions：对数据集计算后返回一个数值value给驱动程序;例如：Reduce将数据集的所有元素用某个函数聚合后，将最终结果返回给程序。<br>b.Transformation：根据数据集创建一个新的数据集，计算后返回一个新RDD;例如：Map将数据的每个元素经过某个函数计算后，返回一个姓的分布式数据集。</li>
</ul>
</li>
</ul>
<h3 id="数组求topk"><a href="#数组求topk" class="headerlink" title="数组求topk"></a>数组求topk</h3><ul>
<li>小根堆</li>
<li>快排 （需要内存足够）</li>
</ul>
<h3 id="二叉树后序遍历"><a href="#二叉树后序遍历" class="headerlink" title="二叉树后序遍历"></a>二叉树后序遍历</h3><p>先序 ==&gt;（左右交换） 逆后序 ==&gt;（再逆序） 后序</p>
<h3 id="MapReduce介绍一下"><a href="#MapReduce介绍一下" class="headerlink" title="MapReduce介绍一下"></a>MapReduce介绍一下</h3><p>MapReduce是一种计算模型，该模型可以将大型数据处理任务分解成很多单个的、可以在服务器集群中并行执行的任务，而这些任务的计算结果可以合并在一起来计算最终的结果。简而言之，Hadoop Mapreduce是一个易于编程并且能在大型集群（上千节点）快速地并行得处理大量数据的软件框架，以可靠，容错的方式部署在商用机器上。</p>
<p>MapReduce这个术语来自两个基本的数据转换操作：map过程和reduce过程。</p>
<p>一、map</p>
<p>map操作会将集合中的元素从一种形式转化成另一种形式，在这种情况下，输入的键值对会被转换成零到多个键值对输出。其中输入和输出的键必须完全不同，而输入和输出的值则可能完全不同。</p>
<p>二、reduce</p>
<p>某个键的所有键值对都会被分发到同一个reduce操作中。确切的说，这个键和这个键所对应的所有值都会被传递给同一个Reducer。reduce过程的目的是将值的集合转换成一个值（例如求和或者求平均），或者转换成另一个集合。这个Reducer最终会产生一个键值对。需要说明的是，如果job不需要reduce过程的话，那么reduce过程也是可以不用的。</p>
<h3 id="mapreduce中间有个combine是干嘛的，有什么好处，有什么使用限制吗？"><a href="#mapreduce中间有个combine是干嘛的，有什么好处，有什么使用限制吗？" class="headerlink" title="mapreduce中间有个combine是干嘛的，有什么好处，有什么使用限制吗？"></a>mapreduce中间有个combine是干嘛的，有什么好处，有什么使用限制吗？</h3><p>combiner是reduce的实现，在map端运行计算任务，减少map端的输出数据。</p>
<p>作用就是优化。</p>
<p>但是combiner的使用场景是mapreduce的map输出结果和reduce输入输出一样。</p>
<p>partition的默认实现是hashpartition，是map端将数据按照reduce个数取余，进行分区，不同的reduce来copy自己的数据。</p>
<p>partition的作用是将数据分到不同的reduce进行计算，加快计算效果。</p>
<h3 id="编写MapReduce作业时，如何做到在Reduce阶段，先对key排序，再对value排序？"><a href="#编写MapReduce作业时，如何做到在Reduce阶段，先对key排序，再对value排序？" class="headerlink" title="编写MapReduce作业时，如何做到在Reduce阶段，先对key排序，再对value排序？"></a>编写MapReduce作业时，如何做到在Reduce阶段，先对<em>key</em>排序，再对<em>value</em>排序？</h3><p>该问题通常称为“二次排序”，最常用的方法是将value放到key中，实现一个组合Key，然后自定义key排序规则（为key实现一个WritableComparable）。</p>
<h3 id="如何使用MapReduce实现两表join，可考虑以下几种情况：（1）一个表大，一个表小（可放到内存中）-（2）两个表均是大表"><a href="#如何使用MapReduce实现两表join，可考虑以下几种情况：（1）一个表大，一个表小（可放到内存中）-（2）两个表均是大表" class="headerlink" title="如何使用MapReduce实现两表join，可考虑以下几种情况：（1）一个表大，一个表小（可放到内存中） （2）两个表均是大表"></a>如何使用MapReduce实现两表join，可考虑以下几种情况：（1）一个表大，一个表小（可放到内存中） （2）两个表均是大表</h3><p>map join：<br>map side join 是针对一下场景进行的优化。两个待连接的表中，有一个表非常大，而另一个非常小，以至于小表可以直接存放到内存中。这样，我们可以将小表复制多份，让每一个map task内存中存在一份（比如放在hash table中），然后只扫描大表：对于大表中的每一条记录key/value，在hash table中查找是否具有相同key的记录，入股有，则连接后输出即可。<br>场景：MapJoin 适用于有一份数据较小的连接情况。<br>做法：直接将较小的数据加载到内存中，按照连接的关键字建立索引，大份数据作为MapTask的输入数据对 map()方法的每次输入都去内存当中直接去匹配连接。然后把连接结果按 key 输出，这种方法要使用 hadoop中的 DistributedCache 把小份数据分布到各个计算节点，每个 maptask 执行任务的节点都需要加载该数据到内存，并且按连接关键字建立索引。</p>
<p>reduce side join是一种最简单的join方式，其主要思想如下：</p>
<p>在map阶段，map函数同时读取两个文件File1和File2，为了区分两种来源的key/value数据对，对每条数据打一个标签（tag）,比如：tag=0表示来自文件File1，tag=2表示来自文件File2。即：map阶段的主要任务是对不同文件中的数据打标签。</p>
<p>在reduce阶段，reduce函数获取key相同的来自File1和File2文件的value list， 然后对于同一个key，对File1和File2中的数据进行join（笛卡尔乘积）。即：reduce阶段进行实际的连接操作。</p>
<p>SemiJoin，也叫半连接，是从分布式数据库中借鉴过来的方法。它的产生动机是：对于reduce side join，跨机器的数据传输量非常大，这成了join操作的一个瓶颈，如果能够在map端过滤掉不会参加join操作的数据，则可以大大节省网络IO。</p>
<p>实现方法很简单：选取一个小表，假设是File1，将其参与join的key抽取出来，保存到文件File3中，File3文件一般很小，可以放到内存中。在map阶段，使用DistributedCache将File3复制到各个TaskTracker上，然后将File2中不在File3中的key对应的记录过滤掉，剩下的reduce阶段的工作与reduce side join相同。</p>
<h3 id="当面试的时候问到，MapReduce-架构设计、Yarn架构设计、Yarn的工作流程、MapReduce-job-提交到-Yarn的工作流程-（面试题为同一题），其实都是同一个问题。"><a href="#当面试的时候问到，MapReduce-架构设计、Yarn架构设计、Yarn的工作流程、MapReduce-job-提交到-Yarn的工作流程-（面试题为同一题），其实都是同一个问题。" class="headerlink" title="当面试的时候问到，MapReduce 架构设计、Yarn架构设计、Yarn的工作流程、MapReduce job 提交到 Yarn的工作流程 （面试题为同一题），其实都是同一个问题。"></a>当面试的时候问到，MapReduce 架构设计、Yarn架构设计、Yarn的工作流程、MapReduce job 提交到 Yarn的工作流程 （面试题为同一题），其实都是同一个问题。</h3><p>1.用户向yarn提交应用程序（job），其中包括application Master程序，启动application Master命令等</p>
<p>2.RM为该job分配了一个容器，并于对应的NM通信，要求它在这个容器中启动job的MR application Master程序<br>3.启动程序之后，application Master首先向ApplicationsManager注册，用户就可以直接在web界面上查看job的整个运行状态和日志<br>4.applicationMaster向Resource Scheduler采用轮询的方式通过RPC协议去申请和领取资源列表</p>
<p>5.一旦applicationMaster申请到资源后，便与对应的NM节点通信，要求启动任务</p>
<p>6.NM为任务task设置好运行环境（环境变量，jar），将任务的启动命令写在一个脚本文件中，并通过脚本文件【启动任务<br>7.各个task通过RPC协议向applicationMaster汇报自己的状态和进度，以让applicationMaster随时掌握各个任务的运行状态，从而可以在任务运行时重新启动任务，则web界面可以实时查看job当前运行状态。<br>8.job运行完成后，applicationMaster向RM注销自己并关闭自己</p>
<h3 id="如何使用MapReduce实现全排序（即数据全局key有序）？-你给出的算法可能要求仅启动一个Reduce-Task，那么如何对算法改进，可以同时启动多个Reduce-Task提高排序效率。"><a href="#如何使用MapReduce实现全排序（即数据全局key有序）？-你给出的算法可能要求仅启动一个Reduce-Task，那么如何对算法改进，可以同时启动多个Reduce-Task提高排序效率。" class="headerlink" title="如何使用MapReduce实现全排序（即数据全局key有序）？ 你给出的算法可能要求仅启动一个Reduce Task，那么如何对算法改进，可以同时启动多个Reduce Task提高排序效率。"></a>如何使用MapReduce实现全排序（即数据全局key有序）？ 你给出的算法可能要求仅启动一个Reduce Task，那么如何对算法改进，可以同时启动多个Reduce Task提高排序效率。</h3><ul>
<li><p>使用一个reduce实现全局排序<br>我们知道，MapReduce默认情况下只保证同一个分区中的key是有序的，不能保证全局有序。如果我们将所有的数据都用一个reduce来处理，就可以实现全局有序。<br>缺点：此方法的缺点也很明显，所有数据发送到一个reduce进行排序，不但不能充分利用集群的分布式资源，在数据量很大的情况下，很容易出现OOM的情况，效率也很低</p>
</li>
<li><p>自定义分区函数实现全局排序</p>
<pre><code>MapReduce默认的分区函数为HashPartitioner，其具体的实现原理是计算map的输出key的hashCode值，然后用该值对reduce个数取余，不同的余数对应不同的reduce，余数相同的key将会被发送到同一个reduce中。</code></pre><p>如果我们能够实现一个分区函数，使得<br>所以的key &lt; 1000的数据都发送到reduce0；<br>所有1000 &lt; key &lt; 10000的数据都发送到reduce1；<br>……<br>其余的key都发送到reduceN;</p>
<pre><code>这样就实现了reduce0的数据一定全部小于reduce1的数据，reduce1的数据全部小于reduce2的数据，……，同时每个reduce中数据是局部有序的，这样就实现了全局有序</code></pre><p>缺点：该方法的难点在于我们必须手动找到各个reduce的分界点，且尽量是的分散到每个reduce的数据均衡。如果想增加或减少reduce个数，必须再次寻找分界点，非常不灵活。</p>
</li>
<li><p>使用TotalOrderPartitioner实现全局排序</p>
<pre><code>MapReduce内部包含一个TotalOrderPartitoner的分区实现类，主要解决全排序问题。其原理和方法2类似，根据key的分界点将不同的key发送到不同的reduce中，区别在于方法2需要人工寻找分界点，而该方法是有程序计算得到。</code></pre><p>数据抽样</p>
<pre><code>为了寻找到合适的key分界点，我们需要对数据的整体分布有所了解；如果数据量很大，我们就不可能对所有数据进行分析然后选出N-1的分界点（假设reduce个数为N），最适合的方式就是通过数据抽样，对抽样的数据进行分析从而选出合适的分割点。MapReduce提供了三种抽样方法：</code></pre><p>SplitSampler：从n个split中选择前m条记录取样（不适合有序数据）<br>RandomSampler：随机取样（通用采样器）<br>IntervalSampler：从n个split中按照一定间隔取样（通常适用于有序数据）</p>
<pre><code>三种抽样方法都通过getSample(InputFormat inf, Job job)函数返回抽样得到的key数组，除了IntervalSampler方法返回的抽样key是有序的，其他都是无序的</code></pre><p>获取到抽样的key之后就可以对其进行排序，然后选择N-1个分界点，最后将这些key的分界点存储到指定的HDFS文件中。</p>
</li>
</ul>
<h3 id="mapreduce优化"><a href="#mapreduce优化" class="headerlink" title="mapreduce优化"></a>mapreduce优化</h3><p><strong>1）数据输入小文件处理：</strong></p>
<p>（1）合并小文件：对小文件进行归档（har）、自定义 inputformat 将小文件存储成</p>
<p>sequenceFile 文件。</p>
<p>（2）采用 ConbinFileInputFormat 来作为输入，解决输入端大量小文件场景。</p>
<p>（3）对于大量小文件 Job，可以开启 JVM 重用。 </p>
<p><strong>2）map 阶段</strong></p>
<p>（1）增大环形缓冲区大小。由 100m 扩大到 200m </p>
<p>（2）增大环形缓冲区溢写的比例。由 80%扩大到 90% </p>
<p>（3）减少对溢写文件的 merge 次数。 </p>
<p>（4）不影响实际业务的前提下，采用 combiner 提前合并，减少 I/O。 </p>
<p><strong>3）reduce 阶段</strong></p>
<p>（1）合理设置 map 和 reduce 数：两个都不能设置太少，也不能设置太多。太少，会</p>
<p>导致 task 等待，延长处理时间；太多，会导致 map、reduce 任务间竞争资源，造成处理</p>
<p>超时等错误。</p>
<p>（2）设置 map、reduce 共存：调整 slowstart.completedmaps 参数，使 map 运行到一</p>
<p>定程度后，reduce 也开始运行，减少 reduce 的等待时间。</p>
<p>（3）规避使用 reduce，因为 Reduce 在用于连接数据集的时候将会产生大量的网络消耗。</p>
<p>（4）增加每个 reduce 去 map 中拿数据的并行数</p>
<p>（5）集群性能可以的前提下，增大 reduce 端存储数据内存的大小。</p>
<p><strong>4）IO 传输</strong></p>
<p>（1）采用数据压缩的方式，减少网络 IO 的的时间。安装 Snappy 和 LZOP 压缩编码器。 </p>
<p>（2）使用 SequenceFile 二进制文件</p>
<p><strong>5）整体</strong></p>
<p>（1）MapTask 默认内存大小为 1G，可以增加 MapTask 内存大小为 4-5g</p>
<p>（2）ReduceTask 默认内存大小为 1G，可以增加 ReduceTask 内存大小为 4-5g</p>
<p>（3）可以增加 MapTask 的 cpu 核数，增加 ReduceTask 的 cpu 核数</p>
<p>（4）增加每个 container 的 cpu 核数和内存大小</p>
<p>（5）调整每个 Map Task 和 Reduce Task 最大重试次数</p>
<h3 id="Hadoop-MapReduce中资源管理模型是怎样的，有什么缺点，如何改进？"><a href="#Hadoop-MapReduce中资源管理模型是怎样的，有什么缺点，如何改进？" class="headerlink" title="Hadoop MapReduce中资源管理模型是怎样的，有什么缺点，如何改进？"></a>Hadoop MapReduce中资源管理模型是怎样的，有什么缺点，如何改进？</h3><p>MapReduce将资源划分成map slot和reduce slot，其中map slot供map task使用，reduce slot供reduce slot使用，这种模型存在以下几个问题：</p>
<p>（1）slot之间不能共享，导致资源利用率低（比如map slot空闲而reduce slot紧缺时，Reduce Task不能使用空闲的map slot）。</p>
<p>（2）slot数目静态配置，不能动态修改 </p>
<p>（3）slot这种划分资源方式粒度过大，导致集群资源利用情况不好精细化控制，比如一个map task可能无法充分利用一个map slot对应的资源。</p>
<p>hadoop2.0已经采用基于真实资源需求量的资源管理方案</p>
<h3 id="解释什么是Capacity-Scheduler？"><a href="#解释什么是Capacity-Scheduler？" class="headerlink" title="解释什么是Capacity Scheduler？"></a>解释什么是Capacity Scheduler？</h3><p>答：Capacity Scheduler是Hadoop自带的一个多租户多队列作业调度器，它按照队列组织作业和用户，以达到多用户共享集群的目的。</p>
<h3 id="Hadoop-2-0中YARN是什么？（最近部分明年毕业的同学找工作过程遇到了关于YARN的笔试题，比如今天的EMC笔试题）"><a href="#Hadoop-2-0中YARN是什么？（最近部分明年毕业的同学找工作过程遇到了关于YARN的笔试题，比如今天的EMC笔试题）" class="headerlink" title="Hadoop 2.0中YARN是什么？（最近部分明年毕业的同学找工作过程遇到了关于YARN的笔试题，比如今天的EMC笔试题）"></a>Hadoop 2.0中YARN是什么？（最近部分明年毕业的同学找工作过程遇到了关于YARN的笔试题，比如今天的EMC笔试题）</h3><p>答：YARN是Yet Another Resource Negotiator的简称，是Hadoop 2.0新引入的资源管理系统，负责集群中资源的统一管理和调度，它允许用户在其上运行各类计算框架，而不仅仅限于MapReduce一种。</p>
<h3 id="YARN和MapReduce有什么关系？"><a href="#YARN和MapReduce有什么关系？" class="headerlink" title="YARN和MapReduce有什么关系？"></a>YARN和MapReduce有什么关系？</h3><p>答：MapReduce是一种两阶段计算框架，它由资源管理和任务调度两部分组成，其中资源管理部分是通用的模块，不仅MapReduce需要，其他计算框架，比如Spark等也需要这种模块，于是Hadoop 2.0将资源管理部分独立出来，创建了YARN这一分支。YARN出现后，多个计算框架运行在一个集群中共享资源成为可能。YARN可看做一个“云操作系统”，而MapReduce可看做运行在该操作系统上的“应用程序”。</p>
<h3 id="hdfs小文件过多会怎么样"><a href="#hdfs小文件过多会怎么样" class="headerlink" title="hdfs小文件过多会怎么样"></a>hdfs小文件过多会怎么样</h3><ul>
<li><p>先说对小文件的定义，一般来说小于等于30M的文件，都叫小文件。在HDFS中，通常NN维护一个文件的名称，目录结构等大约是250字节。现实中，HDFS的小文件如果不做任何操作增长会很快，现在假设NN节点的内存为4G，差不多42亿字节，现在在HDFS上有一亿个小文件，那么需要250乘一亿大约是250亿字节，这样会将NN撑爆。小文件到达一定数目，就会将NN节点撑爆。就算NN能够存储，对于hive，spark计算时，小文件意味着需要更多的task和资源，同样也会将节点弄挂掉。</p>
</li>
<li><p>解决方案</p>
<p>生产上首先需要设置小文件的阈值，到达这个值对小文件进行合并。对于这个合并，一种是在HDFS存储之前就进行合并，还有一种就是计算完之后根据业务周期来进行合并。后一种需要在计算时额外对小文件进行调整。</p>
</li>
</ul>
<h3 id="spark中job，stage，task之间的关系"><a href="#spark中job，stage，task之间的关系" class="headerlink" title="spark中job，stage，task之间的关系"></a>spark中job，stage，task之间的关系</h3><ul>
<li><p>什么是job</p>
<p>Job简单讲就是提交给spark的任务。</p>
</li>
<li><p>什么是stage</p>
<p>Stage是每一个job处理过程要分为的几个阶段。</p>
</li>
<li><p>什么是task</p>
<p>Task是每一个job处理过程要分几为几次任务。Task是任务运行的最小单位。最终是要以task为单位运行在executor中。</p>
</li>
<li><p>Job和stage和task之间有什么关系</p>
<p>Job &lt;—&gt; 一个或多个stage &lt;—&gt; 一个或多个task</p>
</li>
<li><p>一个stage的task的数量是有谁来决定的？</p>
<p>是由输入文件的切片个数来决定的。在HDFS中不大于128m的文件算一个切片（默认128m）。通过算子修改了某一个rdd的分区数量，task数量也会同步修改。</p>
</li>
<li><p>一个job任务的task数量是由谁来决定的？</p>
<p>一个job任务可以有一个或多个stage，一个stage又可以有一个或多个task。所以一个job的task数量是  （多个stage的task数量）的总和。</p>
</li>
<li><p>每一个stage中的task最大的并行度？</p>
<p>并行度：是指指令并行执行的最大条数。在指令流水中，同时执行多条指令称为指令并行。</p>
<p>理论上：每一个stage下有多少的分区，就有多少的task，task的数量就是我们任务的最大的并行度。（一般情况下，我们一个task运行的时候，使用一个cores）</p>
<p>实际上：最大的并行度，取决于我们的application任务运行时使用的executor拥有的cores的数量。</p>
</li>
<li><p>如果我们的task数量超过这个cores的总数怎么办？</p>
<p>–num-executors 2 –executor-memory 512m –executor-cores 2</p>
<p>当前stage有200个task，先执行cores个数量的task，然后等待cpu资源空闲后，继续执行剩下的task。</p>
</li>
<li><p>spark执行时读条中的内容讲解</p>
<p>Stage60：当前的stage编号</p>
<p>(105+2)/200：200：当前stage的task的数量；105：已完成的task数量；4：等待执行的task数量。</p>
</li>
</ul>
]]></content>
      <categories>
        <category>bigdata</category>
      </categories>
      <tags>
        <tag>面试题</tag>
      </tags>
  </entry>
  <entry>
    <title>应用监控与调优</title>
    <url>/2021/01/05/%E5%BA%94%E7%94%A8%E7%9B%91%E6%8E%A7%E4%B8%8E%E8%B0%83%E4%BC%98/</url>
    <content><![CDATA[<h1 id="应用监控与调优"><a href="#应用监控与调优" class="headerlink" title="应用监控与调优"></a>应用监控与调优</h1><h2 id="监控"><a href="#监控" class="headerlink" title="监控"></a>监控</h2><h3 id="skywalking快速上手"><a href="#skywalking快速上手" class="headerlink" title="skywalking快速上手"></a>skywalking快速上手</h3><p>java agent配置 手记：<a href="https://www.imooc.com/article/306297" target="_blank" rel="noopener">https://www.imooc.com/article/306297</a></p>
<p>java agent插件 手记：<a href="https://www.imooc.com/article/305592" target="_blank" rel="noopener">https://www.imooc.com/article/305592</a></p>
<p>编写sky walking 手记: <a href="https://www.imooc.com/article/312560" target="_blank" rel="noopener">https://www.imooc.com/article/312560</a></p>
<p>动态配置: <a href="https://www.imooc.com/article/312690" target="_blank" rel="noopener">https://www.imooc.com/article/312690</a></p>
<h3 id="spring-boot-actuator"><a href="#spring-boot-actuator" class="headerlink" title="spring boot actuator"></a>spring boot actuator</h3><p>actuator可视化：</p>
<p>​    spring boot admin</p>
<p>​    prometheus + grafana</p>
<h3 id="JavaMelody"><a href="#JavaMelody" class="headerlink" title="JavaMelody"></a>JavaMelody</h3><h3 id="Tomcat-内置工具"><a href="#Tomcat-内置工具" class="headerlink" title="Tomcat 内置工具"></a>Tomcat 内置工具</h3><p>​    Tomcat Manager</p>
<h3 id="PSI-Probe"><a href="#PSI-Probe" class="headerlink" title="PSI Probe"></a>PSI Probe</h3><h2 id="池化技术"><a href="#池化技术" class="headerlink" title="池化技术"></a>池化技术</h2><h3 id="对象池"><a href="#对象池" class="headerlink" title="对象池"></a>对象池</h3><p>​    复用对象</p>
<p>​    开源项目：commons-pool2</p>
<h3 id="线程池"><a href="#线程池" class="headerlink" title="线程池"></a>线程池</h3><p>​    复用线程</p>
<p>​    ThreadPoolExecutor</p>
<p>​    SheduledThreadExecutor</p>
<p>​    ForkJoinPool</p>
<pre><code>#### 线程池调优实战</code></pre><p><a href="https://www.javacodegeeks.com/2012/04/threading-stories-about-robust-thread.html" target="_blank" rel="noopener">https://www.javacodegeeks.com/2012/04/threading-stories-about-robust-thread.html</a></p>
<h3 id="连接池"><a href="#连接池" class="headerlink" title="连接池"></a>连接池</h3><p>​    复用连接</p>
<p>​    example: </p>
<p>​        数据库连接池</p>
<p>​        redis连接池</p>
<p>​        http连接池</p>
<p>​            HttpClient、OKHTTP自带连接池</p>
<p>​            RestTemplate、Feign底层可使用HttpClient/OKHTTP的连接池</p>
<ul>
<li><p>本地调用异步化</p>
<ul>
<li><p>创建线程</p>
</li>
<li><p>线程池</p>
</li>
<li><p>@Async</p>
<p>Ps: </p>
<ul>
<li>@Async注解标注的方法必须返回void或future</li>
<li>建议将@Async标注的方法放到独立的类中去</li>
<li>建议自定义BlockingQueue的大小</li>
</ul>
</li>
</ul>
</li>
<li><p>远程调用异步化</p>
<ul>
<li>AsyncRestTemplate使用</li>
<li>WebClient使用</li>
<li>基于MQ</li>
</ul>
</li>
</ul>
<h3 id="锁机制"><a href="#锁机制" class="headerlink" title="锁机制"></a>锁机制</h3><h4 id="synchronized关键字"><a href="#synchronized关键字" class="headerlink" title="synchronized关键字"></a>synchronized关键字</h4><p>可重入锁</p>
<p>通过指定jvm参数来实现：</p>
<p>锁分级</p>
<p>锁消除</p>
<p>锁粗化</p>
<p><img src="https://lixiangbetter.github.io/2021/01/04/%E5%BA%94%E7%94%A8%E7%9B%91%E6%8E%A7%E4%B8%8E%E8%B0%83%E4%BC%98/1609927026486.jpg" alt></p>
<h4 id="ReentrantLock"><a href="#ReentrantLock" class="headerlink" title="ReentrantLock"></a>ReentrantLock</h4><p>完全互斥</p>
<h4 id="ReentrantReadWriteLock"><a href="#ReentrantReadWriteLock" class="headerlink" title="ReentrantReadWriteLock"></a>ReentrantReadWriteLock</h4><p>读锁共享，写锁互斥</p>
]]></content>
      <categories>
        <category>back_end</category>
      </categories>
  </entry>
  <entry>
    <title>dubbo学习</title>
    <url>/2021/01/04/dubbo%E5%AD%A6%E4%B9%A0/</url>
    <content><![CDATA[<h1 id="Dubbo学习"><a href="#Dubbo学习" class="headerlink" title="Dubbo学习"></a>Dubbo学习</h1><h2 id="可选注册中心"><a href="#可选注册中心" class="headerlink" title="可选注册中心"></a>可选注册中心</h2><p>Multicast、zookeeper、nacos、redis、simple</p>
<h2 id="案例"><a href="#案例" class="headerlink" title="案例"></a>案例</h2><p>使用dubbo框架，dubbo-api,dubbo-client,dubbo-consumer完成一个完整的api调用。</p>
<p>Dubbo中注入service是@Reference</p>
<h2 id="Dubbo-Admin"><a href="#Dubbo-Admin" class="headerlink" title="Dubbo-Admin"></a>Dubbo-Admin</h2><p>服务治理可视化</p>
<p>基本功能：</p>
<p>​    条件路由</p>
<p>​    标签路由</p>
<p>​    黑白名单</p>
<p>​    服务权重</p>
<p>​    负载均衡</p>
<p>​    服务测试</p>
]]></content>
      <categories>
        <category>back_end</category>
      </categories>
      <tags>
        <tag>dubbo</tag>
      </tags>
  </entry>
  <entry>
    <title>人工智能基础</title>
    <url>/2020/11/23/%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%9F%BA%E7%A1%80/</url>
    <content><![CDATA[<h1 id="人工智能基础"><a href="#人工智能基础" class="headerlink" title="人工智能基础"></a>人工智能基础</h1><h2 id="python基础教程"><a href="#python基础教程" class="headerlink" title="python基础教程"></a>python基础教程</h2><h3 id="输入输出"><a href="#输入输出" class="headerlink" title="输入输出"></a>输入输出</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">print(<span class="string">"hello world"</span>)</span><br><span class="line">print(<span class="string">"hello"</span>,<span class="string">"world"</span>)</span><br><span class="line">print(<span class="string">"hello,i'm %s,%d years old!"</span>%(<span class="string">'tom'</span>,<span class="number">20</span>))  <span class="comment"># 字符串 整数</span></span><br><span class="line">print(<span class="string">"%2d"</span>%<span class="number">3</span>)	<span class="comment"># 整数</span></span><br><span class="line">print(<span class="string">"%02d"</span>%<span class="number">3</span>)	<span class="comment"># 两位整数</span></span><br><span class="line">print(<span class="string">"%.2f"</span>%<span class="number">3.1415926</span>)</span><br><span class="line">print(<span class="string">"%.2f%%"</span>%<span class="number">25</span>) <span class="comment"># 百分数</span></span><br><span class="line">str = <span class="string">"i'm &#123;0&#125;,&#123;1&#125; years old"</span>.format(<span class="string">"tom"</span>,<span class="number">20</span>) <span class="comment"># 按索引</span></span><br></pre></td></tr></table></figure>

<h3 id="元组"><a href="#元组" class="headerlink" title="元组"></a>元组</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">students = (<span class="string">"jim"</span>,<span class="string">"jack"</span>)</span><br><span class="line"><span class="keyword">for</span> s <span class="keyword">in</span> students:</span><br><span class="line">    print(s)</span><br><span class="line">len(students)</span><br><span class="line">students[<span class="number">1</span>],students[<span class="number">-1</span>],students[<span class="number">10</span>]</span><br></pre></td></tr></table></figure>

<p>元组不可被修改</p>
<h3 id="List"><a href="#List" class="headerlink" title="List"></a>List</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">students = [<span class="string">"jim"</span>,<span class="string">"jack"</span>]</span><br><span class="line">students.append(<span class="string">"rose"</span>)	<span class="comment"># 添加</span></span><br><span class="line">students.insert(<span class="number">2</span>,<span class="string">"lee"</span>) <span class="comment"># 按位置插入</span></span><br><span class="line"><span class="comment"># students.pop() # 弹出</span></span><br><span class="line"><span class="comment"># students.pop(2)</span></span><br><span class="line">students.sort() <span class="comment"># 排序</span></span><br><span class="line"><span class="keyword">for</span> s <span class="keyword">in</span> students:</span><br><span class="line">    print(s)</span><br></pre></td></tr></table></figure>

<h3 id="Set"><a href="#Set" class="headerlink" title="Set"></a>Set</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">set1 = &#123;<span class="number">1</span>,<span class="number">3</span>,<span class="number">5</span>,<span class="number">7</span>,<span class="number">9</span>&#125;</span><br><span class="line">set2 = &#123;<span class="number">1</span>,<span class="number">3</span>,<span class="number">5</span>,<span class="number">7</span>,<span class="number">9</span>,<span class="number">3</span>,<span class="number">7</span>,<span class="number">9</span>&#125;</span><br><span class="line">set1.add(<span class="number">11</span>),set1.remove(<span class="number">5</span>)</span><br><span class="line">s1 = &#123;<span class="number">1</span>,<span class="number">3</span>,<span class="number">5</span>,<span class="number">7</span>&#125;</span><br><span class="line">s2 = &#123;<span class="number">5</span>,<span class="number">7</span>,<span class="number">9</span>,<span class="number">11</span>&#125;</span><br><span class="line">s3 = s1 &amp; s2</span><br><span class="line">s4 = s1 | s2</span><br><span class="line">s4 = set([<span class="number">1</span>,<span class="number">3</span>,<span class="number">5</span>,<span class="number">7</span>])</span><br><span class="line">s5 = set(list(range(<span class="number">1</span>,<span class="number">10</span>)))</span><br></pre></td></tr></table></figure>

<p>set不可重复</p>
<h3 id="Dict"><a href="#Dict" class="headerlink" title="Dict"></a>Dict</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">d = &#123;<span class="string">"s01"</span>:<span class="string">"jim"</span>,<span class="string">"s02"</span>:<span class="string">"tom"</span>,<span class="string">"s03"</span>:<span class="string">"jack"</span>&#125;</span><br><span class="line">d[<span class="string">"s05"</span>] = <span class="string">"rose"</span></span><br><span class="line">d[<span class="string">"s02"</span>]</span><br><span class="line">d.get(<span class="string">"s01"</span>)</span><br><span class="line"><span class="keyword">if</span> <span class="string">"s05"</span> <span class="keyword">in</span> d:</span><br><span class="line">    d.pop(<span class="string">"s05"</span>)</span><br><span class="line">    </span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> d:</span><br><span class="line">    print(<span class="string">"%s-&gt;%s"</span>%(k,d.get(k)))</span><br><span class="line">    </span><br><span class="line"><span class="keyword">for</span> v <span class="keyword">in</span> d.values():</span><br><span class="line">    print(<span class="string">"%s-&gt;%s"</span>%(k,v))</span><br></pre></td></tr></table></figure>

<h3 id="列表生成式"><a href="#列表生成式" class="headerlink" title="列表生成式"></a>列表生成式</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">my_list = list(range(<span class="number">1</span>,<span class="number">10</span>,<span class="number">2</span>))</span><br><span class="line">my_list = [x * x <span class="keyword">for</span> x <span class="keyword">in</span> range(<span class="number">1</span>,<span class="number">10</span>,<span class="number">2</span>)]</span><br><span class="line">print(my_list)</span><br><span class="line">my_list = [x * y <span class="keyword">for</span> x <span class="keyword">in</span> range(<span class="number">1</span>,<span class="number">10</span>) <span class="keyword">for</span> y <span class="keyword">in</span> range(<span class="number">1</span>,<span class="number">10</span>)]</span><br><span class="line">print(my_list)</span><br><span class="line">my_list = [x * x <span class="keyword">for</span> x <span class="keyword">in</span> range(<span class="number">1</span>,<span class="number">21</span>) <span class="keyword">if</span> x % <span class="number">3</span> == <span class="number">0</span>]</span><br><span class="line">print(my_list)</span><br><span class="line">my_list = [m + n.upper() <span class="keyword">for</span> m <span class="keyword">in</span> <span class="string">"hello"</span> <span class="keyword">for</span> n <span class="keyword">in</span> <span class="string">"world"</span>]</span><br><span class="line">print(my_list)</span><br></pre></td></tr></table></figure>

<h3 id="Slice-切片"><a href="#Slice-切片" class="headerlink" title="Slice(切片)"></a>Slice(切片)</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">my_list = list(range(<span class="number">20</span>)) </span><br><span class="line">my_list[<span class="number">0</span>:<span class="number">5</span>],my_list[<span class="number">5</span>:<span class="number">10</span>],my_list[:<span class="number">10</span>],my_list[<span class="number">10</span>:]</span><br><span class="line">my_list[<span class="number">-5</span>:<span class="number">-1</span>],my_list[<span class="number">-10</span>:],my_list[:<span class="number">-10</span>]</span><br><span class="line">my_list[<span class="number">1</span>:<span class="number">10</span>:<span class="number">3</span>],my_list[:],my_list[::<span class="number">3</span>]</span><br><span class="line"></span><br><span class="line">my_tuple = tuple(list(range(<span class="number">20</span>)))  <span class="comment"># 元组也可以使用切片操作</span></span><br><span class="line">my_tuple[<span class="number">3</span>:<span class="number">-10</span>:<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">my_str = <span class="string">"hello world"</span>  <span class="comment"># 字符串可以看成一个list</span></span><br><span class="line">my_str[<span class="number">3</span>:<span class="number">5</span>]</span><br><span class="line">my_str[<span class="number">3</span>:<span class="number">-2</span>]</span><br><span class="line">my_str[<span class="number">3</span>:<span class="number">-2</span>:<span class="number">2</span>]</span><br></pre></td></tr></table></figure>

<h3 id="generator-生成器"><a href="#generator-生成器" class="headerlink" title="generator(生成器)"></a>generator(生成器)</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">g = (x <span class="keyword">for</span> x <span class="keyword">in</span> range(<span class="number">1</span>,<span class="number">11</span>))</span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">fib</span><span class="params">(max_num)</span>:</span></span><br><span class="line">    i,j = <span class="number">1</span>,<span class="number">1</span></span><br><span class="line">    <span class="keyword">yield</span> i</span><br><span class="line">    <span class="keyword">while</span> j&lt;max_num:</span><br><span class="line">        <span class="keyword">yield</span> j</span><br><span class="line">        i,j = j,i+j</span><br><span class="line"><span class="keyword">for</span> num <span class="keyword">in</span> fib(<span class="number">10</span>):</span><br><span class="line">    print(num)</span><br></pre></td></tr></table></figure>

<h2 id="numpy入门"><a href="#numpy入门" class="headerlink" title="numpy入门"></a>numpy入门</h2><h3 id="基础知识"><a href="#基础知识" class="headerlink" title="基础知识"></a>基础知识</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 初始化nparray</span></span><br><span class="line">my_array = np.array([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>]) <span class="comment"># list初始化</span></span><br><span class="line">np.arange(<span class="number">10</span>)										 <span class="comment"># 自动生成</span></span><br><span class="line">np.linspace(<span class="number">1</span>,<span class="number">19</span>,<span class="number">10</span>,endpoint=<span class="literal">False</span>) <span class="comment"># 线性函数</span></span><br><span class="line">np.zeros(<span class="number">20</span>,np.int) <span class="comment"># 0</span></span><br><span class="line">np.ones(<span class="number">20</span>,np.int)  <span class="comment"># 1</span></span><br><span class="line">np.random.randn(<span class="number">10</span>) <span class="comment"># 随机数</span></span><br><span class="line">a = np.arange(<span class="number">1</span>,<span class="number">10</span>,<span class="number">1</span>) <span class="comment"># 间隔</span></span><br><span class="line">b = np.arange(<span class="number">24</span>).reshape(<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>) <span class="comment"># 向量变为矩阵</span></span><br><span class="line">b[<span class="number">1</span>,<span class="number">1</span>,<span class="number">3</span>],b[<span class="number">0</span>,<span class="number">1</span>,:],b[<span class="number">0</span>,<span class="number">2</span>] <span class="comment"># 访问矩阵</span></span><br><span class="line">c = np.arange(<span class="number">1</span>,<span class="number">20</span>,<span class="number">1</span>)</span><br><span class="line">c[c&gt;=<span class="number">15</span>]	<span class="comment"># 条件过滤</span></span><br><span class="line">c[~(c&gt;=<span class="number">15</span>)]</span><br><span class="line">c[(c&gt;=<span class="number">5</span>)&amp;(c&lt;=<span class="number">15</span>)]</span><br><span class="line"></span><br><span class="line">mat1 = [[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>],[<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>]] </span><br><span class="line">np.mat(mat1)		<span class="comment"># 初始化矩阵</span></span><br><span class="line">np.mat(mat1)*<span class="number">8</span>  <span class="comment"># 矩阵运算</span></span><br><span class="line">mat1*mat2       <span class="comment"># 乘法</span></span><br><span class="line">mat2.T          <span class="comment"># 转秩矩阵</span></span><br><span class="line">np.mat(np.zeros((<span class="number">3</span>,<span class="number">3</span>))) <span class="comment"># 3*3 0矩阵</span></span><br><span class="line">mat6 = np.mat(np.eye(<span class="number">2</span>,<span class="number">2</span>,dtype=int)) <span class="comment"># 对称矩阵</span></span><br><span class="line">a1 = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>]</span><br><span class="line">a2 = np.mat(np.diag(a1)) <span class="comment"># 对角矩阵</span></span><br></pre></td></tr></table></figure>

<h3 id="项目实战"><a href="#项目实战" class="headerlink" title="项目实战"></a>项目实战</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">data = pd.read_csv(<span class="string">'/home/admin/jupyter/download/ai/python/data/1/test.csv'</span>)</span><br><span class="line">c = data[<span class="string">'摄氏度'</span>]</span><br><span class="line">v = data[<span class="string">'销量'</span>]</span><br><span class="line"></span><br><span class="line">vmap = np.average(c,weights = v) <span class="comment"># 加权平均数</span></span><br><span class="line">mean = np.mean(c) <span class="comment"># 平均数</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 时间加权平均价格，TWAP，属于另一种平均价格的指标，近期的价格给以较高的权重</span></span><br><span class="line">t = np.arange(len(c))</span><br><span class="line">twap = np.average(c,weights=t)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 最大值和最小值</span></span><br><span class="line"><span class="comment"># 最高价的最大值和最低价的最小值</span></span><br><span class="line">h,l = np.loadtxt(<span class="string">'data.csv'</span>,delimiter=<span class="string">','</span>,usecols=(<span class="number">4</span>,<span class="number">5</span>),unpack=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">highest = np.max(h)</span><br><span class="line">lowest = np.min(l)</span><br><span class="line"></span><br><span class="line"><span class="comment">#ptp函数可以计算数组的取值范围，返回数组元素的最大值和最小值之间的差值</span></span><br><span class="line">spread_high_price = np.ptp(h)</span><br><span class="line">spread_low_price = np.ptp(l)</span><br><span class="line"></span><br><span class="line"><span class="comment">#简单统计分析</span></span><br><span class="line"><span class="comment">#中位数</span></span><br><span class="line">median = np.median(c)</span><br><span class="line"></span><br><span class="line"><span class="comment">#方差</span></span><br><span class="line">variance = np.var(c)</span><br><span class="line"></span><br><span class="line"><span class="comment">#计算简单的收益率</span></span><br><span class="line"><span class="comment">#简单收益率指的是相邻两个价格的变化率，对数收益率，指的是所有价格取对数后两两之间的差值</span></span><br><span class="line">c = np.loadtxt(<span class="string">'data.csv'</span>,delimiter=<span class="string">','</span>,usecols=(<span class="number">6</span>,),unpack=<span class="literal">True</span>)</span><br><span class="line">returns = np.diff(c)/c[:<span class="number">-1</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment">#用std函数计算标准差</span></span><br><span class="line">std_deviation = np.std(returns)</span><br><span class="line"></span><br><span class="line"><span class="comment">#计算对数收益率</span></span><br><span class="line">logreturns = np.diff(np.log(c))</span><br><span class="line"></span><br><span class="line"><span class="comment">#波动率</span></span><br><span class="line">annual_volatility = np.std(logreturns)/np.mean(logreturns)</span><br><span class="line">annual_volatility = annual_volatility/np.sqrt(<span class="number">1.</span>/<span class="number">252.</span>)</span><br></pre></td></tr></table></figure>

<h2 id="Pandas数据处理"><a href="#Pandas数据处理" class="headerlink" title="Pandas数据处理"></a>Pandas数据处理</h2><h3 id="Pandas入门"><a href="#Pandas入门" class="headerlink" title="Pandas入门"></a>Pandas入门</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> pandas <span class="keyword">import</span> Series,DataFrame</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="comment"># 列表创建</span></span><br><span class="line">s1 = pd.Series([<span class="number">157</span>,<span class="number">80</span>,<span class="number">167</span>,<span class="number">90</span>])</span><br><span class="line">s2 = pd.Series([<span class="number">100</span>,<span class="number">78</span>,<span class="number">65</span>,<span class="number">77</span>],index = [<span class="string">"chinese"</span>,<span class="string">"english"</span>,<span class="string">"history"</span>,<span class="string">"maths"</span>]   )</span><br><span class="line"><span class="comment"># dict创建</span></span><br><span class="line">d1 = &#123;<span class="string">"name"</span>:<span class="string">"张三"</span>,<span class="string">"Gender"</span>:<span class="string">"男"</span>,<span class="string">"age"</span>:<span class="number">20</span>,<span class="string">"height"</span>:<span class="number">180</span>,<span class="string">"weight"</span>:<span class="number">66</span>&#125;</span><br><span class="line">s3 = pd.Series(d1)</span><br><span class="line"><span class="comment"># dataframe</span></span><br><span class="line">dfPerson = &#123;</span><br><span class="line">    <span class="string">"name"</span>:[<span class="string">"tom"</span>,<span class="string">"jack"</span>,<span class="string">"kitty"</span>,<span class="string">"eric"</span>],</span><br><span class="line">    <span class="string">"age"</span>:[<span class="number">20</span>,<span class="number">19</span>,<span class="number">21</span>,<span class="number">22</span>],</span><br><span class="line">    <span class="string">"height"</span>:[<span class="number">180</span>,<span class="number">178</span>,<span class="number">170</span>,<span class="number">182</span>],</span><br><span class="line">    <span class="string">"weight"</span>:[<span class="number">66</span>,<span class="number">65</span>,<span class="number">52</span>,<span class="number">75</span>]</span><br><span class="line">&#125;</span><br><span class="line">personSheet = pd.DataFrame(dfPerson)</span><br><span class="line">personSheet.head()</span><br><span class="line"><span class="comment"># numpy创建dataframe</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">numframe = np.random.randn(<span class="number">10</span>,<span class="number">5</span>)</span><br><span class="line">numSheet = pd.DataFrame(numframe)</span><br></pre></td></tr></table></figure>

<figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 1.1 常见操作</span></span><br><span class="line">shape()可以获取excel文件的行和列，以元组形式返回；</span><br><span class="line"></span><br><span class="line">info()获取数据类型；</span><br><span class="line"></span><br><span class="line">isnull()判断哪个值是缺失值；</span><br><span class="line"></span><br><span class="line">head()传入的参数代表获取前几行；</span><br><span class="line"></span><br><span class="line">describe()掌握数值的分布情况，如均值，最值，方差，分位数；</span><br><span class="line"></span><br><span class="line">drop()是否将原索引删掉，设置参数inplace来确认是否修改原数据表；</span><br><span class="line"></span><br><span class="line">dropna()删除有缺失值的行，返回删除后的数据，传入参数how=all，要全为空值才会删除；</span><br><span class="line"></span><br><span class="line">fillna()括号内可直接填入要填充的值，也可指定列填充，以字典形式传参；</span><br><span class="line"></span><br><span class="line"><span class="comment"># 2.2 排序</span></span><br><span class="line">dataSort = pd.Series(range(<span class="number">5</span>),index=[<span class="string">'b'</span>,<span class="string">'a'</span>,<span class="string">'e'</span>,<span class="string">'c'</span>,<span class="string">'d'</span>])</span><br><span class="line">dataSort.sort_index()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 直接写入参数test_dict</span></span><br><span class="line">test_dict = &#123;<span class="string">'id'</span>:[<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>],<span class="string">'name'</span>:[<span class="string">'Alice'</span>,<span class="string">'Bob'</span>,<span class="string">'Cindy'</span>,<span class="string">'Eric'</span>,<span class="string">'Helen'</span>,<span class="string">'Grace '</span>],<span class="string">'math'</span>:[<span class="number">90</span>,<span class="number">89</span>,<span class="number">99</span>,<span class="number">78</span>,<span class="number">97</span>,<span class="number">93</span>],<span class="string">'english'</span>:[<span class="number">89</span>,<span class="number">94</span>,<span class="number">80</span>,<span class="number">94</span>,<span class="number">94</span>,<span class="number">90</span>]&#125;</span><br><span class="line">personInfo = pd.DataFrame(test_dict)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 合并dataframe</span></span><br><span class="line">personInfo2 = pd.DataFrame(&#123;<span class="string">'Gender'</span>:&#123;<span class="number">0</span>:<span class="string">'男'</span>,<span class="number">1</span>:<span class="string">'男'</span>,<span class="number">2</span>:<span class="string">'男'</span>,<span class="number">3</span>:<span class="string">'男'</span>,<span class="number">4</span>:<span class="string">'男'</span>,<span class="number">5</span>:<span class="string">'男'</span>&#125;&#125;)       </span><br><span class="line">personInfo = pd.concat([personInfo,personInfo2],axis=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>

<h3 id="项目实战-1"><a href="#项目实战-1" class="headerlink" title="项目实战"></a>项目实战</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">data = pd.read_csv(<span class="string">'/home/admin/jupyter/download/tianchi_learn_project/data/1/titanic_train.csv'</span>)</span><br><span class="line">data.head()</span><br><span class="line">data.loc[<span class="number">5</span>:<span class="number">10</span>,[<span class="string">'Name'</span>,<span class="string">'Sex'</span>,<span class="string">'Survived'</span>]] <span class="comment"># 取5-10行的数据</span></span><br><span class="line">data.Survived.value_counts()						 <span class="comment"># survived数值分布</span></span><br><span class="line">data.Pclass.value_counts()</span><br><span class="line">data.isnull().sum()											 <span class="comment"># 空行数</span></span><br><span class="line"></span><br><span class="line">data.groupby([<span class="string">'Sex'</span>,<span class="string">'Survived'</span>])[<span class="string">'Survived'</span>].count() <span class="comment"># 按性别分组，并统计存储存活数</span></span><br><span class="line"></span><br><span class="line">pd.crosstab(data.Pclass,data.Survived,margins=<span class="literal">True</span>) <span class="comment"># 交叉列表取值</span></span><br><span class="line">pd.crosstab([data.Embarked,data.Pclass],[data.Sex,data.Survived],margins=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line">data.isnull().sum()</span><br><span class="line">data[<span class="string">'Age'</span>] = data[<span class="string">'Age'</span>].fillna(data[<span class="string">'Age'</span>].mean()) <span class="comment"># 填充空值</span></span><br></pre></td></tr></table></figure>

<h2 id="Matplotlib数据可视化分析"><a href="#Matplotlib数据可视化分析" class="headerlink" title="Matplotlib数据可视化分析"></a>Matplotlib数据可视化分析</h2><h3 id="基础知识-1"><a href="#基础知识-1" class="headerlink" title="基础知识"></a>基础知识</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="comment"># 散点图</span></span><br><span class="line">x1 = np.array([<span class="number">6</span>])</span><br><span class="line">y1 = np.array([<span class="number">4</span>])</span><br><span class="line">plt.scatter(x1,y1)</span><br><span class="line">plt.show</span><br><span class="line"><span class="comment"># 线图</span></span><br><span class="line">x2 = np.array([<span class="number">2</span>,<span class="number">6</span>])</span><br><span class="line">y2 = np.array([<span class="number">2</span>,<span class="number">4</span>])</span><br><span class="line">plt.plot(x2,y2)</span><br><span class="line">plt.show</span><br><span class="line"><span class="comment"># 两张画布</span></span><br><span class="line">plt.figure()</span><br><span class="line">plt.scatter(x1,y1,color=<span class="string">"blue"</span>)</span><br><span class="line">plt.figure()</span><br><span class="line">plt.plot(x2,y2,color=<span class="string">"red"</span>,linestyle=<span class="string">'--'</span>)</span><br><span class="line"><span class="comment"># 横纵坐标范围</span></span><br><span class="line">plt.xlim(<span class="number">0</span>,<span class="number">8</span>)</span><br><span class="line">plt.ylim(<span class="number">0</span>,<span class="number">5</span>)</span><br><span class="line"><span class="comment"># 制定横纵坐标的刻度，并替换</span></span><br><span class="line">x_ticks = [<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">5</span>,<span class="number">6</span>]</span><br><span class="line">plt.xticks(x_ticks)</span><br><span class="line">plt.yticks([<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">5</span>,<span class="number">6</span>],[<span class="string">'one'</span>,<span class="string">'two'</span>,<span class="string">'three'</span>,<span class="string">'four'</span>,<span class="string">'five'</span>])</span><br><span class="line"><span class="comment"># 横纵坐标的标记</span></span><br><span class="line">plt.xlabel(<span class="string">"i am x"</span>,fontsize=<span class="number">15</span>)</span><br><span class="line">plt.ylabel(<span class="string">"i am y"</span>,fontsize=<span class="number">15</span>)</span><br><span class="line"><span class="comment"># 图例</span></span><br><span class="line">plt.legend(loc=<span class="string">'lower right'</span>)</span><br><span class="line"><span class="comment"># 子图（用于在一个画图上，画多个图）</span></span><br><span class="line">plt.subplot(<span class="number">2</span>,<span class="number">2</span>,<span class="number">1</span>) <span class="comment"># 2行1列，位置第一个</span></span><br></pre></td></tr></table></figure>

<h3 id="画图"><a href="#画图" class="headerlink" title="画图"></a>画图</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 散点图</span></span><br><span class="line">n = <span class="number">1024</span></span><br><span class="line">x2 = np.random.normal(<span class="number">0</span>,<span class="number">1</span>,n)</span><br><span class="line">y2 = np.random.normal(<span class="number">0</span>,<span class="number">1</span>,n)</span><br><span class="line">plt.scatter(x2,y2,s=<span class="number">80</span>,alpha=<span class="number">0.5</span>)</span><br><span class="line">plt.show()</span><br><span class="line"><span class="comment"># 折线图</span></span><br><span class="line">x3_1 = np.array([<span class="string">'2014'</span>,<span class="string">'2015'</span>,<span class="string">'2016'</span>,<span class="string">'2017'</span>,<span class="string">'2018'</span>])</span><br><span class="line">y3_1 = np.array([<span class="number">12312</span>,<span class="number">12321</span>,<span class="number">45345</span>,<span class="number">54565</span>,<span class="number">23432</span>])</span><br><span class="line">plt.title(<span class="string">"visitors"</span>,fontsize=<span class="number">15</span>)</span><br><span class="line">plt.xlabel(<span class="string">"year"</span>,fontsize=<span class="number">15</span>)</span><br><span class="line">plt.ylabel(<span class="string">"num(10 thousand)"</span>,fontsize=<span class="number">15</span>)</span><br><span class="line">plt.plot(x3_1,y3_1)</span><br><span class="line">plt.scatter(x3_1,y3_1)</span><br><span class="line">plt.show()</span><br><span class="line"><span class="comment"># 柱形图</span></span><br><span class="line">x4 = [<span class="string">'2014'</span>,<span class="string">'2015'</span>,<span class="string">'2016'</span>,<span class="string">'2017'</span>,<span class="string">'2018'</span>]</span><br><span class="line">y4 = [<span class="number">200</span>,<span class="number">300</span>,<span class="number">150</span>,<span class="number">350</span>,<span class="number">800</span>]</span><br><span class="line">rects = plt.bar(x4,height=y4,width=<span class="number">0.4</span>,alpha=<span class="number">0.8</span>,color=<span class="string">'g'</span>)</span><br><span class="line">plt.bar(x4,height=y4,width=<span class="number">0.4</span>,alpha=<span class="number">0.8</span>,color=<span class="string">'g'</span>)</span><br><span class="line">plt.ylim(<span class="number">0</span>,<span class="number">900</span>)</span><br><span class="line">plt.ylabel(<span class="string">'sale(10 thousand)'</span>,fontsize=<span class="number">15</span>)</span><br><span class="line">plt.xlabel(<span class="string">'year'</span>,fontsize=<span class="number">15</span>)</span><br><span class="line">plt.title(<span class="string">'income'</span>,fontsize=<span class="number">15</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 标出数字</span></span><br><span class="line"><span class="keyword">for</span> rect <span class="keyword">in</span> rects:</span><br><span class="line">    height = rect.get_height()</span><br><span class="line">    plt.text(rect.get_x() + rect.get_width() / <span class="number">2</span>,height + <span class="number">10</span>,str(height),ha=<span class="string">"center"</span>)</span><br><span class="line">    </span><br><span class="line">plt.show()</span><br><span class="line"><span class="comment"># 直方图</span></span><br><span class="line">scoresT1 = np.random.randint(<span class="number">0</span>,<span class="number">100</span>,<span class="number">100</span>)</span><br><span class="line">scoresT2 = np.random.randint(<span class="number">0</span>,<span class="number">100</span>,<span class="number">100</span>)</span><br><span class="line">x = [scoresT1,scoresT2]</span><br><span class="line">colors = [<span class="string">"g"</span>,<span class="string">"b"</span>]</span><br><span class="line">labels = [<span class="string">"class A"</span>,<span class="string">"class B"</span>]</span><br><span class="line">bins =range(<span class="number">0</span>,<span class="number">101</span>,<span class="number">10</span>)</span><br><span class="line">list(bins)</span><br><span class="line">plt.hist(x,bins=bins,color=colors,histtype=<span class="string">"bar"</span>,label=labels)</span><br><span class="line">plt.legend(loc=<span class="string">"upper left"</span>)</span><br><span class="line">plt.show()</span><br><span class="line"><span class="comment"># 饼图</span></span><br><span class="line">labels = [<span class="string">'one'</span>,<span class="string">'two'</span>,<span class="string">'three'</span>,<span class="string">'four'</span>]</span><br><span class="line">sizes = [<span class="number">2332</span>,<span class="number">3413</span>,<span class="number">23423</span>,<span class="number">79832</span>]</span><br><span class="line">colors = [<span class="string">'r'</span>,<span class="string">'g'</span>,<span class="string">'y'</span>,<span class="string">'b'</span>]</span><br><span class="line">explode = (<span class="number">0.1</span>,<span class="number">0</span>,<span class="number">0</span>,<span class="number">0</span>)</span><br><span class="line">plt.pie(sizes,labels=labels,colors=colors,explode=explode,shadow=<span class="literal">True</span>,radius=<span class="number">1.0</span>,autopct=<span class="string">"%1.1F%%"</span>)   </span><br><span class="line">plt.title(<span class="string">"支付方式"</span>)</span><br><span class="line">plt.show()</span><br><span class="line"><span class="comment"># 3D图</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> mpl_toolkits.mplot3d <span class="keyword">import</span> Axes3D</span><br><span class="line"></span><br><span class="line">fig = plt.figure()</span><br><span class="line">ax = Axes3D(fig)</span><br><span class="line"></span><br><span class="line">X = np.arange(<span class="number">-4</span>,<span class="number">4</span>,<span class="number">0.25</span>)</span><br><span class="line">Y = np.arange(<span class="number">-4</span>,<span class="number">4</span>,<span class="number">0.25</span>)</span><br><span class="line">X,Y = np.meshgrid(X,Y)</span><br><span class="line">Z = X ** <span class="number">2</span> + Y ** <span class="number">2</span></span><br><span class="line">ax.scatter(X,Y,Z)</span><br><span class="line"></span><br><span class="line">ax.plot_surface(X,Y,Z,rstride=<span class="number">1</span>,cstride=<span class="number">2</span>,cmap=plt.get_cmap(<span class="string">'rainbow'</span>))</span><br><span class="line"></span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment">### 案例2-商户支付数据分析案例</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">data = pd.read_csv(<span class="string">"data.csv"</span>,encoding=<span class="string">"gbk"</span>)</span><br><span class="line">data[<span class="string">'商户名称'</span>]=<span class="string">"隐藏商户名"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 最近10天支付总额和订单数统计</span></span><br><span class="line">plt.figure(figsize=(<span class="number">15</span>,<span class="number">8</span>))</span><br><span class="line">data.dropna(axis=<span class="number">0</span>,how=<span class="string">'any'</span>,inplance=<span class="literal">True</span>) <span class="comment"># 填充空数值</span></span><br><span class="line">data[<span class="string">'支付时间'</span>] = pd.to_datetime(data[<span class="string">'支付时间'</span>]) <span class="comment"># 支付时间string to date</span></span><br><span class="line">data[<span class="string">'支付日期'</span>] = [x.strftime(<span class="string">"%Y-%m-%d"</span>) <span class="keyword">for</span> x <span class="keyword">in</span> data[<span class="string">'支付时间'</span>]]</span><br><span class="line">data2 = data.groupby([<span class="string">'支付日期'</span>])[<span class="string">'金额'</span>].sum() <span class="comment"># 按照时间，对金额进行汇总</span></span><br><span class="line">x4 = data2.index[<span class="number">-10</span>:]</span><br><span class="line">y4 = data2[<span class="number">-10</span>:]</span><br><span class="line"></span><br><span class="line">rects1 = plt.bar(x4,height=y4,width=<span class="number">0.4</span>,alpha=<span class="number">0.8</span>,color=<span class="string">'g'</span>)</span><br><span class="line">plt.ylabel(<span class="string">"订单日期"</span>,fontsize=<span class="number">15</span>)</span><br><span class="line">plt.xlabel(<span class="string">"日期"</span>,fontsize=<span class="number">15</span>)</span><br><span class="line">plt.title(<span class="string">"最近10天支付总额"</span>,fontsize=<span class="number">15</span>)</span><br><span class="line"><span class="keyword">for</span> rect <span class="keyword">in</span> rects1:</span><br><span class="line">    height = rect.get_height()</span><br><span class="line">    plt.text( rect.get_x() + rect.get_width() / <span class="number">2</span>, height + <span class="number">1000</span>, str(height), ha=<span class="string">"center"</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment">### 时间段统计</span></span><br><span class="line">fig = plt.figure(figsize=(<span class="number">10</span>,<span class="number">6</span>))</span><br><span class="line">data[<span class="string">'几点'</span>] = [x.strftime(<span class="string">'%H'</span>) <span class="keyword">for</span> x <span class="keyword">in</span> data[<span class="string">'支付时间'</span>]]</span><br><span class="line"></span><br><span class="line">data5 = data.groupby(<span class="string">'几点'</span>)[<span class="string">'金额'</span>].size()</span><br><span class="line">x5 = data5.index</span><br><span class="line">y5 = data5</span><br><span class="line"></span><br><span class="line">plt.bar(x5,y5)</span><br><span class="line">plt.xlabel(<span class="string">"时间段"</span>,fontsize=<span class="number">15</span>)</span><br><span class="line">plt.ylabel(<span class="string">"订单数"</span>,fontsize=<span class="number">15</span>)</span><br><span class="line">plt.title(<span class="string">"时间段统计"</span>,fontsize=<span class="number">15</span>)</span><br><span class="line">plt.show()</span><br></pre></td></tr></table></figure>

<h2 id="数据分析"><a href="#数据分析" class="headerlink" title="数据分析"></a>数据分析</h2><h3 id="基础知识-2"><a href="#基础知识-2" class="headerlink" title="基础知识"></a>基础知识</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">缺失数据</span><br><span class="line">数据缺失的原因</span><br><span class="line">    <span class="number">1.</span>无意的</span><br><span class="line">    <span class="number">2.</span>有意的</span><br><span class="line">    <span class="number">3.</span>不存在</span><br><span class="line">数据缺失的类型</span><br><span class="line">    完全随机缺失（mcar）</span><br><span class="line">    随机缺失（mar）</span><br><span class="line">    非随机缺失（mnar）</span><br><span class="line">数据缺失的处理方法</span><br><span class="line">    删除记录</span><br><span class="line">    数据填补的方法</span><br><span class="line">        替换缺失值</span><br><span class="line">        拟合缺失值</span><br><span class="line">    虚拟变量</span><br><span class="line">    不处理</span><br><span class="line">    </span><br><span class="line">    </span><br><span class="line"><span class="comment"># 导入数据包</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="comment"># 常见缺失数据</span></span><br><span class="line">dict = &#123;</span><br><span class="line">    <span class="string">'one'</span>:[<span class="number">100</span>,<span class="number">90</span>,np.nan,<span class="number">95</span>],</span><br><span class="line">    <span class="string">'two'</span>:[<span class="number">30</span>,<span class="number">45</span>,<span class="number">56</span>,np.nan],</span><br><span class="line">    <span class="string">'three'</span>:[np.nan,<span class="number">40</span>,<span class="number">80</span>,<span class="number">90</span>]</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">df = pd.DataFrame(dict)</span><br><span class="line"><span class="comment"># 判断数据是否不为空</span></span><br><span class="line">df.notnull()</span><br><span class="line"><span class="comment"># 选择不为空的数据</span></span><br><span class="line">bool_series = pd.isnull(df[<span class="string">"one"</span>])</span><br><span class="line">df[bool_series]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 缺失数据展示</span></span><br><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> missingno <span class="keyword">as</span> msno</span><br><span class="line"><span class="comment"># 产生缺失数据集</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_random_nan_list</span><span class="params">(list_data,num=<span class="number">100</span>)</span>:</span></span><br><span class="line">    p = np.random.uniform(<span class="number">0</span>,<span class="number">1</span>,size=(len(list_data))) <span class="comment">#随机采样</span></span><br><span class="line">    p = p / np.sum(p)</span><br><span class="line">    data = np.random.choice(list_data,num,p=p) <span class="comment"># p为取每个元素的概率</span></span><br><span class="line">    <span class="keyword">return</span> data</span><br><span class="line">dict = &#123;</span><br><span class="line">    <span class="string">'First Score'</span>:get_random_nan_list([<span class="number">100</span>,np.nan,np.nan,<span class="number">95</span>]),</span><br><span class="line">    <span class="string">'Second Score'</span>:get_random_nan_list([<span class="number">30</span>,np.nan,<span class="number">45</span>,<span class="number">56</span>]),</span><br><span class="line">    <span class="string">'Third Score'</span>:get_random_nan_list([<span class="number">52</span>,np.nan,<span class="number">80</span>,<span class="number">98</span>]),</span><br><span class="line">    <span class="string">'Fourth Score'</span>:get_random_nan_list([<span class="number">60</span>,<span class="number">67</span>,<span class="number">68</span>,<span class="number">65</span>]),</span><br><span class="line">&#125;</span><br><span class="line"></span><br><span class="line">df = pd.DataFrame(dict)</span><br><span class="line"></span><br><span class="line"><span class="comment">## 填充</span></span><br><span class="line"><span class="comment"># 用0填充缺失值</span></span><br><span class="line">df.fillna(<span class="number">0</span>)</span><br><span class="line"><span class="comment"># 某一列用特定的数据填充</span></span><br><span class="line">df[<span class="string">'Second Score'</span>].fillna(np.mean(df[<span class="string">'Second Score'</span>]))</span><br><span class="line"><span class="comment"># 采用replace方法填充</span></span><br><span class="line">df.replace(to_replace=np.nan,value=<span class="number">-99</span>)</span><br><span class="line"><span class="comment"># 采用线性插值填充</span></span><br><span class="line">df.interpolate(method=<span class="string">'linear'</span>,limit_direction=<span class="string">'forward'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">## 删除</span></span><br><span class="line"><span class="comment"># 删除带有nan的数据</span></span><br><span class="line">df.dropna()</span><br><span class="line"><span class="comment"># 删除全部都是nan的数据</span></span><br><span class="line">df.dropna(how=<span class="string">'all'</span>)</span><br><span class="line"><span class="comment"># 删除某列至少有一个nan的数据</span></span><br><span class="line">df.dropna(axis=<span class="number">1</span>)</span><br><span class="line"><span class="comment"># 删除行中至少又一个nan的数据</span></span><br><span class="line">df.dropna(axis=<span class="number">0</span>,how=<span class="string">'any'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment">## sklearn</span></span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> Imputer</span><br><span class="line">imp = Imputer(missing_values=<span class="string">'NaN'</span>,strategy=<span class="string">'mean'</span>,axis=<span class="number">0</span>)</span><br><span class="line">X = np.array([[<span class="number">1</span>,<span class="number">2</span>],[np.nan,<span class="number">3</span>],[<span class="number">7</span>,<span class="number">6</span>]])</span><br><span class="line">Y = [[np.nan,<span class="number">2</span>],[<span class="number">6</span>,np.nan],[<span class="number">7</span>,<span class="number">6</span>]]</span><br><span class="line">imp.fit(X)</span><br><span class="line">imp.transform(Y)</span><br></pre></td></tr></table></figure>

<h3 id="数据分析实战一"><a href="#数据分析实战一" class="headerlink" title="数据分析实战一"></a>数据分析实战一</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line">train = pd.read_csv(<span class="string">'/home/admin/jupyter/download/tianchi_learn_project/data/1/titanic_train.csv'</span>)</span><br><span class="line">test = pd.read_csv(<span class="string">'/home/admin/jupyter/download/tianchi_learn_project/data/1/titanic_test.csv'</span>)</span><br><span class="line"></span><br><span class="line">%matplotlib inline</span><br><span class="line"><span class="comment"># 导入缺失值数值图像展示的包</span></span><br><span class="line"><span class="keyword">import</span> missingno <span class="keyword">as</span> msno</span><br><span class="line"></span><br><span class="line">msno.matrix(train,labels=<span class="literal">True</span>) <span class="comment"># 很有用，一眼可以看出数据的缺失情况</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 缺失值处理</span></span><br><span class="line">train.fillna(<span class="number">0</span>)</span><br><span class="line">test.fillna(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 用前一个值进行填充</span></span><br><span class="line">train.fillna(method=<span class="string">'pad'</span>)</span><br><span class="line"><span class="comment"># 后一个值</span></span><br><span class="line">train.fillna(method=<span class="string">'bfill'</span>)</span><br><span class="line"></span><br><span class="line">train = train.fillna(method=<span class="string">'pad'</span>,inplace=<span class="literal">True</span>) <span class="comment"># 用前一个值填充</span></span><br><span class="line">train[<span class="string">'Age'</span>] = train[<span class="string">'Age'</span>].fillna(np.mean(train[<span class="string">'Age'</span>])) <span class="comment"># 用平均值填充</span></span><br><span class="line">test[<span class="string">'Age'</span>] = test[<span class="string">'Age'</span>].fillna(np.mean(train[<span class="string">'Age'</span>])) <span class="comment"># 用平均值填充</span></span><br><span class="line">train.replace(to_replace=np.nan,value=<span class="number">-99</span>) <span class="comment"># 用指定值来进行替换</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 采用线性插值填充</span></span><br><span class="line"><span class="comment"># 采用线性插值填充</span></span><br><span class="line">train.interpolate(method=<span class="string">'linear'</span>,limit_direction=<span class="string">'forward'</span>)</span><br><span class="line"><span class="comment"># 删除缺失值</span></span><br><span class="line">train.dropna()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用sklearn的Imputer方法填充</span></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> Imputer</span><br><span class="line">imp = Imputer(missing_values=<span class="string">'NaN'</span>,strategy=<span class="string">'mean'</span>,axis=<span class="number">0</span>)</span><br><span class="line">imp.fit(train[<span class="string">'Age'</span>].values.reshape(<span class="number">-1</span>,<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">train_age = imp.transform(train[<span class="string">'Age'</span>].values.reshape(<span class="number">-1</span>,<span class="number">1</span>))</span><br><span class="line">test_age = imp.transform(test[<span class="string">'Age'</span>].values.reshape(<span class="number">-1</span>,<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 取大于所有的99%的数</span></span><br><span class="line">move_data = [np.percentile(train[<span class="string">'Fare'</span>],<span class="number">99</span>) <span class="keyword">if</span> d &gt; np.percentile(train[<span class="string">'Fare'</span>],<span class="number">99</span>) <span class="keyword">else</span> d <span class="keyword">for</span> d <span class="keyword">in</span> train.Fare]</span><br><span class="line">train.Fare.hist(bins=<span class="number">32</span>)</span><br><span class="line"></span><br><span class="line">move_data = [np.percentile(train[<span class="string">'Fare'</span>],<span class="number">99</span>) <span class="keyword">if</span> d &gt; np.percentile(train[<span class="string">'Fare'</span>],<span class="number">99</span>) <span class="keyword">else</span> d <span class="keyword">for</span> d <span class="keyword">in</span> train.Fare]</span><br><span class="line">plt.hist(move_data,bins=<span class="number">100</span>)</span><br><span class="line"></span><br><span class="line">np.max(move_data) <span class="comment"># 求最大值</span></span><br></pre></td></tr></table></figure>

<h3 id="数据特征分析概要"><a href="#数据特征分析概要" class="headerlink" title="数据特征分析概要"></a>数据特征分析概要</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line">统计分布分析</span><br><span class="line">	一维离散分布分析</span><br><span class="line">	一维连续分布分析</span><br><span class="line">	联合分布分析</span><br><span class="line">统计量分析</span><br><span class="line">	均值</span><br><span class="line">	中位数</span><br><span class="line">	众数</span><br><span class="line">	极差</span><br><span class="line">	标准差</span><br><span class="line">	变异系数</span><br><span class="line">	分位数</span><br><span class="line">  </span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> scipy <span class="keyword">import</span> stats</span><br><span class="line"></span><br><span class="line"><span class="comment"># 密度分布曲线</span></span><br><span class="line"><span class="comment"># 伯努利分布</span></span><br><span class="line">X = np.arange(<span class="number">0</span>,<span class="number">2</span>,<span class="number">1</span>)</span><br><span class="line">p1 = <span class="number">0.5</span></span><br><span class="line">plist1 = stats.bernoulli.pmf(X,p1)</span><br><span class="line">plt.plot(X,plist1,marker=<span class="string">'o'</span>,linestyle=<span class="string">'None'</span>)</span><br><span class="line">plt.vlines(X,<span class="number">0</span>,plist1)</span><br><span class="line"><span class="comment"># plt.xlabel('随机概率:抛硬币1次')</span></span><br><span class="line"><span class="comment"># plt.ylabel('概率')</span></span><br><span class="line"><span class="comment"># plt.title('伯努利分布：p1=%.2f' % p1)</span></span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line">n = <span class="number">5</span></span><br><span class="line">p2 = <span class="number">0.5</span></span><br><span class="line">X1 = np.arange(<span class="number">0</span>,n+<span class="number">1</span>,<span class="number">1</span>)</span><br><span class="line">plist2 = stats.binom.pmf(X1,n,p2)</span><br><span class="line">plt.plot(X1,plist2,marker=<span class="string">'o'</span>,linestyle=<span class="string">'None'</span>)</span><br><span class="line">plt.vlines(X1,<span class="number">0</span>,plist2)</span><br><span class="line"><span class="comment"># plt.xlabel('随机概率: 抛硬币正面朝上次数')</span></span><br><span class="line"><span class="comment"># plt.ylabel('概率')</span></span><br><span class="line"><span class="comment"># plt.title('二项分布：n=%i,p2=%.2f' % (n,p2))</span></span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用numpy和stats中的函数产生均值、方差、偏度、峰度</span></span><br><span class="line">x = np.random.randn(<span class="number">10000</span>)</span><br><span class="line">mu = np.mean(x,axis=<span class="number">0</span>)</span><br><span class="line">sigma = np.std(x,axis=<span class="number">0</span>)</span><br><span class="line">skew = stats.skew(x)</span><br><span class="line">kurtosis = stats.kurtosis(x)</span><br><span class="line"></span><br><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> statsmodels.tsa.seasonal <span class="keyword">import</span> seasonal_decompose</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> matplotlib.pylab <span class="keyword">import</span> rcParams</span><br><span class="line">rcParams[<span class="string">'figure.figsize'</span>] = <span class="number">15</span>,<span class="number">6</span></span><br><span class="line"></span><br><span class="line">data = pd.read_csv(<span class="string">'/home/admin/jupyter/download/ai/python/data/3/AirPassengers.csv'</span>)</span><br><span class="line">data[<span class="string">'Month'</span>] = pd.to_datetime(data[<span class="string">'Month'</span>])</span><br><span class="line">data = data.set_index([<span class="string">'Month'</span>]) <span class="comment"># 设置时间为索引</span></span><br><span class="line">data.head()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 分析时间序列</span></span><br><span class="line">decomposition = seasonal_decompose(data)</span><br><span class="line">trend = decomposition.trend</span><br><span class="line">seasonal = decomposition.seasonal</span><br><span class="line">resid = decomposition.resid</span><br><span class="line"><span class="comment"># 时间序列分解画面</span></span><br><span class="line">plt.subplot(<span class="number">411</span>)</span><br><span class="line">plt.plot(data,label=<span class="string">'Original'</span>)</span><br><span class="line">plt.subplot(<span class="number">412</span>)</span><br><span class="line">plt.plot(trend,label=<span class="string">'Trend'</span>)</span><br><span class="line">plt.subplot(<span class="number">413</span>)</span><br><span class="line">plt.plot(seasonal,label=<span class="string">'Seasonal'</span>)</span><br><span class="line">plt.subplot(<span class="number">414</span>)</span><br><span class="line">plt.plot(resid,label=<span class="string">'resid'</span>)</span><br><span class="line">plt.tight_layout()</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制C0特征和target的相关性</span></span><br><span class="line">fcols = <span class="number">2</span></span><br><span class="line">frows = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">8</span>,<span class="number">4</span>))</span><br><span class="line"></span><br><span class="line">ax = plt.subplot(<span class="number">1</span>,<span class="number">2</span>,<span class="number">1</span>)</span><br><span class="line">sns.regplot(x=df[<span class="string">'C0'</span>],y=<span class="string">'target'</span>,data=df,ax=ax,</span><br><span class="line">            scatter_kws=&#123;<span class="string">'marker'</span>:<span class="string">'.'</span>,<span class="string">'s'</span>:<span class="number">3</span>,<span class="string">'alpha'</span>:<span class="number">0.3</span>&#125;,</span><br><span class="line">            line_kws=&#123;<span class="string">'color'</span>:<span class="string">'k'</span>&#125;</span><br><span class="line">);</span><br><span class="line"></span><br><span class="line">plt.xlabel(<span class="string">'C0'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'target'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 绘制c0的直方图</span></span><br><span class="line">ax = plt.subplot(<span class="number">1</span>,<span class="number">2</span>,<span class="number">2</span>)</span><br><span class="line">sns.distplot(df[<span class="string">'C0'</span>].dropna())</span><br><span class="line">plt.xlabel(<span class="string">'C0'</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 相关性矩阵</span></span><br><span class="line">df.corr()</span><br><span class="line"><span class="comment"># 画出相关性热力图</span></span><br><span class="line">ax = plt.subplots(figsize=(<span class="number">20</span>,<span class="number">16</span>))</span><br><span class="line">ax = sns.heatmap(df.corr(), vmax=<span class="number">.8</span>, square=<span class="literal">True</span>, annot=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<h2 id="特征工程"><a href="#特征工程" class="headerlink" title="特征工程"></a>特征工程</h2><h3 id="基本特征工程"><a href="#基本特征工程" class="headerlink" title="基本特征工程"></a>基本特征工程</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">train = pd.read_csv(<span class="string">'/home/admin/jupyter/download/tianchi_learn_project/data/1/titanic_train.csv'</span>)</span><br><span class="line">test = pd.read_csv(<span class="string">'/home/admin/jupyter/download/tianchi_learn_project/data/1/titanic_test.csv'</span>)</span><br><span class="line">df_all = pd.concat([train,test])[train.columns]</span><br><span class="line"><span class="comment"># 基本特征工程</span></span><br><span class="line"><span class="comment"># 清理数据 txt_string中在substrings的留下，其他置为空</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">substrings_in_string</span><span class="params">(txt_string, substrings)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> substring <span class="keyword">in</span> substrings:</span><br><span class="line">        <span class="keyword">if</span> substring <span class="keyword">in</span> txt_string:</span><br><span class="line">            <span class="keyword">return</span> substring</span><br><span class="line">    <span class="keyword">return</span> np.nan</span><br><span class="line">  </span><br><span class="line">title_list = [<span class="string">'Mrs'</span>,<span class="string">'Mr'</span>,<span class="string">'Master'</span>,<span class="string">'Miss'</span>,<span class="string">'Major'</span>,<span class="string">'Rev'</span>,<span class="string">'Dr'</span>,</span><br><span class="line">              <span class="string">'Ms'</span>,<span class="string">'Mlle'</span>,<span class="string">'Col'</span>,<span class="string">'Capt'</span>,<span class="string">'Mme'</span>,<span class="string">'Countess'</span>,<span class="string">'Don'</span>,<span class="string">'Jonkheer'</span></span><br><span class="line">]</span><br><span class="line"></span><br><span class="line">df_all[<span class="string">'Title'</span>] = df_all[<span class="string">'Name'</span>].map(<span class="keyword">lambda</span> x:substrings_in_string(x,title_list))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 转换处理数据</span></span><br><span class="line">cabin_list = [<span class="string">'A'</span>,<span class="string">'B'</span>,<span class="string">'C'</span>,<span class="string">'D'</span>,<span class="string">'E'</span>,<span class="string">'F'</span>,<span class="string">'T'</span>,<span class="string">'G'</span>,<span class="string">'Unknown'</span>]</span><br><span class="line">df_all[<span class="string">'Deck'</span>] = df_all[<span class="string">'Cabin'</span>].map(<span class="keyword">lambda</span> x:substrings_in_string(str(x),cabin_list))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 人工抽取特征</span></span><br><span class="line">df_all[<span class="string">'Family_Size'</span>] = df_all[<span class="string">'SibSp'</span>] + df_all[<span class="string">'Parch'</span>]</span><br><span class="line">df_all[<span class="string">'Age*Class'</span>] = df_all[<span class="string">'Age'</span>] * df_all[<span class="string">'Pclass'</span>]</span><br><span class="line">df_all[<span class="string">'Fare_Per_Person'</span>] = df_all[<span class="string">'Fare'</span>] / (df_all[<span class="string">'Family_Size'</span>] + <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 删除无用特征</span></span><br><span class="line">def_col = [<span class="string">'Name'</span>,<span class="string">'Ticket'</span>,<span class="string">'Cabin'</span>]</span><br><span class="line"><span class="keyword">for</span> col <span class="keyword">in</span> del_col:</span><br><span class="line">    <span class="keyword">del</span> df_all[col]</span><br><span class="line">    </span><br><span class="line"><span class="comment"># 离散变量处理</span></span><br><span class="line"><span class="keyword">for</span> col <span class="keyword">in</span> [<span class="string">'Sex'</span>,<span class="string">'Embarked'</span>,<span class="string">'Title'</span>,<span class="string">'Deck'</span>]:</span><br><span class="line">    df_temp = pd.get_dummies(df_all[col])</span><br><span class="line">    df_temp.columns = [col+c <span class="keyword">for</span> c <span class="keyword">in</span> df_temp]</span><br><span class="line">    df_all = pd.concat([df_all, df_temp])</span><br><span class="line">    <span class="keyword">del</span> df_all[col]</span><br><span class="line">    </span><br><span class="line"><span class="comment"># 连续值归一化处理</span></span><br><span class="line"><span class="keyword">for</span> col <span class="keyword">in</span> [<span class="string">'Pclass'</span>,<span class="string">'Age'</span>,<span class="string">'Fare'</span>,<span class="string">'Age*Class'</span>,<span class="string">'Fare_Per_Person'</span>]:</span><br><span class="line">    df_all[col] = (df_all - np.min(df_all[col])) / (np.max(df_all[col]) - np.min(df_all[col]))</span><br></pre></td></tr></table></figure>

<h3 id="采样"><a href="#采样" class="headerlink" title="采样"></a>采样</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="comment"># 有放回采样 Random_Sampling</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">random_sampling_back</span><span class="params">(data, sample_num, p=<span class="number">0.5</span>)</span>:</span></span><br><span class="line">    sample_cnt = <span class="number">0</span></span><br><span class="line">    len_data = len(data)</span><br><span class="line">    sample_data = []</span><br><span class="line">    <span class="keyword">while</span>(<span class="number">1</span>):</span><br><span class="line">        <span class="keyword">for</span> d <span class="keyword">in</span> data:</span><br><span class="line">            rp = np.random.uniform(<span class="number">0</span>,<span class="number">1</span>)</span><br><span class="line">            <span class="keyword">if</span> rp &gt; p:</span><br><span class="line">                sample_data.append(d)</span><br><span class="line">                sample_cnt += <span class="number">1</span></span><br><span class="line">            </span><br><span class="line">            <span class="keyword">if</span> sample_cnt &gt;= sample_num:</span><br><span class="line">                <span class="keyword">return</span> sample_data</span><br><span class="line">              </span><br><span class="line">data = list(np.random.random(<span class="number">100</span>))</span><br><span class="line">data_sample = random_sampling_back(data,<span class="number">10</span>,p=<span class="number">0.5</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 无放回采样 Random Sampling</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">random_sampling_no_back</span><span class="params">(data,sample_num,p=<span class="number">0.5</span>)</span>:</span></span><br><span class="line">    sample_cnt = <span class="number">0</span></span><br><span class="line">    len_data = len(data)</span><br><span class="line">    sample_data = []</span><br><span class="line">    sample_map = &#123;&#125;</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> sample_num &gt;= len_data:</span><br><span class="line">        <span class="keyword">raise</span> RuntimeError(<span class="string">'采样样本不能超过数据集'</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">while</span>(<span class="number">1</span>):</span><br><span class="line">        <span class="keyword">for</span> d <span class="keyword">in</span> data:</span><br><span class="line">            rp = np.random.uniform(<span class="number">0</span>,<span class="number">1</span>)</span><br><span class="line">            <span class="keyword">if</span> (rp &gt; p) <span class="keyword">and</span> (d <span class="keyword">not</span> <span class="keyword">in</span> sample_map):</span><br><span class="line">                sample_data.append(d)</span><br><span class="line">                sample_cnt += <span class="number">1</span></span><br><span class="line">                sample_map[d] = <span class="number">1</span></span><br><span class="line">            </span><br><span class="line">            <span class="keyword">if</span> sample_cnt &gt;= sample_num:</span><br><span class="line">                <span class="keyword">return</span> sample_data</span><br><span class="line">data_sample_no_back = random_sampling_no_back(data,<span class="number">10</span>,p=<span class="number">0.5</span>)</span><br><span class="line">data_sample_no_back</span><br><span class="line"></span><br><span class="line"><span class="comment"># 分层采样</span></span><br><span class="line">data1 = [<span class="number">1</span>] * <span class="number">10</span></span><br><span class="line">data2 = [<span class="number">2</span>] * <span class="number">20</span></span><br><span class="line">data3 = [<span class="number">3</span>] * <span class="number">30</span></span><br><span class="line"></span><br><span class="line">data_sample = []</span><br><span class="line">data_sample += random_sampling_back(data1, <span class="number">5</span>) + random_sampling_back(data2, <span class="number">10</span>) + random_sampling_back(data3, <span class="number">15</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 按概率采样</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sampling_prob</span><span class="params">(data,num,p_list)</span>:</span></span><br><span class="line">    data_len = len(data)</span><br><span class="line">    <span class="keyword">if</span> data_len != len(p_list):</span><br><span class="line">        <span class="keyword">raise</span> RuntimeError(<span class="string">'需要设置单个样本采样概率。。。数据量和概率值一致'</span>)</span><br><span class="line">    <span class="keyword">return</span> list(np.random.choice(data,num,p=p_list))</span><br><span class="line">  </span><br><span class="line">data1 = list(range(<span class="number">100</span>))</span><br><span class="line">p_list = np.array(data1) / np.sum(data1)</span><br><span class="line">sampling_prob(data1, <span class="number">10</span>, p_list)</span><br><span class="line"></span><br><span class="line"><span class="comment">## 不平衡样本学习</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_classification</span><br><span class="line"></span><br><span class="line"><span class="comment"># 产生分类数据</span></span><br><span class="line">X,y = make_classification(</span><br><span class="line">    n_classes=<span class="number">2</span>, class_sep=<span class="number">1.5</span>, weights=[<span class="number">0.9</span>, <span class="number">0.1</span>],</span><br><span class="line">    n_informative=<span class="number">3</span>, n_redundant=<span class="number">1</span>, flip_y=<span class="number">0</span>,</span><br><span class="line">    n_features=<span class="number">20</span>, n_clusters_per_class=<span class="number">1</span>,</span><br><span class="line">    n_samples=<span class="number">100</span>, random_state=<span class="number">10</span></span><br><span class="line">)</span><br><span class="line"></span><br><span class="line">df = pd.DataFrame(X)</span><br><span class="line">df[<span class="string">'target'</span>] = y </span><br><span class="line">df.target.value_counts().plot(kind=<span class="string">'bar'</span>, title=<span class="string">'Count (target)'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 画图展示比例</span></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> confusion_matrix</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line">conf_mat = confusion_matrix(y_true=y, y_pred=y)</span><br><span class="line">print(<span class="string">'Confusion matrix:\n'</span>,conf_mat)</span><br><span class="line"></span><br><span class="line">labels = [<span class="string">'Class 0'</span>,<span class="string">'Class 1'</span>]</span><br><span class="line">fig = plt.figure()</span><br><span class="line">ax = fig.add_subplot(<span class="number">111</span>)</span><br><span class="line">cax = ax.matshow(conf_mat,cmap=plt.cm.Blues)</span><br><span class="line">fig.colorbar(cax)</span><br><span class="line">ax.set_xticklabels([<span class="string">''</span>] + labels)</span><br><span class="line">ax.set_yticklabels([<span class="string">''</span>] + labels)</span><br><span class="line">plt.xlabel(<span class="string">'Predicted'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'Expected'</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 统计label计数</span></span><br><span class="line">count_class_0,count_class_1  = df.target.value_counts()</span><br><span class="line">print(count_class_0,count_class_1)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 选择标签样本</span></span><br><span class="line">df_class_0 = df[df[<span class="string">'target'</span>] == <span class="number">0</span>]</span><br><span class="line">df_class_1 = df[df[<span class="string">'target'</span>] == <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 随机过采样标签数据</span></span><br><span class="line">df_class_1_over = df_class_1.sample(count_class_0, replace=<span class="literal">True</span>)</span><br><span class="line">df_test_over = pd.concat([df_class_0, df_class_1_over],axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">print(<span class="string">'random over-sampling:'</span>)</span><br><span class="line">print(df_test_over.target.value_counts())</span><br><span class="line"></span><br><span class="line">df_test_over.target.value_counts().plot(kind=<span class="string">'bar'</span>, title=<span class="string">'Count (target)'</span>)</span><br></pre></td></tr></table></figure>

<h3 id="降维"><a href="#降维" class="headerlink" title="降维"></a>降维</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np </span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> make_classification</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line">X,y = make_classification(</span><br><span class="line">    n_classes=<span class="number">2</span>,class_sep=<span class="number">1.5</span>,weights=[<span class="number">0.5</span>,<span class="number">0.5</span>],</span><br><span class="line">    n_informative=<span class="number">3</span>,n_redundant=<span class="number">1</span>,flip_y=<span class="number">0</span>,</span><br><span class="line">    n_features=<span class="number">20</span>,n_clusters_per_class=<span class="number">1</span>,</span><br><span class="line">    n_samples=<span class="number">100</span>,random_state=<span class="number">10</span></span><br><span class="line">)</span><br><span class="line">df = pd.DataFrame(X)</span><br><span class="line">df[<span class="string">'target'</span>] = y</span><br><span class="line"></span><br><span class="line"><span class="comment"># sklearn pca</span></span><br><span class="line"><span class="comment"># 切分数据</span></span><br><span class="line">x_train, x_test, y_train, y_test = train_test_split(X,y,test_size=<span class="number">0.3</span>,random_state=<span class="number">42</span>)</span><br><span class="line"><span class="comment"># 数据正则化(归一化)</span></span><br><span class="line">x_train_N = (x_train - x_train.mean()) / (x_train.max() - x_train.min())</span><br><span class="line">x_train_N = (x_test - x_test.mean()) / (x_test.max() - x_test.min())</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.decomposition <span class="keyword">import</span> PCA</span><br><span class="line">pca = PCA()</span><br><span class="line">pca.fit(x_train_N)</span><br><span class="line">plt.figure(<span class="number">1</span>, figsize=(<span class="number">14</span>,<span class="number">13</span>))</span><br><span class="line">plt.clf()</span><br><span class="line">plt.axes([<span class="number">.2</span>,<span class="number">.2</span>,<span class="number">.4</span>,<span class="number">.4</span>])</span><br><span class="line">plt.plot(pca.explained_variance_ratio_,linewidth=<span class="number">2</span>)</span><br><span class="line">plt.axis(<span class="string">'tight'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'n_components'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'explained_variance_ratio_'</span>)</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment">## 特征选择</span></span><br><span class="line"><span class="comment"># 相关性系数选择</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sim</span><span class="params">(x1,x2)</span>:</span></span><br><span class="line">  x1 = (x1 - np.min(x1)) / (np.max(x1) - np.min(x1))</span><br><span class="line">  x2 = (x2 - np.min(x2)) / (np.max(x2) - np.min(x2))</span><br><span class="line">  vec = np.sum(x1 * x2)</span><br><span class="line">  vec_x1 = np.sqrt(np.sum(x1 ** <span class="number">2</span>))</span><br><span class="line">  vec_x2 = np.sqrt(np.sum(x2 ** <span class="number">2</span>))</span><br><span class="line">  <span class="keyword">return</span> vec / (vec_x1 * vec_x2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 过滤相关性低的样本</span></span><br><span class="line">col_list = df.columns</span><br><span class="line">col_sim = []</span><br><span class="line"><span class="keyword">for</span> col <span class="keyword">in</span> col_list:</span><br><span class="line">    col_sim.append(sim(df[col],df[<span class="string">'target'</span>]))</span><br><span class="line"></span><br><span class="line">plt.figure(<span class="number">1</span>,figsize=(<span class="number">14</span>,<span class="number">13</span>))</span><br><span class="line">plt.title(<span class="string">"Feature Similarity Importances"</span>)</span><br><span class="line">plt.bar(range(len(col_list)), [int(x*<span class="number">100</span>) <span class="keyword">for</span> x <span class="keyword">in</span> col_sim], color = <span class="string">"g"</span>,yerr = <span class="number">1</span>,align=<span class="string">"center"</span>)</span><br><span class="line">plt.xticks(range(len(col_list)), col_list, rotation=<span class="number">90</span>)</span><br><span class="line">plt.xlim([<span class="number">-1</span>, len(col_list)])</span><br><span class="line">plt.show()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 过滤法</span></span><br><span class="line"><span class="comment"># 相关性选择</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">col_selector</span><span class="params">(X, y, top_k=<span class="number">10</span>)</span>:</span></span><br><span class="line">    cor_list = []</span><br><span class="line">    <span class="comment"># calculate the correlation with y for each feature</span></span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> X.columns.tolist():</span><br><span class="line">        cor = np.corrcoef(X[i],y)[<span class="number">0</span>,<span class="number">1</span>]</span><br><span class="line">        cor_list.append(cor)</span><br><span class="line">    <span class="comment"># replace Nan with 0 </span></span><br><span class="line">    cor_list = [<span class="number">0</span> <span class="keyword">if</span> np.isnan(i) <span class="keyword">else</span> i <span class="keyword">for</span> i <span class="keyword">in</span> cor_list]</span><br><span class="line">    <span class="comment"># feature name</span></span><br><span class="line">    cor_feature = X.iloc[:,np.argsort(np.abs(cor_list))[-top_k:]]</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> cor_feature</span><br><span class="line">cor_feature = col_selector(pd.DataFrame(X),y) <span class="comment"># 选出了相关性比较高的10个特征</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># sklearn chi2选择</span></span><br><span class="line">X = pd.DataFrame(X)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> SelectKBest</span><br><span class="line"><span class="keyword">from</span> sklearn.feature_selection <span class="keyword">import</span> chi2</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> MinMaxScaler</span><br><span class="line"></span><br><span class="line">X_norm = MinMaxScaler().fit_transform(X)</span><br><span class="line">chi_selector = SelectKBest(chi2,k=<span class="number">10</span>)</span><br><span class="line">chi_selector.fit(X_norm,y)</span><br><span class="line">chi_support = chi_selector.get_support()</span><br><span class="line">chi_feature = X.loc[:,chi_support].columns.tolist()</span><br></pre></td></tr></table></figure>

<h3 id="特征工程项目一（数据分析-清洗-）"><a href="#特征工程项目一（数据分析-清洗-）" class="headerlink" title="特征工程项目一（数据分析[清洗]）"></a>特征工程项目一（数据分析[清洗]）</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="comment"># 读取数据</span></span><br><span class="line">model_sample = pd.read_csv(<span class="string">'/home/admin/jupyter/download/tianchi_learn_project/data/3/model_sample.csv'</span>)</span><br><span class="line">model_sample.y.value_counts()</span><br><span class="line"></span><br><span class="line">model_sample.x_008.value_counts()</span><br><span class="line"></span><br><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> seaborn <span class="keyword">as</span> sns</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看数据特征，直方图</span></span><br><span class="line">columns = [c <span class="keyword">for</span> c <span class="keyword">in</span> model_sample.columns <span class="keyword">if</span> c <span class="keyword">not</span> <span class="keyword">in</span> [<span class="string">'user_id'</span>]][:<span class="number">10</span>]</span><br><span class="line">fcols = <span class="number">6</span></span><br><span class="line">frows = len(columns)</span><br><span class="line">plt.figure(figsize= (<span class="number">5</span>*fcols,<span class="number">4</span>*frows))</span><br><span class="line"></span><br><span class="line">i=<span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> col <span class="keyword">in</span> columns:</span><br><span class="line">    i+=<span class="number">1</span></span><br><span class="line">    ax = plt.subplot(frows,fcols,i)</span><br><span class="line">    model_sample[col].hist()</span><br><span class="line">    plt.xlabel(col)</span><br><span class="line">    plt.ylabel(<span class="string">'nums'</span>)</span><br></pre></td></tr></table></figure>

<h3 id="特征工程项目二（数据采样）"><a href="#特征工程项目二（数据采样）" class="headerlink" title="特征工程项目二（数据采样）"></a>特征工程项目二（数据采样）</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 数据采样</span></span><br><span class="line">%matplotlib inline</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line">data =  pd.read_csv(<span class="string">'/home/admin/jupyter/download/tianchi_learn_project/data/3/model_sample.csv'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 无效矩阵的数据密度显示</span></span><br><span class="line"><span class="keyword">import</span> missingno <span class="keyword">as</span> msno</span><br><span class="line">msno.matrix(data,labels=<span class="literal">True</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 查看数据标签对比</span></span><br><span class="line">data.y.value_counts()</span><br><span class="line"><span class="comment"># 统计label计数</span></span><br><span class="line">count_class_0,count_class_1  = data.y.value_counts()</span><br><span class="line">print(count_class_0,count_class_1)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 选择标签样本</span></span><br><span class="line">df_class_0 = data[data[<span class="string">'y'</span>] == <span class="number">0</span>]</span><br><span class="line">df_class_1 = data[data[<span class="string">'y'</span>] == <span class="number">1</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 随机过采样标签数据</span></span><br><span class="line">df_class_1_over = df_class_1.sample(count_class_0, replace=<span class="literal">True</span>)</span><br><span class="line">df_test_over = pd.concat([df_class_0, df_class_1_over],axis=<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 数据切分</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> StratifiedKFold</span><br><span class="line">SK = StratifiedKFold(n_splits=<span class="number">5</span>,shuffle=<span class="literal">True</span>,random_state=<span class="number">1</span>)</span><br><span class="line">features_columns = [c <span class="keyword">for</span> c <span class="keyword">in</span> data.columns <span class="keyword">if</span> c <span class="keyword">not</span> <span class="keyword">in</span> [<span class="string">'y'</span>,<span class="string">'user_id'</span>]]</span><br><span class="line">X = data[features_columns].values</span><br><span class="line">y = data[<span class="string">'y'</span>].values</span><br><span class="line"><span class="keyword">for</span> k,(train_index,test_index) <span class="keyword">in</span> enumerate(SK.split(X,y)):</span><br><span class="line">    print(<span class="string">'选择哪些样本的index:'</span>)</span><br><span class="line">    print(train_index,test_index)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 数据从样本筛选出 训练集和测试集</span></span><br><span class="line">    X_train = X[train_index]</span><br><span class="line">    y_train = y[train_index]</span><br><span class="line">    </span><br><span class="line">    X_test = X[test_index]</span><br><span class="line">    y_test = y[test_index]</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 样本的个数</span></span><br><span class="line">    print(<span class="string">'样本个数:'</span>)</span><br><span class="line">    print(len(X_train))</span><br><span class="line">    print(len(y_train))</span><br><span class="line">    print(len(X_test))</span><br><span class="line">    print(len(y_test))</span><br></pre></td></tr></table></figure>

<h3 id="特征工程项目三（数据降维）"><a href="#特征工程项目三（数据降维）" class="headerlink" title="特征工程项目三（数据降维）"></a>特征工程项目三（数据降维）</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="comment"># 读取相关数据</span></span><br><span class="line">model_sample =  pd.read_csv(<span class="string">'/home/admin/jupyter/download/tianchi_learn_project/data/3/model_sample.csv'</span>)</span><br><span class="line"><span class="comment"># 相关系数进行初步选择</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 缺失值填充mean 均值填充</span></span><br><span class="line">columns = [c <span class="keyword">for</span> c <span class="keyword">in</span> model_sample.columns <span class="keyword">if</span> c <span class="keyword">not</span> <span class="keyword">in</span> [<span class="string">'user_id'</span>,<span class="string">'y'</span>]]</span><br><span class="line"><span class="keyword">for</span> col <span class="keyword">in</span> columns:</span><br><span class="line">    model_sample[col] = model_sample[col].fillna(np.mean(model_sample[col]))</span><br><span class="line">    </span><br><span class="line"><span class="comment"># norm 归一化处理</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">max_min</span><span class="params">(data, col, epsilon=<span class="number">1e-5</span>, fillna=<span class="number">0</span>)</span>:</span></span><br><span class="line">    data_norm = data.copy()</span><br><span class="line">    <span class="keyword">for</span> col_i <span class="keyword">in</span> col:</span><br><span class="line">        data_norm[col_i] = (data[col_i] - data.min()) / (data[col_i].max() - data[col_i].min())</span><br><span class="line">    data_norm = data_norm.fillna(fillna)</span><br><span class="line">    <span class="keyword">return</span> data_norm</span><br><span class="line">  </span><br><span class="line">data_norm = max_min(model_sample,columns)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mean_std</span><span class="params">(data,col,epsilon=<span class="number">1e-5</span>,fillna=<span class="number">0</span>)</span>:</span></span><br><span class="line">    data_norm = data.copy()</span><br><span class="line">    <span class="keyword">for</span> col_i <span class="keyword">in</span> col:</span><br><span class="line">        data_norm[col_i] = (data[col_i] - data[col_i].mean()) / (data[col_i].std() + epsilon)</span><br><span class="line">    data_norm = data_norm.fillna(fillna)</span><br><span class="line">    <span class="keyword">return</span> data_norm</span><br><span class="line">data_norm = mean_std(model_sample,columns)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算相关系数矩阵</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">corr_</span><span class="params">(x,y)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> np.abs(np.corrcoef(x,y)[<span class="number">0</span>][<span class="number">1</span>])</span><br><span class="line">  </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">corr_col</span><span class="params">(data_norm, col, y=<span class="string">'y'</span>, percentile=<span class="number">0.25</span>)</span>:</span></span><br><span class="line">    col_corr = []</span><br><span class="line">    <span class="keyword">for</span> col_i <span class="keyword">in</span> col:</span><br><span class="line">        col_corr.append(corr_(data_norm[col_i],data_norm[y]))</span><br><span class="line">    df_corr = pd.DataFrame(&#123;<span class="string">'col'</span>:col,<span class="string">'corr'</span>:col_corr&#125;).dropna()</span><br><span class="line">    col_corr_ = df_corr[<span class="string">'corr'</span>]</span><br><span class="line">    col_corr_percentile = np.percentile(col_corr_,percentile)</span><br><span class="line">    <span class="keyword">return</span> df_corr, col_corr, col_corr_percentile</span><br></pre></td></tr></table></figure>

<h2 id="机器学习"><a href="#机器学习" class="headerlink" title="机器学习"></a>机器学习</h2><h3 id="sklearn"><a href="#sklearn" class="headerlink" title="sklearn"></a>sklearn</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line">iris = datasets.load_iris()</span><br><span class="line">digits = datasets.load_digits()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 手写数字</span></span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> svm</span><br><span class="line">clf = svm.SVC(gamma=<span class="number">0.001</span>, C=<span class="number">100.</span>)</span><br><span class="line">clf.fit(digits.data[:<span class="number">-1</span>], digits.target[:<span class="number">-1</span>])</span><br><span class="line">clf.predict(digits.data[<span class="number">-1</span>:])</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line">plt.imshow(digits.images[<span class="number">-1</span>], cmap=plt.cm.gray_r)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 鸢尾花</span></span><br><span class="line">clf = svm.SVC()</span><br><span class="line">X,y = iris.data, iris.target</span><br><span class="line">clf.fit(X,y)</span><br><span class="line">clf.predict(iris.data[<span class="number">-1</span>:])</span><br><span class="line"></span><br><span class="line"><span class="comment"># 模型保存</span></span><br><span class="line"><span class="keyword">import</span> pickle</span><br><span class="line">s = pickle.dumps(clf)</span><br><span class="line">clf2 = pickle.loads(s)</span><br><span class="line">clf2.predict(X[<span class="number">0</span>:<span class="number">1</span>])</span><br><span class="line"><span class="keyword">from</span> joblib <span class="keyword">import</span> dump, load</span><br><span class="line">dump(clf, <span class="string">'filename.joblib'</span>)</span><br><span class="line">clf = load(<span class="string">'filename.joblib'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 二分类转化为多分类</span></span><br><span class="line"><span class="keyword">from</span> sklearn.multiclass <span class="keyword">import</span> OneVsRestClassifier</span><br><span class="line"></span><br><span class="line">X = [[<span class="number">1</span>,<span class="number">2</span>], [<span class="number">2</span>,<span class="number">4</span>], [<span class="number">4</span>,<span class="number">5</span>], [<span class="number">3</span>,<span class="number">2</span>], [<span class="number">3</span>,<span class="number">1</span>]]</span><br><span class="line">y = [<span class="number">0</span>,<span class="number">0</span>,<span class="number">1</span>,<span class="number">1</span>,<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">classif = OneVsRestClassifier(estimator=svm.SVC(random_state=<span class="number">0</span>))</span><br><span class="line">classif.fit(X,y).predict(X)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 多标签</span></span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> MultiLabelBinarizer</span><br><span class="line"></span><br><span class="line">y = [[<span class="number">0</span>,<span class="number">1</span>], [<span class="number">0</span>,<span class="number">2</span>], [<span class="number">1</span>,<span class="number">3</span>], [<span class="number">0</span>,<span class="number">2</span>,<span class="number">3</span>], [<span class="number">2</span>,<span class="number">4</span>]]</span><br><span class="line">y = MultiLabelBinarizer().fit_transform(y)</span><br><span class="line">y</span><br><span class="line">classif.fit(X,y).predict(X)</span><br></pre></td></tr></table></figure>

<h3 id="项目案例一"><a href="#项目案例一" class="headerlink" title="项目案例一"></a>项目案例一</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"></span><br><span class="line">iris = datasets.load_iris()</span><br><span class="line">X = iris.data</span><br><span class="line">y = iris.target</span><br><span class="line"></span><br><span class="line">X[:<span class="number">10</span>,:] <span class="comment"># 前10行，所有列</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(iris.data, iris.target, test_size=<span class="number">0.2</span>, random_state=<span class="number">666</span>) <span class="comment"># 训练集和测试集</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler  <span class="comment"># 标准化，可以得到一些标准化的参数</span></span><br><span class="line"></span><br><span class="line">standardScaler = StandardScaler()</span><br><span class="line"></span><br><span class="line">standardScaler.fit(X_train)</span><br><span class="line">standardScaler.mean_</span><br><span class="line">standardScaler.scale_</span><br><span class="line"></span><br><span class="line">X_train_stand = standardScaler.transform(X_train) <span class="comment"># 得到训练集</span></span><br><span class="line">X_train_stand[:<span class="number">10</span>,:]</span><br><span class="line">X_test_stand = standardScaler.transform(X_test) <span class="comment"># 测试集</span></span><br><span class="line">X_test_stand[:<span class="number">10</span>,:]</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line">knn_clf = KNeighborsClassifier(n_neighbors=<span class="number">3</span>)</span><br><span class="line">knn_clf.fit(X_train,y_train)</span><br><span class="line"></span><br><span class="line">knn_clf.score(X_test_stand, y_test)</span><br><span class="line"></span><br><span class="line">knn_clf.score(X_test, y_test)</span><br><span class="line"></span><br><span class="line">knn_clf.predict(X_test_stand[<span class="number">-1</span>:,:])</span><br><span class="line"><span class="comment"># 预测值和真实值进行对比</span></span><br><span class="line">y_test[<span class="number">-1</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 第二个案例</span></span><br><span class="line">digits = datasets.load_digits()</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(digits.data, digits.target, test_size=<span class="number">0.2</span>, random_state=<span class="number">666</span>)</span><br><span class="line"></span><br><span class="line">sk_knn_clf = KNeighborsClassifier(n_neighbors=<span class="number">4</span>,weights=<span class="string">'uniform'</span>)</span><br><span class="line">sk_knn_clf.fit(X_train,y_train)</span><br><span class="line">sk_knn_clf.score(X_test,y_test) <span class="comment"># 模型准确度测试</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 寻找最优参数</span></span><br><span class="line">param_grid = [</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment"># 搜索的第一组参数</span></span><br><span class="line">        <span class="string">'weights'</span>:[<span class="string">'uniform'</span>],</span><br><span class="line">        <span class="string">'n_neighbors'</span>:[i <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,<span class="number">11</span>)]</span><br><span class="line">    &#125;,</span><br><span class="line">    &#123;</span><br><span class="line">        <span class="comment"># 搜索的第二组参数</span></span><br><span class="line">        <span class="string">'weights'</span>:[<span class="string">'distance'</span>],</span><br><span class="line">        <span class="string">'n_neighbors'</span>:[i <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>,<span class="number">11</span>)]</span><br><span class="line">    &#125;</span><br><span class="line">]</span><br><span class="line">knn_clf = KNeighborsClassifier()</span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> GridSearchCV</span><br><span class="line"></span><br><span class="line">grid_search = GridSearchCV(knn_clf, param_grid)</span><br><span class="line">grid_search.fit(X_train,y_train)</span><br><span class="line">knn_clf = grid_search.best_estimator_</span><br><span class="line">grid_search.best_score_     <span class="comment"># 最好的准确度</span></span><br><span class="line">grid_search.best_params_    <span class="comment"># 对应的参数</span></span><br><span class="line">knn_clf.score(X_test,y_test) <span class="comment"># 模型准确度测试</span></span><br></pre></td></tr></table></figure>

<h3 id="项目案例二"><a href="#项目案例二" class="headerlink" title="项目案例二"></a>项目案例二</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line">df = pd.read_csv(<span class="string">'/home/admin/jupyter/download/data/4/BostonHousing.csv'</span>)</span><br><span class="line">df.head()</span><br><span class="line"></span><br><span class="line">target = df[<span class="string">'medv'</span>]</span><br><span class="line"></span><br><span class="line">split_num = int(len(features) * <span class="number">0.7</span>)</span><br><span class="line"></span><br><span class="line">X_train = features[:split_num] <span class="comment"># 训练集 前70%</span></span><br><span class="line">y_train = target[:split_num]</span><br><span class="line"></span><br><span class="line">X_test = features[split_num:] <span class="comment"># 测试集  后70%</span></span><br><span class="line">y_test = target[split_num:]</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line"></span><br><span class="line">model = LinearRegression()</span><br><span class="line">model.fit(X_train,y_train)</span><br><span class="line">model.coef_, model.intercept_</span><br><span class="line"></span><br><span class="line">preds = model.predict(X_test)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mae_value</span><span class="params">(y_true, y_pred)</span>:</span></span><br><span class="line">    n = len(y_true)</span><br><span class="line">    mae = sum(np.abs(y_true - y_pred)) / n</span><br><span class="line">    <span class="keyword">return</span> mae</span><br><span class="line">  </span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">mse_value</span><span class="params">(y_true, y_pred)</span>:</span></span><br><span class="line">    n = len(y_true)</span><br><span class="line">    mse = sum(np.square(y_true - y_pred)) / n</span><br><span class="line">    <span class="keyword">return</span> mse</span><br><span class="line">  </span><br><span class="line">mae = mae_value(y_test.values, preds)</span><br><span class="line">mse = mse_value(y_test.values, preds)</span><br><span class="line">print(<span class="string">'MAE:'</span>, mae)</span><br><span class="line">print(<span class="string">'MSE:'</span>, mse)</span><br></pre></td></tr></table></figure>

<h3 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 1.线性回归介绍</span></span><br><span class="line"><span class="comment"># 2.一元线性回归的实现</span></span><br><span class="line"><span class="comment"># 3.平方损失函数</span></span><br><span class="line"><span class="comment"># 4.最小二乘法及代数求解</span></span><br><span class="line"><span class="comment"># 5.线性回归的实现</span></span><br><span class="line"><span class="comment"># 6.最小二乘法的矩阵推导</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># sklearn方式</span></span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line"></span><br><span class="line">model = LinearRegression()</span><br><span class="line">model.fit(x.reshape(len(x),<span class="number">1</span>),y)</span><br><span class="line">model.intercept_, model.coef_</span><br><span class="line"></span><br><span class="line">y_temp = model.predict(x.reshape(len(x),<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">plt.scatter(x.tolist(),y.tolist())</span><br><span class="line">plt.plot(x, y_temp, <span class="string">'r'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 小技巧</span></span><br><span class="line"><span class="keyword">from</span> numpy <span class="keyword">import</span> *</span><br><span class="line"><span class="comment"># 删除numpy矩阵中的一行或者一列</span></span><br><span class="line">x=delete(x,<span class="number">0</span>,axis=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>

<h3 id="多项式回归"><a href="#多项式回归" class="headerlink" title="多项式回归"></a>多项式回归</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 1.多项式回归介绍</span></span><br><span class="line"><span class="comment"># 2.多项式回归基础</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># 使用sklearn方法</span></span><br><span class="line">x = [<span class="number">4</span>, <span class="number">8</span>, <span class="number">12</span>, <span class="number">25</span>, <span class="number">32</span>, <span class="number">43</span>, <span class="number">58</span>, <span class="number">63</span>, <span class="number">69</span>, <span class="number">79</span>]</span><br><span class="line">y = [<span class="number">20</span>, <span class="number">33</span>, <span class="number">50</span>, <span class="number">56</span>, <span class="number">42</span>, <span class="number">31</span>, <span class="number">33</span>, <span class="number">46</span>, <span class="number">65</span>, <span class="number">75</span>]</span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> PolynomialFeatures</span><br><span class="line"></span><br><span class="line">x = np.array(x).reshape(len(x), <span class="number">1</span>)</span><br><span class="line">y = np.array(y).reshape(len(y), <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line">poly_features = PolynomialFeatures(degree=<span class="number">2</span>, include_bias=<span class="literal">False</span>)</span><br><span class="line">poly_x = poly_features.fit_transform(x)</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line"></span><br><span class="line">model = LinearRegression()</span><br><span class="line">model.fit(poly_x, y)</span><br><span class="line">model.intercept_, model.coef_</span><br><span class="line"></span><br><span class="line">x_temp = np.array(x_temp).reshape(len(x_temp), <span class="number">1</span>)</span><br><span class="line">poly_x_temp = poly_features.fit_transform(x_temp)</span><br><span class="line"></span><br><span class="line">plt.plot(x_temp, model.predict(poly_x_temp), <span class="string">'r'</span>)</span><br><span class="line">plt.scatter(x,y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 多项式回归实战</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line">df = pd.read_csv(<span class="string">"/home/admin/jupyter/download/tianchi_learn_project/data/5/course-6-vaccine.csv"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 测试数据集的分离</span></span><br><span class="line">train_df = df[:int(len(df)* <span class="number">0.7</span>)]</span><br><span class="line">test_df = df[int(len(df)* <span class="number">0.7</span>):]</span><br><span class="line"></span><br><span class="line">X_train = train_df[<span class="string">'Year'</span>].values</span><br><span class="line">y_train = train_df[<span class="string">'Values'</span>].values</span><br><span class="line"></span><br><span class="line">X_test = test_df[<span class="string">'Year'</span>].values</span><br><span class="line">y_test = test_df[<span class="string">'Values'</span>].values</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.linear_model <span class="keyword">import</span> LinearRegression</span><br><span class="line"></span><br><span class="line">model = LinearRegression()</span><br><span class="line">model.fit(X_train.reshape(len(X_train), <span class="number">1</span>),y_train.reshape(len(X_train), <span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">results = model.predict(X_test.reshape(len(X_test), <span class="number">1</span>))</span><br><span class="line">results <span class="comment"># 线性回归在测试集上预测的结果</span></span><br><span class="line"></span><br><span class="line">plt.scatter(X_test,y_test)</span><br><span class="line">plt.plot(X_test,results,<span class="string">'r'</span>) <span class="comment"># 可视化</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_absolute_error</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> mean_squared_error</span><br><span class="line"></span><br><span class="line">print(<span class="string">'线性回归平均绝对误差:'</span>,mean_absolute_error(y_test, results.flatten()))</span><br><span class="line">print(<span class="string">'线性回归均方误差:'</span>,mean_squared_error(y_test, results.flatten()))</span><br><span class="line"></span><br><span class="line"><span class="comment"># make_pipeline 合并 PolynomialFeatures + LinearRegression</span></span><br><span class="line"><span class="keyword">from</span> sklearn.pipeline <span class="keyword">import</span> make_pipeline</span><br><span class="line"></span><br><span class="line">X_train = X_train.reshape(len(X_train), <span class="number">1</span>)</span><br><span class="line">X_test = X_test.reshape(len(X_test), <span class="number">1</span>)</span><br><span class="line">y_train = y_train.reshape(len(y_train), <span class="number">1</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> m <span class="keyword">in</span>[<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>]:</span><br><span class="line">    model = make_pipeline(PolynomialFeatures(m,include_bias=<span class="literal">False</span>), LinearRegression())</span><br><span class="line">    model.fit(X_train, y_train)</span><br><span class="line">    pre_y = model.predict(X_test)</span><br><span class="line">    print(<span class="string">'&#123;&#125;次多项式回归平均绝对误差:'</span>.format(m),mean_absolute_error(y_test, pre_y.flatten()))</span><br><span class="line">print(<span class="string">'&#123;&#125;次多项式回归均方误差:'</span>.format(m),mean_squared_error(y_test, pre_y.flatten()))</span><br><span class="line"></span><br><span class="line"><span class="comment"># 计算mse随着多项式次数的增加的变化曲线</span></span><br><span class="line"></span><br><span class="line">mse = []</span><br><span class="line">m = <span class="number">1</span></span><br><span class="line">m_max = <span class="number">10</span></span><br><span class="line"><span class="keyword">while</span> m &lt;= m_max:</span><br><span class="line">    model = make_pipeline(PolynomialFeatures(m,include_bias=<span class="literal">False</span>), LinearRegression())</span><br><span class="line">    model.fit(X_train, y_train)</span><br><span class="line">    pre_y = model.predict(X_test)</span><br><span class="line">    </span><br><span class="line">    mse.append(mean_squared_error(y_test,pre_y.flatten()))</span><br><span class="line">    </span><br><span class="line">    m+=<span class="number">1</span></span><br><span class="line"></span><br><span class="line">plt.plot([i <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, m_max+<span class="number">1</span>)], mse, <span class="string">'r'</span>)</span><br><span class="line">plt.scatter([i <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">1</span>, m_max+<span class="number">1</span>)], mse)</span><br><span class="line">plt.title(<span class="string">'MSE of m degree of polynomial regression'</span>)</span><br><span class="line">plt.xlabel(<span class="string">'m'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'MSE'</span>)</span><br></pre></td></tr></table></figure>

<h3 id="k近邻"><a href="#k近邻" class="headerlink" title="k近邻"></a>k近邻</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 1.最近邻算法</span></span><br><span class="line"><span class="comment"># 2.k近邻算法</span></span><br><span class="line"><span class="comment"># 3.knn算法流程</span></span><br><span class="line"><span class="comment"># 4.度量距离</span></span><br><span class="line"><span class="comment"># 5.决策规划</span></span><br><span class="line"><span class="comment"># 6.knn算法实现</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># k近邻</span></span><br><span class="line"><span class="comment"># 1.数据准备</span></span><br><span class="line"><span class="comment"># 2.计算距离</span></span><br><span class="line"><span class="comment"># 3.寻找邻居</span></span><br><span class="line"><span class="comment"># 4.决策分类</span></span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="comment"># sklearn实现</span></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line">lilac_data = pd.read_csv(<span class="string">'/home/admin/jupyter/download/tianchi_learn_project/data/6/course-9-syringa.csv'</span>)</span><br><span class="line"><span class="keyword">from</span> matplotlib <span class="keyword">import</span> pyplot <span class="keyword">as</span> plt</span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line"><span class="string">'''绘制丁香花特征子图'''</span></span><br><span class="line">fig, axes = plt.subplots(<span class="number">2</span>, <span class="number">3</span>, figsize=(<span class="number">20</span>,<span class="number">10</span>))</span><br><span class="line">fig.subplots_adjust(hspace=<span class="number">0.3</span>, wspace=<span class="number">0.2</span>)</span><br><span class="line"></span><br><span class="line">axes[<span class="number">0</span>, <span class="number">0</span>].set_xlabel(<span class="string">'sepal_length'</span>)</span><br><span class="line">axes[<span class="number">0</span>, <span class="number">0</span>].set_ylabel(<span class="string">'sepal_width'</span>)</span><br><span class="line">axes[<span class="number">0</span>, <span class="number">0</span>].scatter(lilac_data.sepal_length[:<span class="number">50</span>], lilac_data.sepal_width[:<span class="number">50</span>], c=<span class="string">'b'</span>)</span><br><span class="line">axes[<span class="number">0</span>, <span class="number">0</span>].scatter(lilac_data.sepal_length[<span class="number">50</span>:<span class="number">100</span>], lilac_data.sepal_width[<span class="number">50</span>:<span class="number">100</span>], c=<span class="string">'g'</span>)</span><br><span class="line">axes[<span class="number">0</span>, <span class="number">0</span>].scatter(lilac_data.sepal_length[<span class="number">100</span>:], lilac_data.sepal_width[<span class="number">100</span>:], c=<span class="string">'r'</span>)</span><br><span class="line">axes[<span class="number">0</span>, <span class="number">0</span>].legend([<span class="string">'daphne'</span>,<span class="string">'syringa'</span>, <span class="string">'willow'</span>], loc=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">axes[<span class="number">0</span>, <span class="number">1</span>].set_xlabel(<span class="string">'petal_length'</span>)</span><br><span class="line">axes[<span class="number">0</span>, <span class="number">1</span>].set_ylabel(<span class="string">'petal_width'</span>)</span><br><span class="line">axes[<span class="number">0</span>, <span class="number">1</span>].scatter(lilac_data.petal_length[:<span class="number">50</span>], lilac_data.petal_width[:<span class="number">50</span>], c=<span class="string">'b'</span>)</span><br><span class="line">axes[<span class="number">0</span>, <span class="number">1</span>].scatter(lilac_data.petal_length[<span class="number">50</span>:<span class="number">100</span>], lilac_data.petal_width[<span class="number">50</span>:<span class="number">100</span>], c=<span class="string">'g'</span>)</span><br><span class="line">axes[<span class="number">0</span>, <span class="number">1</span>].scatter(lilac_data.petal_length[<span class="number">100</span>:], lilac_data.petal_width[<span class="number">100</span>:], c=<span class="string">'r'</span>)</span><br><span class="line"></span><br><span class="line">axes[<span class="number">0</span>, <span class="number">2</span>].set_xlabel(<span class="string">'sepal_length'</span>)</span><br><span class="line">axes[<span class="number">0</span>, <span class="number">2</span>].set_ylabel(<span class="string">'petal_length'</span>)</span><br><span class="line">axes[<span class="number">0</span>, <span class="number">2</span>].scatter(lilac_data.sepal_length[:<span class="number">50</span>], lilac_data.petal_length[:<span class="number">50</span>], c=<span class="string">'b'</span>)</span><br><span class="line">axes[<span class="number">0</span>, <span class="number">2</span>].scatter(lilac_data.sepal_length[<span class="number">50</span>:<span class="number">100</span>], lilac_data.petal_length[<span class="number">50</span>:<span class="number">100</span>], c=<span class="string">'g'</span>)</span><br><span class="line">axes[<span class="number">0</span>, <span class="number">2</span>].scatter(lilac_data.sepal_length[<span class="number">100</span>:], lilac_data.petal_length[<span class="number">100</span>:], c=<span class="string">'r'</span>)</span><br><span class="line"></span><br><span class="line">axes[<span class="number">1</span>, <span class="number">0</span>].set_xlabel(<span class="string">'sepal_width'</span>)</span><br><span class="line">axes[<span class="number">1</span>, <span class="number">0</span>].set_ylabel(<span class="string">'petal_width'</span>)</span><br><span class="line">axes[<span class="number">1</span>, <span class="number">0</span>].scatter(lilac_data.sepal_width[:<span class="number">50</span>], lilac_data.petal_width[:<span class="number">50</span>], c=<span class="string">'b'</span>)</span><br><span class="line">axes[<span class="number">1</span>, <span class="number">0</span>].scatter(lilac_data.sepal_width[<span class="number">50</span>:<span class="number">100</span>], lilac_data.petal_width[<span class="number">50</span>:<span class="number">100</span>], c=<span class="string">'g'</span>)</span><br><span class="line">axes[<span class="number">1</span>, <span class="number">0</span>].scatter(lilac_data.sepal_width[<span class="number">100</span>:], lilac_data.petal_width[<span class="number">100</span>:], c=<span class="string">'r'</span>)</span><br><span class="line"></span><br><span class="line">axes[<span class="number">1</span>, <span class="number">1</span>].set_xlabel(<span class="string">'sepal_length'</span>)</span><br><span class="line">axes[<span class="number">1</span>, <span class="number">1</span>].set_ylabel(<span class="string">'petal_width'</span>)</span><br><span class="line">axes[<span class="number">1</span>, <span class="number">1</span>].scatter(lilac_data.sepal_length[:<span class="number">50</span>], lilac_data.petal_width[:<span class="number">50</span>], c=<span class="string">'b'</span>)</span><br><span class="line">axes[<span class="number">1</span>, <span class="number">1</span>].scatter(lilac_data.sepal_length[<span class="number">50</span>:<span class="number">100</span>], lilac_data.petal_width[<span class="number">50</span>:<span class="number">100</span>], c=<span class="string">'g'</span>)</span><br><span class="line">axes[<span class="number">1</span>, <span class="number">1</span>].scatter(lilac_data.sepal_length[<span class="number">100</span>:], lilac_data.petal_width[<span class="number">100</span>:], c=<span class="string">'r'</span>)</span><br><span class="line"></span><br><span class="line">axes[<span class="number">1</span>, <span class="number">2</span>].set_xlabel(<span class="string">'sepal_width'</span>)</span><br><span class="line">axes[<span class="number">1</span>, <span class="number">2</span>].set_ylabel(<span class="string">'petal_length'</span>)</span><br><span class="line">axes[<span class="number">1</span>, <span class="number">2</span>].scatter(lilac_data.sepal_width[:<span class="number">50</span>], lilac_data.petal_length[:<span class="number">50</span>], c=<span class="string">'b'</span>)</span><br><span class="line">axes[<span class="number">1</span>, <span class="number">2</span>].scatter(lilac_data.sepal_width[<span class="number">50</span>:<span class="number">100</span>], lilac_data.petal_length[<span class="number">50</span>:<span class="number">100</span>], c=<span class="string">'g'</span>)</span><br><span class="line">axes[<span class="number">1</span>, <span class="number">2</span>].scatter(lilac_data.sepal_width[<span class="number">100</span>:], lilac_data.petal_length[<span class="number">100</span>:], c=<span class="string">'r'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 分离数据集</span></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line">feature_data = lilac_data.iloc[:,:<span class="number">-1</span>]</span><br><span class="line">label_data = lilac_data[<span class="string">'labels'</span>]</span><br><span class="line"></span><br><span class="line">X_train,X_test,y_train,y_test = train_test_split(feature_data, label_data, test_size=<span class="number">0.3</span>, random_state=<span class="number">2</span>)</span><br><span class="line"></span><br><span class="line">X_test.head()</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.neighbors <span class="keyword">import</span> KNeighborsClassifier</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">sklearn_classify</span><span class="params">(train_data, label_data, test_data, k_num)</span>:</span></span><br><span class="line">    knn = KNeighborsClassifier(n_neighbors=k_num)</span><br><span class="line">    knn.fit(train_data, label_data)</span><br><span class="line">    </span><br><span class="line">    predict_label = knn.predict(test_data)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">return</span> predict_label</span><br><span class="line">  </span><br><span class="line">y_predict = sklearn_classify(X_train, y_train, X_test, <span class="number">3</span>)</span><br><span class="line">y_predict</span><br><span class="line"></span><br><span class="line"><span class="comment"># 准确率 = 正确的样本/总测试样本</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_accuracy</span><span class="params">(test_labels, pred_labels)</span>:</span></span><br><span class="line">    correct = np.sum(test_labels == pred_labels) <span class="comment"># 预测正确的个数</span></span><br><span class="line">    </span><br><span class="line">    n = len(test_labels)</span><br><span class="line">    </span><br><span class="line">    accur = correct / n</span><br><span class="line">    <span class="keyword">return</span> accur</span><br><span class="line">  </span><br><span class="line">get_accuracy(y_test,y_predict)</span><br><span class="line">  </span><br><span class="line"><span class="comment"># 查看k取值不同，准确率的变化</span></span><br><span class="line">normal_accuracy = []</span><br><span class="line">k_value = range(<span class="number">2</span>, <span class="number">11</span>)</span><br><span class="line"><span class="keyword">for</span> k <span class="keyword">in</span> k_value:</span><br><span class="line">    y_predict = sklearn_classify(X_train, y_train, X_test, k)</span><br><span class="line">    </span><br><span class="line">    accuracy = get_accuracy(y_test,y_predict)</span><br><span class="line">    </span><br><span class="line">    normal_accuracy.append(accuracy)</span><br><span class="line">    </span><br><span class="line">plt.xlabel(<span class="string">'k'</span>)</span><br><span class="line">plt.ylabel(<span class="string">'accuracy'</span>)</span><br><span class="line">new_ticks = np.linspace(<span class="number">0.6</span>, <span class="number">0.9</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">plt.yticks(new_ticks)</span><br><span class="line">plt.plot(k_value, normal_accuracy, c=<span class="string">'r'</span>)</span><br><span class="line">plt.grid(<span class="literal">True</span>) <span class="comment"># 增加网格</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># sklearn 使用kd树 （时间可以更快？）</span></span><br><span class="line">kd_x = np.random.random((<span class="number">100000</span>,<span class="number">2</span>))</span><br><span class="line">kd_y = np.random.randint(<span class="number">4</span>, size=<span class="number">100000</span>)</span><br><span class="line"></span><br><span class="line">kd_knn = KNeighborsClassifier(n_neighbors=<span class="number">5</span>, algorithm=<span class="string">'kd_tree'</span>)</span><br></pre></td></tr></table></figure>

<h3 id="支持向量机"><a href="#支持向量机" class="headerlink" title="支持向量机"></a>支持向量机</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 1.支持向量机介绍</span></span><br><span class="line"><span class="comment"># 2.线性支持向量机分类</span></span><br><span class="line"><span class="comment"># 3.非线性支持向量机分类</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.datasets <span class="keyword">import</span> samples_generator</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line"><span class="comment"># 生成团状的数据</span></span><br><span class="line">x, y = samples_generator.make_blobs(n_samples=<span class="number">60</span>, centers=<span class="number">2</span> ,random_state=<span class="number">30</span>, cluster_std=<span class="number">0.8</span>)</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">10</span>, <span class="number">8</span>))</span><br><span class="line">plt.scatter(x[:,<span class="number">0</span>], x[:,<span class="number">1</span>], c=y, s=<span class="number">40</span>, cmap=<span class="string">'bwr'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 支持向量机是要找到一条最合理的线</span></span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> SVC</span><br><span class="line"></span><br><span class="line">linear_svc = SVC(kernel=<span class="string">'linear'</span>)</span><br><span class="line">linear_svc.fit(x, y)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 获取支持向量</span></span><br><span class="line">linear_svc.support_vectors_</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">svc_plot</span><span class="params">(model)</span>:</span></span><br><span class="line">    <span class="comment"># 获得当前的Axes对象</span></span><br><span class="line">    ax = plt.gca()</span><br><span class="line">    x = np.linspace(ax.get_xlim()[<span class="number">0</span>], ax.get_xlim()[<span class="number">1</span>], <span class="number">50</span>)</span><br><span class="line">    y = np.linspace(ax.get_ylim()[<span class="number">0</span>], ax.get_ylim()[<span class="number">1</span>], <span class="number">50</span>)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 生成网格数据，x:所有网格点的x坐标，形状也是网格性nxm。yy同样</span></span><br><span class="line">    Y,X = np.meshgrid(y, x)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 元组转数组</span></span><br><span class="line">    xy = np.vstack([X.ravel(), Y.ravel()]).T</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 计算样本点到分割超平面的函数距离</span></span><br><span class="line">    P = model.decision_function(xy).reshape(X.shape)</span><br><span class="line">    </span><br><span class="line">    <span class="comment"># 对网格中每个点的值等于一系列值的时候做出一条条轮廓线，类似于等高线 </span></span><br><span class="line">    ax.contour(X,Y,P, color=<span class="string">'g'</span>, levels=[<span class="number">-1</span>,<span class="number">0</span>,<span class="number">1</span>], linestyle=[<span class="string">'--'</span>,<span class="string">'-'</span>,<span class="string">'--'</span>])</span><br><span class="line">    </span><br><span class="line">    ax.scatter(model.support_vectors_[:, <span class="number">0</span>], model.support_vectors_[:, <span class="number">1</span>], c=<span class="string">'g'</span>, s=<span class="number">100</span>)</span><br><span class="line">    </span><br><span class="line">plt.figure(figsize=(<span class="number">10</span>, <span class="number">8</span>))</span><br><span class="line">plt.scatter(x[:,<span class="number">0</span>], x[:,<span class="number">1</span>], c=y, s=<span class="number">40</span>, cmap=<span class="string">'bwr'</span>)</span><br><span class="line">svc_plot(linear_svc)</span><br><span class="line"></span><br><span class="line"><span class="comment">## 非线性支持向量机分类</span></span><br><span class="line">x2, y2 = samples_generator.make_circles(<span class="number">150</span>, factor=<span class="number">.5</span>, noise=<span class="number">.1</span>, random_state=<span class="number">30</span>)</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">8</span>, <span class="number">8</span>))</span><br><span class="line">plt.scatter(x2[:, <span class="number">0</span>], x2[:, <span class="number">1</span>], c=y2, s=<span class="number">40</span>, cmap=<span class="string">'bwr'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 核函数（采用多项式核函数）</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">kernel_function</span><span class="params">(xi, xj)</span>:</span></span><br><span class="line">    poly = xi**<span class="number">2</span> + xj**<span class="number">2</span></span><br><span class="line">    <span class="keyword">return</span> poly</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> mpl_toolkits <span class="keyword">import</span> mplot3d</span><br><span class="line"><span class="keyword">from</span> ipywidgets <span class="keyword">import</span> interact, fixed</span><br><span class="line"></span><br><span class="line">r = kernel_function(x2[:,<span class="number">0</span>], x2[:,<span class="number">1</span>])</span><br><span class="line">plt.figure(figsize=(<span class="number">10</span>, <span class="number">8</span>))</span><br><span class="line">ax = plt.subplot(projection=<span class="string">'3d'</span>)</span><br><span class="line">ax.scatter3D(x2[:, <span class="number">0</span>], x2[:, <span class="number">1</span>], r, c=y2, s=<span class="number">40</span>, cmap=<span class="string">'bwr'</span>)</span><br><span class="line">ax.set_xlabel(<span class="string">'x'</span>)</span><br><span class="line">ax.set_ylabel(<span class="string">'y'</span>)</span><br><span class="line">ax.set_zlabel(<span class="string">'r'</span>) </span><br><span class="line"></span><br><span class="line">rbf_svc = SVC(kernel=<span class="string">'rbf'</span>, gamma=<span class="string">'auto'</span>)</span><br><span class="line">rbf_svc.fit(x2,y2)</span><br><span class="line"></span><br><span class="line">plt.figure(figsize=(<span class="number">8</span>, <span class="number">8</span>))</span><br><span class="line">plt.scatter(x2[:,<span class="number">0</span>], x2[:,<span class="number">1</span>], c=y2, s=<span class="number">40</span>, cmap=<span class="string">'bwr'</span>)</span><br><span class="line">svc_plot(rbf_svc)</span><br><span class="line"></span><br><span class="line"><span class="comment">## 支持向量机项目案例</span></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> matplotlib.pyplot <span class="keyword">as</span> plt</span><br><span class="line">%matplotlib inline</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn <span class="keyword">import</span> datasets</span><br><span class="line"></span><br><span class="line">irirs = datasets.load_iris()</span><br><span class="line"></span><br><span class="line">X = irirs.data</span><br><span class="line">y = irirs.target</span><br><span class="line"></span><br><span class="line">X = X[y&lt;<span class="number">2</span>, :<span class="number">2</span>]</span><br><span class="line">y = y[y&lt;<span class="number">2</span>]</span><br><span class="line"></span><br><span class="line">plt.scatter(X[y==<span class="number">0</span>, <span class="number">0</span>], X[y==<span class="number">0</span>, <span class="number">1</span>], color=<span class="string">'red'</span>)</span><br><span class="line">plt.scatter(X[y==<span class="number">1</span>, <span class="number">0</span>], X[y==<span class="number">1</span>, <span class="number">1</span>], color=<span class="string">'blue'</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 标准化处理</span></span><br><span class="line"><span class="keyword">from</span> sklearn.preprocessing <span class="keyword">import</span> StandardScaler</span><br><span class="line"></span><br><span class="line">standardScaler = StandardScaler()</span><br><span class="line">standardScaler.fit(X)</span><br><span class="line"></span><br><span class="line">X_standard = standardScaler.transform(X)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 支持向量机模型</span></span><br><span class="line"><span class="keyword">from</span> sklearn.svm <span class="keyword">import</span> LinearSVC</span><br><span class="line"></span><br><span class="line">svc = LinearSVC(C=<span class="number">1e9</span>)</span><br><span class="line">svc.fit(X_standard, y)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">plot_decision_boundary</span><span class="params">(model, axis)</span>:</span></span><br><span class="line">    </span><br><span class="line">    x0, x1 = np.meshgrid(</span><br><span class="line">        np.linspace(axis[<span class="number">0</span>], axis[<span class="number">1</span>], int((axis[<span class="number">1</span>] - axis[<span class="number">0</span>]) * <span class="number">100</span>)).reshape(<span class="number">-1</span>, <span class="number">1</span>),</span><br><span class="line">        np.linspace(axis[<span class="number">2</span>], axis[<span class="number">3</span>], int((axis[<span class="number">3</span>] - axis[<span class="number">2</span>]) * <span class="number">100</span>)).reshape(<span class="number">-1</span>, <span class="number">1</span>)</span><br><span class="line">    )</span><br><span class="line">    </span><br><span class="line">    X_new = np.c_[x0.ravel(), x1.ravel()]</span><br><span class="line">    </span><br><span class="line">    y_predict = model.predict(X_new)</span><br><span class="line">    zz = y_predict.reshape(x0.shape)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">from</span> matplotlib.colors <span class="keyword">import</span> ListedColormap</span><br><span class="line">    custom_map = ListedColormap([<span class="string">'#ef9a9a'</span>, <span class="string">'#fff59d'</span>, <span class="string">'#90caf9'</span>])</span><br><span class="line">    plt.contourf(x0, x1, zz, linewidth=<span class="number">5</span>, cmap=custom_map)</span><br><span class="line"></span><br><span class="line">plot_decision_boundary(svc, axis=[<span class="number">-3</span>, <span class="number">3</span>, <span class="number">-3</span>, <span class="number">3</span>])</span><br><span class="line">plt.scatter(X_standard[y==<span class="number">0</span>, <span class="number">0</span>], X_standard[y==<span class="number">0</span>, <span class="number">1</span>], color=<span class="string">'red'</span>)</span><br><span class="line">plt.scatter(X_standard[y==<span class="number">1</span>, <span class="number">0</span>], X_standard[y==<span class="number">1</span>, <span class="number">1</span>], color=<span class="string">'blue'</span>)</span><br></pre></td></tr></table></figure>

<h3 id="决策树"><a href="#决策树" class="headerlink" title="决策树"></a>决策树</h3><figure class="highlight python"><table><tr><td class="code"><pre><span class="line"><span class="comment"># 1.什么是决策树</span></span><br><span class="line"><span class="comment"># 2.决策树算法流程</span></span><br><span class="line"><span class="comment"># 3.信息增益（id3)</span></span><br><span class="line"><span class="comment"># 4.连续值处理</span></span><br><span class="line"><span class="comment"># 5.决策树的实现</span></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"></span><br><span class="line">stu_grade = pd.read_csv(<span class="string">'/home/admin/jupyter/download/data/10/students_academic_performance.csv'</span>)</span><br><span class="line">stu_grade.head()</span><br><span class="line"></span><br><span class="line">new_data = stu_grade.iloc[:, [<span class="number">0</span>,<span class="number">1</span>,<span class="number">2</span>,<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>,<span class="number">6</span>,<span class="number">8</span>,<span class="number">9</span>,<span class="number">10</span>,<span class="number">11</span>,<span class="number">14</span>,<span class="number">15</span>]]</span><br><span class="line">new_data.head()</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">choice_2</span><span class="params">(x)</span>:</span></span><br><span class="line">    x = int(x)</span><br><span class="line">    <span class="keyword">if</span> x &lt; <span class="number">5</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">'bad'</span></span><br><span class="line">    <span class="keyword">elif</span> x &gt;= <span class="number">5</span> <span class="keyword">and</span> x &lt; <span class="number">10</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">'medium'</span></span><br><span class="line">    <span class="keyword">elif</span> x &gt;= <span class="number">10</span> <span class="keyword">and</span> x &lt; <span class="number">15</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">'good'</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">'excellent'</span></span><br><span class="line">    </span><br><span class="line">stu_data = new_data.copy()</span><br><span class="line">stu_data[<span class="string">'G1'</span>] = pd.Series(map(<span class="keyword">lambda</span> x: choice_2(x), stu_data[<span class="string">'G1'</span>]))</span><br><span class="line">stu_data[<span class="string">'G2'</span>] = pd.Series(map(<span class="keyword">lambda</span> x: choice_2(x), stu_data[<span class="string">'G2'</span>]))</span><br><span class="line">stu_data[<span class="string">'G3'</span>] = pd.Series(map(<span class="keyword">lambda</span> x: choice_2(x), stu_data[<span class="string">'G3'</span>]))</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">choice_3</span><span class="params">(x)</span>:</span></span><br><span class="line">    x = int(x)</span><br><span class="line">    <span class="keyword">if</span> x &lt; <span class="number">3</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">'high'</span></span><br><span class="line">    <span class="keyword">elif</span> x &gt; <span class="number">1.5</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">'medium'</span></span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> <span class="string">'low'</span></span><br><span class="line">    </span><br><span class="line">stu_data[<span class="string">'Pedu'</span>] = pd.Series(map(<span class="keyword">lambda</span> x: choice_3(x), stu_data[<span class="string">'Pedu'</span>]))</span><br><span class="line">stu_data.head()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 特征</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">replace_feature</span><span class="params">(data)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> each <span class="keyword">in</span> data.columns:</span><br><span class="line">        feature_list = data[each]</span><br><span class="line">        unique_value = set(feature_list)</span><br><span class="line">        i = <span class="number">0</span></span><br><span class="line">        <span class="keyword">for</span> fea_value <span class="keyword">in</span> unique_value:</span><br><span class="line">            data[each] = data[each].replace(fea_value, i)</span><br><span class="line">            i += <span class="number">1</span></span><br><span class="line">            </span><br><span class="line">    <span class="keyword">return</span> data</span><br><span class="line">  </span><br><span class="line">stu_data = replace_feature(stu_data)</span><br><span class="line">stu_data.head()</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.model_selection <span class="keyword">import</span> train_test_split</span><br><span class="line"></span><br><span class="line">X_train, X_test, y_train, y_test = train_test_split(stu_data.iloc[:, :<span class="number">-1</span>], stu_data[<span class="string">'G3'</span>], test_size=<span class="number">0.3</span>, random_state=<span class="number">5</span>)</span><br><span class="line">X_test.head()</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.tree <span class="keyword">import</span> DecisionTreeClassifier</span><br><span class="line"></span><br><span class="line">dt_model = DecisionTreeClassifier(criterion=<span class="string">'entropy'</span>, random_state=<span class="number">666</span>)</span><br><span class="line">dt_model.fit(X_train)</span><br><span class="line"></span><br><span class="line">y_pred = dt_model.predict(X_test)</span><br><span class="line">y_pred</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> sklearn.metrics <span class="keyword">import</span> accuracy_score</span><br><span class="line">accuracy_score(y_test, y_pred)</span><br></pre></td></tr></table></figure>

]]></content>
      <categories>
        <category>ai</category>
      </categories>
      <tags>
        <tag>python</tag>
        <tag>数据分析</tag>
        <tag>机器学习</tag>
      </tags>
  </entry>
  <entry>
    <title>网络通信-netty</title>
    <url>/2020/11/17/%E7%BD%91%E7%BB%9C%E9%80%9A%E4%BF%A1-netty/</url>
    <content><![CDATA[<h1 id="网络通信-netty"><a href="#网络通信-netty" class="headerlink" title="网络通信-netty"></a>网络通信-netty</h1><h2 id="netty编解码技术"><a href="#netty编解码技术" class="headerlink" title="netty编解码技术"></a>netty编解码技术</h2><h3 id="编码技术Marshalling"><a href="#编码技术Marshalling" class="headerlink" title="编码技术Marshalling"></a>编码技术Marshalling</h3><h3 id="编码技术Protobuf"><a href="#编码技术Protobuf" class="headerlink" title="编码技术Protobuf"></a>编码技术Protobuf</h3><h2 id="Netty最佳实践"><a href="#Netty最佳实践" class="headerlink" title="Netty最佳实践"></a>Netty最佳实践</h2><h3 id="业务场景-高可靠性架构设计分析"><a href="#业务场景-高可靠性架构设计分析" class="headerlink" title="业务场景-高可靠性架构设计分析"></a>业务场景-高可靠性架构设计分析</h3><p><img src="https://lixiangbetter.github.io/2020/11/17/%E7%BD%91%E7%BB%9C%E9%80%9A%E4%BF%A1-netty/1605595405623.jpg" alt></p>
<h3 id="项目整体业务"><a href="#项目整体业务" class="headerlink" title="项目整体业务"></a>项目整体业务</h3><p><img src="https://lixiangbetter.github.io/2020/11/17/%E7%BD%91%E7%BB%9C%E9%80%9A%E4%BF%A1-netty/1605596382205.jpg" alt></p>
]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>netty</tag>
      </tags>
  </entry>
  <entry>
    <title>分布式文件系统</title>
    <url>/2020/11/16/%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/</url>
    <content><![CDATA[<h1 id="分布式文件系统"><a href="#分布式文件系统" class="headerlink" title="分布式文件系统"></a>分布式文件系统</h1><h2 id="FastDfs"><a href="#FastDfs" class="headerlink" title="FastDfs"></a>FastDfs</h2><h3 id="概念"><a href="#概念" class="headerlink" title="概念"></a>概念</h3><p>FastDFS是一个开源的分布式文件系统，她对文件进行管理，功能包括：文件存储、文件同步、文件访问（文件上传、文件下载）等，解决了大容量存储和负载均衡的问题。特别适合以文件为载体的在线服务，如相册网站、视频网站等等。</p>
<h3 id="FastDfs架构图"><a href="#FastDfs架构图" class="headerlink" title="FastDfs架构图"></a>FastDfs架构图</h3><p><img src="https://lixiangbetter.github.io/2020/11/16/%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/1605512482736.jpg" alt></p>
<h3 id="FastDfs上传过程"><a href="#FastDfs上传过程" class="headerlink" title="FastDfs上传过程"></a>FastDfs上传过程</h3><p><img src="https://lixiangbetter.github.io/2020/11/16/%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/1605512731008.jpg" alt></p>
<h3 id="FastDfs下载过程"><a href="#FastDfs下载过程" class="headerlink" title="FastDfs下载过程"></a>FastDfs下载过程</h3><p><img src="https://lixiangbetter.github.io/2020/11/16/%E5%88%86%E5%B8%83%E5%BC%8F%E6%96%87%E4%BB%B6%E7%B3%BB%E7%BB%9F/1605512731008.jpg" alt></p>
<h3 id="FastDfs环境准备"><a href="#FastDfs环境准备" class="headerlink" title="FastDfs环境准备"></a>FastDfs环境准备</h3><p>文档链接：<a href="https://github.com/happyfish100/fastdfs/wiki" target="_blank" rel="noopener">https://github.com/happyfish100/fastdfs/wiki</a></p>
<h2 id="OSS"><a href="#OSS" class="headerlink" title="OSS"></a>OSS</h2>]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>fastDfs</tag>
        <tag>oss</tag>
      </tags>
  </entry>
  <entry>
    <title>读写分离、分库分表</title>
    <url>/2020/11/05/%E8%AF%BB%E5%86%99%E5%88%86%E7%A6%BB%E3%80%81%E5%88%86%E5%BA%93%E5%88%86%E8%A1%A8/</url>
    <content><![CDATA[<h1 id="读写分离、分库分表"><a href="#读写分离、分库分表" class="headerlink" title="读写分离、分库分表"></a>读写分离、分库分表</h1><h2 id="1-读写分离、分库分表"><a href="#1-读写分离、分库分表" class="headerlink" title="1.读写分离、分库分表"></a>1.读写分离、分库分表</h2><p>​    如何选择垂直切分、水平切分</p>
<p>​    mycat快速体验</p>
<p>​    mycat用户配置</p>
<p>​    mycat的schema配置实操</p>
<p>​    mysql主从配置</p>
<p>​    mycat粉片规则配置（枚举分片、取模）</p>
<p>​    mycat全局表、子表</p>
<p>​    mycat高可用架构原理解析</p>
<p>​    mycat高可用架构-反向代理（haproxy)</p>
<p>​    sharding-jdbc分片表</p>
<p>​    sharding-jdbc全局表</p>
<p>​    sharding-jdbc子表</p>
<h2 id="2-分布式全局ID、分布式事务和数据一致性"><a href="#2-分布式全局ID、分布式事务和数据一致性" class="headerlink" title="2.分布式全局ID、分布式事务和数据一致性"></a>2.分布式全局ID、分布式事务和数据一致性</h2><p>​    分布式全局ID的多种解决方案</p>
<p>​    使用UUID作为分布式全局唯一主键ID</p>
<p>​    基于mycat实现全局唯一主键ID（本地文件和数据库）</p>
<p>​    基于雪花算法实现全局唯一主键ID</p>
<p>​    分布式系统CAP、BASE原理和ACID原则</p>
<p>​    分布式事务问题解析</p>
<p>​    基于XA协议的两阶段提交解决数据一致性问题</p>
<p>​    使用Atomikos做分布式事务</p>
<p>​    mycat和sharding-jdbc分布式事务    </p>
<p>​    基于事务补偿机制解决数据一致性</p>
<p>​    基于本地消息表+定时任务解决数据一致性</p>
<p>​    基于MQ生产消费模型解决数据一致性</p>
<h2 id="3-分布式接口幂等性、分布式限流"><a href="#3-分布式接口幂等性、分布式限流" class="headerlink" title="3.分布式接口幂等性、分布式限流"></a>3.分布式接口幂等性、分布式限流</h2><p>​    基于唯一索引解决delete操作的幂等性问题</p>
<p>​    基于乐观锁解决update的幂等性操作问题</p>
<p>​        update有行锁</p>
<p>​    基于token机制解决insert的幂等操作问题</p>
<p>​    分布式限流技术选型</p>
<p>​    限流常用算法讲解</p>
<p>​    基于nginx的分布式限流 – ip限制</p>
<p>​    基于nginx的分布式限流 – 连接数限制</p>
<p>​    基于网关层实现分布式限流</p>
<p>​    分布式限流落地</p>
<p>​    分布式限流要注意的问题</p>
<p>​    </p>
]]></content>
      <categories>
        <category>架构</category>
      </categories>
      <tags>
        <tag>数据库</tag>
        <tag>分布式事务</tag>
      </tags>
  </entry>
  <entry>
    <title>Storm学习</title>
    <url>/2020/10/22/Storm%E5%AD%A6%E4%B9%A0/</url>
    <content><![CDATA[<h1 id="Storm学习"><a href="#Storm学习" class="headerlink" title="Storm学习"></a>Storm学习</h1><h2 id="1-导学"><a href="#1-导学" class="headerlink" title="1.导学"></a>1.导学</h2><p>VMware Fusion<br>    Mac上搭建：为了给大家演示如何使用我们的OOTB环境<br>    Hadoop环境：虚拟机，我是远程登录<br>    Mac<br>        那么就不需要使用我们的OOTB环境<br>        VMware Fusion+OOTB</p>
<pre><code>Window：VMware</code></pre><p>hadoop/hadoop<br>root用户的密码是什么？<br>修改配置文件，是需要root权限的，怎么办？<br>    sudo command</p>
<p>只有一个地方需要修改：ip地址<br>/etc/hosts<br>    192.168.199.128 hadoop000<br>    192.168.199.128 localhost</p>
<h2 id="2-初识实时流处理Storm"><a href="#2-初识实时流处理Storm" class="headerlink" title="2.初识实时流处理Storm"></a>2.初识实时流处理Storm</h2><p>Apache Storm is a free and open source distributed realtime computation system<br>    免费<br>    开源<br>    分布式<br>    实时计算系统</p>
<p>Storm makes it easy to reliably process unbounded streams of data<br>    unbounded: 无界<br>    bounded: Hadoop/Spark SQL   离线(input …. output)</p>
<p>Storm has many use cases:<br>    realtime analytics,<br>    online machine learning,<br>    continuous computation,<br>    distributed RPC,<br>    ETL, and more. </p>
<p>Storm特点<br>    fast：a million tuples processed per second per node<br>    scalable,<br>    fault-tolerant,<br>    guarantees your data will be processed<br>    and is easy to set up and operate.</p>
<p>小结：Strom能实现高频数据和大规模数据的实时处理</p>
<p>Storm产生于BackType(被Twitter收购)公司</p>
<p>#…#</p>
<p>需求：大数据的实时处理</p>
<p>自己来实现实时系统，要考虑的因素：</p>
<p>1) 健壮性<br>2) 扩展性/分布式<br>3) 如何使得数据不丢失，不重复<br>4) 高性能、低延时</p>
<p>Storm开源<br>    2011.9<br>    Apache<br>    Clojure Java</p>
<p>Storm技术网站</p>
<p>1) 官网： storm.apache.org<br>2) GitHub: github.com/apache/storm<br>3) wiki: <a href="https://en.wikipedia.org/wiki/Storm_(event_processor)" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Storm_(event_processor)</a></p>
<p>Storm vs Hadoop<br>    数据源/处理领域<br>    处理过程<br>        Hadoop: Map  Reduce<br>        Storm:  Spout  Bolt<br>    进程是否结束<br>    处理速度<br>    使用场景    </p>
<p>发展趋势<br>    1) 社区的发展、活跃度<br>    2) 企业的需求<br>    3) 大数据相关的大会， Storm主题的数量上升<br>    4) 互联网  JStorm</p>
<h2 id="3-Storm核心概念"><a href="#3-Storm核心概念" class="headerlink" title="3.Storm核心概念"></a>3.Storm核心概念</h2><p>核心概念<br>    Topologies<br>        拓扑，将整个流程串起来<br>    Streams<br>        流，数据流，水流<br>    Spouts<br>        产生数据/水的东西<br>    Bolts<br>        处理数据/水的东西    水壶/水桶<br>    Tuple<br>        数据/水    </p>
<p>制约中国互联网发展的最大瓶颈是什么？ 后厂村路<br>13号线：回龙观==&gt;龙泽==&gt;西二旗</p>
<p>Storm核心概念总结<br>    Topology： 计算拓扑，由spout和bolt组成的<br>    Stream：消息流，抽象概念，没有边界的tuple构成<br>    Tuple：消息/数据  传递的基本单元<br>    Spout：消息流的源头，Topology的消息生产者<br>    Bolt：消息处理单元，可以做过滤、聚合、查询/写数据库的操作</p>
<p><img src="https://lixiangbetter.github.io/2020/10/22/Storm%E5%AD%A6%E4%B9%A0/A.png" alt></p>
<p><img src="https://lixiangbetter.github.io/2020/10/22/Storm%E5%AD%A6%E4%B9%A0/B.png" alt></p>
<h2 id="4-Storm编程"><a href="#4-Storm编程" class="headerlink" title="4.Storm编程"></a>4.Storm编程</h2><p>搭建开发环境<br>    jdk: 1.8<br>        windows: exe<br>        linux/mac(dmg): tar …..   把jdk指定到系统环境变量(~/.bash_profile)<br>            export JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk1.8.0_121.jdk/Contents/Home<br>            export PATH=$JAVA_HOME/bin:$PATH</p>
<pre><code>        source ~/.bash_profile
        echo $JAVA_HOME
        java -version
IDEA: 
Maven: 3.3+
    windows/linux/mac 下载安装包
    tar .... -C ~/app

    把maven指定到系统环境变量(~/.bash_profile)
    export MAVEN_HOME=/Users/rocky/app/apache-maven-3.3.9
    export PATH=$MAVEN_HOME/bin:$PATH
    source ~/.bash_profile
    echo $JAVA_HOME
    mvn -v

    调整maven依赖下载的jar所在位置： $MAVEN_HOME/conf/setting.xml
    &lt;localRepository&gt;/Users/rocky/maven_repos&lt;/localRepository&gt;

在pom.xml中添加storm的maven依赖
    &lt;dependency&gt;
      &lt;groupId&gt;org.apache.storm&lt;/groupId&gt;
      &lt;artifactId&gt;storm-core&lt;/artifactId&gt;
      &lt;version&gt;1.1.1&lt;/version&gt;
    &lt;/dependency&gt;</code></pre><p>ISpout<br>    概述<br>        核心接口(interface)，负责将数据发送到topology中去处理<br>        Storm会跟踪Spout发出去的tuple的DAG<br>        ack/fail<br>        tuple: message id<br>        ack/fail/nextTuple是在同一个线程中执行的，所以不用考虑线程安全方面</p>
<pre><code>核心方法
    open： 初始化操作
    close： 资源释放操作
    nextTuple： 发送数据   core api
    ack： tuple处理成功，storm会反馈给spout一个成功消息
    fail：tuple处理失败，storm会发送一个消息给spout，处理失败

实现类
    public abstract class BaseRichSpout extends BaseComponent implements IRichSpout {
    public interface IRichSpout extends ISpout, IComponent 
    DRPCSpout
    ShellSpout</code></pre><p>IComponent接口<br>    概述：<br>        public interface IComponent extends Serializable<br>        为topology中所有可能的组件提供公用的方法</p>
<pre><code>    void declareOutputFields(OutputFieldsDeclarer declarer);
    用于声明当前Spout/Bolt发送的tuple的名称
    使用OutputFieldsDeclarer配合使用


实现类：
public abstract class BaseComponent implements IComponent</code></pre><p>IBolt接口<br>    概述<br>        职责：接收tuple处理，并进行相应的处理(filter/join/….)<br>        hold住tuple再处理<br>        IBolt会在一个运行的机器上创建，使用Java序列化它，然后提交到主节点(nimbus)上去执行<br>        nimbus会启动worker来反序列化，调用prepare方法，然后才开始处理tuple处理</p>
<pre><code>方法
    prepare：初始化
    execute：处理一个tuple暑假，tuple对象中包含了元数据信息
    cleanup：shutdown之前的资源清理操作


实现类：
    public abstract class BaseRichBolt extends BaseComponent implements IRichBolt {
    public interface IRichBolt extends IBolt, IComponent 
    RichShellBolt</code></pre><p>求和案例<br>    需求：1 + 2 + 3 + ….   = ???<br>    实现方案：<br>        Spout发送数字作为input<br>        使用Bolt来处理业务逻辑：求和<br>        将结果输出到控制台<br>    拓扑设计： DataSourceSpout  –&gt; SumBolt    </p>
<p>词频统计<br>    需求：读取指定目录的数据，并实现单词计数功能<br>    实现方案：<br>        Spout来读取指定目录的数据，作为后续Bolt处理的input<br>        使用一个Bolt把input的数据，切割开，我们按照逗号进行分割<br>        使用一个Bolt来进行最终的单词的次数统计操作<br>        并输出<br>    拓扑设计： DataSourceSpout ==&gt; SplitBolt ==&gt; CountBolt    </p>
<p>Storm编程注意事项</p>
<p>1) Exception in thread “main” java.lang.IllegalArgumentException: Spout has already been declared for id DataSourceSpout<br>2) org.apache.storm.generated.InvalidTopologyException: null<br>3) topology的名称不是重复： local似乎没问题， 等我们到集群测试的时候再来验证这个问题</p>
<p><img src="https://lixiangbetter.github.io/2020/10/22/Storm%E5%AD%A6%E4%B9%A0/C.png" alt></p>
<h2 id="5-Storm周边框架使用"><a href="#5-Storm周边框架使用" class="headerlink" title="5.Storm周边框架使用"></a>5.Storm周边框架使用</h2><p>环境前置说明：<br>    通过我们的客户端(终端，CRT，XShell)<br>    ssh hadoop@hadoop000<br>    ssh <a href="mailto:hadoop@192.168.199.102" target="_blank" rel="noopener">hadoop@192.168.199.102</a></p>
<pre><code>远程服务器的用户名是hadoop，密码也是hadoop
有没有提供root权限，sudo command

hadoop000(192.168.199.102)是远程服务器的hostname

如果你想在本地通过ssh hadoop@hadoop000远程登录，
那么你本地的hosts肯定要添加ip和hostname的映射

192.168.199.102  hadoop000</code></pre><p>JDK的安装<br>    将所有的软件都安装到~/app<br>        tar -zxvf jdk-8u91-linux-x64.tar.gz -C ~/app/</p>
<pre><code>建议将jdk的bin目录配置到系统环境变量中： ~/.bash_profile
    export JAVA_HOME=/home/hadoop/app/jdk1.8.0_91
    export PATH=$JAVA_HOME/bin:$PATH

让系统环境变量生效
    source ~/.bash_profile

验证
    java -version</code></pre><p>ZooKeeper安装<br>    下载ZK的安装包：<a href="http://archive.cloudera.com/cdh5/cdh/5/" target="_blank" rel="noopener">http://archive.cloudera.com/cdh5/cdh/5/</a><br>    解压：tar -zxvf zookeeper-3.4.5-cdh5.7.0.tar.gz -C ~/app/<br>    建议ZK_HOME/bin添加到系统环境变量: ~/.bash_profile<br>        export ZK_HOME=/home/hadoop/app/zookeeper-3.4.5-cdh5.7.0<br>        export PATH=$ZK_HOME/bin:$PATH<br>    让系统环境变量生效<br>        source ~/.bash_profile<br>    修改ZK的配置： $ZK_HOME/conf/zoo.cfg<br>        dataDir=/home/hadoop/app/tmp/zookeeper<br>    启动zk: $ZK_HOME/bin/<br>        zkServer.sh start<br>    验证: jps<br>        多了一个QuorumPeerMain进程，就表示zk启动成功了</p>
<p>ELK:<br>    <a href="http://www.elastic.co" target="_blank" rel="noopener">www.elastic.co</a></p>
<p>Kafka概述<br>    和消息系统类似</p>
<pre><code>消息中间件：生产者和消费者

妈妈：生产者
你：消费者
馒头：数据流、消息

    正常情况下： 生产一个  消费一个
    其他情况：  
        一直生产，你吃到某一个馒头时，你卡主(机器故障)， 馒头就丢失了
        一直生产，做馒头速度快，你吃来不及，馒头也就丢失了

    拿个碗/篮子，馒头做好以后先放到篮子里，你要吃的时候去篮子里面取出来吃

篮子/框： Kafka
    当篮子满了，馒头就装不下了，咋办？ 
    多准备几个篮子 === Kafka的扩容</code></pre><p>Kafka架构<br>    producer：生产者，就是生产馒头(老妈)<br>    consumer：消费者，就是吃馒头的(你)<br>    broker：篮子<br>    topic：主题，给馒头带一个标签，topica的馒头是给你吃的，topicb的馒头是给你弟弟吃</p>
<p>单节点单broker的部署及使用</p>
<p>$KAFKA_HOME/config/server.properties<br>broker.id=0<br>listeners<br>host.name<br>log.dirs<br>zookeeper.connect</p>
<p>启动Kafka<br>kafka-server-start.sh<br>USAGE: /home/hadoop/app/kafka_2.11-0.9.0.0/bin/kafka-server-start.sh [-daemon] server.properties [–override property=value]*</p>
<p>kafka-server-start.sh $KAFKA_HOME/config/server.properties</p>
<p>创建topic: zk<br>kafka-topics.sh –create –zookeeper hadoop000:2181 –replication-factor 1 –partitions 1 –topic hello_topic</p>
<p>查看所有topic<br>kafka-topics.sh –list –zookeeper hadoop000:2181</p>
<p>发送消息: broker<br>kafka-console-producer.sh –broker-list hadoop000:9092 –topic hello_topic</p>
<p>消费消息: zk<br>kafka-console-consumer.sh –zookeeper hadoop000:2181 –topic hello_topic –from-beginning</p>
<p>–from-beginning的使用</p>
<p>查看所有topic的详细信息：kafka-topics.sh –describe –zookeeper hadoop000:2181<br>查看指定topic的详细信息：kafka-topics.sh –describe –zookeeper hadoop000:2181 –topic hello_topic</p>
<p>单节点多broker<br>server-1.properties<br>    log.dirs=/home/hadoop/app/tmp/kafka-logs-1<br>    listeners=PLAINTEXT://:9093<br>    broker.id=1</p>
<p>server-2.properties<br>    log.dirs=/home/hadoop/app/tmp/kafka-logs-2<br>    listeners=PLAINTEXT://:9094<br>    broker.id=2</p>
<p>server-3.properties<br>    log.dirs=/home/hadoop/app/tmp/kafka-logs-3<br>    listeners=PLAINTEXT://:9095<br>    broker.id=3</p>
<p>kafka-server-start.sh -daemon $KAFKA_HOME/config/server-1.properties &amp;<br>kafka-server-start.sh -daemon $KAFKA_HOME/config/server-2.properties &amp;<br>kafka-server-start.sh -daemon $KAFKA_HOME/config/server-3.properties &amp;</p>
<p>kafka-topics.sh –create –zookeeper hadoop000:2181 –replication-factor 3 –partitions 1 –topic my-replicated-topic</p>
<p>kafka-console-producer.sh –broker-list hadoop000:9093,hadoop000:9094,hadoop000:9095 –topic my-replicated-topic<br>kafka-console-consumer.sh –zookeeper hadoop000:2181 –topic my-replicated-topic</p>
<p>kafka-topics.sh –describe –zookeeper hadoop000:2181 –topic my-replicated-topic</p>
<h2 id="6-Storm架构及部署"><a href="#6-Storm架构及部署" class="headerlink" title="6.Storm架构及部署"></a>6.Storm架构及部署</h2><p>Storm架构<br>    类似于Hadoop的架构，主从(Master/Slave)<br>    Nimbus: 主<br>        集群的主节点，负责任务(task)的指派和分发、资源的分配<br>    Supervisor: 从<br>        可以启动多个Worker，具体几个呢？可以通过配置来指定<br>        一个Topo可以运行在多个Worker之上，也可以通过配置来指定<br>        集群的从节点，(负责干活的)，负责执行任务的具体部分<br>        启动和停止自己管理的Worker进程<br>    无状态，在他们上面的信息(元数据)会存储在ZK中<br>    Worker: 运行具体组件逻辑(Spout/Bolt)的进程<br>    =====================分割线===================<br>    task：<br>        Spout和Bolt<br>        Worker中每一个Spout和Bolt的线程称为一个Task<br>    executor： spout和bolt可能会共享一个线程</p>
<p><img src="https://lixiangbetter.github.io/2020/10/22/Storm%E5%AD%A6%E4%B9%A0/D.png" alt></p>
<p>Storm部署的前置条件<br>    jdk7+<br>    python2.6.6+</p>
<p>我们课程使用的Storm版本是：1.1.1</p>
<p>Storm部署<br>    下载<br>    解压到<del>/app<br>    添加到系统环境变量:</del>/.bash_profile<br>        export STORM_HOME=/home/hadoop/app/apache-storm-1.1.1<br>        export PATH=$STORM_HOME/bin:$PATH<br>    使其生效: source ~/.bash_profile</p>
<pre><code>目录结构
    bin
    examples
    conf
    lib</code></pre><p>Storm启动<br>    $STORM_HOME/bin/storm   如何使用  执行storm就能看到很多详细的命令<br>        dev-zookeeper  启动zk<br>            storm dev-zookeeper  前台启动<br>            nohup sh storm dev-zookeeper &amp;<br>            jps ： dev_zookeeper<br>        nimbus  启动主节点<br>            nohup sh storm nimbus &amp;<br>        supervisor 启动从节点<br>            nohup sh storm supervisor &amp;<br>        ui  启动UI界面<br>            nohup sh storm ui &amp;<br>        logviewer 启动日志查看服务<br>            nohup sh storm logviewer &amp;</p>
<p>注意事项<br>    1) 为什么是4个slot<br>    2) 为什么有2个Nimbus</p>
<p>Storm如何运行我们自己开发的应用程序呢?<br>    Syntax: storm jar topology-jar-path class args0 args1 args2</p>
<p>storm jar /home/hadoop/lib/storm-1.0.jar com.imooc.bigdata.ClusterSumStormTopology</p>
<p>问题: 3个executor，那么页面就看到spout1个和bolt1个，那么还有一个去哪了？</p>
<p>如何修改将跑在本地的storm app改成运行在集群上的<br>StormSubmitter.submitTopology(topoName,new Config(), builder.createTopology());</p>
<p>storm 其他命令的使用</p>
<pre><code>list
        Syntax: storm list
        List the running topologies and their statuses.

如何停止作业
    kill
        Syntax: storm kill topology-name [-w wait-time-secs]

如何停止集群
    hadoop： stop-all.sh
    kill -9 pid,pid....</code></pre><p>Storm集群的部署规划<br>    hadoop000   192.168.199.102<br>    hadoop001   192.168.199.247<br>    hadoop002   192.168.199.138 </p>
<pre><code>每台机器的host映射：/etc/hosts
    192.168.199.102 hadoop000
    192.168.199.247 hadoop001
    192.168.199.138 hadoop002


hadoop000: zk nimbus  supervisor
hadoop001: zk           supervisor
hadoop002: zk         supervisor</code></pre><p>安装包的分发: 从hadoop000机器做为出发点<br>    scp  xxxx  hadoop@hadoop001:<del>/software<br>    scp  xxxx  hadoop@hadoop002:</del>/software</p>
<p>jdk的安装<br>    解压<br>    配置到系统环境变量<br>    验证</p>
<p>ZK分布式环境的安装<br>server.1=hadoop000:2888:3888<br>server.2=hadoop001:2888:3888<br>server.3=hadoop002:2888:3888</p>
<p>hadoop000的dataDir目录: myid的值1<br>hadoop001的dataDir目录: myid的值2<br>hadoop002的dataDir目录: myid的值3</p>
<p>在每个节点上启动zk: zkServer.sh start<br>在每个节点上查看当前机器zk的状态: zkServer.sh status</p>
<p>Storm集群<br>    $STORM_HOME/conf/storm.yaml<br>        storm.zookeeper.servers:<br>             - “hadoop000”<br>             - “hadoop001”<br>             - “hadoop002”</p>
<pre><code>storm.local.dir: &quot;/home/hadoop/app/tmp/storm&quot;

supervisor.slots.ports:
     - 6700
     - 6701
     - 6702
     - 6703</code></pre><p>启动<br>    hadoop000: nimbus  supervisor(ui,logviewer)<br>    hadoop001: supervisor(logviewer)<br>    hadoop002: supervisor(logviewer)</p>
<p>nimbus  启动主节点<br>    nohup sh storm nimbus &amp;<br>supervisor 启动从节点<br>    nohup sh storm supervisor &amp;<br>ui  启动UI界面<br>    nohup sh storm ui &amp;<br>logviewer 启动日志查看服务<br>    nohup sh storm logviewer &amp;</p>
<p>启动完所有的进程之后，查看<br>[hadoop@hadoop000 bin]$ jps<br>7424 QuorumPeerMain<br>8164 Supervisor<br>7769 nimbus<br>8380 logviewer<br>7949 core</p>
<p>[hadoop@hadoop001 bin]$ jps<br>3142 logviewer<br>2760 QuorumPeerMain<br>2971 Supervisor</p>
<p>[hadoop@hadoop002 bin]$ jps<br>3106 logviewer<br>2925 Supervisor<br>2719 QuorumPeerMain</p>
<p>storm jar /home/hadoop/lib/storm-1.0.jar com.imooc.bigdata.ClusterSumStormTopology</p>
<p>目录树<br>    storm.local.dir<br>        nimbus/inbox:stormjar-….jar<br>        supervisor/stormdist<br>            ClusterSumStormTopology-1-1511599690<br>            │   │       ├── stormcode.ser<br>            │   │       ├── stormconf.ser<br>            │   │       └── stormjar.jar</p>
<h2 id="7-并行度"><a href="#7-并行度" class="headerlink" title="7.并行度"></a>7.并行度</h2><p>并行度<br>    一个worker进程执行的是一个topo的子集<br>    一个worker进程会启动1..n个executor线程来执行一个topo的component<br>    一个运行的topo就是由集群中多台物理机上的多个worker进程组成</p>
<pre><code>executor是一个被worker进程启动的单独线程，每个executor只会运行1个topo的一个component
task是最终运行spout或者bolt代码的最小执行单元

默认：
    一个supervisor节点最多启动4个worker进程  ?
    每一个topo默认占用一个worker进程         ?
    每个worker进程会启动一个executor        ?
    每个executor启动一个task                ?</code></pre><p>Total slots:4<br>Executors: 3 ????  spout + bolt = 2  why 3?<br>    acker 导致的</p>
<h2 id="8-分组策略"><a href="#8-分组策略" class="headerlink" title="8.分组策略"></a>8.分组策略</h2><p>A stream grouping defines how that stream should be partitioned among the bolt’s tasks</p>
<p>storm jar /home/had/lib/storm-1.0.jar com.imooc.bigdata.ClusterSumShuffleGroupingStormTopology</p>
<h2 id="9-Storm可靠性"><a href="#9-Storm可靠性" class="headerlink" title="9.Storm可靠性"></a>9.Storm可靠性</h2><p><img src="https://lixiangbetter.github.io/2020/10/22/Storm%E5%AD%A6%E4%B9%A0/E.png" alt></p>
]]></content>
      <categories>
        <category>bigdata</category>
      </categories>
      <tags>
        <tag>Storm</tag>
      </tags>
  </entry>
  <entry>
    <title>hadoop基础</title>
    <url>/2020/10/21/hadoop%E5%9F%BA%E7%A1%80/</url>
    <content><![CDATA[<h1 id="hadoop基础"><a href="#hadoop基础" class="headerlink" title="hadoop基础"></a>hadoop基础</h1><h2 id="1-大数据概述"><a href="#1-大数据概述" class="headerlink" title="1.大数据概述"></a>1.大数据概述</h2><p>大数据带来的技术变革<br>    技术驱动：数据量大<br>        存储：文件存储 ==&gt; 分布式存储<br>        计算：单机    ==&gt; 分布式计算<br>        网络：万兆<br>        DB： RDBMS  ==&gt; NoSQL(HBase/Redis….)</p>
<p>大数据技术概念<br>    单机：CPU Memory Disk<br>    分布式并行计算/处理</p>
<p>船的选择<br>    廉价：<br>    中高价值：</p>
<p>运输过程拆开<br>    货物搬到船上： 数据采集   数据存储<br>    处理：小于多少的石头扔了   精细化的筛选 </p>
<pre><code>数据采集：Flume Sqoop
数据存储：Hadoop
数据处理、分析、挖掘：Hadoop、Spark、Flink....
可视化：</code></pre><h2 id="2-初识Hadoop"><a href="#2-初识Hadoop" class="headerlink" title="2.初识Hadoop"></a>2.初识Hadoop</h2><p>Nutch、Hadoop：Doug Cutting<br>Spring：</p>
<p>学习一个新的框架，我的风格是直接查看该项目的官网地址<br>Hadoop<br>Hive</p>
<p>Apache社区的顶级项目：xxxx.apache.org<br>    hadoop.apache.org<br>    hive.apache.org<br>    hbase.apache.org<br>    spark.apache.org<br>    flink.apache.org<br>    storm.apache.org</p>
<p>reliable, scalable, distributed computing.</p>
<p>单机存储<br>单机计算</p>
<p>Hadoop：提供分布式的存储（一个文件被拆分成很多个块，并且以副本的方式存储在各个节点中）和计算<br>    是一个分布式的系统基础架构：用户可以在不了解分布式底层细节的情况下进行使用。</p>
<p>分布式文件系统：HDFS实现将文件分布式存储在很多的服务器上<br>分布式计算框架：MapReduce实现在很多机器上分布式并行计算<br>分布式资源调度框架：YARN实现集群资源管理以及作业的调度</p>
<p>文件、块、副本<br>    文件：test.log  200M<br>    块(block)：默认的blocksize是128M， 2个块 = 1个128M + 1个72M<br>    副本：HDFS默认3副本</p>
<pre><code>node1：blk1  blk2  X  
node2：blk2
node3：blk1  blk2
node4：
node5：blk1</code></pre><p>去IoE</p>
<p>常用的Hadoop发行版<br>    Apache<br>        优点：纯开源<br>        缺点：不同版本/不同框架之间整合 jar冲突… 吐血</p>
<pre><code>CDH：https://www.cloudera.com/   60-70%
    优点：cm(cloudera manager) 通过页面一键安装各种框架、升级、impala
    缺点：cm不开源、与社区版本有些许出入

Hortonworks：HDP  企业发布自己的数据平台可以直接基于页面框架进行改造
    优点：原装Hadoop、纯开源、支持tez
    缺点：企业级安全不开源

MapR</code></pre><h2 id="3-分布式文件系统HDFS"><a href="#3-分布式文件系统HDFS" class="headerlink" title="3.分布式文件系统HDFS"></a>3.分布式文件系统HDFS</h2><p>HDFS概述<br>    1) 分布式<br>    2）commodity hardware<br>    3）fault-tolerant 容错<br>    4) high throughput<br>    5) large data sets</p>
<pre><code>HDFS是一个分布式的文件系统

文件系统：Linux、Windows、Mac....
    目录结构: C    /
    存放的是文件或者文件夹
    对外提供服务：创建、修改、删除、查看、移动等等

普通文件系统 vs  分布式文件系统
    单机
    分布式文件系统能够横跨N个机器</code></pre><p>HDFS前提和设计目标<br>    Hardware Failure  硬件错误<br>        每个机器只存储文件的部分数据，blocksize=128M<br>        block存放在不同的机器上的，由于容错，HDFS默认采用3副本机制<br>    Streaming Data Access  流式数据访问<br>        The emphasis is on high throughput of data access<br>        rather than low latency of data access.<br>    Large Data Sets  大规模数据集<br>    Moving Computation is Cheaper than Moving Data    移动计算比移动数据更划算</p>
<p>HDFS的架构 *****</p>
<pre><code>1) NameNode(master) and DataNodes(slave)
2) master/slave的架构
3) NN: 
    the file system namespace
        /home/hadoop/software
                    /app
    regulates access to files by clients
4）DN：storage
5）HDFS exposes a file system namespace and allows user data to be stored in files.
6）a file is split into one or more blocks
    blocksize: 128M 
    150M拆成2个block
7）blocks are stored in a set of DataNodes
    为什么？ 容错！！！
8）NameNode executes file system namespace operations：CRUD
9）determines the mapping of blocks to DataNodes
    a.txt  150M   blocksize=128M
    a.txt 拆分成2个block   一个是block1：128M  另一个是block2：22M
    block1存放在哪个DN？block2存放在哪个DN？

    a.txt
        block1：128M, 192.168.199.1
        block2：22M,  192.168.199.2

    get a.txt

    这个过程对于用户来说是不感知的

10）通常情况下：1个Node部署一个组件</code></pre><p>课程环境介绍：<br>本课程录制的系统是Mac，所以我采用的linux客户端是mac自带的shell<br>如果你们是win：xshell、crt<br>服务器/linux地址：192.168.199.233<br>连接到linux环境<br>    登陆：ssh <a href="mailto:hadoop@192.168.199.233" target="_blank" rel="noopener">hadoop@192.168.199.233</a><br>    登陆成功以后：[hadoop@hadoop000 ~]$<br>    linux机器：用户名hadoop、密码123456、hostname是hadoop000<br>    创建课程中所需要的目录（合适的文件存放在合适的目录）<br>        [hadoop@hadoop000 ~]$ mkdir software  存放课程所使用的软件安装包<br>        [hadoop@hadoop000 ~]$ mkdir app       存放课程所有软件的安装目录<br>        [hadoop@hadoop000 ~]$ mkdir data      存放课程中使用的数据<br>        [hadoop@hadoop000 ~]$ mkdir lib       存放课程中开发过的作业jar存放的目录<br>        [hadoop@hadoop000 ~]$ mkdir shell     存放课程中相关的脚本<br>        [hadoop@hadoop000 ~]$ mkdir maven_resp 存放课程中使用到的maven依赖包存放的目录<br>    学员问：root密码<br>        切换hadoop到root用户：[hadoop@hadoop000 ~]$ sudo -i<br>        切换root到hadoop用户：[root@hadoop000 ~]# su hadoop<br>        我OOTB环境中创建的hadoop用户是有sudo权限：sudo vi /etc/hosts<br>    Linux版本：<br>        以前的课程是centos6.4，本次课程升级成centos7</p>
<p>Hadoop环境搭建<br>    使用的Hadoop相关版本：CDH<br>    CDH相关软件包下载地址：<a href="http://archive.cloudera.com/cdh5/cdh/5/" target="_blank" rel="noopener">http://archive.cloudera.com/cdh5/cdh/5/</a><br>    Hadoop使用版本：hadoop-2.6.0-cdh5.15.1<br>    Hadoop下载：wget <a href="http://archive.cloudera.com/cdh5/cdh/5/hadoop-2.6.0-cdh5.15.1.tar.gz" target="_blank" rel="noopener">http://archive.cloudera.com/cdh5/cdh/5/hadoop-2.6.0-cdh5.15.1.tar.gz</a><br>    Hive使用版本：hive-1.1.0-cdh5.15.1</p>
<p>Hadoop/Hive/Spark相关框架的学习：<br>    使用单机版足够  <strong><strong>*<br>        如果使用集群学习会导致：从入门到放弃<br>    使用Linux/Mac学习<br>        一定不要使用Windows搭建Hadoop环境<br>        所以Linux基础是要会的  *</strong></strong></p>
<p>Hadoop安装前置要求<br>    Java  1.8+<br>    ssh</p>
<p>安装Java<br>    拷贝本地软件包到服务器：scp jdk-8u91-linux-x64.tar.gz <a href="mailto:hadoop@192.168.199.233" target="_blank" rel="noopener">hadoop@192.168.199.233</a>:<del>/software/<br>    解压jdk到</del>/app/：tar -zvxf jdk-8u91-linux-x64.tar.gz -C ~/app/<br>    把jdk配置系统环境变量中： ~/.bash_profile<br>        export JAVA_HOME=/home/hadoop/app/jdk1.8.0_91<br>        export PATH=$JAVA_HOME/bin:$PATH<br>    使得配置修改生效：source .bash_profile<br>    验证：java -version</p>
<p>安装ssh无密码登陆<br>    ls<br>    ls -a<br>    ls -la  并没有发现一个.ssh的文件夹</p>
<pre><code>ssh-keygen -t rsa  一路回车
cd ~/.ssh
[hadoop@hadoop000 .ssh]$ ll
总用量 12
-rw------- 1 hadoop hadoop 1679 10月 15 02:54 id_rsa  私钥
-rw-r--r-- 1 hadoop hadoop  398 10月 15 02:54 id_rsa.pub 公钥
-rw-r--r-- 1 hadoop hadoop  358 10月 15 02:54 known_hosts

cat id_rsa.pub &gt;&gt; authorized_keys
chmod 600 authorized_keys</code></pre><p>Hadoop(HDFS)安装<br>    下载<br>    解压：~/app<br>    添加HADOOP_HOME/bin到系统环境变量<br>    修改Hadoop配置文件<br>        hadoop-env.sh<br>            export JAVA_HOME=/home/hadoop/app/jdk1.8.0_91</p>
<pre><code>    core-site.xml
        &lt;property&gt;
            &lt;name&gt;fs.defaultFS&lt;/name&gt;
            &lt;value&gt;hdfs://hadoop000:8020&lt;/value&gt;
        &lt;/property&gt;

    hdfs-site.xml
        &lt;property&gt;
            &lt;name&gt;dfs.replication&lt;/name&gt;
            &lt;value&gt;1&lt;/value&gt;
        &lt;/property&gt;

        &lt;property&gt;
            &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;
            &lt;value&gt;/home/hadoop/app/tmp&lt;/value&gt;
        &lt;/property&gt;

    slaves
        hadoop000
启动HDFS：
    第一次执行的时候一定要格式化文件系统，不要重复执行: hdfs namenode -format
    启动集群：$HADOOP_HOME/sbin/start-dfs.sh
    验证:
        [hadoop@hadoop000 sbin]$ jps
        60002 DataNode
        60171 SecondaryNameNode
        59870 NameNode

        http://192.168.199.233:50070
        如果发现jps ok，但是浏览器不OK？ 十有八九是防火墙问题
        查看防火墙状态：sudo firewall-cmd --state
        关闭防火墙: sudo systemctl stop firewalld.service
        禁止防火墙开机启动：</code></pre><p>hadoop软件包常见目录说明<br>    bin：hadoop客户端名单<br>    etc/hadoop：hadoop相关的配置文件存放目录<br>    sbin：启动hadoop相关进程的脚本<br>    share：常用例子</p>
<p>注意：<br>    start/stop-dfs.sh与hadoop-daemons.sh的关系<br>    start-dfs.sh =<br>        hadoop-daemons.sh start namenode<br>        hadoop-daemons.sh start datanode<br>        hadoop-daemons.sh start secondarynamenode<br>    stop-dfs.sh =<br>        ….</p>
<p>HDFS命令行操作   *****<br>    shell-like<br>        mkdir  ls ….</p>
<p>hadoop fs [generic options]<br>    [-appendToFile <localsrc> … <dst>]<br>    [-cat [-ignoreCrc] <src> …]<br>    [-chgrp [-R] GROUP PATH…]<br>    [-chmod [-R] &lt;MODE[,MODE]… | OCTALMODE&gt; PATH…]<br>    [-chown [-R] [OWNER][:[GROUP]] PATH…]<br>    [-copyFromLocal [-f] [-p] [-l] <localsrc> … <dst>]<br>    [-copyToLocal [-p] [-ignoreCrc] [-crc] <src> … <localdst>]<br>    [-count [-q] [-h] [-v] [-x] <path></path> …]<br>    [-cp [-f] [-p | -p[topax]] <src> … <dst>]<br>    [-df [-h] [<path></path> …]]<br>    [-du [-s] [-h] [-x] <path></path> …]<br>    [-get [-p] [-ignoreCrc] [-crc] <src> … <localdst>]<br>    [-getmerge [-nl] <src> <localdst>]<br>    [-ls [-C] [-d] [-h] [-q] [-R] [-t] [-S] [-r] [-u] [<path></path> …]]<br>    [-mkdir [-p] <path></path> …]<br>    [-moveFromLocal <localsrc> … <dst>]<br>    [-moveToLocal <src> <localdst>]<br>    [-mv <src> … <dst>]<br>    [-put [-f] [-p] [-l] <localsrc> … <dst>]<br>    [-rm [-f] [-r|-R] [-skipTrash] <src> …]<br>    [-rmdir [–ignore-fail-on-non-empty] <dir> …]<br>    [-text [-ignoreCrc] <src> …]</src></dir></src></dst></localsrc></dst></src></localdst></src></dst></localsrc></localdst></src></localdst></src></dst></src></localdst></src></dst></localsrc></src></dst></localsrc></p>
<p>hadoop常用命令：<br>hadoop fs -ls /<br>hadoop fs -put<br>hadoop fs -copyFromLocal<br>hadoop fs -moveFromLocal<br>hadoop fs -cat<br>hadoop fs -text<br>hadoop fs -get<br>hadoop fs -mkdir<br>hadoop fs -mv  移动/改名<br>hadoop fs -getmerge<br>hadoop fs -rm<br>hadoop fs -rmdir<br>hadoop fs -rm -r</p>
<p>HDFS存储扩展：<br>    put: 1file ==&gt; 1…n block ==&gt; 存放在不同的节点上的<br>    get: 去nn上查找这个file对应的元数据信息<br>    了解底层的存储机制这才是我们真正要学习的东西，掌握API那是毛毛雨</p>
<p>使用HDFS API的方式来操作HDFS文件系统<br>    IDEA/Eclipse<br>    Java<br>    使用Maven来管理项目<br>        拷贝jar包<br>        我的所有课程都是使用maven来进行管理的</p>
<p>Caused by: org.apache.hadoop.ipc.RemoteException<br>(org.apache.hadoop.security.AccessControlException):<br>Permission denied: user=rocky, access=WRITE,<br>inode=”/“:hadoop:supergroup:drwxr-xr-x</p>
<p>HDFS操作：shell + Java API<br>综合性的HDFS实战：使用HDFS Java API才完成HDFS文件系统上的文件的词频统计<br>词频统计：wordcount<br>/path/1.txt<br>hello world hello</p>
<p>/path/2.txt<br>hello world hello</p>
<p>==&gt; (hello,4) (world,2)<br>将统计完的结果输出到HDFS上去。</p>
<p>假设：有的小伙伴了解过mr、spark等等，觉得这个操作很简单<br>本实战的要求：只允许使用HDFS API进行操作</p>
<p>目的：<br>    1）掌握HDFS API的操作<br>    2）通过这个案例，让你们对于后续要学习的mr有一个比较好的认识</p>
<p>硬编码 ： 非常忌讳的<br>==&gt; 可配置</p>
<p>可插拔的开发/管理方式  plugin</p>
<p>副本摆放策略<br>    1-本rack的一个节点上<br>    2-另外一个rack的节点上<br>    3-与2相同的rack的另外一个节点上</p>
<pre><code>1-本rack的一个节点上
2-本rack的另外一个节点上
3-不同rack的一个节点上</code></pre><p><img src="https://lixiangbetter.github.io/2020/10/21/hadoop%E5%9F%BA%E7%A1%80/HDFS%E8%AF%BB%E6%95%B0%E6%8D%AE%E6%B5%81%E7%A8%8B.png" alt="HDFS读数据流程"></p>
<p><img src="https://lixiangbetter.github.io/2020/10/21/hadoop%E5%9F%BA%E7%A1%80/HDFS%E5%86%99%E6%95%B0%E6%8D%AE%E6%B5%81%E7%A8%8B.png" alt="HDFS写数据流程"></p>
<p>HDFS的元数据管理<br>    元数据：HDFS的目录结构以及每个文件的BLOCK信息(id，副本系数、block存放在哪个DN上)<br>    存在什么地方：对应配置 ${hadoop.tmp.dir}/name/……<br>    元数据存放在文件中：</p>
<pre><code>/test1
/test1/a.txt
/test2
/test2/1.txt
/test2/2.txt</code></pre><p>![HDFS checkpoint](<a href="https://lixiangbetter.github.io/2020/10/21/hadoop基础/HDFS" target="_blank" rel="noopener">https://lixiangbetter.github.io/2020/10/21/hadoop基础/HDFS</a> checkpoint.png)    </p>
<h2 id="4-分布式计算框架MapReduce"><a href="#4-分布式计算框架MapReduce" class="headerlink" title="4.分布式计算框架MapReduce"></a>4.分布式计算框架MapReduce</h2><p>词频统计、流量统计</p>
<p><img src="https://lixiangbetter.github.io/2020/10/21/hadoop%E5%9F%BA%E7%A1%80/Combiner.png" alt="Combiner"></p>
<p><img src="https://lixiangbetter.github.io/2020/10/21/hadoop%E5%9F%BA%E7%A1%80/Partitioner.png" alt="Combiner"></p>
<h2 id="5-资源调度框架YARN"><a href="#5-资源调度框架YARN" class="headerlink" title="5.资源调度框架YARN"></a>5.资源调度框架YARN</h2><p>YARN产生背景<br>    MapReduce1.x ==&gt; MapReduce2.x<br>        master/slave : JobTracker/TaskTracker<br>        JobTracker：单点、压力大<br>        仅仅只能够支持mapreduce作业</p>
<pre><code>资源利用率
    所有的计算框架运行在一个集群中，共享一个集群的资源，按需分配！</code></pre><p>master: resource management：ResourceManager (RM)<br>job scheduling/monitoring：per-application ApplicationMaster (AM)<br>slave: NodeManager (NM)</p>
<p>YARN架构<br>    Client、ResourceManager、NodeManager、ApplicationMaster<br>    master/slave: RM/NM</p>
<p>Client: 向RM提交任务、杀死任务等<br>ApplicationMaster：<br>    每个应用程序对应一个AM<br>    AM向RM申请资源用于在NM上启动对应的Task<br>    数据切分<br>    为每个task向RM申请资源（container）<br>    NodeManager通信<br>    任务的监控</p>
<p>NodeManager： 多个<br>    干活<br>    向RM发送心跳信息、任务的执行情况<br>    接收来自RM的请求来启动任务<br>    处理来自AM的命令</p>
<p>ResourceManager:集群中同一时刻对外提供服务的只有1个，负责资源相关<br>    处理来自客户端的请求：提交、杀死<br>    启动/监控AM<br>    监控NM<br>    资源相关</p>
<p>container：任务的运行抽象<br>    memory、cpu….<br>    task是运行在container里面的<br>    可以运行am、也可以运行map/reduce task</p>
<p>提交自己开发的MR作业到YARN上运行的步骤：<br>1）mvn clean package -DskipTests<br>    windows/Mac/Linux ==&gt; Maven<br>2）把编译出来的jar包(项目根目录/target/…jar)以及测试数据上传到服务器<br>    scp xxxx hadoop@hostname:directory<br>3) 把数据上传到HDFS<br>    hadoop fs -put xxx hdfspath<br>4) 执行作业<br>    hadoop jar xxx.jar 完整的类名(包名+类名) args…..<br>5) 到YARN UI(8088) 上去观察作业的运行情况<br>6）到输出目录去查看对应的输出结果</p>
<h2 id="6-电商项目实战Hadoop实现"><a href="#6-电商项目实战Hadoop实现" class="headerlink" title="6.电商项目实战Hadoop实现"></a>6.电商项目实战Hadoop实现</h2><p>用户行为日志：<br>    每一次访问的行为(访问、搜索)产生的日志<br>    历史行为数据 &lt;== 历史订单<br>    ==&gt; 推荐<br>    ==&gt; 订单的转换量/率</p>
<p>原始日志字段说明:<br>    第二个字段：url<br>    第十四字段：ip<br>    第十八字段：time</p>
<p>==&gt; 字段的解析<br>    ip =&gt; 地市：国家、省份、城市<br>    url =&gt; 页面ID</p>
<pre><code>referer</code></pre><h2 id="7-数据仓库Hive"><a href="#7-数据仓库Hive" class="headerlink" title="7.数据仓库Hive"></a>7.数据仓库Hive</h2><p>HDFS上的文件并没有schema的概念<br>    schema？</p>
<p>Hive底层执行引擎支持：MR/Tez/Spark</p>
<p>统一元数据管理：<br>    Hive数据是存放在HDFS<br>    元数据信息(记录数据的数据)是存放在MySQL中<br>    SQL on Hadoop： Hive、Spark SQL、impala….</p>
<p>Hive体系架构<br>    client：shell、thrift/jdbc(server/jdbc)、WebUI(HUE/Zeppelin)<br>    metastore：==&gt; MySQL<br>        database：name、location、owner….<br>        table：name、location、owner、column name/type ….</p>
<p>Hive部署<br>    1）下载<br>    2）解压到~/app<br>    3）添加HIVE_HOME到系统环境变量<br>    4）修改配置<br>        hive-env.sh<br>        hive-site.xml<br>    5) 拷贝MySQL驱动包到$HIVE_HOME/lib<br>    6) 前提是要准备安装一个MySQL数据库，yum install去安装一个MySQL数据库<br>    <a href="https://www.cnblogs.com/julyme/p/5969626.html" target="_blank" rel="noopener">https://www.cnblogs.com/julyme/p/5969626.html</a></p>
<pre><code>&lt;?xml version=&quot;1.0&quot;?&gt;
&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;

&lt;configuration&gt;
&lt;property&gt;
  &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;
  &lt;value&gt;jdbc:mysql://hadoop000:3306/hadoop_hive?createDatabaseIfNotExist=true&lt;/value&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;
  &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;
  &lt;value&gt;root&lt;/value&gt;
&lt;/property&gt;

&lt;property&gt;
  &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;
  &lt;value&gt;root&lt;/value&gt;
&lt;/property&gt;
&lt;/configuration&gt;</code></pre><p>Hive部署架构图</p>
<p><img src="https://lixiangbetter.github.io/2020/10/21/hadoop%E5%9F%BA%E7%A1%80/Hive%E9%83%A8%E7%BD%B2%E6%9E%B6%E6%9E%84.png" alt></p>
<p>DDL：Hive Data Definition Language<br>    create、delete、alter…</p>
<p>Hive数据抽象/结构<br>    database     HDFS一个目录<br>        table    HDFS一个目录<br>            data  文件<br>            partition 分区表  HDFS一个目录<br>                data  文件<br>                bucket  分桶   HDFS一个文件</p>
<pre><code>CREATE (DATABASE|SCHEMA) [IF NOT EXISTS] database_name
  [COMMENT database_comment]
  [LOCATION hdfs_path]
  [WITH DBPROPERTIES (property_name=property_value, ...)];</code></pre><p>CREATE DATABASE IF NOT EXISTS hive;</p>
<p>CREATE DATABASE IF NOT EXISTS hive2 LOCATION ‘/test/location’;</p>
<p>CREATE DATABASE IF NOT EXISTS hive3<br>WITH DBPROPERTIES(‘creator’=’pk’);</p>
<p>/user/hive/warehouse是Hive默认的存储在HDFS上的路径</p>
<p>CREATE TABLE emp(<br>empno int,<br>ename string,<br>job string,<br>mgr int,<br>hiredate string,<br>sal double,<br>comm double,<br>deptno int<br>) ROW FORMAT DELIMITED FIELDS TERMINATED BY ‘\t’;</p>
<p>LOAD DATA LOCAL INPATH ‘/home/hadoop/data/emp.txt’ OVERWRITE INTO TABLE emp;</p>
<pre><code>CREATE [TEMPORARY] [EXTERNAL] TABLE [IF NOT EXISTS] [db_name.]table_name    -- (Note: TEMPORARY available in Hive 0.14.0 and later)
  [(col_name data_type [COMMENT col_comment], ... [constraint_specification])]
  [COMMENT table_comment]
  [PARTITIONED BY (col_name data_type [COMMENT col_comment], ...)]
  [CLUSTERED BY (col_name, col_name, ...) [SORTED BY (col_name [ASC|DESC], ...)] INTO num_buckets BUCKETS]
  [SKEWED BY (col_name, col_name, ...)                  -- (Note: Available in Hive 0.10.0 and later)]
     ON ((col_value, col_value, ...), (col_value, col_value, ...), ...)
     [STORED AS DIRECTORIES]
  [
   [ROW FORMAT row_format] 
   [STORED AS file_format]
     | STORED BY &apos;storage.handler.class.name&apos; [WITH SERDEPROPERTIES (...)]  -- (Note: Available in Hive 0.6.0 and later)
  ]
  [LOCATION hdfs_path]
  [TBLPROPERTIES (property_name=property_value, ...)]   -- (Note: Available in Hive 0.6.0 and later)
  [AS select_statement];   -- (Note: Available in Hive 0.5.0 and later; not supported for external tables)


LOAD DATA [LOCAL] INPATH &apos;filepath&apos; [OVERWRITE] INTO TABLE tablename [PARTITION (partcol1=val1, partcol2=val2 ...)]</code></pre><p>LOCAL：本地系统，如果没有local那么就是指的HDFS的路径<br>OVERWRITE：是否数据覆盖，如果没有那么就是数据追加</p>
<p>LOAD DATA LOCAL INPATH ‘/home/hadoop/data/emp.txt’ OVERWRITE INTO TABLE emp;</p>
<p>LOAD DATA INPATH ‘hdfs://hadoop000:8020/data/emp.txt’ INTO TABLE emp;</p>
<p>INSERT OVERWRITE LOCAL DIRECTORY ‘/tmp/hive/‘<br>ROW FORMAT DELIMITED FIELDS TERMINATED BY ‘\t’<br>select empno,ename,sal,deptno from emp;</p>
<p>聚合： max/min/sum/avg</p>
<p>分组函数： group by</p>
<pre><code>求每个部门的平均工资
    出现在select中的字段，如果没有出现在聚合函数里，那么一定要实现在group by里
    select deptno, avg(sal) from emp group by deptno;

求每个部门、工作岗位的平均工资
select deptno,job avg(sal) from emp group by deptno,job;

求每个部门的平均工资大于2000的部门
select deptno, avg(sal) avg_sal from emp group by deptno where avg_sal&gt;2000;

对于分组函数过滤要使用having
select deptno, avg(sal) avg_sal from emp group by deptno having avg_sal&gt;2000;    </code></pre><p>join ： 多表</p>
<p>emp<br>dept</p>
<p>CREATE TABLE dept(<br>deptno int,<br>dname string,<br>loc string<br>) ROW FORMAT DELIMITED FIELDS TERMINATED BY ‘\t’;</p>
<p>LOAD DATA LOCAL INPATH ‘/home/hadoop/data/dept.txt’ OVERWRITE INTO TABLE dept;</p>
<p>explain EXTENDED<br>select<br>e.empno,e.ename,e.sal,e.deptno,d.dname<br>from emp e join dept d<br>on e.deptno=d.deptno;</p>
<h2 id="8-电商项目实战Hive实现"><a href="#8-电商项目实战Hive实现" class="headerlink" title="8.电商项目实战Hive实现"></a>8.电商项目实战Hive实现</h2><p>技术架构图</p>
<p><img src="https://lixiangbetter.github.io/2020/10/21/hadoop%E5%9F%BA%E7%A1%80/%E6%8A%80%E6%9C%AF%E6%9E%B6%E6%9E%84.png" alt></p>
<p>Hive外部表</p>
<p>CREATE TABLE emp(<br>empno int,<br>ename string,<br>job string,<br>mgr int,<br>hiredate string,<br>sal double,<br>comm double,<br>deptno int<br>) ROW FORMAT DELIMITED FIELDS TERMINATED BY ‘\t’;</p>
<p>LOAD DATA LOCAL INPATH ‘/home/hadoop/data/emp.txt’ OVERWRITE INTO TABLE emp;</p>
<p>MANAGED_TABLE:内部表<br>删除表：HDFS上的数据被删除 &amp; Meta也被删除</p>
<p>CREATE EXTERNAL TABLE emp_external(<br>empno int,<br>ename string,<br>job string,<br>mgr int,<br>hiredate string,<br>sal double,<br>comm double,<br>deptno int<br>) ROW FORMAT DELIMITED FIELDS TERMINATED BY ‘\t’<br>location ‘/external/emp/‘;</p>
<p>LOAD DATA LOCAL INPATH ‘/home/hadoop/data/emp.txt’ OVERWRITE INTO TABLE emp_external;</p>
<p>EXTERNAL_TABLE<br>    HDFS上的数据不被删除 &amp; Meta被删除</p>
<p>分区表</p>
<p>create external table track_info(<br>ip string,<br>country string,<br>province string,<br>city string,<br>url string,<br>time string,<br>page string<br>) partitioned by (day string)<br>ROW FORMAT DELIMITED FIELDS TERMINATED BY ‘\t’<br>location ‘/project/trackinfo/‘;</p>
<p>crontab表达式进行调度<br>Azkaban调度：ETLApp==&gt;其他的统计分析<br>    PySpark及调度系统<br>        <a href="https://coding.imooc.com/class/chapter/249.html#Anchor" target="_blank" rel="noopener">https://coding.imooc.com/class/chapter/249.html#Anchor</a></p>
<p>LOAD DATA INPATH ‘hdfs://hadoop000:8020/project/input/etl’ OVERWRITE INTO TABLE track_info partition(day=’2013-07-21’);</p>
<p>select count(*) from track_info where day=’2013-07-21’  ;</p>
<p>select province,count(*) as cnt from track_info where day=’2013-07-21’ group by province ;</p>
<p>省份统计表<br>create table track_info_province_stat(<br>province string,<br>cnt bigint<br>) partitioned by (day string)<br>ROW FORMAT DELIMITED FIELDS TERMINATED BY ‘\t’;</p>
<p>insert overwrite table track_info_province_stat partition(day=’2013-07-21’)<br>select province,count(*) as cnt from track_info where day=’2013-07-21’ group by province ;</p>
<p>到现在为止，我们统计的数据已经在Hive表track_info_province_stat<br>而且这个表是一个分区表，后续统计报表的数据可以直接从这个表中查询<br>也可以将hive表的数据导出到RDBMS（sqoop）</p>
<p>1）ETL<br>2）把ETL输出的数据加载到track_info分区表里<br>3）各个维度统计结果的数据输出到各自维度的表里（track_info_province_stat）<br>4）将数据导出（optional）</p>
<p>如果一个框架不能落地到SQL层面，这个框架就不是一个非常适合的框架</p>
<h2 id="9-Hadoop分布式集群搭建"><a href="#9-Hadoop分布式集群搭建" class="headerlink" title="9.Hadoop分布式集群搭建"></a>9.Hadoop分布式集群搭建</h2><p>Hadoop集群规划<br>    HDFS: NN DN<br>    YARN: RM NM</p>
<p>hadoop000 192.168.199.234<br>    NN RM<br>    DN NM<br>hadoop001 192.168.199.235<br>    DN NM<br>hadoop002 192.168.199.236<br>    DN NM</p>
<p>(每台)<br>/etc/hostname: 修改hostname(hadoop000/hadoop001/hadoop002)<br>/etc/hosts： ip和hostname的映射关系<br>    192.168.199.234 hadoop000<br>    192.168.199.235 hadoop001<br>    192.168.199.236 hadoop002<br>    192.168.199.234 localhost</p>
<p>前置安装 ssh<br>(每台)ssh免密码登陆：ssh-keygen -t rsa<br>在hadoop000机器上进行caozuo<br>ssh-copy-id -i ~/.ssh/id_rsa.pub hadoop000<br>ssh-copy-id -i ~/.ssh/id_rsa.pub hadoop001<br>ssh-copy-id -i ~/.ssh/id_rsa.pub hadoop002</p>
<p>JDK安装<br>1）先在hadoop000机器上部署了jdk<br>2）将jdk bin配置到系统环境变量<br>3）将jdk拷贝到其他节点上去(从hadoop000机器出发)<br>scp -r jdk1.8.0_91 hadoop@hadoop001:<del>/app/<br>scp -r jdk1.8.0_91 hadoop@hadoop002:</del>/app/</p>
<p>scp <del>/.bash_profile hadoop@hadoop001:</del>/<br>scp <del>/.bash_profile hadoop@hadoop002:</del>/</p>
<p>Hadoop部署<br>1）hadoop-env.sh<br>    JAVA_HOME<br>2) core-site.xml<br><property><br>    <name>fs.default.name</name><br>    <value>hdfs://hadoop000:8020</value><br></property></p>
<p>3) hdfs-site.xml<br><property><br>  <name>dfs.namenode.name.dir</name><br>  <value>/home/hadoop/app/tmp/dfs/name</value><br></property></p>
<property>
  <name>dfs.datanode.data.dir</name>
  <value>/home/hadoop/app/tmp/dfs/data</value>
</property>

<p>4) yarn-site.xml<br><property><br>  <name>yarn.nodemanager.aux-services</name><br>  <value>mapreduce_shuffle</value><br> </property></p>
<property>
    <name>yarn.resourcemanager.hostname</name>
    <value>hadoop000</value>
</property>
5) mapred-site.xml
<property>
    <name>mapreduce.framework.name</name>
    <value>yarn</value>
</property>

<p>6) slaves</p>
<p>7) 分发hadoop到其他机器<br>scp -r hadoop-2.6.0-cdh5.15.1 hadoop@hadoop001:<del>/app/<br>scp -r hadoop-2.6.0-cdh5.15.1 hadoop@hadoop002:</del>/app/</p>
<p>scp <del>/.bash_profile hadoop@hadoop001:</del>/<br>scp <del>/.bash_profile hadoop@hadoop002:</del>/</p>
<p>8) NN格式化： hadoop namenode -format<br>9) 启动HDFS<br>10) 启动YARN</p>
]]></content>
      <categories>
        <category>bigdata</category>
      </categories>
      <tags>
        <tag>Hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title>flink学习笔记</title>
    <url>/2020/10/16/flink%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<h1 id="Flink学习笔记"><a href="#Flink学习笔记" class="headerlink" title="Flink学习笔记"></a>Flink学习笔记</h1><h2 id="1-初识FLink"><a href="#1-初识FLink" class="headerlink" title="1.初识FLink"></a>1.初识FLink</h2><h3 id="Flink是什么"><a href="#Flink是什么" class="headerlink" title="Flink是什么"></a>Flink是什么</h3><p>Apache Flink是一个框架和分布式处理引擎，用于对<strong>无限制和有限制的</strong>数据流进行有状态的计算。Flink被设计为可在<strong>所有常见的集群环境中</strong>运行，<strong>以内存速度</strong>和<strong>任何规模</strong>执行计算。</p>
<p>Unbounded data: 有头无尾</p>
<p>Bounded data: 有头有尾</p>
<p>==&gt; 都可以使用flink来进行处理，对应的就是流处理和批处理</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Spark: Streaming 结构化流 批处理为主</span><br><span class="line">				流处理是批处理的一个特例（mini batch)</span><br><span class="line">Flink: 流式为主，批处理是流式处理的一个特例</span><br><span class="line">Storm: 流式 tuple</span><br></pre></td></tr></table></figure>

<h2 id="2-快速上手开发第一个Flink应用程序"><a href="#2-快速上手开发第一个Flink应用程序" class="headerlink" title="2.快速上手开发第一个Flink应用程序"></a>2.快速上手开发第一个Flink应用程序</h2><p>环境准备<br>    JDK:<br>        下载地址：<a href="https://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html" target="_blank" rel="noopener">https://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html</a><br>        Mac ：dmg<br>        Linux： tar.gz<br>        Windows： exe</p>
<pre><code>Maven
    官网：maven.apache.org
    下载地址：https://archive.apache.org/dist/maven/maven-3/3.3.9/binaries/apache-maven-3.3.9-bin.tar.gz
    Linux/Mac/Windows：解压
        tar -zxvf apache-maven-3.3.9-bin.tar.gz -C ~/app
    conf/setting.xml
    &lt;localRepository&gt;/Users/rocky/maven_repos&lt;/localRepository&gt;</code></pre><p>Flink开发批处理应用程序<br>    需求：词频统计(word count)<br>        一个文件，统计文件中每个单词出现的次数<br>        分隔符是\t<br>        统计结果我们直接打印在控制台（生产上肯定是Sink到目的地）<br>    实现：<br>        Flink + Java<br>            前置条件： Maven 3.0.4 (or higher) and Java 8.x</p>
<pre><code>    第一种创建项目的方式：
        mvn archetype:generate                               \
        -DarchetypeGroupId=org.apache.flink              \
        -DarchetypeArtifactId=flink-quickstart-java      \
        -DarchetypeVersion=1.7.0 \
        -DarchetypeCatalog=local

        out of the box：OOTB 开箱即用

    开发流程/开发八股文编程
        1）set up the batch execution environment
        2）read
        3）transform operations  开发的核心所在：开发业务逻辑
        4）execute program

    功能拆解
        1）读取数据  
            hello    welcome
        2）每一行的数据按照指定的分隔符拆分
            hello
            welcome
        3）为每一个单词赋上次数为1
            (hello,1)
            (welcome,1)    
        4) 合并操作  groupBy    


Flink + Scala
    前置条件：Maven 3.0.4 (or higher) and Java 8.x 

    mvn archetype:generate                               \
    -DarchetypeGroupId=org.apache.flink              \
    -DarchetypeArtifactId=flink-quickstart-scala     \
    -DarchetypeVersion=1.7.0 \
    -DarchetypeCatalog=local

Flink Java vs Scala
    1) 算子  map  filter  
    2）简洁性</code></pre><h2 id="3-编程模型及核心概念"><a href="#3-编程模型及核心概念" class="headerlink" title="3.编程模型及核心概念"></a>3.编程模型及核心概念</h2><p>大数据处理的流程：<br>    MapReduce：input -&gt; map(reduce) -&gt; output<br>    Storm:  input -&gt; Spout/Bolt -&gt; output<br>    Spark: input -&gt; transformation/action –&gt; output<br>    Flink:  input -&gt;  transformation/sink –&gt; output</p>
<p>DataSet and DataStream<br>    immutable<br>    批处理：DataSet<br>    流处理：DataStream</p>
<p>Flink编程模型<br>    1）获取执行环境<br>    2）获取数据<br>    3）transformation<br>    4）sink<br>    5）触发执行</p>
<p>select a.<em>, b.</em> from a join b on a.id = b.id</p>
<p>&lt;a1,(a.*)&gt;</p>
<p>&lt;b1,(b.*)&gt;</p>
<h2 id="4-DataSet-API编程"><a href="#4-DataSet-API编程" class="headerlink" title="4.DataSet API编程"></a>4.DataSet API编程</h2><h2 id="5-DataStream-API编程"><a href="#5-DataStream-API编程" class="headerlink" title="5.DataStream API编程"></a>5.DataStream API编程</h2><h2 id="6-Flink-Table-API-amp-SQL编程"><a href="#6-Flink-Table-API-amp-SQL编程" class="headerlink" title="6.Flink Table API &amp; SQL编程"></a>6.Flink Table API &amp; SQL编程</h2><p>DataSet&amp;DataStream API<br>    1) 熟悉两套API：DataSet/DataStream   Java/Scala<br>        MapReduce ==&gt; Hive  SQL<br>        Spark ==&gt; Spark SQL<br>        Flink ==&gt; SQL<br>    2) Flink是支持批处理/流处理，如何做到API层面的统一    </p>
<p>==&gt; Table &amp; SQL API  关系型API</p>
<p>Everybody knows SQL</p>
<h2 id="7-Flink中的Time及Windows的使用"><a href="#7-Flink中的Time及Windows的使用" class="headerlink" title="7.Flink中的Time及Windows的使用"></a>7.Flink中的Time及Windows的使用</h2><p>对于Flink里面的三种时间<br>    事件时间  10:30<br>    摄取时间  11:00<br>    处理时间  11:30</p>
<p>思考：<br>对于流处理来说，你们觉得应该是以哪个时间作为基准时间来进行业务逻辑的处理呢？</p>
<p>幂等性</p>
<p>env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)<br>思考：默认的TimeCharacteristic是什么？</p>
<p>窗口分配器：定义如何将数据分配给窗口</p>
<p>A WindowAssigner is responsible for assigning each incoming element to one or more windows<br>每个传入的数据分配给一个或者多个窗口</p>
<p>tumbling windows 滚动窗口<br>    have a fixed size and do not overlap<br>sliding windows  滑动窗口<br>    overlapping   业务案例：每隔半小时，统计前一个小时的top n商品。<br>session windows  会话窗口<br>global windows   全局窗口</p>
<p>[start timestamp , end timestamp)</p>
<p>ReduceFunction: 两两进行处理</p>
<p>ProcessWindowFunction：整个窗口全部到了，再处理。场景：对窗口中的数据排序</p>
<p><a href="https://blog.csdn.net/lmalds/article/details/52704170" target="_blank" rel="noopener">https://blog.csdn.net/lmalds/article/details/52704170</a></p>
<h2 id="8-Flink-Connectors"><a href="#8-Flink-Connectors" class="headerlink" title="8.Flink Connectors"></a>8.Flink Connectors</h2><p>ZooKeeper<br>    <a href="https://archive.cloudera.com/cdh5/cdh/5/" target="_blank" rel="noopener">https://archive.cloudera.com/cdh5/cdh/5/</a></p>
<pre><code>ssh hadoop@192.168.199.233
1) 从~/software下解压到~/app目录下
2) 配置系统环境变量  ~/.bash_profile
3) 配置文件  $ZK_HOME/conf.zoo.cfg  dataDir不要放在默认的/tmp下
4) 启动ZK   $ZK_HOME/bin/zkServer.sh start
5) 检查是否启动成功   jps  QuorumPeerMain</code></pre><p>Kafka<br>    wget <a href="http://mirrors.tuna.tsinghua.edu.cn/apache/kafka/1.1.1/kafka_2.11-1.1.1.tgz" target="_blank" rel="noopener">http://mirrors.tuna.tsinghua.edu.cn/apache/kafka/1.1.1/kafka_2.11-1.1.1.tgz</a></p>
<pre><code>ssh hadoop@192.168.199.233
1) 从~/software下解压到~/app目录下
2) 配置系统环境变量  ~/.bash_profile
3) 配置文件 $KAFKA_HOME/config/server.properties
    log.dirs 不要放在默认的/tmp下
4) 启动Kafka$KAFKA_HOME/bin/kafka-server-start.sh -daemon /home/hadoop/app/kafka_2.11-1.1.1/config/server.properties 
5) 检查是否启动成功 jps Kafka
6) 创建topic</code></pre><p>./kafka-topics.sh –create –zookeeper hadoop000:2181 –replication-factor 1 –partitions 1 –topic pktest<br>    7) 查看所有的topic<br>./kafka-topics.sh –list –zookeeper hadoop000:2181<br>    8) 启动生产者<br>./kafka-console-producer.sh –broker-list hadoop000:9092 –topic pktest<br>    9) 启动消费者<br>./kafka-console-consumer.sh –bootstrap-server hadoop000:9092 –topic pktest    </p>
<p>作业：请使用Java语言开发FlinkKafkaConsumer/FlinkKafkaProducer实例</p>
<h2 id="9-Flink部署及作业提交"><a href="#9-Flink部署及作业提交" class="headerlink" title="9.Flink部署及作业提交"></a>9.Flink部署及作业提交</h2><p>Flink的单机部署方式<br>    开发/测试</p>
<p>前置条件：<br>    JDK8<br>    Maven3</p>
<p>ssh <a href="mailto:hadoop@192.168.199.233" target="_blank" rel="noopener">hadoop@192.168.199.233</a></p>
<p>通过下载Flink源码进行编译，不是使用直接下载二进制包<br>下载到:<br>    1）服务器：~/source  wget <a href="https://github.com/apache/flink/archive/release-1.7.0.tar.gz" target="_blank" rel="noopener">https://github.com/apache/flink/archive/release-1.7.0.tar.gz</a><br>    2) 本地：<a href="https://github.com/apache/flink/archive/release-1.7.0.tar.gz" target="_blank" rel="noopener">https://github.com/apache/flink/archive/release-1.7.0.tar.gz</a></p>
<p>mvn clean install -DskipTests -Pvendor-repos -Dfast -Dhadoop.version=2.6.0-cdh5.15.1</p>
<p>第一次编译是需要花费很长时间的，因为需要去中央仓库下载flink源码中所有的依赖包</p>
<p>Standalone的最简单的方式</p>
<p>./bin/flink run examples/streaming/SocketWindowWordCount.jar –port 9000</p>
<p>./bin/flink # 路径  $FLINK_HOME</p>
<p>Standalone-分布式<br>1） Java 1.8.x or higher<br>2） ssh  多个机器之间要互通     Hadoop详细讲解<br>    ping hadoop000<br>    ping hadoop001<br>    ping hadoop002</p>
<pre><code>JDK
Flink  同一个目录  集群里面的机器 部署的目录都是一样

每个机器需要添加ip和hostname的映射关系</code></pre><p>3) conf<br>    flink-conf.yaml<br>        jobmanager.rpc.address: 10.0.0.1  配置主节点的ip</p>
<pre><code>jobmanager   主节点
taskmanager  从节点

slaves
    每一行配置一个ip/host</code></pre><p>4）常用配置<br>    jobmanager.rpc.address   master节点的地址<br>    jobmanager.heap.mb  jobmanager节点可用的内存<br>    taskmanager.heap.mb taskmanager节点可用的内存<br>    taskmanager.numberOfTaskSlots 每个机器可用的cpu个数<br>    parallelism.default   任务的并行度<br>    taskmanager.tmp.dirs  taskmanager的临时数据存储目录</p>
<p>扩展或者容错</p>
<p>ON YARN是企业级用的最多的方式  *****</p>
<p>-n taskmanager的数量<br>-jm jobmanager的内存<br>-tm taskmanager的内存</p>
<p>./bin/flink run ./examples/batch/WordCount.jar <br>-input hdfs://hadoop000:8020/LICENSE-2.0.txt <br>-output hdfs://hadoop000:8020/wordcount-result.txt</p>
<p>./bin/flink run -m yarn-cluster -yn 1 ./examples/batch/WordCount.jar</p>
<p>作业：<br>1） 快速开发一个Flink应用程序<br>    Scala&amp;Java<br>    批处理&amp;流处理</p>
<p>批处理Scala和批处理Java的Flink作业提交到YARN上去执行，任意YARN模式</p>
<p>2）可选 HA的配置<br><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.7/ops/jobmanager_high_availability.html" target="_blank" rel="noopener">https://ci.apache.org/projects/flink/flink-docs-release-1.7/ops/jobmanager_high_availability.html</a></p>
<h2 id="10-Flink监控及调优"><a href="#10-Flink监控及调优" class="headerlink" title="10.Flink监控及调优"></a>10.Flink监控及调优</h2><p>History Server<br>    Hadoop MapReduce<br>    Spark<br>    Flink</p>
<p>start/stop-xxx.sh<br>    看一下这些脚本的写法<br>    shell对于bigdata有用吗？ lower</p>
<p>配置：<br>historyserver.web.address: 0.0.0.0<br>historyserver.web.port: 8082<br>historyserver.archive.fs.refresh-interval: 10000</p>
<p>jobmanager.archive.fs.dir: hdfs://hadoop000:8020/completed-jobs-pk/<br>historyserver.archive.fs.dir: hdfs://hadoop000:8020/completed-jobs-pk/</p>
<p>启动：./historyserver.sh start</p>
<p>思考：有了HistoryServer之后为什么还需要提供REST API？</p>
<p>Ganglia</p>
<p>Flink中常用的优化策略<br>1）资源<br>2）并行度<br>    默认是1   适当的调整：好几种  ==&gt; 项目实战<br>3）数据倾斜<br>    100task  98-99跑完了  1-2很慢   ==&gt; 能跑完 、 跑不完<br>    group by： 二次聚合<br>        random_key  + random<br>        key  - random<br>    join on xxx=xxx<br>        repartition-repartition strategy  大大<br>        broadcast-forward strategy  大小</p>
<p><a href="https://dzone.com/articles/four-ways-to-optimize-your-flink-applications" target="_blank" rel="noopener">https://dzone.com/articles/four-ways-to-optimize-your-flink-applications</a></p>
<h2 id="11-基于Flink的互联网直播平台日志分析项目实战"><a href="#11-基于Flink的互联网直播平台日志分析项目实战" class="headerlink" title="11.基于Flink的互联网直播平台日志分析项目实战"></a>11.基于Flink的互联网直播平台日志分析项目实战</h2><p>项目背景<br>aliyun    CN    A    E    [17/Jul/2018:17:07:50 +0800]    2    223.104.18.110    -    112.<br>29.213.35:80    0    v2.go2yd.com    GET    <a href="http://v1.go2yd.com/user_upload/1531633977627104fdec" target="_blank" rel="noopener">http://v1.go2yd.com/user_upload/1531633977627104fdec</a><br>dc68fe7a2c4b96b2226fd3f4c.mp4_bd.mp4    HTTP/1.1    -    bytes 13869056-13885439/25136186    TCP_HIT/206    112.29.213.35    video/mp4    17168    16384    -:0    0    0    -    -    11451601    -    “JSP3/2.0.14”    “-“    “-“    “-“    http    -    2    v1.g<br>o2yd.com    0.002    25136186    16384    -    -    -    -    -    -    -    1531818470104-114516<br>01-112.29.213.66#2705261172    644514568</p>
<p>aliyun<br>CN<br>E<br>[17/Jul/2018:17:07:50 +0800]<br>223.104.18.110<br>v2.go2yd.com<br>17168</p>
<p>接入的数据类型就是日志<br>离线：Flume==&gt;HDFS<br>实时：Kafka==&gt;流处理引擎==&gt;ES==&gt;Kibana</p>
<p>项目架构图</p>
<p><img src="https://lixiangbetter.github.io/2020/10/16/flink%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1589529851438.jpg" alt></p>
<p>项目功能<br>1）统计一分钟内每个域名访问产生的流量<br>    Flink接收Kafka的进行处理<br>2）统计一分钟内每个用户产生的流量<br>    域名和用户是有对应关系的<br>    Flink接收Kafka的进行 + Flink读取域名和用户的配置数据  进行处理</p>
<p>数据：Mock  *****</p>
<p>Mock数据：务必要掌握的<br>    数据敏感<br>    多团队协作，你依赖了其他团队提供的服务或者接口</p>
<pre><code>通过Mock的方式往Kafka的broker里面发送数据

Java/Scala Code：producer
kafka控制台消费者：consumer</code></pre><p>需求：最近一分钟每个域名对应的流量</p>
<p>问题：<br>    可以到QQ群或者问答区进行交流，<br>    我在群里的，<br>    问答区的问题我会用最快的速度答疑</p>
<p>ES部署<br>    1） CentOS7.x<br>    2） 非root     hadoop</p>
<p>ELK</p>
<p>Kibana部署</p>
<p>curl -XPUT ‘<a href="http://hadoop000:9200/cdn&#39;" target="_blank" rel="noopener">http://hadoop000:9200/cdn&#39;</a></p>
<p>curl -H “Content-Type: application/json” -XPOST ‘<a href="http://hadoop000:9200/cdn/traffic/_mapping?pretty&#39;" target="_blank" rel="noopener">http://hadoop000:9200/cdn/traffic/_mapping?pretty&#39;</a> -d ‘{<br>“traffic”:{<br>    “properties”:{<br>        “domain”:{“type”:”text”},<br>        “traffics”:{“type”:”long”},<br>        “time”:{“type”:”date”,”format”: “yyyy-MM-dd HH:mm”}<br>        }<br>    }<br>}<br>‘</p>
<p>curl -XDELETE ‘hadoop000:9200/cdn’</p>
<p>curl -H “Content-Type: application/json” -XPOST ‘<a href="http://hadoop000:9200/cdn/traffic/_mapping?pretty&#39;" target="_blank" rel="noopener">http://hadoop000:9200/cdn/traffic/_mapping?pretty&#39;</a> -d ‘{<br>“traffic”:{<br>    “properties”:{<br>        “domain”:{“type”:”keyword”},<br>        “traffics”:{“type”:”long”},<br>        “time”:{“type”:”date”,”format”: “yyyy-MM-dd HH:mm”}<br>        }<br>    }<br>}<br>‘</p>
<p>作业：<br>1）代码我们都是本地IDEA中运行的，将代码打包，运行在YARN上<br>2）把代码中写死的信息(ip port)改成读配置的方式</p>
<p>需求：CDN业务<br>userid对应多个域名</p>
<p>userid: 8000000</p>
<p>domains:<br>    v1.go2yd.com<br>    v2.go2yd.com<br>    v3.go2yd.com<br>    v4.go2yd.com<br>    vmi.go2yd.com</p>
<p>userid: 8000001<br>    test.gifshow.com</p>
<p>用户id和域名的映射关系<br>    从日志里能拿到domain，还得从另外一个表(MySQL)里面去获取userid和domain的映射关系</p>
<p>CREATE TABLE user_domain_config(<br>id int unsigned auto_increment,<br>user_id varchar(40) not null,<br>domain varchar(40) not null,<br>primary key (id)<br>);</p>
<p>insert into user_domain_config(user_id,domain) values(‘8000000’,’v1.go2yd.com’);<br>insert into user_domain_config(user_id,domain) values(‘8000000’,’v2.go2yd.com’);<br>insert into user_domain_config(user_id,domain) values(‘8000000’,’v3.go2yd.com’);<br>insert into user_domain_config(user_id,domain) values(‘8000000’,’v4.go2yd.com’);<br>insert into user_domain_config(user_id,domain) values(‘8000000’,’vmi.go2yd.com’);</p>
<p>在做实时数据清洗的时候，不仅需要处理raw日志，还需要关联MySQL表里的数据</p>
<p>自定义一个Flink去读MySQL数据的数据源，然后把两个Stream关联起来</p>
<p>Flink进行数据的清洗<br>    读取Kafka的数据<br>    读取MySQL的数据<br>    connect</p>
<pre><code>业务逻辑的处理分析：水印 WindowFunction
==&gt; ES 注意数据类型  &lt;= Kibana 图形化的统计结果展示</code></pre><p>Kibana：各个环节的监控  监控图形化</p>
<p>1 30<br>2 40<br>3 300<br>4 35</p>
<p>我们已经实现的 +  CDN业务文档的描述  ==&gt; 扩展</p>
<h2 id="12-Flink版本升级"><a href="#12-Flink版本升级" class="headerlink" title="12.Flink版本升级"></a>12.Flink版本升级</h2><p>1）代码层面 pom.xml flink.version</p>
<p>2）服务器运行环境的层面</p>
<p>​    standalone    每个服务器都得升级Flink版本</p>
<p>​    yarn    Flink仅仅是作为一个客户端进行作业的提交的，只需要在你的flink作业的提交机器上升级flink即可</p>
<p>3）Flink的部署包也要升级</p>
<p>​    获取到最新的Flink的源码，然后根据你的hadoop版本如何做好升级呢？</p>
<p><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/ops/upgrading.html" target="_blank" rel="noopener">https://ci.apache.org/projects/flink/flink-docs-release-1.11/ops/upgrading.html</a></p>
]]></content>
      <categories>
        <category>bigdata</category>
      </categories>
      <tags>
        <tag>flink</tag>
      </tags>
  </entry>
  <entry>
    <title>架构设计&amp;分布式&amp;数据结构与算法笔记</title>
    <url>/2020/07/19/%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1-%E5%88%86%E5%B8%83%E5%BC%8F-%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<h1 id="架构设计-分布式-数据结构与算法笔记"><a href="#架构设计-分布式-数据结构与算法笔记" class="headerlink" title="架构设计-分布式-数据结构与算法笔记"></a>架构设计-分布式-数据结构与算法笔记</h1><h2 id="架构设计"><a href="#架构设计" class="headerlink" title="架构设计"></a>架构设计</h2><h3 id="请列举出在JDK中几个常用的设计模式？"><a href="#请列举出在JDK中几个常用的设计模式？" class="headerlink" title="请列举出在JDK中几个常用的设计模式？"></a>请列举出在JDK中几个常用的设计模式？</h3><p>单例模式（Singleton pattern）用于Runtime，Calendar和其他的一些类中。工厂模式（Factory pattern）被用于各种不可变的类如 Boolean，像Boolean.valueOf，观察者模式（Observer pattern）被用于 Swing 和很多的事件监听中。装饰器设计模式（Decorator design pattern）被用于多个 Java IO 类中。</p>
<h3 id="什么是设计模式？你是否在你的代码里面使用过任何设计模式？"><a href="#什么是设计模式？你是否在你的代码里面使用过任何设计模式？" class="headerlink" title="什么是设计模式？你是否在你的代码里面使用过任何设计模式？"></a>什么是设计模式？你是否在你的代码里面使用过任何设计模式？</h3><p>设计模式是软件开发人员在软件开发过程中面临的一般问题的解决方案。这些解决方案是众多软件开发人员经过相当长的一段时间的试验和错误总结出来的。设计模式是代码可用性的延伸。</p>
<p>设计模式分类：创建型模式，结构型模式，行为型模式</p>
<h3 id="静态代理、JDK动态代理以及CGLIB动态代理"><a href="#静态代理、JDK动态代理以及CGLIB动态代理" class="headerlink" title="静态代理、JDK动态代理以及CGLIB动态代理"></a>静态代理、JDK动态代理以及CGLIB动态代理</h3><p>代理模式是java中最常用的设计模式之一，尤其是在spring框架中广泛应用。对于java的代理模式，一般可分为：静态代理、动态代理、以及CGLIB实现动态代理。</p>
<p>对于上述三种代理模式，分别进行说明。</p>
<h4 id="静态代理"><a href="#静态代理" class="headerlink" title="静态代理"></a>静态代理</h4><p>静态代理其实就是在程序运行之前，提前写好被代理方法的代理类，编译后运行。在程序运行之前，class已经存在。<br>下面我们实现一个静态代理demo:</p>
<p><img src="https://lixiangbetter.github.io/2020/07/19/%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1-%E5%88%86%E5%B8%83%E5%BC%8F-%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy8zMjM3NDMyLTVkNmViYWI3YTlmM2Q4MzIucG5n.jpeg" alt></p>
<p>定义一个接口Target</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.test.proxy;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">Target</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">execute</span><span class="params">()</span></span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>TargetImpl 实现接口Target</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.test.proxy;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TargetImpl</span> <span class="keyword">implements</span> <span class="title">Target</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">execute</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        System.out.println(<span class="string">"TargetImpl execute！"</span>);</span><br><span class="line">        <span class="keyword">return</span> <span class="string">"execute"</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.test.proxy;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Proxy</span> <span class="keyword">implements</span> <span class="title">Target</span></span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> Target target;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Proxy</span><span class="params">(Target target)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.target = target;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">execute</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        System.out.println(<span class="string">"perProcess"</span>);</span><br><span class="line">        String result = <span class="keyword">this</span>.target.execute();</span><br><span class="line">        System.out.println(<span class="string">"postProcess"</span>);</span><br><span class="line">        <span class="keyword">return</span> result;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>测试类:</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.test.proxy;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ProxyTest</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        Target target = <span class="keyword">new</span> TargetImpl();</span><br><span class="line">        Proxy p = <span class="keyword">new</span> Proxy(target);</span><br><span class="line">        String result =  p.execute();</span><br><span class="line">        System.out.println(result);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>运行结果:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">perProcess</span><br><span class="line">TargetImpl execute！</span><br><span class="line">postProcess</span><br><span class="line">execut</span><br></pre></td></tr></table></figure>

<p>静态代理需要针对被代理的方法提前写好代理类，如果被代理的方法非常多则需要编写很多代码，因此，对于上述缺点，通过动态代理的方式进行了弥补。</p>
<h4 id="动态代理"><a href="#动态代理" class="headerlink" title="动态代理"></a>动态代理</h4><p>动态代理主要是通过反射机制，在运行时动态生成所需代理的class.</p>
<p><img src="https://lixiangbetter.github.io/2020/07/19/%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1-%E5%88%86%E5%B8%83%E5%BC%8F-%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy8zMjM3NDMyLTYzMzU2NDcwZjY0MmQ2NjUucG5n.jpeg" alt></p>
<p>接口</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.test.dynamic;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">Target</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">execute</span><span class="params">()</span></span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>实现类</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.test.dynamic;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TargetImpl</span> <span class="keyword">implements</span> <span class="title">Target</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">execute</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        System.out.println(<span class="string">"TargetImpl execute！"</span>);</span><br><span class="line">        <span class="keyword">return</span> <span class="string">"execute"</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>代理类</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.test.dynamic;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.lang.reflect.InvocationHandler;</span><br><span class="line"><span class="keyword">import</span> java.lang.reflect.Method;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">DynamicProxyHandler</span> <span class="keyword">implements</span> <span class="title">InvocationHandler</span></span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> Target target;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">DynamicProxyHandler</span><span class="params">(Target target)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.target = target;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Object <span class="title">invoke</span><span class="params">(Object proxy, Method method, Object[] args)</span> <span class="keyword">throws</span> Throwable </span>&#123;</span><br><span class="line">        System.out.println(<span class="string">"========before=========="</span>);</span><br><span class="line">        Object result = method.invoke(target,args);</span><br><span class="line">        System.out.println(<span class="string">"========after==========="</span>);</span><br><span class="line">        <span class="keyword">return</span> result;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>测试类</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.test.dynamic;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.lang.reflect.Proxy;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">DynamicProxyTest</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        Target target = <span class="keyword">new</span> TargetImpl();</span><br><span class="line">        DynamicProxyHandler handler = <span class="keyword">new</span> DynamicProxyHandler(target);</span><br><span class="line">        Target proxySubject = (Target) Proxy.newProxyInstance(TargetImpl<span class="class">.<span class="keyword">class</span>.<span class="title">getClassLoader</span>(),<span class="title">TargetImpl</span>.<span class="title">class</span>.<span class="title">getInterfaces</span>(),<span class="title">handler</span>)</span>;</span><br><span class="line">        String result = proxySubject.execute();</span><br><span class="line">        System.out.println(result);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>运行结果：</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">========before==========</span><br><span class="line">TargetImpl execute！</span><br><span class="line">========after===========</span><br><span class="line">execute</span><br></pre></td></tr></table></figure>

<p>无论是动态代理还是静态代理，都需要定义接口，然后才能实现代理功能。这同样存在局限性，因此，为了解决这个问题，出现了第三种代理方式：cglib代理。</p>
<h4 id="cglib代理"><a href="#cglib代理" class="headerlink" title="cglib代理"></a>cglib代理</h4><p>CGLib采用了非常底层的字节码技术，其原理是通过字节码技术为一个类创建子类，并在子类中采用方法拦截的技术拦截所有父类方法的调用，顺势织入横切逻辑。JDK动态代理与CGLib动态代理均是实现Spring AOP的基础。</p>
<p><img src="https://lixiangbetter.github.io/2020/07/19/%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1-%E5%88%86%E5%B8%83%E5%BC%8F-%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy8zMjM3NDMyLTYzOGY4MmM2ZDRkNTExNDUucG5n.jpeg" alt></p>
<p>目标类</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.test.cglib;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Target</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">execute</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        String message = <span class="string">"-----------test------------"</span>;</span><br><span class="line">        System.out.println(message);</span><br><span class="line">        <span class="keyword">return</span> message;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>通用代理类</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.test.cglib;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> net.sf.cglib.proxy.MethodInterceptor;</span><br><span class="line"><span class="keyword">import</span> net.sf.cglib.proxy.MethodProxy;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.lang.reflect.Method;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MyMethodInterceptor</span> <span class="keyword">implements</span> <span class="title">MethodInterceptor</span></span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Object <span class="title">intercept</span><span class="params">(Object obj, Method method, Object[] args, MethodProxy proxy)</span> <span class="keyword">throws</span> Throwable </span>&#123;</span><br><span class="line">        System.out.println(<span class="string">"&gt;&gt;&gt;&gt;MethodInterceptor start..."</span>);</span><br><span class="line">        Object result = proxy.invokeSuper(obj,args);</span><br><span class="line">        System.out.println(<span class="string">"&gt;&gt;&gt;&gt;MethodInterceptor ending..."</span>);</span><br><span class="line">        <span class="keyword">return</span> <span class="string">"result"</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>测试类</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">package</span> com.test.cglib;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> net.sf.cglib.proxy.Enhancer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CglibTest</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        System.out.println(<span class="string">"***************"</span>);</span><br><span class="line">        Target target = <span class="keyword">new</span> Target();</span><br><span class="line">        CglibTest test = <span class="keyword">new</span> CglibTest();</span><br><span class="line">        Target proxyTarget = (Target) test.createProxy(Target<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        String res = proxyTarget.execute();</span><br><span class="line">        System.out.println(res);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Object <span class="title">createProxy</span><span class="params">(Class targetClass)</span> </span>&#123;</span><br><span class="line">        Enhancer enhancer = <span class="keyword">new</span> Enhancer();</span><br><span class="line">        enhancer.setSuperclass(targetClass);</span><br><span class="line">        enhancer.setCallback(<span class="keyword">new</span> MyMethodInterceptor());</span><br><span class="line">        <span class="keyword">return</span> enhancer.create();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>执行结果:</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">***************</span><br><span class="line">&gt;&gt;&gt;&gt;MethodInterceptor start...</span><br><span class="line">-----------test------------</span><br><span class="line">&gt;&gt;&gt;&gt;MethodInterceptor ending...</span><br><span class="line">result</span><br></pre></td></tr></table></figure>

<p>代理对象的生成过程由Enhancer类实现，大概步骤如下：</p>
<ol>
<li>生成代理类Class的二进制字节码；</li>
<li>通过Class.forName加载二进制字节码，生成Class对象；</li>
<li>通过反射机制获取实例构造，并初始化代理类对象。</li>
</ol>
<h3 id="单例模式"><a href="#单例模式" class="headerlink" title="单例模式"></a>单例模式</h3><p>单例模式（Singleton Pattern）是 Java 中最简单的设计模式之一。这种类型的设计模式属于创建型模式，它提供了一种创建对象的最佳方式。</p>
<p><strong>意图</strong>：保证一个类仅有一个实例，并提供一个访问它的全局访问点。</p>
<p><strong>主要解决</strong>：一个全局使用的类频繁地创建与销毁。</p>
<p>懒汉式，线程安全</p>
<p><strong>代码实例</strong>：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Singleton2</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> Singleton2 instance;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="title">Singleton2</span><span class="params">()</span> </span>&#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">synchronized</span> Singleton2 <span class="title">getInstance</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (instance == <span class="keyword">null</span>) &#123;</span><br><span class="line">            instance = <span class="keyword">new</span> Singleton2();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> instance;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>饿汉式，线程安全</p>
<p><strong>代码实例</strong>：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Singleton3</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> Singleton3 instance = <span class="keyword">new</span> Singleton3();</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="title">Singleton3</span><span class="params">()</span> </span>&#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> Singleton3 <span class="title">getInstance</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> instance;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>双检锁/双重校验锁 + volatile关键字</p>
<p><strong>代码实例</strong>：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Singleton7</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">volatile</span> Singleton7 instance = <span class="keyword">null</span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="title">Singleton7</span><span class="params">()</span> </span>&#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> Singleton7 <span class="title">getInstance</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (instance == <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="keyword">synchronized</span> (Singleton7<span class="class">.<span class="keyword">class</span>) </span>&#123;</span><br><span class="line">                <span class="keyword">if</span> (instance == <span class="keyword">null</span>) &#123;</span><br><span class="line">                    instance = <span class="keyword">new</span> Singleton7();</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> instance;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h3 id="工厂模式"><a href="#工厂模式" class="headerlink" title="工厂模式"></a>工厂模式</h3><p>工厂模式（Factory Pattern）是 Java 中最常用的设计模式之一。这种类型的设计模式属于创建型模式，它提供了一种创建对象的最佳方式。</p>
<p>意图：定义一个创建对象的接口，让其子类自己决定实例化哪一个工厂类，工厂模式使其创建过程延迟到子类进行。</p>
<p>主要解决：主要解决接口选择的问题。</p>
<p>我们将创建一个 Shape 接口和实现 Shape 接口的实体类。下一步是定义工厂类 ShapeFactory。</p>
<p>FactoryPatternDemo，我们的演示类使用 ShapeFactory 来获取 Shape 对象。它将向 ShapeFactory 传递信息（CIRCLE / RECTANGLE / SQUARE），以便获取它所需对象的类型。</p>
<p><img src="https://lixiangbetter.github.io/2020/07/19/%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1-%E5%88%86%E5%B8%83%E5%BC%8F-%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/aHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL0pvdXJXb24vaW1hZ2UvbWFzdGVyLyVFOCVBRSVCRSVFOCVBRSVBMSVFNiVBOCVBMSVFNSVCQyU4Ri8lRTUlQjclQTUlRTUlOEUlODIlRTYlQTglQTElRTUlQkMlOEYuanBn.jpeg" alt></p>
<p>步骤 1</p>
<p>创建一个接口。</p>
<p><em>Shape.java</em></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">Shape</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">draw</span><span class="params">()</span></span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>步骤 2</p>
<p>创建实现接口的实体类。</p>
<p><em>Rectangle.java</em></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Rectangle</span> <span class="keyword">implements</span> <span class="title">Shape</span> </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">draw</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        System.out.println(<span class="string">"Inside Rectangle::draw() method."</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><em>Square.java</em></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Square</span> <span class="keyword">implements</span> <span class="title">Shape</span> </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">draw</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        System.out.println(<span class="string">"Inside Square::draw() method."</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><em>Circle.java</em></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Circle</span> <span class="keyword">implements</span> <span class="title">Shape</span> </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">draw</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        System.out.println(<span class="string">"Inside Circle::draw() method."</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>步骤 3</p>
<p>创建一个工厂，生成基于给定信息的实体类的对象。</p>
<p><em>ShapeFactory.java</em></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ShapeFactory</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//使用 getShape 方法获取形状类型的对象</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Shape <span class="title">getShape</span><span class="params">(String shapeType)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (shapeType == <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        shapeType = shapeType.toLowerCase();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">switch</span> (shapeType) &#123;</span><br><span class="line">            <span class="keyword">case</span> <span class="string">"circle"</span>:</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> Circle();</span><br><span class="line">            <span class="keyword">case</span> <span class="string">"rectangle"</span>:</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> Rectangle();</span><br><span class="line">            <span class="keyword">case</span> <span class="string">"square"</span>:</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> Square();</span><br><span class="line">            <span class="keyword">default</span>:</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>步骤 4</p>
<p>使用该工厂，通过传递类型信息来获取实体类的对象。</p>
<p><em>FactoryPatternDemo.java</em></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FactoryPatternDemo</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        ShapeFactory shapeFactory = <span class="keyword">new</span> ShapeFactory();</span><br><span class="line"></span><br><span class="line">        <span class="comment">//获取 Circle 的对象，并调用它的 draw 方法</span></span><br><span class="line">        Shape shape1 = shapeFactory.getShape(<span class="string">"CIRCLE"</span>);</span><br><span class="line">        <span class="comment">//调用 Circle 的 draw 方法</span></span><br><span class="line">        shape1.draw();</span><br><span class="line"></span><br><span class="line">        <span class="comment">//获取 Rectangle 的对象，并调用它的 draw 方法</span></span><br><span class="line">        Shape shape2 = shapeFactory.getShape(<span class="string">"RECTANGLE"</span>);</span><br><span class="line">        <span class="comment">//调用 Rectangle 的 draw 方法</span></span><br><span class="line">        shape2.draw();</span><br><span class="line"></span><br><span class="line">        <span class="comment">//获取 Square 的对象，并调用它的 draw 方法</span></span><br><span class="line">        Shape shape3 = shapeFactory.getShape(<span class="string">"SQUARE"</span>);</span><br><span class="line">        <span class="comment">//调用 Square 的 draw 方法</span></span><br><span class="line">        shape3.draw();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>步骤 5</p>
<p>验证输出。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Inside Circle::draw() method.</span><br><span class="line">Inside Rectangle::draw() method.</span><br><span class="line">Inside Square::draw() method.</span><br></pre></td></tr></table></figure>

<h3 id="观察者模式"><a href="#观察者模式" class="headerlink" title="观察者模式"></a>观察者模式</h3><p>当对象间存在一对多关系时，则使用观察者模式（Observer Pattern）。比如，当一个对象被修改时，则会自动通知它的依赖对象。观察者模式属于行为型模式。</p>
<p>意图：定义对象间的一种一对多的依赖关系，当一个对象的状态发生改变时，所有依赖于它的对象都得到通知并被自动更新。</p>
<p>主要解决：一个对象状态改变给其他对象通知的问题，而且要考虑到易用和低耦合，保证高度的协作。</p>
<p>实现</p>
<p>观察者模式使用三个类 Subject、Observer 和 Client。Subject 对象带有绑定观察者到 Client 对象和从 Client 对象解绑观察者的方法。我们创建 Subject 类、Observer 抽象类和扩展了抽象类 Observer 的实体类。</p>
<p>ObserverPatternDemo，我们的演示类使用 Subject 和实体类对象来演示观察者模式。</p>
<p><img src="https://lixiangbetter.github.io/2020/07/19/%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1-%E5%88%86%E5%B8%83%E5%BC%8F-%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/aHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL0pvdXJXb24vaW1hZ2UvbWFzdGVyLyVFOCVBRSVCRSVFOCVBRSVBMSVFNiVBOCVBMSVFNSVCQyU4Ri8lRTglQTclODIlRTUlQUYlOUYlRTglODAlODUlRTYlQTglQTElRTUlQkMlOEYuanBn.jpeg" alt></p>
<p>步骤 1</p>
<p>创建 Subject 类。</p>
<p><em>Subject.java</em></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Subject</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> List&lt;Observer&gt; observers = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> state;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getState</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> state;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setState</span><span class="params">(<span class="keyword">int</span> state)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.state = state;</span><br><span class="line">        notifyAllObservers();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">attach</span><span class="params">(Observer observer)</span> </span>&#123;</span><br><span class="line">        observers.add(observer);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">notifyAllObservers</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">for</span> (Observer observer : observers) &#123;</span><br><span class="line">            observer.update();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>步骤 2</p>
<p>创建 Observer 类。</p>
<p><em>Observer.java</em></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">Observer</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">protected</span> Subject subject;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="keyword">void</span> <span class="title">update</span><span class="params">()</span></span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>步骤 3</p>
<p>创建实体观察者类。</p>
<p><em>BinaryObserver.java</em></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">BinaryObserver</span> <span class="keyword">extends</span> <span class="title">Observer</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">BinaryObserver</span><span class="params">(Subject subject)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.subject = subject;</span><br><span class="line">        <span class="keyword">this</span>.subject.attach(<span class="keyword">this</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">update</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        System.out.println(<span class="string">"Binary String: "</span></span><br><span class="line">                + Integer.toBinaryString(subject.getState()));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><em>OctalObserver.java</em></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">OctalObserver</span> <span class="keyword">extends</span> <span class="title">Observer</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">OctalObserver</span><span class="params">(Subject subject)</span></span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.subject = subject;</span><br><span class="line">        <span class="keyword">this</span>.subject.attach(<span class="keyword">this</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">update</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        System.out.println( <span class="string">"Octal String: "</span></span><br><span class="line">                + Integer.toOctalString( subject.getState() ) );</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><em>HexaObserver.java</em></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HexaObserver</span> <span class="keyword">extends</span> <span class="title">Observer</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">HexaObserver</span><span class="params">(Subject subject)</span></span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.subject = subject;</span><br><span class="line">        <span class="keyword">this</span>.subject.attach(<span class="keyword">this</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">update</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        System.out.println( <span class="string">"Hex String: "</span></span><br><span class="line">                + Integer.toHexString( subject.getState() ).toUpperCase() );</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>步骤 4</p>
<p>使用 <em>Subject</em> 和实体观察者对象。</p>
<p><em>ObserverPatternDemo.java</em></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ObserverPatternDemo</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        Subject subject = <span class="keyword">new</span> Subject();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">new</span> BinaryObserver(subject);</span><br><span class="line">        <span class="keyword">new</span> HexaObserver(subject);</span><br><span class="line">        <span class="keyword">new</span> OctalObserver(subject);</span><br><span class="line"></span><br><span class="line">        System.out.println(<span class="string">"First state change: 15"</span>);</span><br><span class="line">        subject.setState(<span class="number">15</span>);</span><br><span class="line">        System.out.println();</span><br><span class="line"></span><br><span class="line">        System.out.println(<span class="string">"Second state change: 10"</span>);</span><br><span class="line">        subject.setState(<span class="number">10</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>步骤 5</p>
<p>验证输出。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">First state change: 15</span><br><span class="line">Binary String: 1111</span><br><span class="line">Hex String: F</span><br><span class="line">Octal String: 17</span><br><span class="line"></span><br><span class="line">Second state change: 10</span><br><span class="line">Binary String: 1010</span><br><span class="line">Hex String: A</span><br><span class="line">Octal String: 12</span><br></pre></td></tr></table></figure>

<h3 id="装饰器模式"><a href="#装饰器模式" class="headerlink" title="装饰器模式"></a>装饰器模式</h3><p>装饰器模式（Decorator Pattern）允许向一个现有的对象添加新的功能，同时又不改变其结构。这种类型的设计模式属于结构型模式，它是作为现有的类的一个包装。</p>
<p>意图：动态地给一个对象添加一些额外的职责。就增加功能来说，装饰器模式相比生成子类更为灵活。</p>
<p>主要解决：一般的，我们为了扩展一个类经常使用继承方式实现，由于继承为类引入静态特征，并且随着扩展功能的增多，子类会很膨胀。</p>
<p>实现</p>
<p>我们将创建一个 Shape 接口和实现了 Shape 接口的实体类。然后我们创建一个实现了 Shape 接口的抽象装饰类 ShapeDecorator，并把 Shape 对象作为它的实例变量。</p>
<p>RedShapeDecorator 是实现了 ShapeDecorator 的实体类。</p>
<p>DecoratorPatternDemo，我们的演示类使用 RedShapeDecorator 来装饰 Shape 对象。</p>
<p><img src="https://lixiangbetter.github.io/2020/07/19/%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1-%E5%88%86%E5%B8%83%E5%BC%8F-%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/aHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL0pvdXJXb24vaW1hZ2UvbWFzdGVyLyVFOCVBRSVCRSVFOCVBRSVBMSVFNiVBOCVBMSVFNSVCQyU4Ri8lRTglQTMlODUlRTklQTUlQjAlRTUlOTklQTglRTYlQTglQTElRTUlQkMlOEYuanBn.jpeg" alt></p>
<p>步骤 1</p>
<p>创建一个接口。</p>
<p><em>Shape.java</em></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">Shape</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">draw</span><span class="params">()</span></span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>步骤 2</p>
<p>创建实现接口的实体类。</p>
<p><em>Rectangle.java</em></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Rectangle</span> <span class="keyword">implements</span> <span class="title">Shape</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">draw</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        System.out.println(<span class="string">"Shape: Rectangle"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p><em>Circle.java</em></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Circle</span> <span class="keyword">implements</span> <span class="title">Shape</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">draw</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        System.out.println(<span class="string">"Shape: Circle"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>步骤 3</p>
<p>创建实现了 <em>Shape</em> 接口的抽象装饰类。</p>
<p><em>ShapeDecorator.java</em></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">ShapeDecorator</span> <span class="keyword">implements</span> <span class="title">Shape</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">protected</span> Shape decoratorShape;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">ShapeDecorator</span><span class="params">(Shape decoratorShape)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.decoratorShape = decoratorShape;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">draw</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        decoratorShape.draw();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>步骤 4</p>
<p>创建扩展了 <em>ShapeDecorator</em> 类的实体装饰类。</p>
<p><em>RedShapeDecorator.java</em></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">RedShapeDecorator</span> <span class="keyword">extends</span> <span class="title">ShapeDecorator</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">RedShapeDecorator</span><span class="params">(Shape decoratorShape)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">super</span>(decoratorShape);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">draw</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        decoratorShape.draw();</span><br><span class="line">        setRedBorder(decoratorShape);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">setRedBorder</span><span class="params">(Shape decoratorShape)</span> </span>&#123;</span><br><span class="line">        System.out.println(<span class="string">"Border Color: Red"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>步骤 5</p>
<p>使用 <em>RedShapeDecorator</em> 来装饰 <em>Shape</em> 对象。</p>
<p><em>DecoratorPatternDemo.java</em></p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">DecoratorPatternDemo</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        Shape circle = <span class="keyword">new</span> Circle();</span><br><span class="line">        Shape redCircle = <span class="keyword">new</span> RedShapeDecorator(<span class="keyword">new</span> Circle());</span><br><span class="line">        Shape redRectangle = <span class="keyword">new</span> RedShapeDecorator(<span class="keyword">new</span> Rectangle());</span><br><span class="line"></span><br><span class="line">        System.out.println(<span class="string">"Circle with normal border"</span>);</span><br><span class="line">        circle.draw();</span><br><span class="line"></span><br><span class="line">        System.out.println(<span class="string">"\nCircle of red border"</span>);</span><br><span class="line">        redCircle.draw();</span><br><span class="line"></span><br><span class="line">        System.out.println(<span class="string">"\nRectangle of red border"</span>);</span><br><span class="line">        redRectangle.draw();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<p>步骤 6</p>
<p>验证输出。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Circle with normal border</span><br><span class="line">Shape: Circle</span><br><span class="line"></span><br><span class="line">Circle of red border</span><br><span class="line">Shape: Circle</span><br><span class="line">Border Color: Red</span><br><span class="line"></span><br><span class="line">Rectangle of red border</span><br><span class="line">Shape: Rectangle</span><br><span class="line">Border Color: Red</span><br></pre></td></tr></table></figure>

<h3 id="秒杀系统设计"><a href="#秒杀系统设计" class="headerlink" title="秒杀系统设计"></a>秒杀系统设计</h3><p><strong>什么是秒杀</strong></p>
<p>通俗一点讲就是网络商家为促销等目的组织的网上限时抢购活动</p>
<p><strong>业务特点</strong></p>
<ul>
<li>高并发：秒杀的特点就是这样<strong>时间极短</strong>、 <strong>瞬间用户量大</strong>。</li>
<li>库存量少：一般秒杀活动商品量很少，这就导致了只有极少量用户能成功购买到。</li>
<li>业务简单：流程比较简单，一般都是下订单、扣库存、支付订单</li>
<li>恶意请求，数据库压力大</li>
</ul>
<p>解决方案</p>
<p>前端：页面资源静态化，按钮控制，使用答题校验码可以防止秒杀器的干扰，让更多用户有机会抢到</p>
<p>nginx：校验恶意请求，转发请求，负载均衡；动静分离，不走tomcat获取静态资源；gzip压缩，减少静态文件传输的体积，节省带宽，提高渲染速度</p>
<p>业务层：集群，多台机器处理，提高并发能力</p>
<p>redis：集群保证高可用，持久化数据；分布式锁（悲观锁）；缓存热点数据（库存）</p>
<p>mq：削峰限流，MQ堆积订单，保护订单处理层的负载，Consumer根据自己的消费能力来取Task，实际上下游的压力就可控了。重点做好路由层和MQ的安全</p>
<p>数据库：读写分离，拆分事务提高并发度</p>
<p><strong>秒杀系统设计小结</strong></p>
<ul>
<li>秒杀系统就是一个“三高”系统，即高并发、高性能和高可用的分布式系统</li>
<li>秒杀设计原则：前台请求尽量少，后台数据尽量少，调用链路尽量短，尽量不要有单点</li>
<li>秒杀高并发方法：访问拦截、分流、动静分离</li>
<li>秒杀数据方法：减库存策略、热点、异步、限流降级</li>
<li>访问拦截主要思路：通过CDN和缓存技术，尽量把访问拦截在离用户更近的层，尽可能地过滤掉无效请求。</li>
<li>分流主要思路：通过分布式集群技术，多台机器处理，提高并发能力。</li>
</ul>
<h2 id="分布式"><a href="#分布式" class="headerlink" title="分布式"></a>分布式</h2><h3 id="分布式概述"><a href="#分布式概述" class="headerlink" title="分布式概述"></a>分布式概述</h3><h4 id="分布式-1"><a href="#分布式-1" class="headerlink" title="分布式"></a>分布式</h4><p>分布式（distributed）是为了解决单个物理服务器容量和性能瓶颈问题而采用的优化手段，将一个业务拆分成不同的子业务，分布在不同的机器上执行。服务之间通过远程调用协同工作，对外提供服务。</p>
<p>该领域需要解决的问题极多，在不同的技术层面上，又包括：分布式缓存、分布式数据库、分布式计算、分布式文件系统等，一些技术如MQ、Redis、zookeeper等都跟分布式有关。</p>
<p>从理念上讲，分布式的实现有两种形式：</p>
<p>水平扩展：当一台机器扛不住流量时，就通过添加机器的方式，将流量平分到所有服务器上，所有机器都可以提供 相同的服务；</p>
<p>垂直拆分：前端有多种查询需求时，一台机器扛不住，可以将不同的业务需求分发到不同的机器上，比如A机器处理余票查询的请求，B机器处理支付的请求。</p>
<h4 id="集群"><a href="#集群" class="headerlink" title="集群"></a>集群</h4><p>集群（cluster）是指在多台不同的服务器中部署相同应用或服务模块，构成一个集群，通过负载均衡设备对外提供服务。</p>
<p>两个特点</p>
<ul>
<li><p><strong>可扩展性</strong>：集群中的服务节点，可以动态的添加机器，从而增加集群的处理能力。</p>
</li>
<li><p><strong>高可用性</strong>：如果集群某个节点发生故障，这台节点上面运行的服务，可以被其他服务节点接管，从而增强集群的高可用性。</p>
</li>
</ul>
<p>两大能力</p>
<ul>
<li><p><strong>负载均衡</strong>：负载均衡能把任务比较均衡地分布到集群环境下的计算和网络资源。</p>
</li>
<li><p><strong>集群容错</strong>：当我们的系统中用到集群环境，因为各种原因在集群调用失败时，集群容错起到关键性的作用。</p>
</li>
</ul>
<h4 id="微服务"><a href="#微服务" class="headerlink" title="微服务"></a>微服务</h4><p>微服务就是很小的服务，小到一个服务只对应一个单一的功能，只做一件事。这个服务可以单独部署运行，服务之间通过远程调用协同工作，每个微服务都是由独立的小团队开发，测试，部署，上线，负责它的整个生命周期。</p>
<h4 id="多线程"><a href="#多线程" class="headerlink" title="多线程"></a>多线程</h4><p>多线程（multi-thread）：多线程是指程序中包含多个执行流，即在一个程序中可以同时运行多个不同的线程来执行不同的任务。多线程是为了提高CPU的利用率。</p>
<h4 id="高并发"><a href="#高并发" class="headerlink" title="高并发"></a>高并发</h4><p>高并发（High Concurrency）是一种系统运行过程中发生了一种“短时间内遇到大量请求”的情况，高并发对应的是访问请求，多线程是解决高并发的方法之一，高并发还可以通过分布式，集群，算法优化，数据库优化等方法解决。</p>
<h3 id="分布式系统设计理念"><a href="#分布式系统设计理念" class="headerlink" title="分布式系统设计理念"></a>分布式系统设计理念</h3><h4 id="分布式系统的目标与要素"><a href="#分布式系统的目标与要素" class="headerlink" title="分布式系统的目标与要素"></a>分布式系统的目标与要素</h4><p>分布式系统的目标是提升系统的整体性能和吞吐量另外还要尽量保证分布式系统的容错性（假如增加10台服务器才达到单机运行效果2倍左右的性能，那么这个分布式系统就根本没有存在的意义）。</p>
<p>即使采用了分布式系统，我们也要尽力运用并发编程、高性能网络框架等等手段提升单机上的程序性能。</p>
<h4 id="分布式系统设计两大思路：中心化和去中心化"><a href="#分布式系统设计两大思路：中心化和去中心化" class="headerlink" title="分布式系统设计两大思路：中心化和去中心化"></a>分布式系统设计两大思路：中心化和去中心化</h4><p><img src="https://lixiangbetter.github.io/2020/07/19/%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1-%E5%88%86%E5%B8%83%E5%BC%8F-%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/aHR0cHM6Ly91c2VyLWdvbGQtY2RuLnhpdHUuaW8vMjAxOC81LzI0LzE2MzkxNWQ1ZGZiM2VjZTk.jpeg" alt></p>
<p>中心化设计</p>
<ul>
<li><p>两个角色： 中心化的设计思想很简单，分布式集群中的节点机器按照角色分工，大体上分为两种角色： “领导” 和 “干活的”</p>
</li>
<li><p>角色职责： “领导”通常负责分发任务并监督“干活的”，发现谁太闲了，就想发设法地给其安排新任务，确保没有一个“干活的”能够偷懒，如果“领导”发现某个“干活的”因为劳累过度而病倒了，则是不会考虑先尝试“医治”他的，而是一脚踢出去，然后把他的任务分给其他人。其中微服务架构 Kubernetes 就恰好采用了这一设计思路。</p>
</li>
<li><p>中心化设计的问题</p>
<ol>
<li>中心化的设计存在的最大问题是“领导”的安危问题，如果“领导”出了问题，则群龙无首，整个集群就奔溃了。但我们难以同时安排两个“领导”以避免单点问题。</li>
<li>中心化设计还存在另外一个潜在的问题，既“领导”的能力问题：可以领导10个人高效工作并不意味着可以领导100个人高效工作，所以如果系统设计和实现得不好，问题就会卡在“领导”身上。</li>
</ol>
</li>
<li><p>领导安危问题的解决办法： 大多数中心化系统都采用了主备两个“领导”的设计方案，可以是热备或者冷备，也可以是自动切换或者手动切换，而且越来越多的新系统都开始具备自动选举切换“领导”的能力，以提升系统的可用性。</p>
</li>
</ul>
<p>去中心化设计</p>
<ul>
<li>众生地位平等： 在去中心化的设计里，通常没有“领导”和“干活的”这两种角色的区分，大家的角色都是一样的，地位是平等的，全球互联网就是一个典型的去中心化的分布式系统，联网的任意节点设备宕机，都只会影响很小范围的功能。</li>
<li>“去中心化”不是不要中心，而是由节点来自由选择中心。 （集群的成员会自发的举行“会议”选举新的“领导”主持工作。最典型的案例就是ZooKeeper及Go语言实现的Etcd）</li>
<li>去中心化设计的问题： 去中心化设计里最难解决的一个问题是 “脑裂”问题 ，这种情况的发生概率很低，但影响很大。脑裂指一个集群由于网络的故障，被分为至少两个彼此无法通信的单独集群，此时如果两个集群都各自工作，则可能会产生严重的数据冲突和错误。一般的设计思路是，当集群判断发生了脑裂问题时，规模较小的集群就“自杀”或者拒绝服务。</li>
</ul>
<h4 id="分布式与集群的区别是什么？"><a href="#分布式与集群的区别是什么？" class="headerlink" title="分布式与集群的区别是什么？"></a>分布式与集群的区别是什么？</h4><ul>
<li>分布式： 一个业务分拆多个子业务，部署在不同的服务器上</li>
<li>集群： 同一个业务，部署在多个服务器上。比如之前做电商网站搭的redis集群以及solr集群都是属于将redis服务器提供的缓存服务以及solr服务器提供的搜索服务部署在多个服务器上以提高系统性能、并发量解决海量存储问题。</li>
</ul>
<h3 id="CAP定理"><a href="#CAP定理" class="headerlink" title="CAP定理"></a>CAP定理</h3><p><img src="https://lixiangbetter.github.io/2020/07/19/%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1-%E5%88%86%E5%B8%83%E5%BC%8F-%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/aHR0cHM6Ly91c2VyLWdvbGQtY2RuLnhpdHUuaW8vMjAxOC81LzI0LzE2MzkxMmU5NzNlY2I5M2M.jpeg" alt></p>
<p>在理论计算机科学中，CAP定理（CAP theorem），又被称作布鲁尔定理（Brewer’s theorem），它指出对于一个分布式计算系统来说，不可能同时满足以下三点：</p>
<table>
<thead>
<tr>
<th>选项</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>Consistency（一致性）</td>
<td>指数据在多个副本之间能够保持一致的特性（严格的一致性）</td>
</tr>
<tr>
<td>Availability（可用性）</td>
<td>指系统提供的服务必须一直处于可用的状态，每次请求都能获取到非错的响应（不保证获取的数据为最新数据）</td>
</tr>
<tr>
<td>Partition tolerance（分区容错性）</td>
<td>分布式系统在遇到任何网络分区故障的时候，仍然能够对外提供满足一致性和可用性的服务，除非整个网络环境都发生了故障</td>
</tr>
</tbody></table>
<p><strong>Spring Cloud在CAP法则上主要满足的是A和P法则，Dubbo和Zookeeper在CAP法则主要满足的是C和P法则</strong></p>
<p>CAP仅适用于原子读写的NOSQL场景中，并不适合数据库系统。现在的分布式系统具有更多特性比如扩展性、可用性等等，在进行系统设计和开发时，我们不应该仅仅局限在CAP问题上。</p>
<p><strong>注意：不是所谓的3选2（不要被网上大多数文章误导了）</strong></p>
<p>现实生活中，大部分人解释这一定律时，常常简单的表述为：“一致性、可用性、分区容忍性三者你只能同时达到其中两个，不可能同时达到”。实际上这是一个非常具有误导性质的说法，而且在CAP理论诞生12年之后，CAP之父也在2012年重写了之前的论文。</p>
<p><strong>当发生网络分区的时候，如果我们要继续服务，那么强一致性和可用性只能2选1。也就是说当网络分区之后P是前提，决定了P之后才有C和A的选择。也就是说分区容错性（Partition tolerance）我们是必须要实现的。</strong></p>
<h2 id="CAP定理的证明"><a href="#CAP定理的证明" class="headerlink" title="CAP定理的证明"></a>CAP定理的证明</h2><p>关于CAP这三个特性我们就介绍完了，接下来我们试着证明一下<strong>为什么CAP不能同时满足</strong>。</p>
<p><img src="https://lixiangbetter.github.io/2020/07/19/%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1-%E5%88%86%E5%B8%83%E5%BC%8F-%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/aHR0cHM6Ly91c2VyLWdvbGQtY2RuLnhpdHUuaW8vMjAyMC8xLzExLzE2ZjkyMTEwOTczNjgzMDY.jpeg" alt></p>
<p>为了简化证明的过程，我们假设整个集群里只有两个N1和N2两个节点，如下图：</p>
<p>N1和N2当中各自有一个应用程序AB和数据库，当系统满足一致性的时候，我们认为N1和N2数据库中的数据保持一致。在满足可用性的时候，我们认为无论用户访问N1还是N2，都可以获得正确的结果，在满足分区容错性的时候，我们认为无论N1还是N2宕机或者是两者的通信中断，都不影响系统的运行。</p>
<p>我们假设一种极端情况，假设某个时刻N1和N2之间的网络通信突然中断了。如果系统满足分区容错性，那么显然可以支持这种异常。问题是在此前提下，一致性和可用性是否可以做到不受影响呢？</p>
<p>我们做个假象实验，如下图，突然某一时刻N1和N2之间的关联断开：</p>
<p><img src="https://lixiangbetter.github.io/2020/07/19/%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1-%E5%88%86%E5%B8%83%E5%BC%8F-%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/aHR0cHM6Ly91c2VyLWdvbGQtY2RuLnhpdHUuaW8vMjAyMC8xLzExLzE2ZjkyMTEzMTkxNmYxODY.jpeg" alt></p>
<p>有用户向N1发送了请求更改了数据，将数据库从V0更新成了V1。由于网络断开，所以N2数据库依然是V0，如果这个时候有一个请求发给了N2，但是N2并没有办法可以直接给出最新的结果V1，这个时候该怎么办呢？</p>
<p>这个时候无法两种方法，一种是将错就错，将错误的V0数据返回给用户。第二种是阻塞等待，等待网络通信恢复，N2中的数据更新之后再返回给用户。显然前者牺牲了一致性，后者牺牲了可用性。</p>
<p>这个例子虽然简单，但是说明的内容却很重要。在分布式系统当中，CAP三个特性我们是无法同时满足的，必然要舍弃一个。三者舍弃一个，显然排列组合一共有三种可能。</p>
<h3 id="BASE理论"><a href="#BASE理论" class="headerlink" title="BASE理论"></a>BASE理论</h3><p>BASE理论由eBay架构师Dan Pritchett提出，在2008年上被发表为论文，并且eBay给出了他们在实践中总结的基于BASE理论的一套新的分布式事务解决方案。</p>
<p>BASE 是 Basically Available（基本可用） 、Soft-state（软状态） 和 Eventually Consistent（最终一致性） 三个短语的缩写。BASE理论是对CAP中一致性和可用性权衡的结果，其来源于对大规模互联网系统分布式实践的总结，是基于CAP定理逐步演化而来的，它大大降低了我们对系统的要求。</p>
<h4 id="BASE理论的核心思想"><a href="#BASE理论的核心思想" class="headerlink" title="BASE理论的核心思想"></a>BASE理论的核心思想</h4><p>即使无法做到强一致性，但每个应用都可以根据自身业务特点，采用适当的方式来使系统达到最终一致性。也就是牺牲数据的一致性来满足系统的高可用性，系统中一部分数据不可用或者不一致时，仍需要保持系统整体“主要可用”。</p>
<p>针对数据库领域，BASE思想的主要实现是对业务数据进行拆分，让不同的数据分布在不同的机器上，以提升系统的可用性，当前主要有以下两种做法：</p>
<ul>
<li>按功能划分数据库</li>
<li>分片（如开源的Mycat、Amoeba等）。</li>
</ul>
<p>由于拆分后会涉及分布式事务问题，所以eBay在该BASE论文中提到了如何用最终一致性的思路来实现高性能的分布式事务。</p>
<h4 id="BASE理论三要素"><a href="#BASE理论三要素" class="headerlink" title="BASE理论三要素"></a>BASE理论三要素</h4><p><img src="https://lixiangbetter.github.io/2020/07/19/%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1-%E5%88%86%E5%B8%83%E5%BC%8F-%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/aHR0cHM6Ly91c2VyLWdvbGQtY2RuLnhpdHUuaW8vMjAxOC81LzI0LzE2MzkxNDgwNmQ5ZTE1YzY.jpeg" alt></p>
<h5 id="1-基本可用"><a href="#1-基本可用" class="headerlink" title="1. 基本可用"></a>1. 基本可用</h5><p>基本可用是指分布式系统在出现不可预知故障的时候，允许损失部分可用性。但是，这绝不等价于系统不可用。</p>
<p>比如：</p>
<ul>
<li>响应时间上的损失：正常情况下，一个在线搜索引擎需要在0.5秒之内返回给用户相应的查询结果，但由于出现故障，查询结果的响应时间增加了1~2秒</li>
<li>系统功能上的损失：正常情况下，在一个电子商务网站上进行购物的时候，消费者几乎能够顺利完成每一笔订单，但是在一些节日大促购物高峰的时候，由于消费者的购物行为激增，为了保护购物系统的稳定性，部分消费者可能会被引导到一个降级页面</li>
</ul>
<h5 id="2-软状态"><a href="#2-软状态" class="headerlink" title="2. 软状态"></a>2. 软状态</h5><p>软状态指允许系统中的数据存在中间状态，并认为该中间状态的存在不会影响系统的整体可用性，即允许系统在不同节点的数据副本之间进行数据同步的过程存在延时</p>
<h5 id="3-最终一致性"><a href="#3-最终一致性" class="headerlink" title="3. 最终一致性"></a>3. 最终一致性</h5><p>最终一致性强调的是系统中所有的数据副本，在经过一段时间的同步后，最终能够达到一个一致的状态。因此，最终一致性的本质是需要系统保证最终数据能够达到一致，而不需要实时保证系统数据的强一致性。</p>
<h2 id="数据结构与算法"><a href="#数据结构与算法" class="headerlink" title="数据结构与算法"></a>数据结构与算法</h2><h3 id="冒泡排序"><a href="#冒泡排序" class="headerlink" title="冒泡排序"></a>冒泡排序</h3><p><img src="https://lixiangbetter.github.io/2020/07/19/%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1-%E5%88%86%E5%B8%83%E5%BC%8F-%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/20190712143006969.gif" alt></p>
<h3 id="选择排序"><a href="#选择排序" class="headerlink" title="选择排序"></a>选择排序</h3><p><img src="https://lixiangbetter.github.io/2020/07/19/%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1-%E5%88%86%E5%B8%83%E5%BC%8F-%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/20190712143023558.gif" alt></p>
<h3 id="快速排序"><a href="#快速排序" class="headerlink" title="快速排序"></a>快速排序</h3><p><img src="https://lixiangbetter.github.io/2020/07/19/%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1-%E5%88%86%E5%B8%83%E5%BC%8F-%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/20190712143139347.gif" alt></p>
<h3 id="二分查找"><a href="#二分查找" class="headerlink" title="二分查找"></a>二分查找</h3><p><img src="https://lixiangbetter.github.io/2020/07/19/%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1-%E5%88%86%E5%B8%83%E5%BC%8F-%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/aHR0cHM6Ly91c2VyLWdvbGQtY2RuLnhpdHUuaW8vMjAxNy81LzI1L2E5ZWE3ZTVkNzQwM2I1NDEwYzJlZmQwOTQ0Njc1MWZk.jpeg" alt></p>
<h3 id="一致性Hash算法"><a href="#一致性Hash算法" class="headerlink" title="一致性Hash算法"></a>一致性Hash算法</h3><h4 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h4><p>一致性Hash是一种特殊的Hash算法，由于其均衡性、持久性的映射特点，被广泛的应用于负载均衡领域和分布式存储，如nginx和memcached都采用了一致性Hash来作为集群负载均衡的方案。</p>
<p>普通的Hash函数最大的作用是散列，或者说是将一系列在形式上具有相似性质的数据，打散成随机的、均匀分布的数据。不难发现，这样的Hash只要集群的数量N发生变化，之前的所有Hash映射就会全部失效。如果集群中的每个机器提供的服务没有差别，倒不会产生什么影响，但对于分布式缓存这样的系统而言，映射全部失效就意味着之前的缓存全部失效，后果将会是灾难性的。一致性Hash通过构建环状的Hash空间代替线性Hash空间的方法解决了这个问题。</p>
<p>良好的分布式cahce系统中的一致性hash算法应该满足以下几个方面：</p>
<ul>
<li><p>平衡性(Balance)<br>平衡性是指哈希的结果能够尽可能分布到所有的缓冲中去，这样可以使得所有的缓冲空间都得到利用。很多哈希算法都能够满足这一条件。</p>
</li>
<li><p>单调性(Monotonicity)<br>单调性是指如果已经有一些内容通过哈希分派到了相应的缓冲中，又有新的缓冲区加入到系统中，那么哈希的结果应能够保证原有已分配的内容可以被映射到新的缓冲区中去，而不会被映射到旧的缓冲集合中的其他缓冲区。</p>
</li>
<li><p>分散性(Spread)<br>在分布式环境中，终端有可能看不到所有的缓冲，而是只能看到其中的一部分。当终端希望通过哈希过程将内容映射到缓冲上时，由于不同终端所见的缓冲范围有可能不同，从而导致哈希的结果不一致，最终的结果是相同的内容被不同的终端映射到不同的缓冲区中。这种情况显然是应该避免的，因为它导致相同内容被存储到不同缓冲中去，降低了系统存储的效率。分散性的定义就是上述情况发生的严重程度。好的哈希算法应能够尽量避免不一致的情况发生，也就是尽量降低分散性。</p>
</li>
<li><p>负载(Load)<br>负载问题实际上是从另一个角度看待分散性问题。既然不同的终端可能将相同的内容映射到不同的缓冲区中，那么对于一个特定的缓冲区而言，也可能被不同的用户映射为不同的内容。与分散性一样，这种情况也是应当避免的，因此好的哈希算法应能够尽量降低缓冲的负荷。</p>
</li>
<li><p>平滑性(Smoothness)<br>平滑性是指缓存服务器的数目平滑改变和缓存对象的平滑改变是一致的。</p>
</li>
</ul>
<h4 id="一致性Hash算法原理"><a href="#一致性Hash算法原理" class="headerlink" title="一致性Hash算法原理"></a>一致性Hash算法原理</h4><p>简单来说，一致性哈希将整个哈希值空间组织成一个虚拟的圆环，如假设某哈希函数H的值空间为0-232-1（即哈希值是一个32位无符号整形），整个哈希空间环如下：整个空间按顺时针方向组织。0和232-1在零点中方向重合。</p>
<p><img src="https://lixiangbetter.github.io/2020/07/19/%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1-%E5%88%86%E5%B8%83%E5%BC%8F-%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/aHR0cHM6Ly91c2VyLWdvbGQtY2RuLnhpdHUuaW8vMjAxOC80LzI2LzE2MmZmZmYwMTlkNDhlZDU.jpeg" alt></p>
<p>下一步将各个服务器使用Hash进行一次哈希，具体可以选择服务器的ip或主机名作为关键字进行哈希，这样每台机器就能确定其在哈希环上的位置，这里假设将上文中四台服务器使用ip地址哈希后在环空间的位置如下：</p>
<p><img src="https://lixiangbetter.github.io/2020/07/19/%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1-%E5%88%86%E5%B8%83%E5%BC%8F-%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/aHR0cHM6Ly91c2VyLWdvbGQtY2RuLnhpdHUuaW8vMjAxOC80LzI2LzE2MmZmZmYwMTlmMWNlY2M.jpeg" alt></p>
<p>接下来使用如下算法定位数据访问到相应服务器：将数据key使用相同的函数Hash计算出哈希值，并确定此数据在环上的位置，从此位置沿环顺时针“行走”，第一台遇到的服务器就是其应该定位到的服务器。</p>
<p>例如我们有Object A、Object B、Object C、Object D四个数据对象，经过哈希计算后，在环空间上的位置如下：</p>
<p><img src="https://lixiangbetter.github.io/2020/07/19/%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1-%E5%88%86%E5%B8%83%E5%BC%8F-%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/aHR0cHM6Ly91c2VyLWdvbGQtY2RuLnhpdHUuaW8vMjAxOC80LzI2LzE2MmZmZmYwMWRkZTliMmI.jpeg" alt></p>
<p>根据一致性哈希算法，数据A会被定为到Node A上，B被定为到Node B上，C被定为到Node C上，D被定为到Node D上。</p>
<p>下面分析一致性哈希算法的容错性和可扩展性。现假设Node C不幸宕机，可以看到此时对象A、B、D不会受到影响，只有C对象被重定位到Node D。一般的，在一致性哈希算法中，如果一台服务器不可用，则受影响的数据仅仅是此服务器到其环空间中前一台服务器（即沿着逆时针方向行走遇到的第一台服务器）之间数据，其它不会受到影响。</p>
<p>下面考虑另外一种情况，如果在系统中增加一台服务器Node X，如下图所示：</p>
<p><img src="https://lixiangbetter.github.io/2020/07/19/%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1-%E5%88%86%E5%B8%83%E5%BC%8F-%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/aHR0cHM6Ly91c2VyLWdvbGQtY2RuLnhpdHUuaW8vMjAxOC80LzI2LzE2MmZmZmYwMWRkZTliMmI.jpeg" alt></p>
<p>此时对象Object A、B、D不受影响，只有对象C需要重定位到新的Node X 。一般的，在一致性哈希算法中，如果增加一台服务器，则受影响的数据仅仅是新服务器到其环空间中前一台服务器（即沿着逆时针方向行走遇到的第一台服务器）之间数据，其它数据也不会受到影响。</p>
<p>综上所述，一致性哈希算法对于节点的增减都只需重定位环空间中的一小部分数据，具有较好的容错性和可扩展性。</p>
]]></content>
      <categories>
        <category>bigdata</category>
      </categories>
      <tags>
        <tag>设计模式</tag>
        <tag>分布式</tag>
        <tag>数据结构</tag>
        <tag>算法</tag>
      </tags>
  </entry>
  <entry>
    <title>ZooKeeper笔记</title>
    <url>/2020/07/18/ZooKeeper%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<h1 id="Zookeeper笔记"><a href="#Zookeeper笔记" class="headerlink" title="Zookeeper笔记"></a>Zookeeper笔记</h1><h2 id="1-ZooKeeper-是什么？"><a href="#1-ZooKeeper-是什么？" class="headerlink" title="1. ZooKeeper 是什么？"></a>1. ZooKeeper 是什么？</h2><p>ZooKeeper 是一个开源的分布式协调服务。它是一个为分布式应用提供一致性服务的软件，分布式应用程序可以基于 Zookeeper 实现诸如数据发布/订阅、负载均衡、命名服务、分布式协调/通知、集群管理、Master选举、分布式锁和分布式队列等功能。</p>
<p>ZooKeeper 的目标就是封装好复杂易出错的关键服务，将简单易用的接口和性能高效、功能稳定的系统提供给用户。</p>
<p>Zookeeper保证了如下分布式一致性特性：</p>
<p>（1）顺序一致性</p>
<p>（2）原子性</p>
<p>（3）单一视图</p>
<p>（4）可靠性</p>
<p>（5）实时性（最终一致性）</p>
<p>客户端的读请求可以被集群中的任意一台机器处理，如果读请求在节点上注册了监听器，这个监听器也是由所连接的 zookeeper 机器来处理。对于写请求，这些请求会同时发给其他 zookeeper 机器并且达成一致后，请求才会返回成功。因此，随着 zookeeper 的集群机器增多，读请求的吞吐会提高但是写请求的吞吐会下降。</p>
<p>有序性是 zookeeper 中非常重要的一个特性，所有的更新都是全局有序的，每个更新都有一个唯一的时间戳，这个时间戳称为 zxid（Zookeeper Transaction Id）。而读请求只会相对于更新有序，也就是读请求的返回结果中会带有这个zookeeper 最新的 zxid。</p>
<h2 id="2-ZooKeeper-提供了什么？"><a href="#2-ZooKeeper-提供了什么？" class="headerlink" title="2. ZooKeeper 提供了什么？"></a>2. ZooKeeper 提供了什么？</h2><ul>
<li>文件系统</li>
<li>通知机制</li>
</ul>
<h2 id="3-Zookeeper-文件系统"><a href="#3-Zookeeper-文件系统" class="headerlink" title="3. Zookeeper 文件系统"></a>3. Zookeeper 文件系统</h2><p>Zookeeper 提供一个多层级的节点命名空间（节点称为 znode）。与文件系统不同的是，这些节点都可以设置关联的数据，而文件系统中只有文件节点可以存放数据而目录节点不行。</p>
<p>Zookeeper 为了保证高吞吐和低延迟，在内存中维护了这个树状的目录结构，这种特性使得 Zookeeper 不能用于存放大量的数据，每个节点的存放数据上限为1M。</p>
<h2 id="4-Zookeeper-怎么保证主从节点的状态同步？"><a href="#4-Zookeeper-怎么保证主从节点的状态同步？" class="headerlink" title="4. Zookeeper 怎么保证主从节点的状态同步？"></a>4. Zookeeper 怎么保证主从节点的状态同步？</h2><p>Zookeeper 的核心是原子广播机制，这个机制保证了各个 server 之间的同步。实现这个机制的协议叫做 Zab 协议。Zab 协议有两种模式，它们分别是恢复模式和广播模式。</p>
<ol>
<li><p>恢复模式</p>
<p>当服务启动或者在领导者崩溃后，Zab就进入了恢复模式，当领导者被选举出来，且大多数 server 完成了和 leader 的状态同步以后，恢复模式就结束了。状态同步保证了 leader 和 server 具有相同的系统状态。</p>
</li>
<li><p>广播模式<br>一旦 leader 已经和多数的 follower 进行了状态同步后，它就可以开始广播消息了，即进入广播状态。这时候当一个 server 加入 ZooKeeper 服务中，它会在恢复模式下启动，发现 leader，并和 leader 进行状态同步。待到同步结束，它也参与消息广播。ZooKeeper 服务一直维持在 Broadcast 状态，直到 leader 崩溃了或者 leader 失去了大部分的 followers 支持。</p>
</li>
</ol>
<h2 id="5-四种类型的数据节点-Znode"><a href="#5-四种类型的数据节点-Znode" class="headerlink" title="5. 四种类型的数据节点 Znode"></a>5. 四种类型的数据节点 Znode</h2><p>（1）PERSISTENT-持久节点</p>
<p>除非手动删除，否则节点一直存在于 Zookeeper 上</p>
<p>（2）EPHEMERAL-临时节点</p>
<p>临时节点的生命周期与客户端会话绑定，一旦客户端会话失效（客户端与zookeeper 连接断开不一定会话失效），那么这个客户端创建的所有临时节点都会被移除。</p>
<p>（3）PERSISTENT_SEQUENTIAL-持久顺序节点</p>
<p>基本特性同持久节点，只是增加了顺序属性，节点名后边会追加一个由父节点维护的自增整型数字。</p>
<p>（4）EPHEMERAL_SEQUENTIAL-临时顺序节点</p>
<p>基本特性同临时节点，增加了顺序属性，节点名后边会追加一个由父节点维护的自增整型数字。</p>
<h2 id="16-zookeeper-是如何保证事务的顺序一致性的？"><a href="#16-zookeeper-是如何保证事务的顺序一致性的？" class="headerlink" title="16. zookeeper 是如何保证事务的顺序一致性的？"></a>16. zookeeper 是如何保证事务的顺序一致性的？</h2><p>zookeeper 采用了全局递增的事务 Id 来标识，所有的 proposal（提议）都在被提出的时候加上了 zxid，zxid 实际上是一个 64 位的数字，高 32 位是 epoch（ 时期; 纪元; 世; 新时代）用来标识 leader 周期，如果有新的 leader 产生出来，epoch会自增，低 32 位用来递增计数。当新产生 proposal 的时候，会依据数据库的两阶段过程，首先会向其他的 server 发出事务执行请求，如果超过半数的机器都能执行并且能够成功，那么就会开始执行。</p>
<h2 id="17-分布式集群中为什么会有-Master主节点？"><a href="#17-分布式集群中为什么会有-Master主节点？" class="headerlink" title="17. 分布式集群中为什么会有 Master主节点？"></a>17. 分布式集群中为什么会有 Master主节点？</h2><p>在分布式环境中，有些业务逻辑只需要集群中的某一台机器进行执行，其他的机器可以共享这个结果，这样可以大大减少重复计算，提高性能，于是就需要进行 leader 选举。</p>
<h2 id="18-zk-节点宕机如何处理？"><a href="#18-zk-节点宕机如何处理？" class="headerlink" title="18. zk 节点宕机如何处理？"></a>18. zk 节点宕机如何处理？</h2><p>Zookeeper 本身也是集群，推荐配置不少于 3 个服务器。Zookeeper 自身也要保证当一个节点宕机时，其他节点会继续提供服务。</p>
<p>如果是一个 Follower 宕机，还有 2 台服务器提供访问，因为 Zookeeper 上的数据是有多个副本的，数据并不会丢失；</p>
<p>如果是一个 Leader 宕机，Zookeeper 会选举出新的 Leader。</p>
<p>ZK 集群的机制是只要超过半数的节点正常，集群就能正常提供服务。只有在 ZK节点挂得太多，只剩一半或不到一半节点能工作，集群才失效。</p>
<p>所以</p>
<p>3 个节点的 cluster 可以挂掉 1 个节点(leader 可以得到 2 票&gt;1.5)</p>
<p>2 个节点的 cluster 就不能挂掉任何 1 个节点了(leader 可以得到 1 票&lt;=1)</p>
<h2 id="19-zookeeper-负载均衡和-nginx-负载均衡区别"><a href="#19-zookeeper-负载均衡和-nginx-负载均衡区别" class="headerlink" title="19. zookeeper 负载均衡和 nginx 负载均衡区别"></a>19. zookeeper 负载均衡和 nginx 负载均衡区别</h2><p>zk 的负载均衡是可以调控，nginx 只是能调权重，其他需要可控的都需要自己写插件；但是 nginx 的吞吐量比 zk 大很多，应该说按业务选择用哪种方式。</p>
<h2 id="20-Zookeeper-有哪几种几种部署模式？"><a href="#20-Zookeeper-有哪几种几种部署模式？" class="headerlink" title="20. Zookeeper 有哪几种几种部署模式？"></a>20. Zookeeper 有哪几种几种部署模式？</h2><p>Zookeeper 有三种部署模式：</p>
<ol>
<li>单机部署：一台集群上运行；</li>
<li>集群部署：多台集群运行；</li>
<li>伪集群部署：一台集群启动多个 Zookeeper 实例运行。</li>
</ol>
<h2 id="21-集群最少要几台机器，集群规则是怎样的？集群中有-3-台服务器，其中一个节点宕机，这个时候-Zookeeper-还可以使用吗？"><a href="#21-集群最少要几台机器，集群规则是怎样的？集群中有-3-台服务器，其中一个节点宕机，这个时候-Zookeeper-还可以使用吗？" class="headerlink" title="21. 集群最少要几台机器，集群规则是怎样的？集群中有 3 台服务器，其中一个节点宕机，这个时候 Zookeeper 还可以使用吗？"></a>21. 集群最少要几台机器，集群规则是怎样的？集群中有 3 台服务器，其中一个节点宕机，这个时候 Zookeeper 还可以使用吗？</h2><p>集群规则为 2N+1 台，N&gt;0，即 3 台。可以继续使用，单数服务器只要没超过一半的服务器宕机就可以继续使用。</p>
<ol start="22">
<li>集群支持动态添加机器吗？<br>其实就是水平扩容了，Zookeeper 在这方面不太好。两种方式：</li>
</ol>
<p>全部重启：关闭所有 Zookeeper 服务，修改配置之后启动。不影响之前客户端的会话。</p>
<p>逐个重启：在过半存活即可用的原则下，一台机器重启不影响整个集群对外提供服务。这是比较常用的方式。</p>
<p>3.5 版本开始支持动态扩容。</p>
<h2 id="23-Zookeeper-对节点的-watch-监听通知是永久的吗？为什么不是永久的"><a href="#23-Zookeeper-对节点的-watch-监听通知是永久的吗？为什么不是永久的" class="headerlink" title="23. Zookeeper 对节点的 watch 监听通知是永久的吗？为什么不是永久的?"></a>23. Zookeeper 对节点的 watch 监听通知是永久的吗？为什么不是永久的?</h2><p>不是。官方声明：一个 Watch 事件是一个一次性的触发器，当被设置了 Watch的数据发生了改变的时候，则服务器将这个改变发送给设置了 Watch 的客户端，以便通知它们。</p>
<p>为什么不是永久的，举个例子，如果服务端变动频繁，而监听的客户端很多情况下，每次变动都要通知到所有的客户端，给网络和服务器造成很大压力。</p>
<p>一般是客户端执行 getData(“/节点 A”,true)，如果节点 A 发生了变更或删除，客户端会得到它的 watch 事件，但是在之后节点 A 又发生了变更，而客户端又没有设置 watch 事件，就不再给客户端发送。</p>
<p>在实际应用中，很多情况下，我们的客户端不需要知道服务端的每一次变动，我只要最新的数据即可。</p>
<h2 id="24-Zookeeper-的-java-客户端都有哪些？"><a href="#24-Zookeeper-的-java-客户端都有哪些？" class="headerlink" title="24. Zookeeper 的 java 客户端都有哪些？"></a>24. Zookeeper 的 java 客户端都有哪些？</h2><p>java 客户端：zk 自带的 zkclient 及 Apache 开源的 Curator。</p>
<h2 id="25-chubby-是什么，和-zookeeper-比你怎么看？"><a href="#25-chubby-是什么，和-zookeeper-比你怎么看？" class="headerlink" title="25. chubby 是什么，和 zookeeper 比你怎么看？"></a>25. chubby 是什么，和 zookeeper 比你怎么看？</h2><p>chubby 是 google 的，完全实现 paxos 算法，不开源。zookeeper 是 chubby的开源实现，使用 zab 协议，paxos 算法的变种。</p>
<h2 id="26-说几个-zookeeper-常用的命令。"><a href="#26-说几个-zookeeper-常用的命令。" class="headerlink" title="26. 说几个 zookeeper 常用的命令。"></a>26. 说几个 zookeeper 常用的命令。</h2><p>常用命令：ls get set create delete 等。</p>
<h2 id="27-ZAB-和-Paxos-算法的联系与区别？"><a href="#27-ZAB-和-Paxos-算法的联系与区别？" class="headerlink" title="27. ZAB 和 Paxos 算法的联系与区别？"></a>27. ZAB 和 Paxos 算法的联系与区别？</h2><p>相同点：</p>
<p>（1）两者都存在一个类似于 Leader 进程的角色，由其负责协调多个 Follower 进程的运行</p>
<p>（2）Leader 进程都会等待超过半数的 Follower 做出正确的反馈后，才会将一个提案进行提交</p>
<p>（3）ZAB 协议中，每个 Proposal 中都包含一个 epoch 值来代表当前的 Leader周期，Paxos 中名字为 Ballot</p>
<p>不同点：</p>
<p>ZAB 用来构建高可用的分布式数据主备系统（Zookeeper），Paxos 是用来构建分布式一致性状态机系统。</p>
<h2 id="28-Zookeeper-的典型应用场景"><a href="#28-Zookeeper-的典型应用场景" class="headerlink" title="28. Zookeeper 的典型应用场景"></a>28. Zookeeper 的典型应用场景</h2><p>Zookeeper 是一个典型的发布/订阅模式的分布式数据管理与协调框架，开发人员可以使用它来进行分布式数据的发布和订阅。</p>
<p>通过对 Zookeeper 中丰富的数据节点进行交叉使用，配合 Watcher 事件通知机制，可以非常方便的构建一系列分布式应用中年都会涉及的核心功能，如：</p>
<p>（1）数据发布/订阅</p>
<p>（2）负载均衡</p>
<p>（3）命名服务</p>
<p>（4）分布式协调/通知</p>
<p>（5）集群管理</p>
<p>（6）Master 选举</p>
<p>（7）分布式锁</p>
<p>（8）分布式队列</p>
<h2 id="29-Zookeeper-都有哪些功能？"><a href="#29-Zookeeper-都有哪些功能？" class="headerlink" title="29. Zookeeper 都有哪些功能？"></a>29. Zookeeper 都有哪些功能？</h2><ol>
<li>集群管理：监控节点存活状态、运行请求等；</li>
<li>主节点选举：主节点挂掉了之后可以从备用的节点开始新一轮选主，主节点选举说的就是这个选举的过程，使用 Zookeeper 可以协助完成这个过程；</li>
<li>分布式锁：Zookeeper 提供两种锁：独占锁、共享锁。独占锁即一次只能有一个线程使用资源，共享锁是读锁共享，读写互斥，即可以有多线线程同时读同一个资源，如果要使用写锁也只能有一个线程使用。Zookeeper 可以对分布式锁进行控制。</li>
<li>命名服务：在分布式系统中，通过使用命名服务，客户端应用能够根据指定名字来获取资源或服务的地址，提供者等信息。</li>
</ol>
<h2 id="30-说一下-Zookeeper-的通知机制？"><a href="#30-说一下-Zookeeper-的通知机制？" class="headerlink" title="30. 说一下 Zookeeper 的通知机制？"></a>30. 说一下 Zookeeper 的通知机制？</h2><p>client 端会对某个 znode 建立一个 watcher 事件，当该 znode 发生变化时，这些 client 会收到 zk 的通知，然后 client 可以根据 znode 变化来做出业务上的改变等。</p>
<h2 id="31-Zookeeper-和-Dubbo-的关系？"><a href="#31-Zookeeper-和-Dubbo-的关系？" class="headerlink" title="31. Zookeeper 和 Dubbo 的关系？"></a>31. Zookeeper 和 Dubbo 的关系？</h2><p>Zookeeper的作用：</p>
<p>zookeeper用来注册服务和进行负载均衡，哪一个服务由哪一个机器来提供必需让调用者知道，简单来说就是ip地址和服务名称的对应关系。当然也可以通过硬编码的方式把这种对应关系在调用方业务代码中实现，但是如果提供服务的机器挂掉调用者无法知晓，如果不更改代码会继续请求挂掉的机器提供服务。zookeeper通过心跳机制可以检测挂掉的机器并将挂掉机器的ip和服务对应关系从列表中删除。至于支持高并发，简单来说就是横向扩展，在不更改代码的情况通过添加机器来提高运算能力。通过添加新的机器向zookeeper注册服务，服务的提供者多了能服务的客户就多了。</p>
<p>dubbo：</p>
<p>是管理中间层的工具，在业务层到数据仓库间有非常多服务的接入和服务提供者需要调度，dubbo提供一个框架解决这个问题。<br>注意这里的dubbo只是一个框架，至于你架子上放什么是完全取决于你的，就像一个汽车骨架，你需要配你的轮子引擎。这个框架中要完成调度必须要有一个分布式的注册中心，储存所有服务的元数据，你可以用zk，也可以用别的，只是大家都用zk。</p>
<p>zookeeper和dubbo的关系：</p>
<p>Dubbo 的将注册中心进行抽象，它可以外接不同的存储媒介给注册中心提供服务，有 ZooKeeper，Memcached，Redis 等。</p>
<p>引入了 ZooKeeper 作为存储媒介，也就把 ZooKeeper 的特性引进来。首先是负载均衡，单注册中心的承载能力是有限的，在流量达到一定程度的时 候就需要分流，负载均衡就是为了分流而存在的，一个 ZooKeeper 群配合相应的 Web 应用就可以很容易达到负载均衡；资源同步，单单有负载均衡还不 够，节点之间的数据和资源需要同步，ZooKeeper 集群就天然具备有这样的功能；命名服务，将树状结构用于维护全局的服务地址列表，服务提供者在启动 的时候，向 ZooKeeper 上的指定节点 /dubbo/${serviceName}/providers 目录下写入自己的 URL 地址，这个操作就完成了服务的发布。 其他特性还有 Master 选举，分布式锁等。</p>
<p><img src="https://lixiangbetter.github.io/2020/07/18/ZooKeeper%E7%AC%94%E8%AE%B0/aHR0cHM6Ly91c2VyLWdvbGQtY2RuLnhpdHUuaW8vMjAxOS8xMC8zMS8xNmUyMTliYzc3MDA5OGRm.jpeg" alt></p>
]]></content>
      <categories>
        <category>bigdata</category>
      </categories>
      <tags>
        <tag>zookeeper</tag>
      </tags>
  </entry>
  <entry>
    <title>Tomcat笔记</title>
    <url>/2020/07/17/Tomcat%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<h1 id="Tomcat笔记"><a href="#Tomcat笔记" class="headerlink" title="Tomcat笔记"></a>Tomcat笔记</h1><h2 id="Tomcat是什么？"><a href="#Tomcat是什么？" class="headerlink" title="Tomcat是什么？"></a>Tomcat是什么？</h2><p>Tomcat 服务器Apache软件基金会项目中的一个核心项目，是一个免费的开放源代码的Web 应用服务器，属于轻量级应用服务器，在中小型系统和并发访问用户不是很多的场合下被普遍使用，是开发和调试JSP 程序的首选。</p>
<h2 id="Tomcat的缺省端口是多少，怎么修改"><a href="#Tomcat的缺省端口是多少，怎么修改" class="headerlink" title="Tomcat的缺省端口是多少，怎么修改"></a>Tomcat的缺省端口是多少，怎么修改</h2><ol>
<li>找到Tomcat目录下的conf文件夹</li>
<li>进入conf文件夹里面找到server.xml文件</li>
<li>打开server.xml文件</li>
<li>在server.xml文件里面找到下列信息</li>
<li>把Connector标签的8080端口改成你想要的端口</li>
</ol>
<figure class="highlight xml"><table><tr><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">Service</span> <span class="attr">name</span>=<span class="string">"Catalina"</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">Connector</span> <span class="attr">port</span>=<span class="string">"8080"</span> <span class="attr">protocol</span>=<span class="string">"HTTP/1.1"</span> </span></span><br><span class="line"><span class="tag">               <span class="attr">connectionTimeout</span>=<span class="string">"20000"</span> </span></span><br><span class="line"><span class="tag">               <span class="attr">redirectPort</span>=<span class="string">"8443"</span> /&gt;</span></span><br></pre></td></tr></table></figure>

<h2 id="tomcat-有哪几种Connector-运行模式-优化-？"><a href="#tomcat-有哪几种Connector-运行模式-优化-？" class="headerlink" title="tomcat 有哪几种Connector 运行模式(优化)？"></a>tomcat 有哪几种Connector 运行模式(优化)？</h2><p>下面，我们先大致了解Tomcat Connector的三种运行模式。</p>
<ul>
<li><p><strong>BIO</strong>：同步并阻塞一个线程处理一个请求。缺点：并发量高时，线程数较多，浪费资源。Tomcat7或以下，在Linux系统中默认使用这种方式。<br> 配制项：protocol=”HTTP/1.1”</p>
</li>
<li><p><strong>NIO</strong>：同步非阻塞IO</p>
<p>利用Java的异步IO处理，可以通过少量的线程处理大量的请求，可以复用同一个线程处理多个connection(多路复用)。</p>
<p>Tomcat8在Linux系统中默认使用这种方式。</p>
<p>Tomcat7必须修改Connector配置来启动。</p>
<p>配制项：protocol=”org.apache.coyote.http11.Http11NioProtocol”</p>
<p>备注：我们常用的Jetty，Mina，ZooKeeper等都是基于java nio实现.</p>
</li>
<li><p><strong>APR</strong>：即Apache Portable Runtime，从操作系统层面解决io阻塞问题。<strong>AIO方式，</strong>异步非阻塞IO(Java NIO2又叫AIO) 主要与NIO的区别主要是操作系统的底层区别.可以做个比喻:比作快递，NIO就是网购后要自己到官网查下快递是否已经到了(可能是多次)，然后自己去取快递；AIO就是快递员送货上门了(不用关注快递进度)。</p>
<p>配制项：protocol=”org.apache.coyote.http11.Http11AprProtocol”</p>
<p>备注：需在本地服务器安装APR库。Tomcat7或Tomcat8在Win7或以上的系统中启动默认使用这种方式。Linux如果安装了apr和native，Tomcat直接启动就支持apr。</p>
</li>
</ul>
<h2 id="Tomcat有几种部署方式？"><a href="#Tomcat有几种部署方式？" class="headerlink" title="Tomcat有几种部署方式？"></a>Tomcat有几种部署方式？</h2><p>在Tomcat中部署Web应用的方式主要有如下几种：</p>
<ol>
<li><p>利用Tomcat的自动部署。</p>
<p>把web应用拷贝到webapps目录。Tomcat在启动时会加载目录下的应用，并将编译后的结果放入work目录下。</p>
</li>
<li><p>使用Manager App控制台部署。</p>
<p>在tomcat主页点击“Manager App” 进入应用管理控制台，可以指定一个web应用的路径或war文件。</p>
</li>
<li><p>修改conf/server.xml文件部署。</p>
<p>修改conf/server.xml文件，增加Context节点可以部署应用。</p>
</li>
<li><p>增加自定义的Web部署文件。</p>
<p>在conf/Catalina/localhost/ 路径下增加 xyz.xml文件，内容是Context节点，可以部署应用。</p>
</li>
</ol>
<h2 id="tomcat容器是如何创建servlet类实例？用到了什么原理？"><a href="#tomcat容器是如何创建servlet类实例？用到了什么原理？" class="headerlink" title="tomcat容器是如何创建servlet类实例？用到了什么原理？"></a>tomcat容器是如何创建servlet类实例？用到了什么原理？</h2><ol>
<li>当容器启动时，会读取在webapps目录下所有的web应用中的web.xml文件，然后对 xml文件进行解析，并读取servlet注册信息。然后，将每个应用中注册的servlet类都进行加载，并通过 反射的方式实例化。（有时候也是在第一次请求时实例化）</li>
<li>在servlet注册时加上1如果为正数，则在一开始就实例化，如果不写或为负数，则第一次请求实例化。</li>
</ol>
<h2 id="Tomcat工作模式"><a href="#Tomcat工作模式" class="headerlink" title="Tomcat工作模式"></a>Tomcat工作模式</h2><p>Tomcat作为servlet容器，有三种工作模式：</p>
<ol>
<li>独立的servlet容器，servlet容器是web服务器的一部分；</li>
<li>进程内的servlet容器，servlet容器是作为web服务器的插件和java容器的实现，web服务器插件在内部地址空间打开一个jvm使得java容器在内部得以运行。反应速度快但伸缩性不足；</li>
<li>进程外的servlet容器，servlet容器运行于web服务器之外的地址空间，并作为web服务器的插件和java容器实现的结合。反应时间不如进程内但伸缩性和稳定性比进程内优；</li>
</ol>
<p>进入Tomcat的请求可以根据Tomcat的工作模式分为如下两类：</p>
<ol>
<li>Tomcat作为应用程序服务器：请求来自于前端的web服务器，这可能是Apache, IIS, Nginx等；</li>
<li>Tomcat作为独立服务器：请求来自于web浏览器；</li>
</ol>
<p>面试时问到Tomcat相关问题的几率并不高，正式因为如此，很多人忽略了对Tomcat相关技能的掌握，下面这一篇文章整理了Tomcat相关的系统架构，介绍了Server、Service、Connector、Container之间的关系，各个模块的功能，可以说把这几个掌握住了，Tomcat相关的面试题你就不会有任何问题了！另外，在面试的时候你还要有意识无意识的往Tomcat这个地方引，就比如说常见的Spring MVC的执行流程，一个URL的完整调用链路，这些相关的题目你是可以往Tomcat处理请求的这个过程去说的！掌握了Tomcat这些技能，面试官一定会佩服你的！</p>
<p>学了本章之后你应该明白的是：</p>
<ul>
<li>Server、Service、Connector、Container四大组件之间的关系和联系，以及他们的主要功能点；</li>
<li>Tomcat执行的整体架构，请求是如何被一步步处理的；</li>
<li>Engine、Host、Context、Wrapper相关的概念关系；</li>
<li>Container是如何处理请求的；</li>
<li>Tomcat用到的相关设计模式；</li>
</ul>
<h2 id="Tomcat顶层架构"><a href="#Tomcat顶层架构" class="headerlink" title="Tomcat顶层架构"></a>Tomcat顶层架构</h2><p>俗话说，站在巨人的肩膀上看世界，一般学习的时候也是先总览一下整体，然后逐个部分个个击破，最后形成思路，了解具体细节，Tomcat的结构很复杂，但是 Tomcat 非常的模块化，找到了 Tomcat 最核心的模块，问题才可以游刃而解，了解了 Tomcat 的整体架构对以后深入了解 Tomcat 来说至关重要！</p>
<p>先上一张Tomcat的顶层结构图（图A），如下：<br><img src="https://lixiangbetter.github.io/2020/07/17/Tomcat%E7%AC%94%E8%AE%B0/20191021215330153.png" alt></p>
<p>Tomcat中最顶层的容器是Server，代表着整个服务器，从上图中可以看出，一个Server可以包含至少一个Service，即可以包含多个Service，用于具体提供服务。</p>
<p>Service主要包含两个部分：Connector和Container。从上图中可以看出 Tomcat 的心脏就是这两个组件，他们的作用如下：</p>
<ul>
<li>Connector用于处理连接相关的事情，并提供Socket与Request请求和Response响应相关的转化;</li>
<li>Container用于封装和管理Servlet，以及具体处理Request请求；</li>
</ul>
<p>一个Tomcat中只有一个Server，一个Server可以包含多个Service，一个Service只有一个Container，但是可以有多个Connectors，这是因为一个服务可以有多个连接，如同时提供Http和Https链接，也可以提供向相同协议不同端口的连接，示意图如下（Engine、Host、Context下面会说到）：</p>
<p><img src="https://lixiangbetter.github.io/2020/07/17/Tomcat%E7%AC%94%E8%AE%B0/20191021215344811.png" alt></p>
<p>多个 Connector 和一个Container 就形成了一个Service，有了 Service 就可以对外提供服务了，但是 Service 还要一个生存的环境，必须要有人能够给她生命、掌握其生死大权，那就非 Server 莫属了！所以整个 Tomcat 的生命周期由 Server 控制。</p>
<p>另外，上述的包含关系或者说是父子关系，都可以在tomcat的conf目录下的server.xml配置文件中看出，下图是删除了注释内容之后的一个完整的server.xml配置文件（Tomcat版本为8.0）</p>
<p><img src="https://lixiangbetter.github.io/2020/07/17/Tomcat%E7%AC%94%E8%AE%B0/20191021215355649.png" alt></p>
<p>上边的配置文件，还可以通过下边的一张结构图更清楚的理解：</p>
<p><img src="https://lixiangbetter.github.io/2020/07/17/Tomcat%E7%AC%94%E8%AE%B0/2019102121541531.png" alt></p>
<p>Server标签设置的端口号为8005，shutdown=”SHUTDOWN” ，表示在8005端口监听“SHUTDOWN”命令，如果接收到了就会关闭Tomcat。一个Server有一个Service，当然还可以进行配置，一个Service有多个Connector，Service左边的内容都属于Container的，Service下边是Connector。</p>
<h3 id="Tomcat顶层架构小结"><a href="#Tomcat顶层架构小结" class="headerlink" title="Tomcat顶层架构小结"></a>Tomcat顶层架构小结</h3><ol>
<li>Tomcat中只有一个Server，一个Server可以有多个Service，一个Service可以有多个Connector和一个Container；</li>
<li>Server掌管着整个Tomcat的生死大权；</li>
<li>Service 是对外提供服务的；</li>
<li>Connector用于接受请求并将请求封装成Request和Response来具体处理；</li>
<li>Container用于封装和管理Servlet，以及具体处理request请求；</li>
</ol>
<p>知道了整个Tomcat顶层的分层架构和各个组件之间的关系以及作用，对于绝大多数的开发人员来说Server和Service对我们来说确实很远，而我们开发中绝大部分进行配置的内容是属于Connector和Container的，所以接下来介绍一下Connector和Container。</p>
<h2 id="Connector和Container的微妙关系"><a href="#Connector和Container的微妙关系" class="headerlink" title="Connector和Container的微妙关系"></a>Connector和Container的微妙关系</h2><p>由上述内容我们大致可以知道一个请求发送到Tomcat之后，首先经过Service然后会交给我们的Connector，Connector用于接收请求并将接收的请求封装为Request和Response来具体处理，Request和Response封装完之后再交由Container进行处理，Container处理完请求之后再返回给Connector，最后在由Connector通过Socket将处理的结果返回给客户端，这样整个请求的就处理完了！</p>
<p>Connector最底层使用的是Socket来进行连接的，Request和Response是按照HTTP协议来封装的，所以Connector同时需要实现TCP/IP协议和HTTP协议！</p>
<p>Tomcat既然需要处理请求，那么肯定需要先接收到这个请求，接收请求这个东西我们首先就需要看一下Connector！</p>
<p><strong>Connector架构分析</strong></p>
<p>Connector用于接受请求并将请求封装成Request和Response，然后交给Container进行处理，Container处理完之后在交给Connector返回给客户端。</p>
<p>因此，我们可以把Connector分为四个方面进行理解：</p>
<ol>
<li>Connector如何接受请求的？</li>
<li>如何将请求封装成Request和Response的？</li>
<li>封装完之后的Request和Response如何交给Container进行处理的？</li>
<li>Container处理完之后如何交给Connector并返回给客户端的？</li>
</ol>
<p>首先看一下Connector的结构图（图B），如下所示：</p>
<p><img src="https://lixiangbetter.github.io/2020/07/17/Tomcat%E7%AC%94%E8%AE%B0/20191021215430677.png" alt></p>
<p>Connector就是使用ProtocolHandler来处理请求的，不同的ProtocolHandler代表不同的连接类型，比如：Http11Protocol使用的是普通Socket来连接的，Http11NioProtocol使用的是NioSocket来连接的。</p>
<p>其中ProtocolHandler由包含了三个部件：Endpoint、Processor、Adapter。</p>
<ol>
<li>Endpoint用来处理底层Socket的网络连接，Processor用于将Endpoint接收到的Socket封装成Request，Adapter用于将Request交给Container进行具体的处理。</li>
<li>Endpoint由于是处理底层的Socket网络连接，因此Endpoint是用来实现TCP/IP协议的，而Processor用来实现HTTP协议的，Adapter将请求适配到Servlet容器进行具体的处理。</li>
<li>Endpoint的抽象实现AbstractEndpoint里面定义的Acceptor和AsyncTimeout两个内部类和一个Handler接口。Acceptor用于监听请求，AsyncTimeout用于检查异步Request的超时，Handler用于处理接收到的Socket，在内部调用Processor进行处理。</li>
</ol>
<p>至此，我们应该很轻松的回答1，2，3的问题了，但是4还是不知道，那么我们就来看一下Container是如何进行处理的以及处理完之后是如何将处理完的结果返回给Connector的？</p>
<h2 id="Container架构分析"><a href="#Container架构分析" class="headerlink" title="Container架构分析"></a>Container架构分析</h2><p>Container用于封装和管理Servlet，以及具体处理Request请求，在Container内部包含了4个子容器，结构图如下（图C）：</p>
<p><img src="https://lixiangbetter.github.io/2020/07/17/Tomcat%E7%AC%94%E8%AE%B0/20191021215443306.png" alt></p>
<p>4个子容器的作用分别是：</p>
<ol>
<li>Engine：引擎，用来管理多个站点，一个Service最多只能有一个Engine；</li>
<li>Host：代表一个站点，也可以叫虚拟主机，通过配置Host就可以添加站点；</li>
<li>Context：代表一个应用程序，对应着平时开发的一套程序，或者一个WEB-INF目录以及下面的web.xml文件；</li>
<li>Wrapper：每一Wrapper封装着一个Servlet；</li>
</ol>
<p>下面找一个Tomcat的文件目录对照一下，如下图所示：</p>
<p><img src="https://lixiangbetter.github.io/2020/07/17/Tomcat%E7%AC%94%E8%AE%B0/20191021215455991.png" alt></p>
<p>Context和Host的区别是Context表示一个应用，我们的Tomcat中默认的配置下webapps下的每一个文件夹目录都是一个Context，其中ROOT目录中存放着主应用，其他目录存放着子应用，而整个webapps就是一个Host站点。</p>
<p>我们访问应用Context的时候，如果是ROOT下的则直接使用域名就可以访问，例如：<a href="http://www.baidu.com，如果是Host（webapps）下的其他应用，则可以使用www.baidu.com/docs进行访问，当然默认指定的根应用（ROOT）是可以进行设定的，只不过Host站点下默认的主应用是ROOT目录下的。" target="_blank" rel="noopener">www.baidu.com，如果是Host（webapps）下的其他应用，则可以使用www.baidu.com/docs进行访问，当然默认指定的根应用（ROOT）是可以进行设定的，只不过Host站点下默认的主应用是ROOT目录下的。</a></p>
<p>看到这里我们知道Container是什么，但是还是不知道Container是如何进行请求处理的以及处理完之后是如何将处理完的结果返回给Connector的？别急！下边就开始探讨一下Container是如何进行处理的！</p>
<h3 id="Container如何处理请求的"><a href="#Container如何处理请求的" class="headerlink" title="Container如何处理请求的"></a>Container如何处理请求的</h3><p>Container处理请求是使用Pipeline-Valve管道来处理的！（Valve是阀门之意）</p>
<p>Pipeline-Valve是责任链模式，责任链模式是指在一个请求处理的过程中有很多处理者依次对请求进行处理，每个处理者负责做自己相应的处理，处理完之后将处理后的结果返回，再让下一个处理者继续处理。</p>
<p><img src="https://lixiangbetter.github.io/2020/07/17/Tomcat%E7%AC%94%E8%AE%B0/20191021215507725.png" alt></p>
<p>但是！Pipeline-Valve使用的责任链模式和普通的责任链模式有些不同！区别主要有以下两点：</p>
<p>每个Pipeline都有特定的Valve，而且是在管道的最后一个执行，这个Valve叫做BaseValve，BaseValve是不可删除的；</p>
<p>在上层容器的管道的BaseValve中会调用下层容器的管道。</p>
<p>我们知道Container包含四个子容器，而这四个子容器对应的BaseValve分别在：StandardEngineValve、StandardHostValve、StandardContextValve、StandardWrapperValve。</p>
<p>Pipeline的处理流程图如下（图D）：</p>
<p><img src="https://lixiangbetter.github.io/2020/07/17/Tomcat%E7%AC%94%E8%AE%B0/20191021215519408.png" alt></p>
<ul>
<li>Connector在接收到请求后会首先调用最顶层容器的Pipeline来处理，这里的最顶层容器的Pipeline就是EnginePipeline（Engine的管道）；</li>
<li>在Engine的管道中依次会执行EngineValve1、EngineValve2等等，最后会执行StandardEngineValve，在StandardEngineValve中会调用Host管道，然后再依次执行Host的HostValve1、HostValve2等，最后在执行StandardHostValve，然后再依次调用Context的管道和Wrapper的管道，最后执行到StandardWrapperValve。</li>
<li>当执行到StandardWrapperValve的时候，会在StandardWrapperValve中创建FilterChain，并调用其doFilter方法来处理请求，这个FilterChain包含着我们配置的与请求相匹配的Filter和Servlet，其doFilter方法会依次调用所有的Filter的doFilter方法和Servlet的service方法，这样请求就得到了处理！</li>
<li>当所有的Pipeline-Valve都执行完之后，并且处理完了具体的请求，这个时候就可以将返回的结果交给Connector了，Connector在通过Socket的方式将结果返回给客户端。</li>
</ul>
<h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>至此，我们已经对Tomcat的整体架构有了大致的了解，从图A、B、C、D可以看出来每一个组件的基本要素和作用。我们在脑海里应该有一个大概的轮廓了！如果你面试的时候，让你简单的聊一下Tomcat，上面的内容你能脱口而出吗？当你能够脱口而出的时候，面试官一定会对你刮目相看的！</p>
]]></content>
      <categories>
        <category>bigdata</category>
      </categories>
      <tags>
        <tag>tomcat</tag>
      </tags>
  </entry>
  <entry>
    <title>flume、sqoop笔记</title>
    <url>/2020/07/12/flume%E3%80%81sqoop%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<h1 id="flume、sqoop笔记"><a href="#flume、sqoop笔记" class="headerlink" title="flume、sqoop笔记"></a>flume、sqoop笔记</h1><h2 id="1-什么是flume"><a href="#1-什么是flume" class="headerlink" title="1.什么是flume"></a>1.什么是flume</h2><p>a.Flume是一个分布式、可靠、和高可用的海量日志采集、聚合和传输的系统。</p>
<p>b.Flume可以采集文件，socket数据包等各种形式源数据，又可以将采集到的数据输出到HDFS、hbase、hive、kafka等众多外部存储系统中</p>
<p>c.一般的采集需求，通过对flume的简单配置即可实现</p>
<p>d.Flume针对特殊场景也具备良好的自定义扩展能力，因此，flume可以适用于大部分的日常数据采集场景</p>
<h2 id="2-flume运行机制"><a href="#2-flume运行机制" class="headerlink" title="2.flume运行机制"></a>2.flume运行机制</h2><ol>
<li>Flume分布式系统中最核心的角色是agent，flume采集系统就是由一个个agent所连接起来形成</li>
<li>每一个agent相当于一个数据传递员，内部有三个组件：<ul>
<li>Source：采集源，用于跟数据源对接，以获取数据</li>
<li>Sink：下沉地，采集数据的传送目的，用于往下一级agent传递数据或者往最终存储系统传递数据</li>
<li>Channel：angent内部的数据传输通道，用于从source将数据传递到sink</li>
</ul>
</li>
</ol>
<p><img src="https://lixiangbetter.github.io/2020/07/12/flume%E3%80%81sqoop%E7%AC%94%E8%AE%B0/20200409122907293.png" alt></p>
<h2 id="3-Flume采集数据到Kafka中丢数据怎么办"><a href="#3-Flume采集数据到Kafka中丢数据怎么办" class="headerlink" title="3.Flume采集数据到Kafka中丢数据怎么办"></a>3.Flume采集数据到Kafka中丢数据怎么办</h2><p><strong>Source到Channel</strong>是事务性的，</p>
<p><strong>Channel到Sink</strong>也是事务性的，</p>
<p>这两个环节都不可能丢失数据。</p>
<p>唯一可能丢失数据的是Channel采用MemoryChannel.</p>
<h2 id="4-Flume怎么进行监控"><a href="#4-Flume怎么进行监控" class="headerlink" title="4.Flume怎么进行监控?"></a>4.Flume怎么进行监控?</h2><p>Ganglia</p>
<h2 id="5-Flume的三层架构，collector、agent、storage"><a href="#5-Flume的三层架构，collector、agent、storage" class="headerlink" title="5.Flume的三层架构，collector、agent、storage"></a>5.Flume的三层架构，collector、agent、storage</h2><p>Flume采用了三层架构，分别为agent，collector和storage，每一层均可以水平扩展。其中，所有agent和collector由master统一管理，这使得系统容易监控和维护，且master允许有多个（使用ZooKeeper进行管理和负载均衡），这就避免了单点故障问题。</p>
<p>1）Agent层：这一层包含了Flume的Agent组件，与需要传输数据的数据源连接在一起</p>
<p>2）Collector：这一层通过多个收集器收集Agent层的数据，然后将这些转发到下一层</p>
<p>3）storage：这一层接收collector层的数据并存储起来</p>
<p>分割线———————————————————————————————————</p>
<h2 id="1-Sqoop底层运行的任务是什么"><a href="#1-Sqoop底层运行的任务是什么" class="headerlink" title="1.Sqoop底层运行的任务是什么"></a>1.Sqoop底层运行的任务是什么</h2><p>只有Map阶段，没有Reduce阶段的任务。</p>
<h2 id="2-Sqoop的迁移数据的原理"><a href="#2-Sqoop的迁移数据的原理" class="headerlink" title="2.Sqoop的迁移数据的原理"></a>2.Sqoop的迁移数据的原理</h2><p>将导入或导出命令翻译成mapreduce程序来实现，在翻译出的mapreduce中主要是对inputformat和outputformat进行定制。</p>
<h2 id="3-Sqoop参数"><a href="#3-Sqoop参数" class="headerlink" title="3.Sqoop参数"></a>3.Sqoop参数</h2><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">/opt/module/sqoop/bin/sqoop import \</span><br><span class="line"></span><br><span class="line">--connect \</span><br><span class="line"></span><br><span class="line">--username \</span><br><span class="line"></span><br><span class="line">--password \</span><br><span class="line"></span><br><span class="line">--target-dir \</span><br><span class="line"></span><br><span class="line">--delete-target-dir \</span><br><span class="line"></span><br><span class="line">--num-mappers \</span><br><span class="line"></span><br><span class="line">--fields-terminated-by   \</span><br><span class="line"></span><br><span class="line">--query   "$2" ' and $CONDITIONS;'</span><br></pre></td></tr></table></figure>

<h2 id="4-Sqoop导入导出Null存储一致性问题"><a href="#4-Sqoop导入导出Null存储一致性问题" class="headerlink" title="4.Sqoop导入导出Null存储一致性问题"></a>4.Sqoop导入导出Null存储一致性问题</h2><p>Hive中的Null在底层是以“\N”来存储，而MySQL中的Null在底层就是Null，为了保证数据两端的一致性。在导出数据时采用–input-null-string和–input-null-non-string两个参数。导入数据时采用–null-string和–null-non-string。</p>
<h2 id="5-Sqoop数据导出一致性问题"><a href="#5-Sqoop数据导出一致性问题" class="headerlink" title="5.Sqoop数据导出一致性问题"></a>5.Sqoop数据导出一致性问题</h2><p>1）场景1：如Sqoop在导出到Mysql时，使用4个Map任务，过程中有2个任务失败，那此时MySQL中存储了另外两个Map任务导入的数据，此时老板正好看到了这个报表数据。而开发工程师发现任务失败后，会调试问题并最终将全部数据正确的导入MySQL，那后面老板再次看报表数据，发现本次看到的数据与之前的不一致，这在生产环境是不允许的。</p>
<p>2）场景2：设置map数量为1个（不推荐，面试官想要的答案不只这个）</p>
<p>多个Map任务时，采用–staging-table方式，仍然可以解决数据一致性问题。</p>
]]></content>
      <categories>
        <category>bigdata</category>
      </categories>
      <tags>
        <tag>flume sqoop</tag>
      </tags>
  </entry>
  <entry>
    <title>hive调优</title>
    <url>/2020/07/12/hive%E8%B0%83%E4%BC%98/</url>
    <content><![CDATA[<h1 id="hive调优"><a href="#hive调优" class="headerlink" title="hive调优"></a>hive调优</h1><p> HIVE调优是一个很大的课题，涉及到hive本身的调优，hive底层的mapreduce计算引擎的调优，sql的调优，数据倾斜调优，小文件问题的调优，数据压缩的调优等</p>
<p>以下提供一些主要的调优总结：</p>
<h2 id="1-数据的压缩与存储格式"><a href="#1-数据的压缩与存储格式" class="headerlink" title="1.数据的压缩与存储格式"></a>1.数据的压缩与存储格式</h2><p> hive底层的计算引擎是mapreduce，而mapreduce在运算时，免不了的就是要从hdfs中读取原始文件，然后在内部的map到reduce之间还要shuffle数据到各task所在的本地磁盘，最后的输出又避免不了要往HDFS中输出文件.</p>
<p>所以，在各个环节中，读写的数据量越小，读写的性能越高，对hive的整体执行效率肯定是有重要作用的，那么，如何降低这些环节上的文件io量以及提高文件io效率呢，一个最主要的办法就是“选择合适的文件格式”+选择合适的压缩编码</p>
<p>设置方式</p>
<ol>
<li>map阶段输出数据压缩 ，在这个阶段，优先选择一个低CPU开销的算法。</li>
</ol>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">set hive.exec.compress.intermediate=true</span><br><span class="line">set mapred.map.output.compression.codec= org.apache.hadoop.io.compress.SnappyCodec</span><br><span class="line">set mapred.map.output.compression.codec=com.hadoop.compression.lzo.LzoCodec;</span><br></pre></td></tr></table></figure>

<ol start="2">
<li>对最终输出结果压缩</li>
</ol>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">set hive.exec.compress.output=true </span><br><span class="line">set mapred.output.compression.codec=org.apache.hadoop.io.compress.SnappyCodec</span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># 当然，也可以在hive建表时指定表的文件格式和压缩编码</span></span></span><br></pre></td></tr></table></figure>

<p>结论，一般选择orcfile/parquet + snappy 方式</p>
<h2 id="2-合理利用分区、分桶"><a href="#2-合理利用分区、分桶" class="headerlink" title="2.合理利用分区、分桶"></a>2.合理利用分区、分桶</h2><p>​        分区是将表的数据在物理上分成不同的文件夹，以便于在查询时可以精准指定所要读取的分区目录，从来降低读取的数据量</p>
<p>​    分桶是将表数据按指定列的hash散列后分在了不同的文件中，将来查询时，hive可以根据分桶结构，快速定位到一行数据所在的分桶文件，从来提高读取效率.</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="comment">-- 创建分桶表示例</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> bucketed_user(<span class="keyword">id</span> <span class="built_in">int</span>,<span class="keyword">name</span> <span class="keyword">string</span>) clustered <span class="keyword">by</span> (<span class="keyword">id</span>)</span><br><span class="line"> sorted <span class="keyword">by</span>(<span class="keyword">name</span>) <span class="keyword">into</span> <span class="number">4</span> buckets <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> <span class="keyword">stored</span> <span class="keyword">as</span> ORCFILE;</span><br></pre></td></tr></table></figure>

<h2 id="3-hive参数优化"><a href="#3-hive参数优化" class="headerlink" title="3.hive参数优化"></a>3.hive参数优化</h2><figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">-- 让可以不走mapreduce任务的，就不走mapreduce任务</span><br><span class="line"><span class="meta">hive&gt;</span><span class="bash"> <span class="built_in">set</span> hive.fetch.task.conversion=more;</span></span><br><span class="line"> </span><br><span class="line">// 开启任务并行执行</span><br><span class="line"> set hive.exec.parallel=true;</span><br><span class="line">// 解释：当一个sql中有多个job时候，且这多个job之间没有依赖，则可以让顺序执行变为并行执行（一般为用到union all的时候）</span><br><span class="line"> </span><br><span class="line"> // 同一个sql允许并行任务的最大线程数 </span><br><span class="line">set hive.exec.parallel.thread.number=8;</span><br><span class="line"> </span><br><span class="line">// 设置jvm重用</span><br><span class="line">// JVM重用对hive的性能具有非常大的 影响，特别是对于很难避免小文件的场景或者task特别多的场景，这类场景大多数执行时间都很短。jvm的启动过程可能会造成相当大的开销，尤其是执行的job包含有成千上万个task任务的情况。</span><br><span class="line">set mapred.job.reuse.jvm.num.tasks=10; </span><br><span class="line"> </span><br><span class="line">// 合理设置reduce的数目</span><br><span class="line">// 方法1：调整每个reduce所接受的数据量大小</span><br><span class="line">set hive.exec.reducers.bytes.per.reducer=500000000; （500M）</span><br><span class="line">// 方法2：直接设置reduce数量</span><br><span class="line">set mapred.reduce.tasks = 20</span><br></pre></td></tr></table></figure>

<h2 id="4-sql优化"><a href="#4-sql优化" class="headerlink" title="4.sql优化"></a>4.sql优化</h2><p>（1）where条件优化<br>优化前（关系数据库不用考虑会自动优化）：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> m.cid,u.id <span class="keyword">from</span> <span class="keyword">order</span> m <span class="keyword">join</span> customer u <span class="keyword">on</span>( m.cid =u.id )<span class="keyword">where</span> m.dt=<span class="string">'20180808'</span>;</span><br></pre></td></tr></table></figure>

<p>优化后(where条件在map端执行而不是在reduce端执行）：</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> m.cid,u.id <span class="keyword">from</span> （<span class="keyword">select</span> * <span class="keyword">from</span> <span class="keyword">order</span> <span class="keyword">where</span> dt=<span class="string">'20180818'</span>） m <span class="keyword">join</span> customer u <span class="keyword">on</span>( m.cid =u.id);</span><br></pre></td></tr></table></figure>

<p>（2）union优化</p>
<p>尽量不要使用union （union 去掉重复的记录）而是使用 union all 然后在用group by 去重</p>
<p>（3）count distinct优化</p>
<p>不要使用count (distinct  cloumn) ,使用子查询</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">count</span>(<span class="number">1</span>) <span class="keyword">from</span> (<span class="keyword">select</span> <span class="keyword">id</span> <span class="keyword">from</span> tablename <span class="keyword">group</span> <span class="keyword">by</span> <span class="keyword">id</span>) tmp;</span><br></pre></td></tr></table></figure>

<p>（4） 用in 来代替join</p>
<p>如果需要根据一个表的字段来约束另为一个表，尽量用in来代替join . in 要比join 快</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">id</span>,<span class="keyword">name</span> <span class="keyword">from</span> tb1  a <span class="keyword">join</span> tb2 b <span class="keyword">on</span>(a.id = b.id);</span><br><span class="line"> </span><br><span class="line"><span class="keyword">select</span> <span class="keyword">id</span>,<span class="keyword">name</span> <span class="keyword">from</span> tb1 <span class="keyword">where</span> <span class="keyword">id</span> <span class="keyword">in</span>(<span class="keyword">select</span> <span class="keyword">id</span> <span class="keyword">from</span> tb2);</span><br></pre></td></tr></table></figure>

<p>（5）消灭子查询内的 group by 、 COUNT(DISTINCT)，MAX，MIN。 可以减少job的数量。</p>
<p>  (6) join 优化：</p>
<p>Common/shuffle/Reduce JOIN 连接发生的阶段，发生在reduce 阶段， 适用于大表 连接 大表(默认的方式)</p>
<p>Map join ： 连接发生在map阶段 ， 适用于小表 连接 大表<br>                       大表的数据从文件中读取<br>                       小表的数据存放在内存中（hive中已经自动进行了优化，自动判断小表，然后进行缓存）</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.auto.convert.join=<span class="literal">true</span>;</span><br></pre></td></tr></table></figure>

<p>SMB join<br>  Sort -Merge -Bucket Join 对大表连接大表的优化，用桶表的概念来进行优化。在一个桶内发送生笛卡尔积连接（需要是两个桶表进行join）</p>
<figure class="highlight sql"><table><tr><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.auto.convert.sortmerge.join=<span class="literal">true</span>;  </span><br><span class="line"><span class="keyword">set</span> hive.optimize.bucketmapjoin = <span class="literal">true</span>;  </span><br><span class="line"><span class="keyword">set</span> hive.optimize.bucketmapjoin.sortedmerge = <span class="literal">true</span>;  </span><br><span class="line"><span class="keyword">set</span> hive.auto.convert.sortmerge.join.noconditionaltask=<span class="literal">true</span>;</span><br></pre></td></tr></table></figure>

<h2 id="5-数据倾斜"><a href="#5-数据倾斜" class="headerlink" title="5.数据倾斜"></a>5.数据倾斜</h2><p>表现：任务进度长时间维持在99%（或100%），查看任务监控页面，发现只有少量（1个或几个）reduce子任务未完成。因为其处理的数据量和其他reduce差异过大。</p>
<p>原因：某个reduce的数据输入量远远大于其他reduce数据的输入量</p>
<p>1)、key分布不均匀</p>
<p>2)、业务数据本身的特性</p>
<p>3)、建表时考虑不周</p>
<p>4)、某些SQL语句本身就有数据倾斜</p>
<table>
<thead>
<tr>
<th>关键词</th>
<th><strong>情形</strong></th>
<th><strong>后果</strong></th>
</tr>
</thead>
<tbody><tr>
<td>join</td>
<td>其中一个表较小，但是key集中</td>
<td>分发到某一个或几个Reduce上的数据远高于平均值</td>
</tr>
<tr>
<td>join</td>
<td>大表与大表，但是分桶的判断字段0值或空值过多</td>
<td>这些空值都由一个reduce处理，非常慢</td>
</tr>
<tr>
<td>group by</td>
<td>group by 维度过小，某值的数量过多</td>
<td>处理某值的reduce非常耗时</td>
</tr>
<tr>
<td>count distinct</td>
<td>某特殊值过多</td>
<td>处理此特殊值reduce耗时</td>
</tr>
</tbody></table>
<p>(1)参数调节</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">set hive.map.aggr=true  // map端聚合，降低传给reduce的数据量</span><br><span class="line">set hive.groupby.skewindata=true // 开启hive内置的数倾优化机制</span><br></pre></td></tr></table></figure>

<p>(2) 熟悉数据的分布，优化sql的逻辑，找出数据倾斜的原因。</p>
<p>比如，如果是在groupby中产生了数据倾斜，是否可以将group by的维度变得更细，如果没法变得更细，就可以在原分组key上添加随机数后分组聚合一次，然后对结果去掉随机数后再分组聚合</p>
<p>比如，在join时，有大量为null的join key，则可以将null转成一个随机字符串，也能让null key数据均匀分散到不同的reduce任务</p>
<h2 id="6-合并小文件"><a href="#6-合并小文件" class="headerlink" title="6.合并小文件"></a>6.合并小文件</h2><p>小文件的产生有三个地方，map输入，map输出，reduce输出，小文件过多也会影响hive的分析效率：</p>
<p>设置map输入的小文件合并</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">set mapred.max.split.size=256000000;  </span><br><span class="line">//一个节点上split的至少的大小(这个值决定了多个DataNode上的文件是否需要合并)</span><br><span class="line">set mapred.min.split.size.per.node=100000000;</span><br><span class="line">//一个交换机下split的至少的大小(这个值决定了多个交换机上的文件是否需要合并)  </span><br><span class="line">set mapred.min.split.size.per.rack=100000000;</span><br><span class="line">//执行Map前进行小文件合并</span><br><span class="line">set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;</span><br></pre></td></tr></table></figure>

<p>设置map输出和reduce输出进行合并的相关参数：</p>
<figure class="highlight shell"><table><tr><td class="code"><pre><span class="line">//设置map端输出进行合并，默认为true</span><br><span class="line">set hive.merge.mapfiles = true</span><br><span class="line">//设置reduce端输出进行合并，默认为false</span><br><span class="line">set hive.merge.mapredfiles = true</span><br><span class="line">//设置合并文件的大小</span><br><span class="line">set hive.merge.size.per.task = 256*1000*1000</span><br><span class="line">//当输出文件的平均大小小于该值时，启动一个独立的MapReduce任务进行文件merge。</span><br><span class="line">set hive.merge.smallfiles.avgsize=16000000</span><br></pre></td></tr></table></figure>

<h2 id="7-查看sql的执行计划"><a href="#7-查看sql的执行计划" class="headerlink" title="7.查看sql的执行计划"></a>7.查看sql的执行计划</h2><p>通过explain select …from ，来查看你的sql的执行计划，从来进行分析寻找，看是否有更优化的sql写法</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">explain sql</span><br></pre></td></tr></table></figure>

<p>学会查看sql的执行计划，优化业务逻辑 ，减少job的数据量。 对调优也非常重要</p>
]]></content>
      <categories>
        <category>bigdata</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title>数仓笔记</title>
    <url>/2020/07/11/%E6%95%B0%E4%BB%93%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<h1 id="数仓笔记"><a href="#数仓笔记" class="headerlink" title="数仓笔记"></a>数仓笔记</h1><h2 id="1-维表和宽表的考查（主要考察维表的使用及维度退化手法）"><a href="#1-维表和宽表的考查（主要考察维表的使用及维度退化手法）" class="headerlink" title="1.维表和宽表的考查（主要考察维表的使用及维度退化手法）"></a>1.维表和宽表的考查（主要考察维表的使用及维度退化手法）</h2><p>维表数据一般根据ods层数据加工生成，在设计宽表的时候，可以适当的用一些维度退化手法，将维度退化到事实表中，减少事实表和维表的关联</p>
<h2 id="2-数仓表命名规范"><a href="#2-数仓表命名规范" class="headerlink" title="2.数仓表命名规范"></a>2.数仓表命名规范</h2><p>每个公司都会有点差别</p>
<p>ODS</p>
<p>ods.库名_表名_df/di/da/dz</p>
<p>CDM(dwd/dws)</p>
<p>dwd.主题_内容_df</p>
<h2 id="3-拉链表的使用场景"><a href="#3-拉链表的使用场景" class="headerlink" title="3.拉链表的使用场景"></a>3.拉链表的使用场景</h2><p>1.数据量比较大</p>
<p>2.表中的部分字段会被更新</p>
<p>3.需要查看某一个时间点或者时间段的历史快照信息</p>
<p>​    查看某一个订单在历史某一个时间点的状态<br>​    某一个用户在过去某一段时间，下单次数</p>
<p>4.更新的比例和频率不是很大<br>         如果表中信息变化不是很大，每天都保留一份全量，那么每次全量中会保存很多不变的信息，对存储是极大的浪费</p>
<h2 id="4-一亿条数据查的很慢-怎么查快一点"><a href="#4-一亿条数据查的很慢-怎么查快一点" class="headerlink" title="4.一亿条数据查的很慢,怎么查快一点"></a>4.一亿条数据查的很慢,怎么查快一点</h2><h2 id="5-有什么维表"><a href="#5-有什么维表" class="headerlink" title="5.有什么维表"></a>5.有什么维表</h2><p>时间维表，用户维表，医院维表等</p>
<h2 id="6-数据源都有哪些"><a href="#6-数据源都有哪些" class="headerlink" title="6.数据源都有哪些"></a>6.数据源都有哪些</h2><p>业务库数据源:mysql,oracle,mongo</p>
<p>日志数据：ng日志，埋点日志</p>
<p>爬虫数据</p>
<h2 id="7-你们最大的表是什么表-数据量多少"><a href="#7-你们最大的表是什么表-数据量多少" class="headerlink" title="7.你们最大的表是什么表,数据量多少"></a>7.你们最大的表是什么表,数据量多少</h2><p>ng日志表，三端(app,web,h5)中app端日志量最大，清洗入库后的数据一天大概xxxxW</p>
<h2 id="8-数仓架构体系"><a href="#8-数仓架构体系" class="headerlink" title="8.数仓架构体系"></a>8.数仓架构体系</h2><p><img src="https://lixiangbetter.github.io/2020/07/11/%E6%95%B0%E4%BB%93%E7%AC%94%E8%AE%B0/20200630165353204.png" alt></p>
<h2 id="9-数据平台是怎样的，用到了阿里的那一套吗？"><a href="#9-数据平台是怎样的，用到了阿里的那一套吗？" class="headerlink" title="9.数据平台是怎样的，用到了阿里的那一套吗？"></a>9.数据平台是怎样的，用到了阿里的那一套吗？</h2><p>没用到阿里那一套，数据平台为自研产品</p>
<h2 id="10-你了解的调度系统有那些？，你们公司用的是哪种调度系统"><a href="#10-你了解的调度系统有那些？，你们公司用的是哪种调度系统" class="headerlink" title="10.你了解的调度系统有那些？，你们公司用的是哪种调度系统"></a>10.你了解的调度系统有那些？，你们公司用的是哪种调度系统</h2><p>airflow，azkaban，ooize，我们公司使用的是airflow</p>
<h2 id="11-你们公司数仓底层是怎么抽数据的？"><a href="#11-你们公司数仓底层是怎么抽数据的？" class="headerlink" title="11.你们公司数仓底层是怎么抽数据的？"></a>11.你们公司数仓底层是怎么抽数据的？</h2><p>业务数据用的是datax</p>
<p>日志数据用的是logstash</p>
<h2 id="12-为什么datax抽数据要比sqoop-快？"><a href="#12-为什么datax抽数据要比sqoop-快？" class="headerlink" title="12.为什么datax抽数据要比sqoop 快？"></a>12.为什么datax抽数据要比sqoop 快？</h2><h2 id="13-埋点数据你们是怎样接入的"><a href="#13-埋点数据你们是怎样接入的" class="headerlink" title="13.埋点数据你们是怎样接入的"></a>13.埋点数据你们是怎样接入的</h2><p>logstash–&gt;kafka–&gt;logstash–&gt;hdfs</p>
<h2 id="14-如果你们业务库的表有更新，你们数仓怎么处理的？"><a href="#14-如果你们业务库的表有更新，你们数仓怎么处理的？" class="headerlink" title="14.如果你们业务库的表有更新，你们数仓怎么处理的？"></a>14.如果你们业务库的表有更新，你们数仓怎么处理的？</h2><p>根据表数据量及表特性，选择用全量表，增量表，追加表和拉链表处理</p>
<h2 id="15-能独立搭建数仓吗"><a href="#15-能独立搭建数仓吗" class="headerlink" title="15.能独立搭建数仓吗"></a>15.能独立搭建数仓吗</h2><h2 id="16-搭建过CDH-集群吗"><a href="#16-搭建过CDH-集群吗" class="headerlink" title="16.搭建过CDH 集群吗"></a>16.搭建过CDH 集群吗</h2><h2 id="17-说一下你们公司的大数据平台架构？你有参与吗？"><a href="#17-说一下你们公司的大数据平台架构？你有参与吗？" class="headerlink" title="17.说一下你们公司的大数据平台架构？你有参与吗？"></a>17.说一下你们公司的大数据平台架构？你有参与吗？</h2><h2 id="18-介绍一下你自己的项目和所用的技术"><a href="#18-介绍一下你自己的项目和所用的技术" class="headerlink" title="18.介绍一下你自己的项目和所用的技术"></a>18.介绍一下你自己的项目和所用的技术</h2><h2 id="19-对目前的流和批处理的认识？就是谈谈自己的感受"><a href="#19-对目前的流和批处理的认识？就是谈谈自己的感受" class="headerlink" title="19.对目前的流和批处理的认识？就是谈谈自己的感受"></a>19.对目前的流和批处理的认识？就是谈谈自己的感受</h2><h2 id="20-你了解那些OLAP-引擎，MPP-知道一些吗？clickHouse-了解一些吗？你自己做过测试性能吗？"><a href="#20-你了解那些OLAP-引擎，MPP-知道一些吗？clickHouse-了解一些吗？你自己做过测试性能吗？" class="headerlink" title="20.你了解那些OLAP 引擎，MPP 知道一些吗？clickHouse 了解一些吗？你自己做过测试性能吗？"></a>20.你了解那些OLAP 引擎，MPP 知道一些吗？clickHouse 了解一些吗？你自己做过测试性能吗？</h2><p>Kylin、druid、presto</p>
<p><strong>MPP架构</strong></p>
<ul>
<li>任务并行执行;</li>
<li>数据分布式存储(本地化);</li>
<li>分布式计算;</li>
<li>私有资源;</li>
<li>横向扩展;</li>
<li>Shared Nothing架构。</li>
</ul>
<p><strong>什么是ClickHouse</strong></p>
<p>ClickHouse是一个快速的开源OLAP数据库管理系统, 它是面向列的，并允许使用SQL查询实时生成分析报告。<br>也是一个新的开源列式数据库。</p>
<p><strong>ClickHouse 亮点</strong></p>
<ol>
<li>急速处理</li>
<li>硬件效率高</li>
<li>线性可扩展</li>
<li>容错</li>
<li>功能丰富</li>
<li>高度可靠</li>
</ol>
<h2 id="21-Kylin-有了解吗？介绍一下原理"><a href="#21-Kylin-有了解吗？介绍一下原理" class="headerlink" title="21.Kylin 有了解吗？介绍一下原理"></a>21.Kylin 有了解吗？介绍一下原理</h2><p>Kylin的出现就是为了解决大数据系统中<code>TB</code>级别数据的数据分析需求，它提供<code>Hadoop/Spark</code>之上的<code>SQL</code>查询接口及多维分析(<code>OLAP</code>)能力以支持超大规模数据，它能在亚秒内查询巨大的<code>Hive</code>表。其核心是预计算，计算结果存在<code>HBase</code>中.</p>
<p><strong>基本原理</strong></p>
<p>Kylin的核心思想是预计算。</p>
<p>理论基础是：以空间换时间。即多维分析可能用到的度量进行预计算，将计算好的结果保存成Cube并存储到HBase中，供查询时直接访问。</p>
<p>大致流程：将数据源(比如Hive)中的数据按照指定的维度和指标，由计算引擎Mapreduce离线计算出所有可能的查询结果(即Cube)存储到HBase中。HBase中每行记录的Rowkey由各维度的值拼接而成，度量会保存在column family中。为了减少存储代价，这里会对维度和度量进行编码。查询阶段，利用HBase列存储的特性就可以保证Kylin有良好的快速响应和高并发。</p>
<h2 id="22-datax-源码有改造过吗"><a href="#22-datax-源码有改造过吗" class="headerlink" title="22.datax 源码有改造过吗"></a>22.datax 源码有改造过吗</h2><p>没有</p>
<h2 id="23-你们数仓的APP-层是怎么对外提供服务的？"><a href="#23-你们数仓的APP-层是怎么对外提供服务的？" class="headerlink" title="23.你们数仓的APP 层是怎么对外提供服务的？"></a>23.你们数仓的APP 层是怎么对外提供服务的？</h2><p>1.直接存入mysql业务库，业务方直接读取</p>
<p>2.数据存入mysql，以接口的形式提供数据</p>
<p>3.数据存入kylin，需求方通过jdbc读取数据</p>
<h2 id="24-数据接入进来，你们是怎样规划的，有考虑数据的膨胀问题吗"><a href="#24-数据接入进来，你们是怎样规划的，有考虑数据的膨胀问题吗" class="headerlink" title="24.数据接入进来，你们是怎样规划的，有考虑数据的膨胀问题吗"></a>24.数据接入进来，你们是怎样规划的，有考虑数据的膨胀问题吗</h2><h2 id="25-简述拉链表，流水表以及快照表的含义和特点"><a href="#25-简述拉链表，流水表以及快照表的含义和特点" class="headerlink" title="25.简述拉链表，流水表以及快照表的含义和特点"></a>25.简述拉链表，流水表以及快照表的含义和特点</h2><p>拉链表：</p>
<p>（1）记录一个事物从开始，一直到当前状态的所有变化的信息；<br>（2）拉链表每次上报的都是历史记录的最终状态，是记录在当前时刻的历史总量；<br>（3）当前记录存的是当前时间之前的所有历史记录的最后变化量（总量）；<br>（4）封链时间可以是2999，3000，9999等等比较大的年份；拉链表到期数据要报0；</p>
<p>流水表：对于表的每一个修改都会记录，可以用于反映实际记录的变更<br>　区别于拉链表：　<br>  拉链表通常是对账户信息的历史变动进行处理保留的结果，流水表是每天的交易形成的历史；<br>  流水表用于统计业务相关情况，拉链表用于统计账户及客户的情况</p>
<p> 快照表：<br> 按天分区，每一天的数据都是截止到那一天mysql的全量数据</p>
<h2 id="26-全量表-df-增量表-di-追加表-da-，拉链表-dz-的区别及使用场景"><a href="#26-全量表-df-增量表-di-追加表-da-，拉链表-dz-的区别及使用场景" class="headerlink" title="26.全量表(df),增量表(di),追加表(da)，拉链表(dz)的区别及使用场景"></a>26.全量表(df),增量表(di),追加表(da)，拉链表(dz)的区别及使用场景</h2><p>全量表</p>
<ol>
<li>全量表，有无变化，都要报</li>
<li>每次上报的数据都是所有的数据（变化的 + 没有变化的）</li>
</ol>
<p>增量表：新增数据，增量数据是上次导出之后的新数据。</p>
<ul>
<li>记录每次增加的量，而不是总量；</li>
<li>增量表，只报变化量，无变化不用报</li>
<li>每天一个分区</li>
<li>业务库表中需要有主键及创建时间，修改时间</li>
</ul>
<h2 id="27-你们公司的数仓分层，每一层是怎么处理数据的"><a href="#27-你们公司的数仓分层，每一层是怎么处理数据的" class="headerlink" title="27.你们公司的数仓分层，每一层是怎么处理数据的"></a>27.你们公司的数仓分层，每一层是怎么处理数据的</h2><p>ODS层</p>
<ul>
<li>存放未经过处理的原始数据至数据仓库系统，结构上与源系统保持一致，是数据仓库的数据准备区</li>
</ul>
<p>数据公共层CDM：</p>
<ul>
<li>公共维度层（DIM）：基于维度建模理念思想，建立整个企业的一致性维度。降低数 据计算口径和算法不统一风险。</li>
<li>公共汇总粒度事实层（DWS）：以分析的主题对象作为建模驱动，基于上层的应用和产品的指标需求，构建公共粒度的汇总指标事实表，以宽表化手段物理化模型。</li>
<li>明细粒度事实层（DWD）：以业务过程作为建模驱动，基于每个具体的业务过程特点，构建最细粒度的明细层事实表。</li>
</ul>
<p>ADS层</p>
<p> 数据应用层ADS（Application Data Service）：存放数据产品个性化的统计指标数据。根据CDM与ODS层加工生成。 </p>
<h2 id="28-什么是事实表，什么是维表"><a href="#28-什么是事实表，什么是维表" class="headerlink" title="28.什么是事实表，什么是维表"></a>28.什么是事实表，什么是维表</h2><p>​    事实表（Fact Table）是指存储有事实记录的表，如系统日志、销售记录等；事实表的记录在不断地动态增长，所以它的体积通常远大于其他表。<br>​    维度表（Dimension Table）或维表，有时也称查找表（Lookup Table），是与事实表相对应的一种表；它保存了维度的属性值，可以跟事实表做关联；相当于将事实表上经常重复出现的属性抽取、规范出来用一张表进行管理。常见的维度表有：日期表（存储与日期对应的周、月、季度等的属性）、地点表（包含国家、省／州、城市等属性）等。</p>
<h2 id="29-星型模型和雪花模型"><a href="#29-星型模型和雪花模型" class="headerlink" title="29.星型模型和雪花模型"></a>29.星型模型和雪花模型</h2><ul>
<li>星形模型中有一张事实表，以及零个或多个维度表，事实表与维度表通过主键外键相关联，维度表之间没有关联，当所有维表都直接连接到“ 事实表”上时，整个图解就像星星一样，故将该模型称为星型模型。</li>
<li>当有一个或多个维表没有直接连接到事实表上，而是通过其他维表连接到事实表上时，其图解就像多个雪花连接在一起，故称雪花模型。</li>
</ul>
<h2 id="30-数据建模一般有哪几种方式，你们公司是用哪种方式进行数据建模的"><a href="#30-数据建模一般有哪几种方式，你们公司是用哪种方式进行数据建模的" class="headerlink" title="30.数据建模一般有哪几种方式，你们公司是用哪种方式进行数据建模的"></a>30.数据建模一般有哪几种方式，你们公司是用哪种方式进行数据建模的</h2><p>ER模型</p>
<p>维度模型</p>
<p>Data Vault模型</p>
<p>Anchor模型</p>
<p>我司用的是维度建模</p>
<h2 id="31-有没有实际工作中碰到的sql调优的例子，举例说明"><a href="#31-有没有实际工作中碰到的sql调优的例子，举例说明" class="headerlink" title="31.有没有实际工作中碰到的sql调优的例子，举例说明"></a>31.有没有实际工作中碰到的sql调优的例子，举例说明</h2><p>见博客hive调优</p>
<h2 id="32-你觉得数据仓库应该如何搭建，数据规范和标准如何落地"><a href="#32-你觉得数据仓库应该如何搭建，数据规范和标准如何落地" class="headerlink" title="32.你觉得数据仓库应该如何搭建，数据规范和标准如何落地"></a>32.你觉得数据仓库应该如何搭建，数据规范和标准如何落地</h2><h2 id="33-如何保证你们公司的数据质量"><a href="#33-如何保证你们公司的数据质量" class="headerlink" title="33.如何保证你们公司的数据质量"></a>33.如何保证你们公司的数据质量</h2><p><strong>如何提升数据质量</strong></p>
<p>1.事前定义数据的监控规则</p>
<ul>
<li>提炼规则：梳理对应指标、确定对象（多表、单表、字段）、通过影响程度确定资产等级、质量规则制定</li>
</ul>
<p>2.事中监控和控制数据生产过程</p>
<ul>
<li>质量监控和工作流无缝对接</li>
<li>支持定时调度</li>
<li>强弱规则控制ETL流程</li>
<li>对脏数据进行清洗</li>
</ul>
<p>3.事后分析和问题跟踪</p>
<ul>
<li>邮件短信报警并及时跟踪处理</li>
<li>稽核报告查询</li>
<li>数据质量报告的概览、历史趋势、异常查询、数据质量表覆盖率</li>
<li>异常评估、严重程度、影响范围、问题分类</li>
</ul>
<h2 id="34-对元数据的理解，元数据管理的意义及应用场景有哪些"><a href="#34-对元数据的理解，元数据管理的意义及应用场景有哪些" class="headerlink" title="34.对元数据的理解，元数据管理的意义及应用场景有哪些"></a>34.对元数据的理解，元数据管理的意义及应用场景有哪些</h2><ul>
<li>元数据主要记录数据仓库中模型的定义、各层级间的映射关系、监控数据仓库的数据状态及 ETL 的任务运行状态。在数据仓库系统中，元数据可以帮助数据仓库管理员和开发人员非常方便地找到他们所关心的数据，用于指导其进行数据管理和开发工作，可以极大的提升工作的效率。</li>
<li>元数据有重要的应用价值，是数据管理、数据内容、数据应用的基础，在数据管理方面为集团数据提供在计算、存储、成本、质量、安全、模型等治理领域上的数据支持。例如在计算上可以利用元数据查找超长运行节点，对这些节点进行专项治理，保障基线产出时间。在数据内容方面为集团数据进行数据域、数据主题、业务属性等的提取和分析提供数据素材。例如可以利用元数据构建知识图谱，给数据打标签，清楚地知道现在有哪些数据。在数据应用方面打通产品及应用链路，保障产品数据准确、及时产出。例如打通DP和应用数据，明确数据产等级，更有效地保障产品数据。</li>
<li>数据的真正价值在于数据驱动决策，通过数据指导运营。通过数据驱动的方法，我们能够判断趋势 ，从而展开有效行动，帮助自己发现问题，推动创新或解决方案的产生。这就是数据化运营。同样，对于元数据，可以用于指导数据相关人员进行日常工作，实现数据化“运营”。 比如对于数据使用者，可以通过元数据让其快速找到所需要的数据；对于ETL 工程师，可以通过元数据指导其进行模型设计、任务优化和任务下线等各种日常ETL 工作；对于运维工程师，可以通过元数据指导其进行整个集群的存储、计算和系统优化等运维工作。</li>
</ul>
<h2 id="35-如何判别模型的好坏，模型设计的原则有哪些"><a href="#35-如何判别模型的好坏，模型设计的原则有哪些" class="headerlink" title="35.如何判别模型的好坏，模型设计的原则有哪些"></a>35.如何判别模型的好坏，模型设计的原则有哪些</h2><p>基本原则：</p>
<ul>
<li><p>高内聚和低耦合</p>
<p>一个逻辑或者物理模型由哪些记录和字段组成，应该遵循最基本的软件设计方法论的高内聚和低耦合原则，主要从数据业务特性和访问特性两个角度来考虑：将业务相近或者相关，粒度相同的数据设计为一个逻辑或者物理模型，将高概率同时访问的数据放一起，将低概率同时访问的数据分开存储</p>
</li>
<li><p>核心模型与扩展模型分离<br>建立核心模型与扩展模型体系，核心模型包括的字段支持常用的核心业务，扩展模型包括的字段支持个性化或少量应用的需要，不能让扩展模型的字段过度侵入核心模型，以免破坏核心模型架构简洁性与可维护性</p>
</li>
<li><p>公共处理逻辑下沉及单一<br>越是底层共用的处理逻辑越应该放在数据调度依赖的底层进行封装与实现，不要让共用的处理逻辑暴露给应用层实现，不要让公共逻辑多处同时存在</p>
</li>
<li><p>成本与性能平衡<br>适当的数据冗余可换取查询和刷新性能，不宜过度数据冗余和数据复制</p>
</li>
<li><p>数据可回滚<br>处理逻辑不变，在不同时间多次运行数据结果确定不变</p>
</li>
<li><p>一致性<br>具有相同含义的字段在不同的表中的命名必须相同，必须使用规范定义中的名称</p>
</li>
<li><p>命名清晰,可理解<br>表命名需清晰，一致，表名需易于消费者理解和使用</p>
</li>
</ul>
<h2 id="36-对于数据中台的理解，和数据仓库，数据湖的区别"><a href="#36-对于数据中台的理解，和数据仓库，数据湖的区别" class="headerlink" title="36.对于数据中台的理解，和数据仓库，数据湖的区别"></a>36.对于数据中台的理解，和数据仓库，数据湖的区别</h2><p><strong>数据仓库(Data Warehouse)</strong>是一个面向主题的（Subject Oriented）、集成的（Integrated）、相对稳定的（Non-Volatile）、反映历史变化的（Time Variant）数据集合，用于支持管理决策和信息的全局共享。</p>
<p><strong>数据湖（Data Lake）是一个存储企业的各种各样原始数据的大型仓库，其中的数据可供存取、处理、分析及传输。</strong></p>
<p><strong>数据中台</strong>是一个承接技术，引领业务，构建规范定义的、全域可连接萃取的、智慧的数据处理平台，建设目标是为了高效满足前台数据分析和应用的需求。数据中台距离业务更近，能更快速的相应业务和应用开发的需求，可追溯，更精准。</p>
<p>数据湖、数据仓库更多地是面向不同对象的不同形态的数据资产，而数据中台更多强调的是服务于前台，实现逻辑、标签、算法、模型的复用沉淀。</p>
<p>数据中台像一个“数据工厂”，涵盖了数据湖、数据仓库等存储组件，随着数据中台的发展，未来很有可能数据湖和数据仓库的概念会被弱化。</p>
<h2 id="37-对于数据仓库的理解，数据仓库主要为解决什么问题"><a href="#37-对于数据仓库的理解，数据仓库主要为解决什么问题" class="headerlink" title="37.对于数据仓库的理解，数据仓库主要为解决什么问题"></a>37.对于数据仓库的理解，数据仓库主要为解决什么问题</h2><p><strong>（1）数据仓库能够为业务部门提供准确、及时的的报表。</strong></p>
<p><strong>（2）数据仓库可以赋予管理人员更强大的分析能力。</strong></p>
<p><strong>（3）数据仓库是进行数据挖掘、知识发现的基础。</strong></p>
<h2 id="38-数据仓库模型的理解，数据仓库分层设计的好处是什么"><a href="#38-数据仓库模型的理解，数据仓库分层设计的好处是什么" class="headerlink" title="38.数据仓库模型的理解，数据仓库分层设计的好处是什么"></a>38.数据仓库模型的理解，数据仓库分层设计的好处是什么</h2><ul>
<li>清晰数据结构：每一个数据分层都有它的作用域，这样我们在使用表的时候能更方便地定位和理解。</li>
<li>数据血缘追踪：简单来讲可以这样理解，我们最终给业务呈现的是一张能直接使用的张业务表，但是它的来源有很多，如果有一张来源表出问题了，我们希望能够快速准确地定位到问题，并清楚它的危害范围。</li>
<li>减少重复开发：规范数据分层，开发一些通用的中间层数据，能够减少极大的重复计算。</li>
<li>把复杂问题简单化：将一个复杂的任务分解成多个步骤来完成，每一层只处理单一的步骤，比较简单和容易理解。而且便于维护数据的准确性，当数据出现问题之后，可以不用修复所有的数据，只需要从有问题的步骤开始修复。</li>
</ul>
<h2 id="39-数仓主题划分的标准和依据"><a href="#39-数仓主题划分的标准和依据" class="headerlink" title="39.数仓主题划分的标准和依据"></a>39.数仓主题划分的标准和依据</h2><p><strong>关于主题：</strong></p>
<p>数据仓库中的数据是面向主题组织的，主题是在较高层次上将企业信息系统中的数据进行综合、归类和分析利用的一个抽象概念，每一个主题基本对应一个宏观的分析领域。如财务分析就是一个分析领域，因此这个数据仓库应用的主题就为“财务分析”。</p>
<p><strong>关于主题域：</strong></p>
<p>主题域通常是联系较为紧密的数据主题的集合。可以根据业务的关注点，将这些数据主题划分到不同的主题域(也说是对某个主题进行分析后确定的主题的边界。)</p>
<p><strong>关于主题域的划分：</strong></p>
<p>主题域的确定必须由最终用户和数据仓库的设计人员共同完成的， 而在划分主题域时，大家的切入点不同可能会造成一些争论、重构等的现象，考虑的点可能会是下方的某些方面：</p>
<ol>
<li>按照业务或业务过程划分：比如一个靠销售广告位置的门户网站主题域可能会有广告域，客户域等，而广告域可能就会有广告的库存，销售分析、内部投放分析等主题；</li>
<li>根据需求方划分：比如需求方为财务部，就可以设定对应的财务主题域，而财务主题域里面可能就会有员工工资分析，投资回报比分析等主题；</li>
<li>按照功能或应用划分：比如微信中的朋友圈数据域、群聊数据域等，而朋友圈数据域可能就会有用户动态信息主题、广告主题等；</li>
<li>按照部门划分：比如可能会有运营域、技术域等，运营域中可能会有工资支出分析、活动宣传效果分析等主题；</li>
</ol>
<h2 id="40-缓慢变化维如何处理，几种方式"><a href="#40-缓慢变化维如何处理，几种方式" class="headerlink" title="40.缓慢变化维如何处理，几种方式"></a>40.缓慢变化维如何处理，几种方式</h2><p>缓慢变化维：</p>
<p>数据仓库的重要特点之一是反映历史变化，所以如何处理维度的变化是维度设计的重要工作之一。缓慢变化维的提出是因为在现实世界中，维度的属性并不是静态的，它会随着时间的流逝发生缓慢的变化，与数据增长较为快速的事实表相比，维度变化相对缓慢。</p>
<p>在一些情况下，保留历史数据没有什么分析价值，而在另一些情况下,保留历史数据是非常重要的，在kimball理论中，有三种处理缓慢变化维的方式</p>
<p><strong>1.重写纬度值</strong></p>
<p>采用此种方式，不保留历史数据，始终取最新数据</p>
<p><strong>2.插入新的维度行</strong></p>
<p>插人新的维度行。采用此种方式，保留历史数据，</p>
<p>维度值变化前的事实和过去的维度值关联，维度值变化后的事实和当前的维度值关联</p>
<p><strong>3.添加维度列</strong></p>
<p>采用第二种处理方式不能将变化前后记录的事实归一为变化前的维度或者归一为变化后的维度。比如根据业务需求，需要将4月份的交易金额全部统计到类目2上，采用第二种处理方式无法实现。针对此问题，采用第三种处理方式，保留历史数据，可以使用任何一个属性列</p>
]]></content>
      <categories>
        <category>bigdata</category>
      </categories>
      <tags>
        <tag>warehouse</tag>
      </tags>
  </entry>
  <entry>
    <title>Hbase笔记</title>
    <url>/2020/07/11/Hbase%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<h1 id="Hbase笔记"><a href="#Hbase笔记" class="headerlink" title="Hbase笔记"></a>Hbase笔记</h1><h2 id="1-Hbase调优"><a href="#1-Hbase调优" class="headerlink" title="1.Hbase调优"></a>1.Hbase调优</h2><ul>
<li>高可用<br>在HBase中Hmaster负责监控RegionServer的生命周期，均衡RegionServer的负载，如果Hmaster挂掉了，那么整个HBase集群将陷入不健康的状态，并且此时的工作状态并不会维持太久。所以HBase支持对Hmaster的高可用配置。</li>
<li>预分区<br>每一个region维护着startRow与endRowKey，如果加入的数据符合某个region维护的rowKey范围，则该数据交给这个region维护。那么依照这个原则，我们可以将数据所要投放的分区提前大致的规划好，以提高HBase性能。</li>
<li>优化RowKey设计<br>一条数据的唯一标识就是rowkey，那么这条数据存储于哪个分区，取决于rowkey处于哪个一个预分区的区间内，设计rowkey的主要目的，就是让数据均匀的分布于所有的region中，在一定程度上防止数据倾斜</li>
<li>内存优化<br>HBase操作过程中需要大量的内存开销，毕竟Table是可以缓存在内存中的，一般会分配整个可用内存的70%给HBase的Java堆。但是不建议分配非常大的堆内存，因为GC过程持续太久会导致RegionServer处于长期不可用状态，一般16~48G内存就可以了，如果因为框架占用内存过高导致系统内存不足，框架一样会被系统服务拖死。</li>
</ul>
<h2 id="2-hbase的rowkey怎么创建好？列族怎么创建比较好？"><a href="#2-hbase的rowkey怎么创建好？列族怎么创建比较好？" class="headerlink" title="2.hbase的rowkey怎么创建好？列族怎么创建比较好？"></a>2.hbase的rowkey怎么创建好？列族怎么创建比较好？</h2><p>hbase存储时，数据按照Row key的字典序(byte order)排序存储。设计key时，要充分排序存储这个特性，将经常一起读取的行存储放到一起。(位置相关性)</p>
<p>一个列族在数据底层是一个文件，所以将经常一起查询的列放到一个列族中，列族尽量少，减少文件的寻址时间。</p>
<p>设计原则</p>
<p>1）rowkey 长度原则</p>
<p>2）rowkey 散列原则</p>
<p>3）rowkey 唯一原则</p>
<p>如何设计</p>
<p>1）生成随机数、hash、散列值</p>
<p>2）字符串反转</p>
<p>3)  字符串拼接</p>
<h2 id="3-hbase过滤器实现用途"><a href="#3-hbase过滤器实现用途" class="headerlink" title="3.hbase过滤器实现用途"></a>3.hbase过滤器实现用途</h2><p>增强hbase查询数据的功能</p>
<p>减少服务端返回给客户端的数据量</p>
<h2 id="4-HBase宕机如何处理"><a href="#4-HBase宕机如何处理" class="headerlink" title="4.HBase宕机如何处理"></a>4.HBase宕机如何处理</h2><p>答：宕机分为HMaster宕机和HRegisoner宕机，如果是HRegisoner宕机，HMaster会将其所管理的region重新分布到其他活动的RegionServer上，由于数据和日志都持久在HDFS中，该操作不会导致数据丢失。所以数据的一致性和安全性是有保障的。</p>
<p>如果是HMaster宕机，HMaster没有单点问题，HBase中可以启动多个HMaster，通过Zookeeper的Master Election机制保证总有一个Master运行。即ZooKeeper会保证总会有一个HMaster在对外提供服务。</p>
<h2 id="5-hive跟hbase的区别是？"><a href="#5-hive跟hbase的区别是？" class="headerlink" title="5.hive跟hbase的区别是？"></a>5.hive跟hbase的区别是？</h2><p>共同点：<br>1.hbase与hive都是架构在hadoop之上的。都是用hadoop作为底层存储</p>
<p>区别：<br>2.Hive是建立在Hadoop之上为了减少MapReduce jobs编写工作的批处理系统，HBase是为了支持弥补Hadoop对实时操作的缺陷的项目 。<br>3.想象你在操作RMDB数据库，如果是全表扫描，就用Hive+Hadoop,如果是索引访问，就用HBase+Hadoop 。<br>4.Hive query就是MapReduce jobs可以从5分钟到数小时不止，HBase是非常高效的，肯定比Hive高效的多。<br>5.Hive本身不存储和计算数据，它完全依赖于HDFS和MapReduce，Hive中的表纯逻辑。<br>6.hive借用hadoop的MapReduce来完成一些hive中的命令的执行<br>7.hbase是物理表，不是逻辑表，提供一个超大的内存hash表，搜索引擎通过它来存储索引，方便查询操作。<br>8.hbase是列存储。<br>9.hdfs作为底层存储，hdfs是存放文件的系统，而Hbase负责组织文件。<br>10.hive需要用到hdfs存储文件，需要用到MapReduce计算框架。</p>
<h2 id="6-hbase写流程"><a href="#6-hbase写流程" class="headerlink" title="6.hbase写流程"></a>6.hbase写流程</h2><p><img src="https://lixiangbetter.github.io/2020/07/11/Hbase%E7%AC%94%E8%AE%B0/20190622100115335.png" alt></p>
<ol>
<li>客户端要连接zookeeper, 从zk的/hbase节点找到hbase:meta表所在的regionserver（host:port）;</li>
<li>regionserver扫描hbase:meta中的每个region的起始行健，对比r000001这条数据在那个region的范围内；</li>
<li>从对应的 info:server key中存储了region是有哪个regionserver(host:port)在负责的；</li>
<li>客户端直接请求对应的regionserver；</li>
<li>regionserver接收到客户端发来的请求之后，就会将数据写入到region中</li>
</ol>
<h2 id="7-hbase读流程"><a href="#7-hbase读流程" class="headerlink" title="7.hbase读流程"></a>7.hbase读流程</h2><p><img src="https://lixiangbetter.github.io/2020/07/11/Hbase%E7%AC%94%E8%AE%B0/20200430133829262.png" alt></p>
<ol>
<li>首先Client连接zookeeper, 找到hbase:meta表所在的regionserver;</li>
<li>请求对应的regionserver，扫描hbase:meta表，根据namespace、表名和rowkey在meta表中找到r00001所在的region是由那个regionserver负责的；</li>
<li>找到这个region对应的regionserver</li>
<li>regionserver收到了请求之后，扫描对应的region返回数据到Client</li>
</ol>
<p>(先从MemStore找数据，如果没有，再到BlockCache里面读；BlockCache还没有，再到StoreFile上读(为了读取的效率)；</p>
<p>如果是从StoreFile里面读取的数据，不是直接返回给客户端，而是先写入BlockCache，再返回给客户端。)</p>
<h2 id="8-hbase数据flush过程"><a href="#8-hbase数据flush过程" class="headerlink" title="8.hbase数据flush过程"></a>8.hbase数据flush过程</h2><ol>
<li>当MemStore数据达到阈值（默认是128M，老版本是64M），将数据刷到硬盘，将内存中的数据删除，同时删除HLog中的历史数据；</li>
<li>并将数据存储到HDFS中；</li>
<li>在HLog中做标记点。</li>
</ol>
<h2 id="9-数据合并过程"><a href="#9-数据合并过程" class="headerlink" title="9.数据合并过程"></a>9.数据合并过程</h2><ol>
<li>当数据块达到4块，hmaster将数据块加载到本地，进行合并</li>
<li>当合并的数据超过256M，进行拆分，将拆分后的region分配给不同的hregionserver管理</li>
<li>当hregionser宕机后，将hregionserver上的hlog拆分，然后分配给不同的hregionserver加载，修改.META.</li>
<li>注意：hlog会同步到hdfs</li>
</ol>
<h2 id="10-Hmaster和HRgionserver职责"><a href="#10-Hmaster和HRgionserver职责" class="headerlink" title="10.Hmaster和HRgionserver职责"></a>10.Hmaster和HRgionserver职责</h2><p>Hmaster</p>
<p>1、管理用户对Table的增、删、改、查操作；</p>
<p>2、记录region在哪台Hregion server上</p>
<p>3、在Region Split后，负责新Region的分配；</p>
<p>4、新机器加入时，管理HRegion Server的负载均衡，调整Region分布</p>
<p>5、在HRegion Server宕机后，负责失效HRegion Server 上的Regions迁移。</p>
<p>HRegion server</p>
<ol>
<li>HRegion Server主要负责响应用户I/O请求，向HDFS文件系统中读写数据，是HBASE中最核心的模块。</li>
<li>HRegion Server管理了很多table的分区，也就是region。</li>
</ol>
<h2 id="11-HBase列族和region的关系？"><a href="#11-HBase列族和region的关系？" class="headerlink" title="11.HBase列族和region的关系？"></a>11.HBase列族和region的关系？</h2><p>HBase有多个RegionServer，每个RegionServer里有多个Region，一个Region中存放着若干行的行键以及所对应的数据，一个列族是一个文件夹，如果经常要搜索整个一条数据，列族越少越好，如果只有一部分的数据需要经常被搜索，那么将经常搜索的建立一个列族，其他不常搜索的建立列族检索较快。</p>
<h2 id="12-请简述Hbase的物理模型是什么"><a href="#12-请简述Hbase的物理模型是什么" class="headerlink" title="12.请简述Hbase的物理模型是什么"></a>12.请简述Hbase的物理模型是什么</h2><p>（1）Table在行的方向上分割为多个Region。<br>（2）Table中的所有行都按照row key的字典序排列，根据rowkey存储在不同的Region上。<br>（3）Region是按大小分割的，每个表开始只有一个region，随着数据增多，region不断增大，当增大到一个阈值的时候，region就会等分成两个新的region，之后会有越来越多的region。<br>（4）Region是HBase中分布式存储和负载均衡的最小单元。不同Region分布到不同RegionServer上。移动的时候是移动一个Region，进行不同RegionServer之间的负载均衡。<br>（5）Region虽然是分布式存储的最小单元，但并不是存储的最小单元，存储的最小单元是Cell。Region由一个或者多个Store组成，每个store保存一个columns family列簇。每个store又由一个memStore和0至多个StoreFile组成。memStore存储在内存中，StoreFile存储在HDFS上。memStore是内存中划分的一个区间，StoreFile是底层存储在HDFS上的文件。<br>（6）每个column family存储在HDFS上的一个单独文件中。Key和Version number在每个column family中均有一份。空值不会被保存。</p>
<h2 id="13-请问如果使用Hbase做即席查询，如何设计二级索引"><a href="#13-请问如果使用Hbase做即席查询，如何设计二级索引" class="headerlink" title="13.请问如果使用Hbase做即席查询，如何设计二级索引"></a>13.请问如果使用Hbase做即席查询，如何设计二级索引</h2><p> 1）索引与主数据存放在同一张表的不同Column Family 中</p>
<ul>
<li><p>索引与主数据划分到同一个Region 上，从索引抓取目标主数据减少RPC 次数，减少网络通信压力，把性能损失降低到最小</p>
</li>
<li><p>索引与主数据分配在不同的Column Family 中，实现了索引与主数据的物理分离</p>
<p>2）索引区的Column Family 不包含任何 Qualifier，是一个典型的“Dummy Column Family”</p>
</li>
<li><p>减少读写压力，快速定位</p>
<p>3）RowKey 格式：RegionStartKey-索引名-索引键-索引值</p>
</li>
<li><p>RegionStartKey：索引RowKey 的前缀固定为当前Region的StartKey，一方面，这个值处在Region 的RowKey 区间之内，它确保了索引必定跟随其主数据被划分到同一个Region 里；另一方面，这个值是RowKey 区间内的最小值，这保证了在同一Region 里所有索引会集中排在主数据之前，实现了索引与主数据的逻辑分离。</p>
</li>
<li><p>索引键：由目标记录各对应字段的值组合而成</p>
</li>
<li><p>索引值：记录对应的RowKey</p>
</li>
</ul>
<h2 id="14-如何避免读、写HBaes时访问热点问题？"><a href="#14-如何避免读、写HBaes时访问热点问题？" class="headerlink" title="14.如何避免读、写HBaes时访问热点问题？"></a>14.如何避免读、写HBaes时访问热点问题？</h2><p><strong>rowkey的散列或预分区</strong></p>
<p>预分区一开始就预建好了一部分region，这些region都维护着自己的start-end keys，我们将rowkey做一些处理，比如RowKey%i，写数据能均衡的命中这些预建的region，就能解决上面的那些缺点，大大提供性能。</p>
<p>而将rowkey散列化就是避免rowkey自增，这样也能解决上面所说的缺点。</p>
<ol>
<li>rowkey前面加随机数</li>
<li>哈希</li>
<li>反转</li>
<li>使用反转的时间戳作为rowkey的一部分</li>
</ol>
<h2 id="15-布隆过滤器在HBASE中的应用"><a href="#15-布隆过滤器在HBASE中的应用" class="headerlink" title="15.布隆过滤器在HBASE中的应用"></a>15.布隆过滤器在HBASE中的应用</h2><p>布隆过滤器的作用是，用户可以立即判断一个文件是否包含特定的行键，从而帮我们过滤掉一些不需要扫描的文件。</p>
<h2 id="16-Hbase是用来干嘛的-什么样的数据会放到hbase"><a href="#16-Hbase是用来干嘛的-什么样的数据会放到hbase" class="headerlink" title="16.Hbase是用来干嘛的?什么样的数据会放到hbase"></a>16.Hbase是用来干嘛的?什么样的数据会放到hbase</h2><p>当我们对于数据结构字段不够确定或杂乱无章很难按一个概念去进行抽取的数据适合用使用什么数据库？</p>
<p>最适合使用Hbase存储的数据是非常稀疏的数据（非结构化或者半结构化的数据）。Hbase之所以擅长存储这类数据，是因为Hbase是column-oriented列导向的存储机制，而我们熟知的RDBMS都是row- oriented行导向的存储机制（郁闷的是我看过N本关于关系数据库的介绍从来没有提到过row- oriented行导向存储这个概念）。在列导向的存储机制下对于Null值得存储是不占用任何空间的。比如，如果某个表 UserTable有10列，但在存储时只有一列有数据，那么其他空值的9列是不占用存储空间的（普通的数据库MySql是如何占用存储空间的呢？）。</p>
<p>Hbase适合存储非结构化的稀疏数据的另一原因是他对列集合 column families 处理机制。 </p>
<p>现在Hbase为未来的DBA也带来了这个激动人心的特性，你只需要告诉你的数据存储到Hbase的那个column families 就可以了，不需要指定它的具体类型：char,varchar,int,tinyint,text等等。</p>
]]></content>
      <categories>
        <category>bigdata</category>
      </categories>
      <tags>
        <tag>hbase</tag>
      </tags>
  </entry>
  <entry>
    <title>Kafka笔记</title>
    <url>/2020/07/10/Kafka%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<h1 id="Kafka笔记"><a href="#Kafka笔记" class="headerlink" title="Kafka笔记"></a>Kafka笔记</h1><h2 id="1-Kafka名词解释和工作方式"><a href="#1-Kafka名词解释和工作方式" class="headerlink" title="1.Kafka名词解释和工作方式"></a>1.Kafka名词解释和工作方式</h2><ol>
<li>Producer ：消息生产者，就是向kafka broker发消息的客户端。</li>
<li>Consumer ：消息消费者，向kafka broker取消息的客户端</li>
<li>Topic ：咋们可以理解为一个队列。</li>
<li>Consumer Group （CG）：这是kafka用来实现一个topic消息的广播（发给所有的consumer）和单播（发给任意一个consumer）的手段。一个topic可以有多个CG。topic的消息会复制（不是真的复制，是概念上的）到所有的CG，但每个partion只会把消息发给该CG中的一个consumer。如果需要实现广播，只要每个consumer有一个独立的CG就可以了。要实现单播只要所有的consumer在同一个CG。用CG还可以将consumer进行自由的分组而不需要多次发送消息到不同的topic。</li>
<li>Broker ：一台kafka服务器就是一个broker。一个集群由多个broker组成。一个broker可以容纳多个topic</li>
<li>Partition：为了实现扩展性，一个非常大的topic可以分布到多个broker（即服务器）上，一个topic可以分为多个partition，每个partition是一个有序的队列。partition中的每条消息都会被分配一个有序的id（offset）。kafka只保证按一个partition中的顺序将消息发给consumer，不保证一个topic的整体（多个partition间）的顺序。</li>
<li>Offset：kafka的存储文件都是按照offset.kafka来命名，用offset做名字的好处是方便查找。例如你想找位于2049的位置，只要找到2048.kafka的文件即可。当然the first offset就是00000000000.kafka</li>
</ol>
<h2 id="2-Consumer与topic关系"><a href="#2-Consumer与topic关系" class="headerlink" title="2.Consumer与topic关系"></a>2.Consumer与topic关系</h2><p>本质上kafka只支持Topic；</p>
<p>每个group中可以有多个consumer，每个consumer属于一个consumer group；</p>
<p>通常情况下，一个group中会包含多个consumer，这样不仅可以提高topic中消息的并发消费能力，而且还能提高”故障容错”性，如果group中的某个consumer失效那么其消费的partitions将会有其他consumer自动接管。</p>
<p>对于Topic中的一条特定的消息，只会被订阅此Topic的每个group中的其中一个consumer消费，此消息不会发送给一个group的多个consumer；</p>
<p>那么一个group中所有的consumer将会交错的消费整个Topic，每个group中consumer消息消费互相独立，我们可以认为一个group是一个”订阅”者。</p>
<p>在kafka中,一个partition中的消息只会被group中的一个consumer消费(同一时刻)；</p>
<p>一个Topic中的每个partions，只会被一个”订阅者”中的一个consumer消费，不过一个consumer可以同时消费多个partitions中的消息。</p>
<p>kafka的设计原理决定,对于一个topic，同一个group中不能有多于partitions个数的consumer同时消费，否则将意味着某些consumer将无法得到消息。</p>
<p><strong>kafka只能保证一个partition中的消息被某个consumer消费时是顺序的；事实上，从Topic角度来说,当有多个partitions时,消息仍不是全局有序的。</strong></p>
<h2 id="3-kafka中生产数据的时候，如何保证写入的容错性？"><a href="#3-kafka中生产数据的时候，如何保证写入的容错性？" class="headerlink" title="3.kafka中生产数据的时候，如何保证写入的容错性？"></a>3.kafka中生产数据的时候，如何保证写入的容错性？</h2><p>设置发送数据是否需要服务端的反馈,有三个值0,1,-1</p>
<p>0: producer不会等待broker发送ack</p>
<p>1: 当leader接收到消息之后发送ack</p>
<p>-1: 当所有的follower都同步消息成功后发送ack</p>
<p>request.required.acks=0</p>
<h2 id="4-如何保证kafka消费者消费数据是全局有序的"><a href="#4-如何保证kafka消费者消费数据是全局有序的" class="headerlink" title="4.如何保证kafka消费者消费数据是全局有序的"></a>4.如何保证kafka消费者消费数据是全局有序的</h2><p>伪命题</p>
<p>每个分区内，每条消息都有一个offset，故只能保证分区内有序。</p>
<p>如果要全局有序的，必须保证生产有序，存储有序，消费有序。</p>
<p>由于生产可以做集群，存储可以分片，消费可以设置为一个consumerGroup，要保证全局有序，就需要保证每个环节都有序。</p>
<p>只有一个可能，就是一个生产者，一个partition，一个消费者。这种场景和大数据应用场景相悖。</p>
<h2 id="5-有两个数据源，一个记录的是广告投放给用户的日志，一个记录用户访问日志，另外还有一个固定的用户基础表记录用户基本信息（比如学历，年龄等等）。现在要分析广告投放对与哪类用户更有效，请采用熟悉的技术描述解决思路。另外如果两个数据源都是实时数据源（比如来自kafka），他们数据在时间上相差5分钟，需要哪些调整来解决实时分析问题？"><a href="#5-有两个数据源，一个记录的是广告投放给用户的日志，一个记录用户访问日志，另外还有一个固定的用户基础表记录用户基本信息（比如学历，年龄等等）。现在要分析广告投放对与哪类用户更有效，请采用熟悉的技术描述解决思路。另外如果两个数据源都是实时数据源（比如来自kafka），他们数据在时间上相差5分钟，需要哪些调整来解决实时分析问题？" class="headerlink" title="5.有两个数据源，一个记录的是广告投放给用户的日志，一个记录用户访问日志，另外还有一个固定的用户基础表记录用户基本信息（比如学历，年龄等等）。现在要分析广告投放对与哪类用户更有效，请采用熟悉的技术描述解决思路。另外如果两个数据源都是实时数据源（比如来自kafka），他们数据在时间上相差5分钟，需要哪些调整来解决实时分析问题？"></a>5.有两个数据源，一个记录的是广告投放给用户的日志，一个记录用户访问日志，另外还有一个固定的用户基础表记录用户基本信息（比如学历，年龄等等）。现在要分析广告投放对与哪类用户更有效，请采用熟悉的技术描述解决思路。另外如果两个数据源都是实时数据源（比如来自kafka），他们数据在时间上相差5分钟，需要哪些调整来解决实时分析问题？</h2><h2 id="6-Kafka和Spark-Streaming如何集成"><a href="#6-Kafka和Spark-Streaming如何集成" class="headerlink" title="6.Kafka和Spark Streaming如何集成?"></a>6.Kafka和Spark Streaming如何集成?</h2><p>两种方式：</p>
<p>1.Direct</p>
<p>2.Receiver</p>
<h2 id="7-列举Kafka的优点，简述Kafka为什么可以做到每秒数十万甚至上百万消息的高效分发？"><a href="#7-列举Kafka的优点，简述Kafka为什么可以做到每秒数十万甚至上百万消息的高效分发？" class="headerlink" title="7.列举Kafka的优点，简述Kafka为什么可以做到每秒数十万甚至上百万消息的高效分发？"></a>7.列举Kafka的优点，简述Kafka为什么可以做到每秒数十万甚至上百万消息的高效分发？</h2><p>1、页缓存技术 + 磁盘顺序写</p>
<p>2、零拷贝技术</p>
<h2 id="8-为什么离线分析要用kafka？"><a href="#8-为什么离线分析要用kafka？" class="headerlink" title="8.为什么离线分析要用kafka？"></a>8.为什么离线分析要用kafka？</h2><p>Kafka的作用是解耦，如果直接从日志服务器上采集的话，实时离线都要采集，等于要采集两份数据，而使用了kafka的话，只需要从日志服务器上采集一份数据，然后在kafka中使用不同的两个组读取就行了</p>
<h2 id="9-Kafka怎么进行监控"><a href="#9-Kafka怎么进行监控" class="headerlink" title="9.Kafka怎么进行监控?"></a>9.Kafka怎么进行监控?</h2><p>Kafka Manager</p>
<h2 id="10-Kafka与传统的消息队列服务有什么不同"><a href="#10-Kafka与传统的消息队列服务有什么不同" class="headerlink" title="10.Kafka与传统的消息队列服务有什么不同"></a>10.Kafka与传统的消息队列服务有什么不同</h2><ul>
<li>较长时间持久化</li>
<li>高性能（7种的有点）</li>
</ul>
<h2 id="11-Kafka-api-low-level与high-level有什么区别，使用low-level需要处理哪些细节"><a href="#11-Kafka-api-low-level与high-level有什么区别，使用low-level需要处理哪些细节" class="headerlink" title="11.Kafka api  low-level与high-level有什么区别，使用low-level需要处理哪些细节"></a>11.Kafka api  low-level与high-level有什么区别，使用low-level需要处理哪些细节</h2><p><strong>High Level Consumer API</strong></p>
<ol>
<li>High Level Consumer API围绕着Consumer Group这个逻辑概念展开，它屏蔽了每个Topic的每个Partition的Offset管理（自动读取zookeeper中该Consumer group的last offset ）、Broker失败转移以及增减Partition、Consumer时的负载均衡(当Partition和Consumer增减时，Kafka自动进行负载均衡）</li>
<li>对于多个Partition，多个Consumer，如果consumer比partition多，是浪费，因为kafka的设计是在一个partition上是不允许并发的，所以consumer数不要大于partition数；</li>
<li>如果consumer比partition少，一个consumer会对应于多个partitions，这里主要合理分配consumer数和partition数，否则会导致partition里面的数据被取的不均匀。最好partiton数目是consumer数目的整数倍，所以partition数目很重要，比如取24，就很容易设定consumer数目</li>
<li>如果consumer从多个partition读到数据，不保证数据间的顺序性，kafka只保证在一个partition上数据是有序的，但多个partition，根据你读的顺序会有不同</li>
<li>增减consumer，broker，partition会导致rebalance，所以rebalance后consumer对应的partition会发生变化</li>
</ol>
<p><strong>Low Level Consumer API</strong></p>
<ol>
<li><p>Low Level Consumer API控制灵活性<br>Low Level Consumer API，作为底层的Consumer API，提供了消费Kafka Message更大的控制，如：<br>Read a message multiple times(重复读取）<br>Consume only a subset of the partitions in a topic in a process（跳读）<br>Manage transactions to make sure a message is processed once and only once（Exactly Once原语）</p>
</li>
<li><p>Low Level Consumer API的复杂性<br>软件没有银弹，Low Level Consumer API提供更大灵活控制是以复杂性为代价的：<br>Offset不再透明<br>Broker自动失败转移需要处理<br>增加Consumer、Partition、Broker需要自己做负载均衡</p>
</li>
</ol>
<h2 id="12-Kafka的ISR副本同步队列"><a href="#12-Kafka的ISR副本同步队列" class="headerlink" title="12.Kafka的ISR副本同步队列"></a>12.Kafka的ISR副本同步队列</h2><p>ISR（In-Sync Replicas），副本同步队列。ISR中包括Leader和Follower。如果Leader进程挂掉，会在ISR队列中选择一个服务作为新的Leader。有replica.lag.max.messages（延迟条数）和replica.lag.time.max.ms（延迟时间）两个参数决定一台服务是否可以加入ISR副本队列，在0.10版本移除了replica.lag.max.messages参数，防止服务频繁的进去队列。</p>
<p>任意一个维度超过阈值都会把Follower剔除出ISR，存入OSR（Outof-Sync Replicas）列表，新加入的Follower也会先存放在OSR中。</p>
<h2 id="13-Kafka消息数据积压，Kafka消费能力不足怎么处理？"><a href="#13-Kafka消息数据积压，Kafka消费能力不足怎么处理？" class="headerlink" title="13.Kafka消息数据积压，Kafka消费能力不足怎么处理？"></a>13.Kafka消息数据积压，Kafka消费能力不足怎么处理？</h2><ol>
<li>如果是Kafka消费能力不足，则可以考虑增加Topic的分区数，并且同时提升消费组的消费者数量，消费者数=分区数。（两者缺一不可）</li>
<li>如果是下游的数据处理不及时：提高每批次拉取的数量。批次拉取数据过少（拉取数据/处理时间&lt;生产速度），使处理的数据小于生产的数据，也会造成数据积压。</li>
</ol>
<h2 id="14-Kafka中的ISR、AR又代表什么？"><a href="#14-Kafka中的ISR、AR又代表什么？" class="headerlink" title="14.Kafka中的ISR、AR又代表什么？"></a>14.Kafka中的ISR、AR又代表什么？</h2><p> ISR：in-sync replica set (ISR)，与leader保持同步的follower集合</p>
<p> AR：分区的所有副本</p>
<h2 id="15-Kafka中的HW、LEO等分别代表什么？"><a href="#15-Kafka中的HW、LEO等分别代表什么？" class="headerlink" title="15.Kafka中的HW、LEO等分别代表什么？"></a>15.Kafka中的HW、LEO等分别代表什么？</h2><p>LEO：每个副本的最后那条消息的offset</p>
<p>HW：一个分区中所有副本最小的offset</p>
<h2 id="16-哪些情景会造成消息漏消费？"><a href="#16-哪些情景会造成消息漏消费？" class="headerlink" title="16.哪些情景会造成消息漏消费？"></a>16.哪些情景会造成消息漏消费？</h2><p>两种情况可能出现重复消费</p>
<ol>
<li><p>当ack=-1时，如果在follower同步完成后，broker发送ack之前，leader发生故障，导致没有返回ack给Producer，由于失败重试机制，又会给新选举出来的leader发送数据，造成数据重复。</p>
</li>
<li><p>（手动管理offset时，先消费后提交offset）消费者消费后没有commit offset(程序崩溃/强行kill/消费耗时/自动提交偏移情况下unscrible)</p>
</li>
</ol>
<p>三种情况可能出现漏消费</p>
<ol>
<li><p>（手动管理offset时，先提交offset后消费）先提交offset，后消费，有可能造成数据的漏消费</p>
<p>如果先提交offset，后消费，可能会出现数据漏消费问题。比如，要消费0,1,2,我先提交offset ，此时<strong>consumer_offsets的值为4，但等我提交完offset之后，还没有消费之前，消费者挂掉了，这时等消费者重新活过来后，读取的</strong>consumer_offsets值为4，就会从4开始消费，导致消息0,1,2出现漏消费问题。</p>
</li>
<li><p>当ack=0时，producer不等待broker的ack，这一操作提供了一个最低的延迟，broker一接收到还没有写入磁盘就已经返回，当broker故障时有可能丢失数据；</p>
</li>
<li><p>当ack=1时，producer等待broker的ack，partition的leader落盘成功后返回ack，如果在follower同步成功之前leader故障，而由于已经返回了ack，系统默认新选举的leader已经有了数据，从而不会进行失败重试，那么将会丢失数据</p>
</li>
</ol>
<h2 id="17-当你使用kafka-topics-sh创建了一个topic之后，Kafka背后会执行什么逻辑？"><a href="#17-当你使用kafka-topics-sh创建了一个topic之后，Kafka背后会执行什么逻辑？" class="headerlink" title="17.当你使用kafka-topics.sh创建了一个topic之后，Kafka背后会执行什么逻辑？"></a>17.当你使用kafka-topics.sh创建了一个topic之后，Kafka背后会执行什么逻辑？</h2><ol>
<li>会在zookeeper中的/brokers/topics节点下创建一个新的topic节点，如：/brokers/topics/first</li>
<li>触发Controller的监听程序</li>
<li>kafka Controller 负责topic的创建工作，并更新metadata cache</li>
</ol>
<h2 id="18-topic的分区数可不可以增加？如果可以怎么增加？如果不可以，那又是为什么？"><a href="#18-topic的分区数可不可以增加？如果可以怎么增加？如果不可以，那又是为什么？" class="headerlink" title="18.topic的分区数可不可以增加？如果可以怎么增加？如果不可以，那又是为什么？"></a>18.topic的分区数可不可以增加？如果可以怎么增加？如果不可以，那又是为什么？</h2><p>可以增加</p>
<p>bin/kafka-topics.sh –zookeeper localhost:2181/kafka –alter –topic topic-config –partitions 3</p>
<h2 id="19-topic的分区数可不可以减少？如果可以怎么减少？如果不可以，那又是为什么？"><a href="#19-topic的分区数可不可以减少？如果可以怎么减少？如果不可以，那又是为什么？" class="headerlink" title="19.topic的分区数可不可以减少？如果可以怎么减少？如果不可以，那又是为什么？"></a>19.topic的分区数可不可以减少？如果可以怎么减少？如果不可以，那又是为什么？</h2><p>不可以减少，被删除的分区数据难以处理。</p>
<h2 id="20-Kafka有内部的topic吗？如果有是什么？有什么所用？"><a href="#20-Kafka有内部的topic吗？如果有是什么？有什么所用？" class="headerlink" title="20.Kafka有内部的topic吗？如果有是什么？有什么所用？"></a>20.Kafka有内部的topic吗？如果有是什么？有什么所用？</h2><p>__consumer_offsets,保存消费者offset</p>
<h2 id="21-聊一聊Kafka-Controller的作用？"><a href="#21-聊一聊Kafka-Controller的作用？" class="headerlink" title="21.聊一聊Kafka Controller的作用？"></a>21.聊一聊Kafka Controller的作用？</h2><p>负责管理集群broker的上下线，所有topic的分区副本分配和leader选举等工作。</p>
<h2 id="22-失效副本是指什么？有那些应对措施？"><a href="#22-失效副本是指什么？有那些应对措施？" class="headerlink" title="22.失效副本是指什么？有那些应对措施？"></a>22.失效副本是指什么？有那些应对措施？</h2><p>不能及时与leader同步，暂时踢出ISR，等其追上leader之后再重新加入</p>
]]></content>
      <categories>
        <category>bigdata</category>
      </categories>
      <tags>
        <tag>kafka</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark笔记</title>
    <url>/2020/07/06/Spark%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<h1 id="Spark笔记"><a href="#Spark笔记" class="headerlink" title="Spark笔记"></a>Spark笔记</h1><h2 id="1-rdd的属性"><a href="#1-rdd的属性" class="headerlink" title="1.rdd的属性"></a>1.rdd的属性</h2><ul>
<li>一组分片（Partition），即数据集的基本组成单位。对于RDD来说，每个分片都会被一个计算任务处理，并决定并行计算的粒度。用户可以在创建RDD时指定RDD的分片个数，如果没有指定，那么就会采用默认值。默认值就是程序所分配到的CPU Core的数目。</li>
<li>一个计算每个分区的函数。Spark中RDD的计算是以分片为单位的，每个RDD都会实现compute函数以达到这个目的。compute函数会对迭代器进行复合，不需要保存每次计算的结果。</li>
<li>RDD之间的依赖关系。RDD的每次转换都会生成一个新的RDD，所以RDD之间就会形成类似于流水线一样的前后依赖关系。在部分分区数据丢失时，Spark可以通过这个依赖关系重新计算丢失的分区数据，而不是对RDD的所有分区进行重新计算。</li>
<li>一个Partitioner，即RDD的分片函数。当前Spark中实现了两种类型的分片函数，一个是基于哈希的HashPartitioner，另外一个是基于范围的RangePartitioner。只有对于key-value的RDD，才会有Partitioner，非key-value的RDD的Parititioner的值是None。Partitioner函数不但决定了RDD本身的分片数量，也决定了parent RDD Shuffle输出时的分片数量。</li>
<li>一个列表，存储存取每个Partition的优先位置（preferred location）。对于一个HDFS文件来说，这个列表保存的就是每个Partition所在的块的位置。按照“移动数据不如移动计算”的理念，Spark在进行任务调度的时候，会尽可能地将计算任务分配到其所要处理数据块的存储位置。</li>
</ul>
<h2 id="2-算子分为哪几类-RDD支持哪几种类型的操作"><a href="#2-算子分为哪几类-RDD支持哪几种类型的操作" class="headerlink" title="2.算子分为哪几类(RDD支持哪几种类型的操作)"></a>2.算子分为哪几类(RDD支持哪几种类型的操作)</h2><p>转换（Transformation）  现有的RDD通过转换生成一个新的RDD。lazy模式，延迟执行。</p>
<p>转换函数包括：map，filter，flatMap，groupByKey，reduceByKey，aggregateByKey，union,join, coalesce 等等。</p>
<p>动作（Action）  在RDD上运行计算，并返回结果给驱动程序(Driver)或写入文件系统。</p>
<p>动作操作包括：reduce，collect，count，first，take，countByKey以及foreach等等。</p>
<p>collect  该方法把数据收集到driver端   Array数组类型</p>
<p>所有的transformation只有遇到action才能被执行。</p>
<p>当触发执行action之后，数据类型不再是rdd了，数据就会存储到指定文件系统中，或者直接打印结果或者收集起来。</p>
<h2 id="3-创建rdd的几种方式"><a href="#3-创建rdd的几种方式" class="headerlink" title="3.创建rdd的几种方式"></a>3.创建rdd的几种方式</h2><ol>
<li><p>集合并行化创建(有数据)</p>
<p>val arr = Array(1,2,3,4,5)</p>
<p>val rdd = sc.parallelize(arr)</p>
<p>val rdd =sc.makeRDD(arr)</p>
</li>
<li><p>读取外部文件系统，如hdfs，或者读取本地文件(最常用的方式)(没数据)</p>
<p>val rdd2 = sc.textFile(“hdfs://hdp-01:9000/words.txt”)</p>
<p>// 读取本地文件</p>
<p>val rdd2 = sc.textFile(“file:///root/words.txt”)</p>
</li>
<li><p>从父RDD转换成新的子RDD</p>
<p>调用Transformation类的方法，RDD</p>
</li>
</ol>
<h2 id="4-spark运行流程"><a href="#4-spark运行流程" class="headerlink" title="4.spark运行流程"></a>4.spark运行流程</h2><p><img src="https://lixiangbetter.github.io/2020/07/06/Spark%E7%AC%94%E8%AE%B0/20200409083030286.png" alt></p>
<p>Worker的功能： 定时和master通信；调度并管理自身的executor</p>
<p>executor： 由Worker启动的，程序最终在executor中运行，（程序运行的一个容器）</p>
<p>spark-submit命令执行时，会根据master地址去向 Master发送请求。</p>
<p>Master接收到Dirver端的任务请求之后，根据任务的请求资源进行调度，（打散的策略），尽可能的把任务资源平均分配，然后向Worker发送指令</p>
<p>Worker收到Master的指令之后，就根据相应的资源，启动executor（cores,memory）</p>
<p>executor会向dirver端建立请求，通知driver，任务已经可以运行了</p>
<p>driver运行任务的时候，会把任务发送到executor中去运行。</p>
<h2 id="5-Spark中coalesce与repartition的区别"><a href="#5-Spark中coalesce与repartition的区别" class="headerlink" title="5.Spark中coalesce与repartition的区别"></a>5.Spark中coalesce与repartition的区别</h2><p>1）关系：</p>
<p>两者都是用来改变 RDD 的 partition 数量的，repartition 底层调用的就是 coalesce 方法：coalesce(numPartitions, shuffle = true)</p>
<p>2）区别：</p>
<p>repartition 一定会发生 shuffle，coalesce 根据传入的参数来判断是否发生 shuffle</p>
<p>一般情况下增大 rdd 的 partition 数量使用 repartition，减少 partition 数量时使用coalesce</p>
<h2 id="6-sortBy-和-sortByKey的区别"><a href="#6-sortBy-和-sortByKey的区别" class="headerlink" title="6.sortBy 和 sortByKey的区别"></a>6.sortBy 和 sortByKey的区别</h2><p>sortBy既可以作用于RDD[K] ，还可以作用于RDD[(k,v)]</p>
<p>sortByKey  只能作用于 RDD[K,V] 类型上。</p>
<h2 id="7-map和mapPartitions的区别"><a href="#7-map和mapPartitions的区别" class="headerlink" title="7.map和mapPartitions的区别"></a>7.map和mapPartitions的区别</h2><p><img src="https://lixiangbetter.github.io/2020/07/06/Spark%E7%AC%94%E8%AE%B0/20200409083305358.png" alt></p>
<h2 id="8-数据存入Redis-优先使用map-mapPartitions-foreach-foreachPartions哪个"><a href="#8-数据存入Redis-优先使用map-mapPartitions-foreach-foreachPartions哪个" class="headerlink" title="8.数据存入Redis  优先使用map mapPartitions  foreach  foreachPartions哪个"></a>8.数据存入Redis  优先使用map mapPartitions  foreach  foreachPartions哪个</h2><p>使用 foreachPartition</p>
<ul>
<li><p>1,map mapPartition   是转换类的算子， 有返回值</p>
</li>
<li><p>2, 写mysql,redis 的连接</p>
<p>foreach  * 100万         100万次的连接</p>
<p>foreachPartions * 200 个分区     200次连接  一个分区中的数据，共用一个连接</p>
</li>
</ul>
<p><strong>foreachParititon 每次迭代一个分区，foreach每次迭代一个元素。</strong></p>
<p>该方法没有返回值，或者Unit</p>
<p>主要作用于，没有返回值类型的操作（打印结果，写入到mysql数据库中）</p>
<p>在写入到redis,mysql的时候，优先使用foreachPartititon</p>
<h2 id="9-reduceByKey和groupBykey的区别"><a href="#9-reduceByKey和groupBykey的区别" class="headerlink" title="9.reduceByKey和groupBykey的区别"></a>9.reduceByKey和groupBykey的区别</h2><p><img src="https://lixiangbetter.github.io/2020/07/06/Spark%E7%AC%94%E8%AE%B0/20200409083531763.png" alt></p>
<p>reduceByKey会传一个聚合函数， 相当于  groupByKey + mapValues</p>
<p>reduceByKey 会有一个分区内聚合，而groupByKey没有  最核心的区别  </p>
<p>结论： reduceByKey有分区内聚合，更高效，优先选择使用reduceByKey。</p>
<h2 id="10-cache和checkPoint的比较"><a href="#10-cache和checkPoint的比较" class="headerlink" title="10.cache和checkPoint的比较"></a>10.cache和checkPoint的比较</h2><p>都是做 RDD 持久化的</p>
<p>1.缓存，是在触发action之后，把数据写入到内存或者磁盘中。不会截断血缘关系</p>
<p>（设置缓存级别为memory_only： 内存不足，只会部分缓存或者没有缓存，缓存会丢失,memory_and_disk :内存不足，会使用磁盘）</p>
<p>2.checkpoint 也是在触发action之后，执行任务。单独再启动一个job，负责写入数据到hdfs中。（把rdd中的数据，以二进制文本的方式写入到hdfs中，有几个分区，就有几个二进制文件）</p>
<p>3.某一个RDD被checkpoint之后，他的父依赖关系会被删除，血缘关系被截断，该RDD转换成了CheckPointRDD，以后再对该rdd的所有操作，都是从hdfs中的checkpoint的具体目录来读取数据。 缓存之后，rdd的依赖关系还是存在的。</p>
<h2 id="11-spark-streaming流式统计单词数量代码"><a href="#11-spark-streaming流式统计单词数量代码" class="headerlink" title="11.spark streaming流式统计单词数量代码"></a>11.spark streaming流式统计单词数量代码</h2><figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">WordCountAll</span> </span>&#123;</span><br><span class="line">  <span class="comment">// newValues当前批次的出现的单词次数， runningCount表示之前运行的单词出现的结果</span></span><br><span class="line"> <span class="comment">/* def updateFunction(newValues: Seq[Int], runningCount: Option[Int]): Option[Int] = &#123;</span></span><br><span class="line"><span class="comment">    val newCount =  newValues.sum + runningCount.getOrElse(0)// 将历史前几个批次的值和当前批次的值进行累加返回当前批次最终的结果</span></span><br><span class="line"><span class="comment">    Some(newCount)</span></span><br><span class="line"><span class="comment">  &#125;*/</span></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * String : 单词 hello</span></span><br><span class="line"><span class="comment">    * Seq[Int] ：单词在当前批次出现的次数</span></span><br><span class="line"><span class="comment">    * Option[Int] ： 历史结果</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">  <span class="keyword">val</span> updateFunc = (iter: <span class="type">Iterator</span>[(<span class="type">String</span>, <span class="type">Seq</span>[<span class="type">Int</span>], <span class="type">Option</span>[<span class="type">Int</span>])]) =&gt; &#123;</span><br><span class="line">    <span class="comment">//iter.flatMap(it=&gt;Some(it._2.sum + it._3.getOrElse(0)).map(x=&gt;(it._1,x)))</span></span><br><span class="line">    iter.flatMap&#123;<span class="keyword">case</span>(x,y,z)=&gt;<span class="type">Some</span>(y.sum + z.getOrElse(<span class="number">0</span>)).map(m=&gt;(x, m))&#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// 屏蔽日志</span></span><br><span class="line">  <span class="type">Logger</span>.getLogger(<span class="string">"org.apache"</span>).setLevel(<span class="type">Level</span>.<span class="type">ERROR</span>)</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line">    <span class="comment">// 必须要开启2个以上的线程，一个线程用来接收数据，另外一个线程用来计算</span></span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[2]"</span>).setAppName(<span class="string">"NetworkWordCount"</span>)</span><br><span class="line">      <span class="comment">// 设置sparkjob计算时所采用的序列化方式</span></span><br><span class="line">      .set(<span class="string">"spark.serializer"</span>, <span class="string">"org.apache.spark.serializer.KryoSerializer"</span>)</span><br><span class="line">      .set(<span class="string">"spark.rdd.compress"</span>, <span class="string">"true"</span>) <span class="comment">// 节约大量的内存内容</span></span><br><span class="line">    <span class="comment">// 如果你的程序出现垃圾回收时间过程，可以设置一下java的垃圾回收参数</span></span><br><span class="line">    <span class="comment">// 同时也会创建sparkContext对象</span></span><br><span class="line">    <span class="comment">// 批次时间 &gt;= 批次处理的总时间 (批次数据量，集群的计算节点数量和配置)</span></span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(conf, <span class="type">Seconds</span>(<span class="number">5</span>))</span><br><span class="line"> </span><br><span class="line">    <span class="comment">//做checkpoint 写入共享存储中</span></span><br><span class="line">    ssc.checkpoint(<span class="string">"c://aaa"</span>)</span><br><span class="line"> </span><br><span class="line">    <span class="comment">// 创建一个将要连接到 hostname:port 的 DStream，如 localhost:9999</span></span><br><span class="line">    <span class="keyword">val</span> lines: <span class="type">ReceiverInputDStream</span>[<span class="type">String</span>] = ssc.socketTextStream(<span class="string">"192.168.175.101"</span>, <span class="number">44444</span>)</span><br><span class="line">    <span class="comment">//updateStateByKey结果可以累加但是需要传入一个自定义的累加函数：updateFunc</span></span><br><span class="line">    <span class="keyword">val</span> results = lines.flatMap(_.split(<span class="string">" "</span>)).map((_,<span class="number">1</span>)).updateStateByKey(updateFunc, <span class="keyword">new</span> <span class="type">HashPartitioner</span>(ssc.sparkContext.defaultParallelism), <span class="literal">true</span>)</span><br><span class="line">    <span class="comment">//打印结果到控制台</span></span><br><span class="line">    results.print()</span><br><span class="line">    <span class="comment">//开始计算</span></span><br><span class="line">    ssc.start()</span><br><span class="line">    <span class="comment">//等待停止</span></span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SparkWordCountApp</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local"</span>).setAppName(<span class="string">"SparkWordCountApp"</span>)</span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> rdd = sc.textFile(<span class="string">"file:////Users/lx/IdeaProjects/sparksql-train/data/input.txt"</span>)</span><br><span class="line">    <span class="comment">//    rdd.collect().foreach(println)</span></span><br><span class="line">    rdd.flatMap(_.split(<span class="string">","</span>))</span><br><span class="line">      .map(word =&gt; (word, <span class="number">1</span>))</span><br><span class="line">        .reduceByKey(_+_).map(x =&gt; (x._2,x._1)).sortByKey(<span class="literal">false</span>)</span><br><span class="line">      .map(x =&gt; (x._1,x._2))</span><br><span class="line">      .collect().foreach(println)</span><br><span class="line"></span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="12-简述map和flatMap的区别和应用场景"><a href="#12-简述map和flatMap的区别和应用场景" class="headerlink" title="12.简述map和flatMap的区别和应用场景"></a>12.简述map和flatMap的区别和应用场景</h2><p>map是对每一个元素进行操作，flatmap是对每一个元素操作后并压平</p>
<p>flatmap对每个元素进行操作后，放入一个对象返回。</p>
<h2 id="13-计算曝光数和点击数"><a href="#13-计算曝光数和点击数" class="headerlink" title="13.计算曝光数和点击数"></a>13.计算曝光数和点击数</h2><h2 id="14-分别列出几个常用的transformation和action算子"><a href="#14-分别列出几个常用的transformation和action算子" class="headerlink" title="14.分别列出几个常用的transformation和action算子"></a>14.分别列出几个常用的transformation和action算子</h2><ul>
<li>转换算子：map,flatmap,filter,reduceByKey,groupByKey,groupBy</li>
<li>行动算子：foreach，foreachpartition,collect,collectAsMap,take,top,first,count,countByKey</li>
</ul>
<h2 id="15-按照需求使用spark编写以下程序，要求使用scala语言"><a href="#15-按照需求使用spark编写以下程序，要求使用scala语言" class="headerlink" title="15.按照需求使用spark编写以下程序，要求使用scala语言"></a>15.按照需求使用spark编写以下程序，要求使用scala语言</h2><p>当前文件a.txt的格式，请统计每个单词出现的次数</p>
<p>A,b,c</p>
<p>B,b,f,e</p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">WordCount</span> </span>&#123;</span><br><span class="line"> </span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">      .setAppName(<span class="keyword">this</span>.getClass.getSimpleName)</span><br><span class="line">      .setMaster(<span class="string">"local[*]"</span>)</span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">var</span> sData: <span class="type">RDD</span>[<span class="type">String</span>] = sc.textFile(<span class="string">"a.txt"</span>)</span><br><span class="line">    <span class="keyword">val</span> sortData: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = sData.flatMap(_.split(<span class="string">","</span>)).map((_,<span class="number">1</span>)).reduceByKey(_+_)</span><br><span class="line">    sortData.foreach(print)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>

<h2 id="16-spark应用程序的执行命令是什么？"><a href="#16-spark应用程序的执行命令是什么？" class="headerlink" title="16.spark应用程序的执行命令是什么？"></a>16.spark应用程序的执行命令是什么？</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">/usr/local/spark-current2.3/bin/spark-submit \</span><br><span class="line"></span><br><span class="line">--class com.wedoctor.Application \</span><br><span class="line"></span><br><span class="line">--master yarn \</span><br><span class="line"></span><br><span class="line">--deploy-mode client \</span><br><span class="line"></span><br><span class="line">--driver-memory 1g \</span><br><span class="line"></span><br><span class="line">--executor-memory 2g \</span><br><span class="line"></span><br><span class="line">--queue root.wedw \</span><br><span class="line"></span><br><span class="line">--num-executors 200 \</span><br><span class="line"></span><br><span class="line">--jars /home/pgxl/liuzc/config-1.3.0.jar,/home/pgxl/liuzc/hadoop-lzo-0.4.20.jar,/home/pgxl/liuzc/elasticsearch-hadoop-hive-2.3.4.jar \</span><br><span class="line"></span><br><span class="line">/home/pgxl/liuzc/sen.jar</span><br></pre></td></tr></table></figure>

<h2 id="17-Spark应用执行有哪些模式，其中哪几种是集群模式"><a href="#17-Spark应用执行有哪些模式，其中哪几种是集群模式" class="headerlink" title="17.Spark应用执行有哪些模式，其中哪几种是集群模式"></a>17.Spark应用执行有哪些模式，其中哪几种是集群模式</h2><ul>
<li>本地local模式</li>
<li>standalone模式</li>
<li>spark on yarn模式</li>
<li>spark on mesos模式</li>
</ul>
<p>其中，standalone模式，spark on yarn模式，spark on mesos模式是集群模式</p>
<h2 id="18-请说明spark中广播变量的用途"><a href="#18-请说明spark中广播变量的用途" class="headerlink" title="18.请说明spark中广播变量的用途"></a>18.请说明spark中广播变量的用途</h2><p>使用广播变量，每个 Executor 的内存中，只驻留一份变量副本，而不是对 每个 task 都传输一次大变量，省了很多的网络传输， 对性能提升具有很大帮助， 而且会通过高效的广播算法来减少传输代价。</p>
<h2 id="19-以下代码会报错吗？如果会怎么解决-val-arr-new-ArrayList-String-arr-foreach-println"><a href="#19-以下代码会报错吗？如果会怎么解决-val-arr-new-ArrayList-String-arr-foreach-println" class="headerlink" title="19.以下代码会报错吗？如果会怎么解决 val arr = new ArrayList[String]; arr.foreach(println)"></a>19.以下代码会报错吗？如果会怎么解决 val arr = new ArrayList[String]; arr.foreach(println)</h2><p>val arr = new ArrayList[String]; 这里会报错，需要改成 val arr: Array[String] = new Array[String](10)</p>
<p>arr.foreach(println)打印不会报空指针</p>
<h2 id="20-写出你用过的spark中的算子，其中哪些会产生shuffle过程"><a href="#20-写出你用过的spark中的算子，其中哪些会产生shuffle过程" class="headerlink" title="20.写出你用过的spark中的算子，其中哪些会产生shuffle过程"></a>20.写出你用过的spark中的算子，其中哪些会产生shuffle过程</h2><p>reduceBykey：</p>
<p>groupByKey：</p>
<p>…ByKey:</p>
<h2 id="21-Spark中rdd与partition的区别"><a href="#21-Spark中rdd与partition的区别" class="headerlink" title="21.Spark中rdd与partition的区别"></a>21.Spark中rdd与partition的区别</h2><p>RDD是什么?弹性分布式数据集。<br>弹性:并不是指他可以动态扩展，而是血统容错机制。<br>分布式:顾名思义，RDD会在多个节点上存储，就和hdfs的分布式道理是一样的。hdfs文件被切分为多个block存储在各个节点上，而RDD是被切分为多个partition。不同的partition可能在不同的节点上。在spark读取hdfs的场景下，spark把hdfs的block读到内存就会抽象为spark的partition。至于后续遇到shuffle的操作，RDD的partition可以根据Hash再次进行划分(一般pairRDD是使用key做Hash再取余来划分partition）。</p>
<h2 id="22-请写出创建Dateset的几种方式"><a href="#22-请写出创建Dateset的几种方式" class="headerlink" title="22.请写出创建Dateset的几种方式"></a>22.请写出创建Dateset的几种方式</h2><ul>
<li><h5 id="由DataFrame-转化成为-Dataset"><a href="#由DataFrame-转化成为-Dataset" class="headerlink" title="由DataFrame 转化成为 Dataset"></a>由DataFrame 转化成为 Dataset</h5></li>
<li><h5 id="通过-SparkSession-createDataset-直接创建"><a href="#通过-SparkSession-createDataset-直接创建" class="headerlink" title="通过 SparkSession.createDataset() 直接创建"></a>通过 SparkSession.createDataset() 直接创建</h5></li>
<li><h5 id="通过toDS-方法意识转换"><a href="#通过toDS-方法意识转换" class="headerlink" title="通过toDS 方法意识转换"></a>通过toDS 方法意识转换</h5></li>
</ul>
<h2 id="23-描述一下RDD，DataFrame，DataSet的区别？"><a href="#23-描述一下RDD，DataFrame，DataSet的区别？" class="headerlink" title="23.描述一下RDD，DataFrame，DataSet的区别？"></a>23.描述一下RDD，DataFrame，DataSet的区别？</h2><p>1）RDD</p>
<p>优点:</p>
<p>编译时类型安全</p>
<p>编译时就能检查出类型错误</p>
<p>面向对象的编程风格</p>
<p>直接通过类名点的方式来操作数据</p>
<p>缺点:</p>
<p>序列化和反序列化的性能开销</p>
<p>无论是集群间的通信, 还是 IO 操作都需要对对象的结构和数据进行序列化和反序列化。</p>
<p>GC 的性能开销，频繁的创建和销毁对象, 势必会增加 GC</p>
<p>2）DataFrame</p>
<p>DataFrame 引入了 schema 和 off-heap</p>
<p>schema : RDD 每一行的数据, 结构都是一样的，这个结构就存储在 schema 中。 Spark 通过 schema 就能够读懂数据, 因此在通信和 IO 时就只需要序列化和反序列化数据, 而结构的部分就可以省略了。</p>
<p>3）DataSet</p>
<p>DataSet 结合了 RDD 和 DataFrame 的优点，并带来的一个新的概念 Encoder。</p>
<p>当序列化数据时，Encoder 产生字节码与 off-heap 进行交互，能够达到按需访问数据的效果，而不用反序列化整个对象。Spark 还没有提供自定义 Encoder 的 API，但是未来会加入。<br>三者之间的转换：</p>
<p><img src="https://lixiangbetter.github.io/2020/07/06/Spark%E7%AC%94%E8%AE%B0/20200409085324709.png" alt></p>
<h2 id="24-描述一下Spark中stage是如何划分的？描述一下shuffle的概念"><a href="#24-描述一下Spark中stage是如何划分的？描述一下shuffle的概念" class="headerlink" title="24.描述一下Spark中stage是如何划分的？描述一下shuffle的概念"></a>24.描述一下Spark中stage是如何划分的？描述一下shuffle的概念</h2><ul>
<li><strong>Stage概念</strong></li>
</ul>
<p>Spark任务会根据<strong>RDD之间的依赖关系，形成一个DAG有向无环图</strong>，DAG会提交给DAGScheduler，DAGScheduler会把DAG划分相互依赖的多个stage，划分stage的依据就是RDD之间的宽窄依赖。<strong>遇到宽依赖就划分stage</strong>,每个stage包含一个或多个task任务。然后将这些task以taskSet的形式提交给<strong>TaskScheduler运行</strong>。   <strong>stage是由一组并行的task组成。</strong></p>
<ul>
<li><p><strong>stage切割规则</strong></p>
<p>切割规则：<strong>从后往前</strong>，<strong>遇到宽依赖就切割stage。</strong></p>
</li>
</ul>
<p><img src="https://lixiangbetter.github.io/2020/07/06/Spark%E7%AC%94%E8%AE%B0/1250469-20180205010816029-1089510889.png" alt="img"></p>
<ul>
<li><p><strong>stage计算模式</strong></p>
<p>pipeline管道计算模式,pipeline只是一种计算思想，模式。</p>
</li>
</ul>
<p>备注：图中几个理解点：</p>
<p>  1、Spark的pipeLine的计算模式，相当于执行了一个高阶函数f3(f2(f1(textFile))) !+!+!=3 也就是来一条数据然后计算一条数据，把所有的逻辑走完，然后落地，准确的说一个task处理遗传分区的数据 因为跨过了不同的逻辑的分区。而MapReduce是 1+1=2,2+1=3的模式，也就是计算完落地，然后在计算，然后再落地到磁盘或内存，最后数据是落在计算节点上，按reduce的hash分区落地。所以这也是比Mapreduce快的原因，完全基于内存计算。</p>
<p>  2、管道中的数据何时落地：<strong>shuffle write的时候，对RDD进行持久化的时候。</strong></p>
<ol start="3">
<li><p><strong>Stage的task并行度是由stage的最后一个RDD的分区数来决定的 。一般来说，一个partiotion对应一个task,但最后reduce的时候可以手动改变reduce的个数，也就是分区数，即改变了并行度。例如reduceByKey(XXX,3),GroupByKey(4)，union由的分区数由前面的相加。</strong></p>
</li>
<li><p>、<strong>如何提高stage的并行度</strong>：reduceBykey(xxx,numpartiotion),join(xxx,numpartiotion)</p>
</li>
</ol>
<h2 id="shuffle-和-stage"><a href="#shuffle-和-stage" class="headerlink" title="shuffle 和 stage"></a>shuffle 和 stage</h2><p>Shuffle:当Map的输出结果要被Reduce使用时，输出结果需要按key哈希，并且分发到每一个Reducer上去，这个过程就是shuffle。</p>
<p>spark中的shuffle:宽依赖指子 RDD 的各个分片会依赖于父RDD 的多个分片,所以会造成父 RDD 的各个分片在集群中重新分片</p>
<h2 id="25-Spark-在yarn上运行需要做哪些关键的配置工作？如何kill-个Spark在yarn运行中Application"><a href="#25-Spark-在yarn上运行需要做哪些关键的配置工作？如何kill-个Spark在yarn运行中Application" class="headerlink" title="25.Spark 在yarn上运行需要做哪些关键的配置工作？如何kill -个Spark在yarn运行中Application"></a>25.Spark 在yarn上运行需要做哪些关键的配置工作？如何kill -个Spark在yarn运行中Application</h2><p><strong>修改 spark-env.sh文件，配置hadoop的配置文件，或者yarn的配置文件即可（两者选择其中一种即可）</strong></p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">yarn application -kill &lt;applicationId&gt;</span><br></pre></td></tr></table></figure>

<h2 id="26-通常来说，Spark与MapReduce相比，Spark运行效率更高。请说明效率更高来源于Spark内置的哪些机制？并请列举常见spark的运行模式？"><a href="#26-通常来说，Spark与MapReduce相比，Spark运行效率更高。请说明效率更高来源于Spark内置的哪些机制？并请列举常见spark的运行模式？" class="headerlink" title="26.通常来说，Spark与MapReduce相比，Spark运行效率更高。请说明效率更高来源于Spark内置的哪些机制？并请列举常见spark的运行模式？"></a>26.通常来说，Spark与MapReduce相比，Spark运行效率更高。请说明效率更高来源于Spark内置的哪些机制？并请列举常见spark的运行模式？</h2><ol>
<li><p>spark中具有DAG有向无环图，DAG有向无环图在此过程中减少了shuffle以及落地磁盘的次数</p>
<p>Spark 计算比 MapReduce 快的根本原因在于 DAG 计算模型。一般而言，DAG 相比MapReduce 在大多数情况下可以减少 shuffle 次数。Spark 的 DAGScheduler 相当于一个改进版的 MapReduce，如果计算不涉及与其他节点进行数据交换，Spark 可以在内存中一次性完成这些操作，也就是中间结果无须落盘，减少了磁盘 IO 的操作。<br>spark把运算的中间数据(shuffle阶段产生的数据)存放在内存，迭代计算效率更高，mapreduce的中间结果需要落地，保存到磁盘</p>
</li>
<li><p>Spark容错性高，它通过弹性分布式数据集RDD来实现高效容错，RDD是一组分布式的存储在 节点内存中的只读性的数据集，这些集合石弹性的，某一部分丢失或者出错，可以通过整个数据集的计算流程的血缘关系来实现重建，mapreduce的容错只能重新计算</p>
</li>
<li><p>spark是粗粒度资源申请，也就是当提交spark application的时候，application会将所有的资源申请完毕，如果申请不到资源就等待，如果申请到资源才执行application，task在执行的时候就不需要自己去申请资源，task执行快，当最后一个task执行完之后task才会被释放。</p>
<p>优点是执行速度快，缺点是不能使集群得到充分的利用</p>
<p>MapReduce是细粒度资源申请，当提交application的时候，task执行时，自己申请资源，自己释放资源，task执行完毕之后，资源立即会被释放，task执行的慢，application执行的相对比较慢。</p>
<p>优点是集群资源得到充分利用，缺点是application执行的相对比较慢。</p>
</li>
</ol>
<h2 id="27-RDD中的数据在哪？"><a href="#27-RDD中的数据在哪？" class="headerlink" title="27.RDD中的数据在哪？"></a>27.RDD中的数据在哪？</h2><p>RDD中的数据在数据源，RDD只是一个抽象的数据集，我们通过对RDD的操作就相当于对数据进行操作。</p>
<h2 id="28-如果对RDD进行cache操作后，数据在哪里？"><a href="#28-如果对RDD进行cache操作后，数据在哪里？" class="headerlink" title="28.如果对RDD进行cache操作后，数据在哪里？"></a>28.如果对RDD进行cache操作后，数据在哪里？</h2><p>数据在第一执行cache算子时会被加载到各个Executor进程的内存中，第二次就会直接从内存中读取而不会区磁盘。</p>
<h2 id="29-Spark中Partition的数量由什么决定"><a href="#29-Spark中Partition的数量由什么决定" class="headerlink" title="29.Spark中Partition的数量由什么决定"></a>29.Spark中Partition的数量由什么决定</h2><p>和Mr一样，但是Spark默认最少有两个分区。</p>
<p>如果是读取hdfs的文件，一般来说，partition的数量等于文件的数量。</p>
<p>如果单个文件的大小大于hdfs的分块大小，partition的数量就等于 “文件大小/分块大小”。</p>
<p>同时，也可以使用rdd的repartition方法重新划分partition。</p>
<p>另外，在使用聚合函数比如 reducebykey, groupbykey，可以通过指定partitioner来指定partition的数量。</p>
<h2 id="30-Scala里面的函数和方法有什么区别"><a href="#30-Scala里面的函数和方法有什么区别" class="headerlink" title="30.Scala里面的函数和方法有什么区别"></a>30.Scala里面的函数和方法有什么区别</h2><p>Scala 有函数和方法，二者在语义上的区别很小。Scala 方法是类的一部分，而函数是一个对象可以赋值给一个变量。换句话来说在类中定义的函数即是方法。</p>
<h2 id="31-SparkStreaming怎么进行监控"><a href="#31-SparkStreaming怎么进行监控" class="headerlink" title="31.SparkStreaming怎么进行监控?"></a>31.SparkStreaming怎么进行监控?</h2><ol>
<li><p>Spark Streaming也提供了Jobs、Stages、Storage、Enviorment、Executors以及Streaming的监控</p>
</li>
<li><p>Spark Streaming能够提供如此优雅的数据监控，是因在对监听器设计模式的使用。如若Spark UI无法满足你所需的监控需要，用户可以定制个性化监控信息。Spark Streaming提供了StreamingListener特质，通过继承此方法，就可以定制所需的监控</p>
</li>
</ol>
<h2 id="32-Spark判断Shuffle的依据"><a href="#32-Spark判断Shuffle的依据" class="headerlink" title="32.Spark判断Shuffle的依据?"></a>32.Spark判断Shuffle的依据?</h2><p>父RDD的一个分区中的数据有可能被分配到子RDD的多个分区中</p>
<h2 id="33-Scala有没有多继承？可以实现多继承么？"><a href="#33-Scala有没有多继承？可以实现多继承么？" class="headerlink" title="33.Scala有没有多继承？可以实现多继承么？"></a>33.Scala有没有多继承？可以实现多继承么？</h2><p>trait实现多继承</p>
<p>由上可见，super.log通常调用trait从最后一个开始，从右往左调用。但是如果右边的trait是左边trait的超类，那么次序会调换，先调用子再调用父。</p>
<h2 id="34-Sparkstreaming和flink做实时处理的区别"><a href="#34-Sparkstreaming和flink做实时处理的区别" class="headerlink" title="34.Sparkstreaming和flink做实时处理的区别"></a>34.Sparkstreaming和flink做实时处理的区别</h2><ul>
<li>Flink的计算模型抽象是有状态的流，即源源不断没有边界的数据，并且数据的状态可以改变，对于批处理则认为是有边界的流进行处理</li>
<li>Spark的计算模型抽象是批，所有数据的表示本质上都是RDD抽象，对于流处理的支持，则是基于时间将流划分为多个批次，依次进行处理</li>
</ul>
<h2 id="35-Sparkcontext的作用"><a href="#35-Sparkcontext的作用" class="headerlink" title="35.Sparkcontext的作用"></a>35.Sparkcontext的作用</h2><p>官方解释：SparkContext是spark功能的主要入口。其代表与spark集群的连接，能够用来在集群上创建RDD、累加器、广播变量。每个JVM里只能存在一个处于激活状态的SparkContext，在创建新的SparkContext之前必须调用stop()来关闭之前的SparkContext。<br>下面我们看下SparkContext究竟有什么作用：<br>首先，每一个Spark应用都是一个SparkContext实例，可以理解为一个SparkContext就是一个spark application的生命周期，一旦SparkContext创建之后，就可以用这个SparkContext来创建RDD、累加器、广播变量，并且可以通过SparkContext访问Spark的服务，运行任务。spark context设置内部服务，并建立与spark执行环境的连接。</p>
<p>SparkContext在spark应用中起到了master的作用，掌控了所有Spark的生命活动，统筹全局，除了具体的任务在executor中执行，其他的任务调度、提交、监控、RDD管理等关键活动均由SparkContext主体来完成。</p>
<h2 id="36-Sparkstreaming读取kafka数据为什么选择直连方式"><a href="#36-Sparkstreaming读取kafka数据为什么选择直连方式" class="headerlink" title="36.Sparkstreaming读取kafka数据为什么选择直连方式"></a>36.Sparkstreaming读取kafka数据为什么选择直连方式</h2><figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Direct方式则采用的是低层次的API，直接连接kafka服务器上读取数据。需要我们自己去手动维护偏移量，代码量稍微大些。不过这种方式的优点有：</span><br><span class="line">1.当我们读取Topic下的数据时，它会自动对应Topic下的Partition生成相对应数量的RDD Partition，提高了计算时的并行度，提高了效率。</span><br><span class="line">2.它不需要通过WAL来维持数据的完整性。采取Direct直连方式时，当数据发生丢失，只要kafka上的数据进行了复制，就可以根据副本来进行数据重新拉取。</span><br><span class="line">3.它保证了数据只消费一次。因为我们将偏移量保存在一个地方，当我们读取数据时，从这里拿到数据的起始偏移量和读取偏移量确定读取范围，通过这些我们可以读取数据，当读取完成后会更新偏移量，这就保证了数据只消费一次。</span><br></pre></td></tr></table></figure>

<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Receive是使用的高级API，需要消费者连接Zookeeper来读取数据。是由Zookeeper来维护偏移量，不用我们来手动维护，这样的话就比较简单一些，减少了代码量。但是天下没有免费的午餐，它也有很多缺点：</span><br><span class="line">1.导致丢失数据。它是由Executor内的Receive来拉取数据并存放在内存中，再由Driver端提交的job来处理数据。这样的话，如果底层节点出现错误，就会发生数据丢失。</span><br><span class="line">2.浪费资源。可以采取WALs方式将数据同步到高可用数据存储平台上（HDFS，S3），那么如果再发生错误，就可以从中再次读取数据。但是这样会导致同样的数据存储了两份，浪费了资源。</span><br><span class="line">3.可能会导致重复读取数据。对于公司来说，一些数据宁可丢失了一小小部分也不能重复读取，但是这种由Zookeeper来记录偏移量的方式，可能会因为Spark和Zookeeper不同步，导致一份数据读取了两次。</span><br><span class="line">4.效率低。因为是分批次执行的，它是接收数据，直到达到了设定的时间间隔，才可是进行计算。而且我们在KafkaUtils.createStream()中设定的partition数量，只会增加receive的数量，不能提高并行计算的效率，但我们可以设定不同的Group和Topic创建DStream，然后再用Union合并DStream，提高并行效率。</span><br></pre></td></tr></table></figure>

<h2 id="37-离线分析什么时候用sparkcore和sparksql"><a href="#37-离线分析什么时候用sparkcore和sparksql" class="headerlink" title="37.离线分析什么时候用sparkcore和sparksql"></a>37.离线分析什么时候用sparkcore和sparksql</h2><p>经过分析，得出结论，Core适合读取结构复杂，多重map嵌套的数据。比如Avro这种数据复杂的文件类型。</p>
<p>SparkSQL适合读取结构简单的数据，比如parquet。</p>
<h2 id="38-Sparkstreaming实时的数据不丢失的问题"><a href="#38-Sparkstreaming实时的数据不丢失的问题" class="headerlink" title="38.Sparkstreaming实时的数据不丢失的问题"></a>38.Sparkstreaming实时的数据不丢失的问题</h2><p>在这种模式下，我们可以使用checkpoint + WAL + ReliableReceiver的方式保证不丢失数据，就是说在driver端打开chechpoint，用于定期的保存driver端的状态信息到HDFS上，保证driver端的状态信息不会丢失；在接收数据Receiver所在的Executor上打开WAL，使得接收到的数据保存在HDFS中，保证接收到的数据不会丢失；因为我们使用的是ReliableReceiver，所以在Receiver挂掉的期间，是不会接收数据，当这个Receiver重启的时候，会从上次消费的地方开始消费。</p>
<h2 id="39-简述宽依赖和窄依赖概念，groupByKey-reduceByKey-map-filter-union五种操作哪些会导致宽依赖，哪些会导致窄依赖"><a href="#39-简述宽依赖和窄依赖概念，groupByKey-reduceByKey-map-filter-union五种操作哪些会导致宽依赖，哪些会导致窄依赖" class="headerlink" title="39.简述宽依赖和窄依赖概念，groupByKey,reduceByKey,map,filter,union五种操作哪些会导致宽依赖，哪些会导致窄依赖?"></a>39.简述宽依赖和窄依赖概念，groupByKey,reduceByKey,map,filter,union五种操作哪些会导致宽依赖，哪些会导致窄依赖?</h2><p><strong>宽依赖：</strong>父RDD的分区被子RDD的多个分区使用  例如 groupByKey、reduceByKey、sortByKey等操作会产生宽依赖，会产生shuffle</p>
<p><strong>窄依赖：</strong>父RDD的每个分区都只被子RDD的一个分区使用 例如map、filter、union等操作会产生窄依赖</p>
<h2 id="40-数据倾斜可能会导致哪些问题，如何监控和排查，在设计之初，要考虑哪些来避免"><a href="#40-数据倾斜可能会导致哪些问题，如何监控和排查，在设计之初，要考虑哪些来避免" class="headerlink" title="40.数据倾斜可能会导致哪些问题，如何监控和排查，在设计之初，要考虑哪些来避免?"></a>40.数据倾斜可能会导致哪些问题，如何监控和排查，在设计之初，要考虑哪些来避免?</h2><p>危害一：任务长时间挂起，资源利用率下降</p>
<p>计算作业通常是分阶段进行的，阶段与阶段之间通常存在数据上的依赖关系，也就是说后一阶段需要等前一阶段执行完才能开始。</p>
<p>举个例子，Stage1在Stage0之后执行，假如Stage1依赖Stage0产生的数据结果，那么Stage1必须等待Stage0执行完成后才能开始，如果这时Stage0因为数据倾斜问题，导致任务执行时长过长，或者直接挂起，那么Stage1将一直处于等待状态，整个作业也就一直挂起。这个时候，资源被这个作业占据，但是却只有极少数task在执行，造成计算资源的严重浪费，利用率下降。</p>
<p>危害二：由引发内存溢出，导致任务失败</p>
<p>数据发生倾斜时，可能导致大量数据集中在少数几个节点上，在计算执行中由于要处理的数据超出了单个节点的能力范围，最终导致内存被撑爆，报OOM异常，直接导致任务失败。</p>
<p>危害三：作业执行时间超出预期，导致后续依赖数据结果的作业出错</p>
<p>有时候作业与作业之间，并没有构建强依赖关系，而是通过执行时间的前后时间差来调度，当前置作业未在预期时间范围内完成执行，那么当后续作业启动时便无法读取到其所需要的最新数据，从而导致连续出错。</p>
<p>可以看出，数据倾斜问题，就像是一个隐藏的杀手，潜伏在数据处理与分析的过程中，只要一出手，非死即伤。那么它又是如何产生的呢？想要解决它，我们就要先了解它。</p>
<p><strong>为什么会产生数据倾斜？</strong></p>
<p>3.1：读入数据的时候就是倾斜的</p>
<p>读入数据是计算任务的开始，但是往往这个阶段就可能已经开始出现问题了。</p>
<p>对于一些本身就可能倾斜的数据源，在读入阶段就可能出现个别partition执行时长过长或直接失败，如读取id分布跨度较大的mysql数据、partition分配不均的kafka数据或不可分割的压缩文件。</p>
<p>这些场景下，数据在读取阶段或者读取后的第一个计算阶段，就会容易执行过慢或报错。</p>
<p>3.2：shuffle产生倾斜</p>
<p>在shuffle阶段造成倾斜，在实际的工作中更加常见，比如特定key值数量过多，导致join发生时，大量数据涌向一个节点，导致数据严重倾斜，个别节点的读写压力是其他节点的好几倍，容易引发OOM错误。</p>
<p>3.3：过滤导致倾斜</p>
<p>有些场景下，数据原本是均衡的，但是由于进行了一系列的数据剔除操作，可能在过滤掉大量数据后，造成数据的倾斜。</p>
<p>例如，大部分节点都被过滤掉了很多数据，只剩下少量数据，但是个别节点的数据被过滤掉的很少，保留着大部分的数据。这种情况下，一般不会OOM，但是倾斜的数据可能会随着计算逐渐累积，最终引发问题。</p>
<p><strong>怎么预防或解决数据倾斜问题？</strong></p>
<p>4.1.尽量保证数据源是均衡的</p>
<p>程序读入的数据源通常是上个阶段其他作业产生的，那么我们在上个阶段作业生成数据时，就要注意这个问题，尽量不要给下游作业埋坑。</p>
<p>如果所有作业都注意到并谨慎处理了这个问题，那么出现读入时倾斜的可能性会大大降低。</p>
<p>这个有个小建议，在程序输出写文件时，尽量不要用coalesce，而是用repartition，这样写出的数据，各文件大小往往是均衡的。</p>
<p>4.2.对大数据集做过滤，结束后做repartition</p>
<p>对比较大的数据集做完过滤后，如果过滤掉了绝大部分数据，在进行下一步操作前，最好可以做一次repartition，让数据重回均匀分布的状态，否则失衡的数据集，在进行后续计算时，可能会逐渐累积倾斜的状态，容易产生错误。</p>
<p>4.3.对小表进行广播</p>
<p>如果两个数据量差异较大的表做join时，发生数据倾斜的常见解决方法，是将小表广播到每个节点去，这样就可以实现map端join，从而省掉shuffle，避免了大量数据在个别节点上的汇聚，执行效率也大大提升。</p>
<p>4.4.编码时要注意，不要人为造成倾斜</p>
<p>在写代码时，也要多加注意不要使用容易出问题的算子，如上文提到的coalesce。</p>
<p>另外，也要注意不要人为造成倾斜，如作者一次在帮别人排查倾斜问题时发现，他在代码中使用开窗函数，其中写到over (partition by 1)，这样就把所有数据分配到一个分区内，人为造成了倾斜。</p>
<p>4.5.join前优化</p>
<p>个别场景下，两个表join，某些特殊key值可能很多，很容易产生数据倾斜，这时可以根据实际计算进行join前优化。</p>
<p>如计算是先join后根据key聚合，那可以改为先根据key聚合然后再join。又如，需求是join后做distinct操作，在不影响结果的前提下，可以改为先distinct，然后再join。这些措施都是可以有效避免重复key过多导致join时倾斜。</p>
<h2 id="41-有一千万条短信，有重复，以文本文件的形式保存，一行一条数据，请用五分钟时间，找出重复出现最多的前10条"><a href="#41-有一千万条短信，有重复，以文本文件的形式保存，一行一条数据，请用五分钟时间，找出重复出现最多的前10条" class="headerlink" title="41.有一千万条短信，有重复，以文本文件的形式保存，一行一条数据，请用五分钟时间，找出重复出现最多的前10条"></a>41.有一千万条短信，有重复，以文本文件的形式保存，一行一条数据，请用五分钟时间，找出重复出现最多的前10条</h2><p>采用内存映射办法。</p>
<p>首先，1千万条短信按现在的短信长度将不会超过1GB空间，使用内存映射文件比较合适，可以一次映射 （如果有更大的数据量，可以采用分段映射），由于不需要频繁使用文件I/O和频繁分配小内存，这将大大提高了数据的加载速度。</p>
<p>其次，对每条短信的第i（i从0到70）个字母按ASCII码进行分组，也就是创建树。i是树的深度，也是短信第i个字母。</p>
<p>这个问题主要是解决两方面的问题：</p>
<ol>
<li><p>内容的加载。</p>
</li>
<li><p>短信内容的比较。</p>
</li>
</ol>
<p>采用内存映射技术可以解决内容加载的性能问题（不仅是不需要调用文件I/O函数，而且也不需要每读出一条短信都要分配一小块内存），而使用树技术可以有效地减少比较次数。</p>
<h2 id="42-现有一文件，格式如下，请用spark统计每个单词出现的次数"><a href="#42-现有一文件，格式如下，请用spark统计每个单词出现的次数" class="headerlink" title="42.现有一文件，格式如下，请用spark统计每个单词出现的次数"></a>42.现有一文件，格式如下，请用spark统计每个单词出现的次数</h2><p>见15</p>
<h2 id="43-共享变量和累加器"><a href="#43-共享变量和累加器" class="headerlink" title="43.共享变量和累加器"></a>43.共享变量和累加器</h2><p>累加器（accumulator）是 Spark 中提供的一种分布式的变量机制，其原理类似于mapreduce，即分布式的改变，然后聚合这些改变。累加器的一个常见用途是在调试时对作业执行过程中的事件进行计数。而广播变量用来高效分发较大的对象。</p>
<p>共享变量出现的原因：</p>
<p>通常在向 Spark 传递函数时，比如使用 map() 函数或者用 filter() 传条件时，可以使用驱动器程序中定义的变量，但是集群中运行的每个任务都会得到这些变量的一份新的副本，更新这些副本的值也不会影响驱动器中的对应变量。</p>
<p>Spark 的两个共享变量，累加器与广播变量，分别为结果聚合与广播这两种常见的通信模式突破了这一限制。</p>
<h2 id="44-当Spark涉及到数据库的操作时，如何减少Spark运行中的数据库连接数？"><a href="#44-当Spark涉及到数据库的操作时，如何减少Spark运行中的数据库连接数？" class="headerlink" title="44.当Spark涉及到数据库的操作时，如何减少Spark运行中的数据库连接数？"></a>44.当Spark涉及到数据库的操作时，如何减少Spark运行中的数据库连接数？</h2><p>使用 foreachPartition 代替 foreach，在 foreachPartition 内获取数据库的连接。</p>
<h2 id="45-特别大的数据，怎么发送到excutor中？"><a href="#45-特别大的数据，怎么发送到excutor中？" class="headerlink" title="45.特别大的数据，怎么发送到excutor中？"></a>45.特别大的数据，怎么发送到excutor中？</h2><p>当在excutor端使用了Driver变量，不使用广播变量，在每个excutor中有多少的task就有多少个Driver端变量副本<br>导致的问题：占用了网络IO,速度慢<br>如果使用广播变量在每一个excutor端只有一份Driver端的变量副本</p>
<h2 id="46-spark调优都做过哪些方面？"><a href="#46-spark调优都做过哪些方面？" class="headerlink" title="46.spark调优都做过哪些方面？"></a>46.spark调优都做过哪些方面？</h2><p>​    <img src="https://lixiangbetter.github.io/2020/07/06/Spark%E7%AC%94%E8%AE%B0/20160617154014702.png" alt></p>
<p>​    了解完了Spark作业运行的基本原理之后，对资源相关的参数就容易理解了。所谓的Spark资源参数调优，其实主要就是对Spark运行过程中各个使用资源的地方，通过调节各种参数，来优化资源使用的效率，从而提升Spark作业的执行性能。以下参数就是Spark中主要的资源参数，每个参数都对应着作业运行原理中的某个部分，我们同时也给出了一个调优的参考值。</p>
<h2 id="num-executors"><a href="#num-executors" class="headerlink" title="num-executors"></a>num-executors</h2><p>　　参数说明：该参数用于设置Spark作业总共要用多少个Executor进程来执行。Driver在向YARN集群管理器申请资源时，YARN集群管理器会尽可能按照你的设置来在集群的各个工作节点上，启动相应数量的Executor进程。这个参数非常之重要，如果不设置的话，默认只会给你启动少量的Executor进程，此时你的Spark作业的运行速度是非常慢的。</p>
<p>　　参数调优建议：<strong>每个Spark作业的运行一般设置50~100个左右的Executor进程比较合适</strong>，设置太少或太多的Executor进程都不好。设置的太少，无法充分利用集群资源；设置的太多的话，大部分队列可能无法给予充分的资源。</p>
<h2 id="executor-memory"><a href="#executor-memory" class="headerlink" title="executor-memory"></a>executor-memory</h2><p>　　参数说明：该参数用于设置每个Executor进程的内存。Executor内存的大小，很多时候直接决定了Spark作业的性能，而且跟常见的JVM OOM异常，也有直接的关联。</p>
<p>　　参数调优建议：每个Executor进程的内存设置4G<del>8G较为合适。但是这只是一个参考值，具体的设置还是得根据不同部门的资源队列来定。可以看看自己团队的资源队列的最大内存限制是多少，num-executors乘以executor-memory，就代表了你的Spark作业申请到的总内存量（也就是所有Executor进程的内存总和），这个量是不能超过队列的最大内存量的。此外，如果你是跟团队里其他人共享这个资源队列，那么申请的总内存量最好不要超过资源队列最大总内存的1/3</del>1/2，避免你自己的Spark作业占用了队列所有的资源，导致别的同学的作业无法运行。</p>
<h2 id="executor-cores"><a href="#executor-cores" class="headerlink" title="executor-cores"></a>executor-cores</h2><p>　　参数说明：该参数用于设置每个Executor进程的CPU core数量。这个参数决定了每个Executor进程并行执行task线程的能力。因为每个CPU core同一时间只能执行一个task线程，因此每个Executor进程的CPU core数量越多，越能够快速地执行完分配给自己的所有task线程。</p>
<p>　　参数调优建议：Executor的CPU core数量设置为2<del>4个较为合适。同样得根据不同部门的资源队列来定，可以看看自己的资源队列的最大CPU core限制是多少，再依据设置的Executor数量，来决定每个Executor进程可以分配到几个CPU core。同样建议，如果是跟他人共享这个队列，那么num-executors * executor-cores不要超过队列总CPU core的1/3</del>1/2左右比较合适，也是避免影响其他同学的作业运行。</p>
<h2 id="driver-memory"><a href="#driver-memory" class="headerlink" title="driver-memory"></a>driver-memory</h2><p>　　参数说明：该参数用于设置Driver进程的内存。</p>
<p>　　参数调优建议：<strong>Driver的内存通常来说不设置，或者设置1G左右应该就够了。</strong>唯一需要注意的一点是，如果需要使用collect算子将RDD的数据全部拉取到Driver上进行处理，那么必须确保Driver的内存足够大，否则会出现OOM内存溢出的问题。</p>
<h2 id="spark-default-parallelism"><a href="#spark-default-parallelism" class="headerlink" title="spark.default.parallelism"></a>spark.default.parallelism</h2><p>　　参数说明：该参数用于设置每个stage的默认task数量。这个参数极为重要，如果不设置可能会直接影响你的Spark作业性能。</p>
<p>　　参数调优建议：<strong>Spark作业的默认task数量为500~1000个较为合适。很多同学常犯的一个错误就是不去设置这个参数</strong>，那么此时就会导致Spark自己根据底层HDFS的block数量来设置task的数量，默认是一个HDFS block对应一个task。通常来说，Spark默认设置的数量是偏少的（比如就几十个task），如果task数量偏少的话，就会导致你前面设置好的Executor的参数都前功尽弃。试想一下，无论你的Executor进程有多少个，内存和CPU有多大，但是task只有1个或者10个，那么90%的Executor进程可能根本就没有task执行，也就是白白浪费了资源！因此Spark官网建议的设置原则是，设置该参数为num-executors * executor-cores的2~3倍较为合适，比如Executor的总CPU core数量为300个，那么设置1000个task是可以的，此时可以充分地利用Spark集群的资源。</p>
<h2 id="spark-storage-memoryFraction"><a href="#spark-storage-memoryFraction" class="headerlink" title="spark.storage.memoryFraction"></a>spark.storage.memoryFraction</h2><p>　　参数说明：该参数用于设置RDD持久化数据在Executor内存中能占的比例，默认是0.6。也就是说，默认Executor 60%的内存，可以用来保存持久化的RDD数据。根据你选择的不同的持久化策略，如果内存不够时，可能数据就不会持久化，或者数据会写入磁盘。</p>
<p>　　参数调优建议：如果Spark作业中，有较多的RDD持久化操作，该参数的值可以适当提高一些，保证持久化的数据能够容纳在内存中。避免内存不够缓存所有的数据，导致数据只能写入磁盘中，降低了性能。但是如果Spark作业中的shuffle类操作比较多，而持久化操作比较少，那么这个参数的值适当降低一些比较合适。此外，如果发现作业由于频繁的gc导致运行缓慢（通过spark web ui可以观察到作业的gc耗时），意味着task执行用户代码的内存不够用，那么同样建议调低这个参数的值。</p>
<h2 id="spark-shuffle-memoryFraction"><a href="#spark-shuffle-memoryFraction" class="headerlink" title="spark.shuffle.memoryFraction"></a>spark.shuffle.memoryFraction</h2><p>　　参数说明：该参数用于设置shuffle过程中一个task拉取到上个stage的task的输出后，进行聚合操作时能够使用的Executor内存的比例，默认是0.2。也就是说，Executor默认只有20%的内存用来进行该操作。shuffle操作在进行聚合时，如果发现使用的内存超出了这个20%的限制，那么多余的数据就会溢写到磁盘文件中去，此时就会极大地降低性能。</p>
<p>　　参数调优建议：如果Spark作业中的RDD持久化操作较少，shuffle操作较多时，建议降低持久化操作的内存占比，提高shuffle操作的内存占比比例，避免shuffle过程中数据过多时内存不够用，必须溢写到磁盘上，降低了性能。此外，如果发现作业由于频繁的gc导致运行缓慢，意味着task执行用户代码的内存不够用，那么同样建议调低这个参数的值。<br>资源参数的调优，没有一个固定的值，需要同学们根据自己的实际情况（包括Spark作业中的shuffle操作数量、RDD持久化操作数量以及spark web ui中显示的作业gc情况），同时参考本篇文章中给出的原理以及调优建议，合理地设置上述参数。</p>
<h2 id="47-spark任务为什么会被yarn-kill掉？"><a href="#47-spark任务为什么会被yarn-kill掉？" class="headerlink" title="47.spark任务为什么会被yarn kill掉？"></a>47.spark任务为什么会被yarn kill掉？</h2><p>因为spark任务是向yarn申请资源的。</p>
<h2 id="48-Spark-on-Yarn作业执行流程？yarn-client和yarn-cluster有什么区别？"><a href="#48-Spark-on-Yarn作业执行流程？yarn-client和yarn-cluster有什么区别？" class="headerlink" title="48.Spark on Yarn作业执行流程？yarn-client和yarn-cluster有什么区别？"></a>48.Spark on Yarn作业执行流程？yarn-client和yarn-cluster有什么区别？</h2><p><strong>Spark on Yarn作业执行流程？</strong></p>
<p>1.Spark Yarn Client 向 Yarn 中提交应用程序。<br>2.ResourceManager 收到请求后，在集群中选择一个 NodeManager，并为该应用程序分配一个 Container，在这个 Container 中启动应用程序的 ApplicationMaster， ApplicationMaster 进行 SparkContext 等的初始化。<br>3.ApplicationMaster 向 ResourceManager 注册，这样用户可以直接通过 ResourceManager 查看应用程序的运行状态，然后它将采用轮询的方式通过RPC协议为各个任务申请资源，并监控它们的运行状态直到运行结束。<br>4.ApplicationMaster 申请到资源（也就是Container）后，便与对应的 NodeManager 通信，并在获得的 Container 中启动 CoarseGrainedExecutorBackend，启动后会向 ApplicationMaster 中的 SparkContext 注册并申请 Task。<br>5.ApplicationMaster 中的 SparkContext 分配 Task 给 CoarseGrainedExecutorBackend 执行，CoarseGrainedExecutorBackend 运行 Task 并向ApplicationMaster 汇报运行的状态和进度，以让 ApplicationMaster 随时掌握各个任务的运行状态，从而可以在任务失败时重新启动任务。<br>6.应用程序运行完成后，ApplicationMaster 向 ResourceManager申请注销并关闭自己。</p>
<p><strong>yarn-client和yarn-cluster有什么区别？</strong></p>
<ol>
<li><p>理解YARN-Client和YARN-Cluster深层次的区别之前先清楚一个概念：Application Master。在YARN中，每个Application实例都有一个ApplicationMaster进程，它是Application启动的第一个容器。它负责和ResourceManager打交道并请求资源，获取资源之后告诉NodeManager为其启动Container。从深层次的含义讲YARN-Cluster和YARN-Client模式的区别其实就是ApplicationMaster进程的区别</p>
<ol start="2">
<li>YARN-Cluster模式下，Driver运行在AM(Application Master)中，它负责向YARN申请资源，并监督作业的运行状况。当用户提交了作业之后，就可以关掉Client，作业会继续在YARN上运行，因而YARN-Cluster模式不适合运行交互类型的作业 </li>
<li>YARN-Client模式下，Application Master仅仅向YARN请求Executor，Client会和请求的Container通信来调度他们工作，也就是说Client不能离开。</li>
</ol>
</li>
</ol>
<h2 id="49-Flatmap底层编码实现？"><a href="#49-Flatmap底层编码实现？" class="headerlink" title="49.Flatmap底层编码实现？"></a>49.Flatmap底层编码实现？</h2><p><strong>Spark flatMap 源码：</strong></p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">   *  Return a new RDD by first applying a function to all elements of this</span></span><br><span class="line"><span class="comment">   *  RDD, and then flattening the results.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">flatMap</span></span>[<span class="type">U</span>: <span class="type">ClassTag</span>](f: <span class="type">T</span> =&gt; <span class="type">TraversableOnce</span>[<span class="type">U</span>]): <span class="type">RDD</span>[<span class="type">U</span>] = withScope &#123;</span><br><span class="line">    <span class="keyword">val</span> cleanF = sc.clean(f)</span><br><span class="line">    <span class="keyword">new</span> <span class="type">MapPartitionsRDD</span>[<span class="type">U</span>, <span class="type">T</span>](<span class="keyword">this</span>, (context, pid, iter) =&gt; iter.flatMap(cleanF))</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p><strong>Scala flatMap 源码：</strong></p>
<figure class="highlight scala"><table><tr><td class="code"><pre><span class="line"><span class="comment">/** Creates a new iterator by applying a function to all values produced by this iterator</span></span><br><span class="line"><span class="comment">   *  and concatenating the results.</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   *  @param f the function to apply on each element.</span></span><br><span class="line"><span class="comment">   *  @return  the iterator resulting from applying the given iterator-valued function</span></span><br><span class="line"><span class="comment">   *           `f` to each value produced by this iterator and concatenating the results.</span></span><br><span class="line"><span class="comment">   *  @note    Reuse: $consumesAndProducesIterator</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">flatMap</span></span>[<span class="type">B</span>](f: <span class="type">A</span> =&gt; <span class="type">GenTraversableOnce</span>[<span class="type">B</span>]): <span class="type">Iterator</span>[<span class="type">B</span>] = <span class="keyword">new</span> <span class="type">AbstractIterator</span>[<span class="type">B</span>] &#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">var</span> cur: <span class="type">Iterator</span>[<span class="type">B</span>] = empty</span><br><span class="line">    <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">nextCur</span></span>() &#123; cur = f(self.next()).toIterator &#125;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">hasNext</span></span>: <span class="type">Boolean</span> = &#123;</span><br><span class="line">      <span class="comment">// Equivalent to cur.hasNext || self.hasNext &amp;&amp; &#123; nextCur(); hasNext &#125;</span></span><br><span class="line">      <span class="comment">// but slightly shorter bytecode (better JVM inlining!)</span></span><br><span class="line">      <span class="keyword">while</span> (!cur.hasNext) &#123;</span><br><span class="line">        <span class="keyword">if</span> (!self.hasNext) <span class="keyword">return</span> <span class="literal">false</span></span><br><span class="line">        nextCur()</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="literal">true</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">next</span></span>(): <span class="type">B</span> =&lt;span style=<span class="string">"color:#ffffff"</span>&gt; &lt;span style=<span class="string">"background-color:rgb(255,0,0)"</span>&gt;(<span class="keyword">if</span> (hasNext) cur <span class="keyword">else</span> empty).next()&lt;/span&gt;&lt;/span&gt;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure>

<p>flatMap其实就是将RDD里的每一个元素执行自定义函数f，这时这个元素的结果转换成iterator，最后将这些再拼接成一个新的RDD，也可以理解成原本的每个元素由横向执行函数f后再变为纵向。画红部分一直在回调，当RDD内没有元素为止。</p>
]]></content>
      <categories>
        <category>bigdata</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title>hive笔记</title>
    <url>/2020/07/05/hive%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<h1 id="hive笔记"><a href="#hive笔记" class="headerlink" title="hive笔记"></a>hive笔记</h1><h2 id="1-大表join小表产生的问题，怎么解决？"><a href="#1-大表join小表产生的问题，怎么解决？" class="headerlink" title="1.大表join小表产生的问题，怎么解决？"></a>1.大表join小表产生的问题，怎么解决？</h2><p>mapjoin优化是在Map阶段进行join，而不是通常那样在Reduce阶段按照join列进行分发后在每个Reduce节点上进行join，不需要分发也就没有倾斜的问题，相反，Hive会将小表全量复制到每个Map任务节点（对于本例是dim_seller表，当然仅全量复制b表sql指定的列），然后每个Map任务节点执行lookup小表即可。</p>
<h2 id="2-udf-udaf-udtf区别"><a href="#2-udf-udaf-udtf区别" class="headerlink" title="2.udf udaf udtf区别"></a>2.udf udaf udtf区别</h2><p>UDF操作作用于单个数据行，并且产生一个数据行作为输出。大多数函数都属于这一类（比如数学函数和字符串函数）。</p>
<p>UDAF 接受多个输入数据行，并产生一个输出数据行。像COUNT和MAX这样的函数就是聚集函数。</p>
<p>UDTF 操作作用于单个数据行，并且产生多个数据行——-一个表作为输出。lateral view explore()</p>
<p>简单来说：</p>
<p>UDF: 返回对应值，一对一</p>
<p>UDAF：返回聚类值，多对一</p>
<p>UDTF：返回拆分值，一对多</p>
<h2 id="3-hive有哪些保存元数据的方式，各有什么特点。"><a href="#3-hive有哪些保存元数据的方式，各有什么特点。" class="headerlink" title="3.hive有哪些保存元数据的方式，各有什么特点。"></a>3.hive有哪些保存元数据的方式，各有什么特点。</h2><ul>
<li>内存数据库derby，安装小，但是数据存在内存，不稳定</li>
<li>mysql数据库，数据存储模式可以自己设置，持久化好，查看方便。</li>
</ul>
<h2 id="4-hive内部表和外部表的区别"><a href="#4-hive内部表和外部表的区别" class="headerlink" title="4.hive内部表和外部表的区别"></a>4.hive内部表和外部表的区别</h2><p>内部表：加载数据到hive所在的hdfs目录，删除时，元数据和数据文件都删除</p>
<p>外部表：不加载数据到hive所在的hdfs目录，删除时，只删除表结构。</p>
<p>这样外部表相对来说更加安全些，数据组织也更加灵活，方便共享源数据。 </p>
<h2 id="5-生产环境中为什么建议使用外部表？"><a href="#5-生产环境中为什么建议使用外部表？" class="headerlink" title="5.生产环境中为什么建议使用外部表？"></a>5.生产环境中为什么建议使用外部表？</h2><ol>
<li>因为外部表不会加载数据到hive，减少数据传输、数据还能共享。</li>
<li>hive不会修改数据，所以无需担心数据的损坏</li>
<li>删除表时，只删除表结构、不删除数据。</li>
</ol>
<h2 id="6-insert-into-和-override-write区别？"><a href="#6-insert-into-和-override-write区别？" class="headerlink" title="6.insert into 和 override write区别？"></a>6.insert into 和 override write区别？</h2><p>insert into：将数据写到表中</p>
<p>override write：覆盖之前的内容。</p>
<h2 id="7-hive的判断函数有哪些"><a href="#7-hive的判断函数有哪些" class="headerlink" title="7.hive的判断函数有哪些"></a>7.hive的判断函数有哪些</h2><p>hive 的条件判断（if、coalesce、case）</p>
<h2 id="8-简单描述一下HIVE的功能？用hive创建表有几种方式？hive表有几种？"><a href="#8-简单描述一下HIVE的功能？用hive创建表有几种方式？hive表有几种？" class="headerlink" title="8.简单描述一下HIVE的功能？用hive创建表有几种方式？hive表有几种？"></a>8.简单描述一下HIVE的功能？用hive创建表有几种方式？hive表有几种？</h2><p>hive主要是做离线分析的</p>
<p>hive建表有三种方式</p>
<ul>
<li>直接建表法</li>
<li>查询建表法(通过AS 查询语句完成建表：将子查询的结果存在新表里，有数据<strong>，</strong>一般用于中间表)</li>
<li>like建表法(会创建结构完全相同的表，但是没有数据)</li>
</ul>
<p>hive表有2种：内部表和外部表</p>
<h2 id="9-线上业务每天产生的业务日志（压缩后-gt-3G），每天需要加载到hive的log表中，将每天产生的业务日志在压缩之后load到hive的log表时，最好使用的压缩算法是哪个-并说明其原因"><a href="#9-线上业务每天产生的业务日志（压缩后-gt-3G），每天需要加载到hive的log表中，将每天产生的业务日志在压缩之后load到hive的log表时，最好使用的压缩算法是哪个-并说明其原因" class="headerlink" title="9.线上业务每天产生的业务日志（压缩后&gt;=3G），每天需要加载到hive的log表中，将每天产生的业务日志在压缩之后load到hive的log表时，最好使用的压缩算法是哪个,并说明其原因"></a>9.线上业务每天产生的业务日志（压缩后&gt;=3G），每天需要加载到hive的log表中，将每天产生的业务日志在压缩之后load到hive的log表时，最好使用的压缩算法是哪个,并说明其原因</h2><ul>
<li>lzo压缩</li>
</ul>
<p>优点：压缩/解压速度也比较快，合理的压缩率；支持split，是hadoop中最流行的压缩格式；支持hadoop native库；可以在linux系统下安装lzop命令，使用方便。</p>
<p>缺点：压缩率比gzip要低一些；hadoop本身不支持，需要安装；在应用中对lzo格式的文件需要做一些特殊处理（为了支持split需要建索引，还需要指定inputformat为lzo格式）。</p>
<p>应用场景：一个很大的文本文件，压缩之后还大于200M以上的可以考虑，而且单个文件越大，lzo优点越明显。</p>
<ul>
<li>snappy压缩</li>
</ul>
<p>优点：高速压缩速度和合理的压缩率；支持hadoop native库。</p>
<p>缺点：不支持split；压缩率比gzip要低；hadoop本身不支持，需要安装；linux系统下没有对应的命令。</p>
<p>应用场景：当mapreduce作业的map输出的数据比较大的时候，作为map到reduce的中间数据的压缩格式；或者作为一个mapreduce作业的输出和另外一个mapreduce作业的输入。</p>
<h2 id="10-若在hive中建立分区仍不能优化查询效率，建表时如何优化"><a href="#10-若在hive中建立分区仍不能优化查询效率，建表时如何优化" class="headerlink" title="10.若在hive中建立分区仍不能优化查询效率，建表时如何优化"></a>10.若在hive中建立分区仍不能优化查询效率，建表时如何优化</h2><p>hive分桶</p>
<p>对于每一个表（table）或者分区，Hive可以进一步组织成桶，也就是说桶是更为细粒度的数据范围划分。<br>Hive是针对某一列进行分桶。<br>Hive采用对列值哈希，然后除以桶的个数求余的方式决定该条记录存放在哪个桶当中。</p>
<h2 id="11-union-all和union的区别"><a href="#11-union-all和union的区别" class="headerlink" title="11.union all和union的区别"></a>11.union all和union的区别</h2><p>union 去重</p>
<p>union all 不去重</p>
<h2 id="12-如何解决hive数据倾斜的问题"><a href="#12-如何解决hive数据倾斜的问题" class="headerlink" title="12.如何解决hive数据倾斜的问题"></a>12.如何解决hive数据倾斜的问题</h2><p>1）group by</p>
<p>注：group by 优于 distinct group</p>
<p>情形：group by 维度过小，某值的数量过多</p>
<p>后果：处理某值的 reduce 非常耗时</p>
<p>解决方式：采用 sum() group by 的方式来替换 count(distinct)完成计算。</p>
<p>2）count(distinct)</p>
<p>count(distinct xx)</p>
<p>情形：某特殊值过多</p>
<p>后果：处理此特殊值的 reduce 耗时；只有一个 reduce 任务</p>
<p>解决方式：count distinct 时，将值为空的情况单独处理，比如可以直接过滤空值的行，</p>
<p>在最后结果中加 1。如果还有其他计算，需要进行 group by，可以先将值为空的记录单独处</p>
<p>理，再和其他计算结果进行 union。</p>
<p>3）mapjoin</p>
<p>​    小表不小不大，怎么用 map join 解决倾斜问题</p>
<p>​    使用 map join 解决小表(记录数少)关联大表的数据倾斜问题，这个方法使用的频率非常高，但如果小表很大，大到map join会出现bug或异常，这时就需要特别的处理。</p>
<p>4）不同数据类型关联产生数据倾斜</p>
<p>情形：比如用户表中 user_id 字段为 int，log 表中 user_id 字段既有 string 类型也有 int 类型。当按照 user_id 进行两个表的 Join 操作时。</p>
<p>后果：处理此特殊值的 reduce 耗时；只有一个 reduce 任务</p>
<p>默认的 Hash 操作会按 int 型的 id 来进行分配，这样会导致所有 string 类型 id 的记录都分配到一个 Reducer 中。</p>
<p>解决方式：把数字类型转换成字符串类型</p>
<p>select * from users a</p>
<p>left outer join logs b</p>
<p>on a.usr_id = cast(b.user_id as string)</p>
<p>5）开启数据倾斜时负载均衡</p>
<p>set hive.groupby.skewindata=true;</p>
<p>思想：就是先随机分发并处理，再按照 key group by 来分发处理。</p>
<p>操作：当选项设定为 true，生成的查询计划会有两个 MRJob。</p>
<p>第一个 MRJob 中，Map 的输出结果集合会随机分布到 Reduce 中，每个 Reduce 做部分聚合操作，并输出结果，这样处理的结果是相同的 GroupBy Key 有可能被分发到不同的Reduce 中，从而达到负载均衡的目的；</p>
<p>第二个 MRJob 再根据预处理的数据结果按照 GroupBy Key 分布到 Reduce 中（这个过程可以保证相同的原始 GroupBy Key 被分布到同一个 Reduce 中），最后完成最终的聚合操作。</p>
<p>点评：它使计算变成了两个 mapreduce，先在第一个中在 shuffle 过程 partition 时随机给 key 打标记，使每个 key 随机均匀分布到各个 reduce 上计算，但是这样只能完成部分计算，因为相同 key 没有分配到相同 reduce 上。</p>
<p>所以需要第二次的 mapreduce,这次就回归正常 shuffle,但是数据分布不均匀的问题在第一次 mapreduce 已经有了很大的改善，因此基本解决数据倾斜。因为大量计算已经在第一次mr 中随机分布到各个节点完成。</p>
<p>6）控制空值分布</p>
<p>将为空的 key 转变为字符串加随机数或纯随机数，将因空值而造成倾斜的数据分不到多个 Reducer。</p>
<p>注：对于异常值如果不需要的话，最好是提前在 where 条件里过滤掉，这样可以使计算量大大减少</p>
<h2 id="13-hive性能优化常用的方法"><a href="#13-hive性能优化常用的方法" class="headerlink" title="13.hive性能优化常用的方法"></a>13.hive性能优化常用的方法</h2><p>1）MapJoin</p>
<p>如果不指定 MapJoin 或者不符合 MapJoin 的条件，那么 Hive 解析器会将 Join 操作转换成 Common Join，即：在 Reduce 阶段完成 join。容易发生数据倾斜。可以用 MapJoin 把小表全部加载到内存在 map 端进行 join，避免 reducer 处理。</p>
<p>2）行列过滤</p>
<p>列处理：在 SELECT 中，只拿需要的列，如果有，尽量使用分区过滤，少用 SELECT *。</p>
<p>行处理：在分区剪裁中，当使用外关联时，如果将副表的过滤条件写在 Where 后面，那么就会先全表关联，之后再过滤。（先子查询，再关联）</p>
<p>3）列式存储</p>
<p>4）采用分区技术</p>
<p>5）合理设置 Map 数</p>
<p>（1）通常情况下，作业会通过 input 的目录产生一个或者多个 map 任务。</p>
<p>主要的决定因素有：input 的文件总个数，input 的文件大小，集群设置的文件块大小。</p>
<p>（2）是不是 map 数越多越好？</p>
<p>答案是否定的。如果一个任务有很多小文件（远远小于块大小 128m），则每个小文件</p>
<p>也会被当做一个块，用一个 map 任务来完成，而一个 map 任务启动和初始化的时间远远大</p>
<p>于逻辑处理的时间，就会造成很大的资源浪费。而且，同时可执行的 map 数是受限的。</p>
<p>（3）是不是保证每个 map 处理接近 128m 的文件块，就高枕无忧了？</p>
<p>答案也是不一定。比如有一个 127m 的文件，正常会用一个 map 去完成，但这个文件只</p>
<p>有一个或者两个小字段，却有几千万的记录，如果 map 处理的逻辑比较复杂，用一个 map</p>
<p>任务去做，肯定也比较耗时。</p>
<p>针对上面的问题 2 和 3，我们需要采取两种方式来解决：即减少 map 数和增加 map 数；</p>
<p>6）小文件进行合并</p>
<p>在 Map 执行前合并小文件，减少 Map 数：CombineHiveInputFormat 具有对小文件进行</p>
<p>合并的功能（系统默认的格式）。HiveInputFormat 没有对小文件合并功能。</p>
<p>7）合理设置 Reduce 数</p>
<p>Reduce 个数并不是越多越好</p>
<p>（1）过多的启动和初始化 Reduce 也会消耗时间和资源；</p>
<p>（2）另外，有多少个 Reduce，就会有多少个输出文件，如果生成了很多个小文件，那</p>
<p>么如果这些小文件作为下一个任务的输入，则也会出现小文件过多的问题；</p>
<p>在设置 Reduce 个数的时候也需要考虑这两个原则：处理大数据量利用合适的 Reduce</p>
<p>数；使单个 Reduce 任务处理数据量大小要合适；</p>
<p>8）常用参数</p>
<p>// 输出合并小文件</p>
<p>SET hive.merge.mapfiles = true; – 默认 true，在 map-only 任务结束时合并</p>
<p>小文件</p>
<p>SET hive.merge.mapredfiles = true; – 默认 false，在 map-reduce 任务结</p>
<p>束时合并小文件</p>
<p>SET hive.merge.size.per.task = 268435456; – 默认 256M</p>
<p>SET hive.merge.smallfiles.avgsize = 16777216; – 当输出文件的平均大小</p>
<p>小于 16m 该值时，启动一个独立的 map-reduce 任务进行文件 merge</p>
<p>9）开启 map 端 combiner（不影响最终业务逻辑）</p>
<p>set hive.map.aggr=true；</p>
<p>10）压缩（选择快的）</p>
<p>设置 map 端输出、中间结果压缩。（不完全是解决数据倾斜的问题，但是减少了 IO 读</p>
<p>写和网络传输，能提高很多效率）</p>
<p>11）开启 JVM 重用</p>
<h2 id="14-简述delete，drop，truncate的区别"><a href="#14-简述delete，drop，truncate的区别" class="headerlink" title="14.简述delete，drop，truncate的区别"></a>14.简述delete，drop，truncate的区别</h2><p>delete 删除数据</p>
<p>drop 删除表</p>
<p>truncate 清空数据</p>
<h2 id="15-四个by的区别"><a href="#15-四个by的区别" class="headerlink" title="15.四个by的区别"></a>15.四个by的区别</h2><ol>
<li><p>Sort By：分区内有序；sort by在每个reducer端都会做排序，为每个reduce产生一个排序文件。也就是说sort by能保证局部有序（每个reducer出来的数据是有序的，但是不能保证所有的数据是有序的，除非只有一个reducer）</p>
</li>
<li><p>Order By：全局排序，只有一个 Reducer；</p>
</li>
<li><p>Distrbute By：类似 MR 中 Partition，进行分区，结合 sort by 使用。DISTRIBUTE BY 是控制在map端如何拆分数据给reduce端的。hive会根据distribute by后面列，对应reduce的个数进行分发，默认是采用hash算法。</p>
<p>相同的将会分配到一个reducer.</p>
</li>
<li><p>Cluster By：当 Distribute by 和 Sorts by 字段相同时，可以使用 Cluster by 方式。Cluster by 除了具有Distribute by 的功能外还兼具 Sort by 的功能。但是排序只能是升序排序，不能 指定排序规则为 ASC 或者 DESC。</p>
</li>
</ol>
<h2 id="16-Hive里边字段的分隔符用的什么？为什么用-t？有遇到过字段里边有-t-的情况吗，怎么处理的？为什么不用Hive默认的分隔符，默认的分隔符是什么？"><a href="#16-Hive里边字段的分隔符用的什么？为什么用-t？有遇到过字段里边有-t-的情况吗，怎么处理的？为什么不用Hive默认的分隔符，默认的分隔符是什么？" class="headerlink" title="16.Hive里边字段的分隔符用的什么？为什么用\t？有遇到过字段里边有\t 的情况吗，怎么处理的？为什么不用Hive默认的分隔符，默认的分隔符是什么？"></a>16.Hive里边字段的分隔符用的什么？为什么用\t？有遇到过字段里边有\t 的情况吗，怎么处理的？为什么不用Hive默认的分隔符，默认的分隔符是什么？</h2><p>hive 默认的字段分隔符为 ascii 码的控制符\001（^A）,建表的时候用 fields terminated by ‘\001’</p>
<p>遇到过字段里边有\t 的情况，自定义 InputFormat，替换为其他分隔符再做后续处理</p>
<h2 id="17-分区分桶的区别，为什么要分区"><a href="#17-分区分桶的区别，为什么要分区" class="headerlink" title="17.分区分桶的区别，为什么要分区"></a>17.分区分桶的区别，为什么要分区</h2><p>分区表： 原来的一个大表存储的时候分成不同的数据目录进行存储。如果说是单分区表，那么在表的目录下就只有一级子目录，如果说是多分区表，那么在表的目录下有多少分区就有多少级子目录。不管是单分区表，还是多分区表，在表的目录下，和非最终分区目录下是不能直接存储数据文件的 </p>
<p>分桶表： 原理和hashpartitioner 一样，将hive中的一张表的数据进行归纳分类的时候，归纳分类规则就是hashpartitioner。（需要指定分桶字段，指定分成多少桶）</p>
<p>分区表和分桶的区别除了存储的格式不同外，最主要的是作用：</p>
<ul>
<li>分区表：细化数据管理，缩小mapreduce程序 需要扫描的数据量。</li>
<li>分桶表：提高join查询的效率，在一份数据会被经常用来做连接查询的时候建立分桶，分桶字段就是连接字段；提高采样的效率。</li>
</ul>
<h2 id="18-mapjoin的原理"><a href="#18-mapjoin的原理" class="headerlink" title="18.mapjoin的原理"></a>18.mapjoin的原理</h2><p>MapJoin通常用于一个很小的表和一个大表进行join的场景，具体小表有多小，由参数hive.mapjoin.smalltable.filesize来决定，该参数表示小表的总大小，默认值为25000000字节，即25M。</p>
<p>Hive0.7之前，需要使用hint提示 /*+ mapjoin(table) */才会执行MapJoin,否则执行Common Join，但在0.7版本之后，默认自动会转换Map Join，由参数hive.auto.convert.join来控制，默认为true.<br>假设a表为一张大表，b为小表，并且hive.auto.convert.join=true,那么Hive在执行时候会自动转化为MapJoin。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">MapJoin简单说就是在Map阶段将小表读入内存，顺序扫描大表完成Join。减少昂贵的shuffle操作及reduce操作</span><br><span class="line">MapJoin分为两个阶段：</span><br><span class="line">- 通过MapReduce Local Task，将小表读入内存，生成HashTableFiles上传至Distributed Cache中，这里会HashTableFiles进行压缩。</span><br><span class="line">- MapReduce Job在Map阶段，每个Mapper从Distributed Cache读取HashTableFiles到内存中，顺序扫描大表，在Map阶段直接进行Join，将数据传递给下一个MapReduce任务。</span><br></pre></td></tr></table></figure>

<h2 id="19-在hive的row-number中distribute-by-和-partition-by的区别"><a href="#19-在hive的row-number中distribute-by-和-partition-by的区别" class="headerlink" title="19.在hive的row_number中distribute by 和 partition by的区别"></a>19.在hive的row_number中distribute by 和 partition by的区别</h2><p>distribute by + sort by = cluster by </p>
<p>partition by + order by : partition by 分组列 order by 排序列</p>
<p>Partition by<br>通常查询时会对整个数据库查询，而这带来了大量的开销，因此引入了partition的概念，在建表的时候通过设置partition的字段, 会根据该字段对数据分区存放，更具体的说是存放在不同的文件夹,这样通过指定设置Partition的字段条件查询时可以减少大量的开销</p>
<h2 id="20-hive开发中遇到什么问题"><a href="#20-hive开发中遇到什么问题" class="headerlink" title="20.hive开发中遇到什么问题?"></a>20.hive开发中遇到什么问题?</h2><p>数据倾斜、调优</p>
<h2 id="21-什么时候使用内部表-什么时候使用外部表"><a href="#21-什么时候使用内部表-什么时候使用外部表" class="headerlink" title="21.什么时候使用内部表,什么时候使用外部表"></a>21.什么时候使用内部表,什么时候使用外部表</h2><p>每天收集到的ng日志和埋点日志数据,需要做大量的统计数据分析,所以可以使用外部表进行存储，方便数据的共享，并且在对表做操作的时候不会误删原始数据。</p>
<p>在做统计分析时候用到的中间表，结果表可以使用内部表，因为这些数据不需要共享，使用内部表更为合适。并且很多时候分区表我们只需要保留最近3天的数据，用外部表的时候删除分区时无法删除数据。</p>
<h2 id="22-hive都有哪些函数，你平常工作中用到哪些"><a href="#22-hive都有哪些函数，你平常工作中用到哪些" class="headerlink" title="22.hive都有哪些函数，你平常工作中用到哪些"></a>22.hive都有哪些函数，你平常工作中用到哪些</h2><ul>
<li><p>数学函数<br>round(DOUBLE a)</p>
<p>floor(DOUBLE a)</p>
<p>ceil(DOUBLE a)</p>
<p>rand()</p>
</li>
<li><p>集合函数<br>size(Map&lt;K.V&gt;)</p>
<p>map_keys(Map&lt;K.V&gt;)</p>
<p>map_values(Map&lt;K.V&gt;)</p>
<p>array_contains(Array<t>, value)</t></p>
<p>sort_array(Array<t>)</t></p>
</li>
<li><p>类型转换函数<br>cast(expr as <type>)</type></p>
</li>
<li><p>日期函数<br>date_format函数（根据格式整理日期）<br>date_add、date_sub函数（加减日期）<br>next_day函数<br>last_day函数（求当月最后一天日期）<br>collect_set函数<br>get_json_object解析json函数</p>
<p>from_unixtime(bigint unixtime, string format)<br>to_date(string timestamp)<br>year(string date)<br>month(string date)<br>hour(string date)<br>weekofyear(string date)<br>datediff(string enddate, string startdate)<br>add_months(string start_date, int num_months)<br>date_format(date/timestamp/string ts, string fmt)</p>
</li>
<li><p>条件函数<br>if(boolean testCondition, T valueTrue, T valueFalseOrNull)</p>
<p>nvl(T value, T default_value)</p>
<p>COALESCE(T v1, T v2, …)</p>
<p>CASE a WHEN b THEN c [WHEN d THEN e]* [ELSE f] END</p>
<p>isnull( a )</p>
<p>isnotnull ( a )</p>
</li>
<li><p>字符函数<br>concat(string|binary A, string|binary B…)</p>
<p>concat_ws(string SEP, string A, string B…)</p>
<p>get_json_object(string json_string, string path)</p>
<p>length(string A)</p>
<p>lower(string A) lcase(string A)</p>
<p>parse_url(string urlString, string partToExtract [, string keyToExtract])</p>
<p>regexp_replace(string INITIAL_STRING, string PATTERN, string REPLACEMENT)</p>
<p>reverse(string A)</p>
<p>split(string str, string pat)</p>
<p>substr(string|binary A, int start) substring(string|binary A, int start)</p>
</li>
<li><p>聚合函数<br>count  sum min max avg</p>
</li>
<li><p>表生成函数<br>explode(array<type> a)</type></p>
<p>explode(ARRAY)</p>
<p>json_tuple(jsonStr, k1, k2, …)</p>
<p>parse_url_tuple(url, p1, p2, …)</p>
</li>
</ul>
]]></content>
      <categories>
        <category>bigdata</category>
      </categories>
      <tags>
        <tag>hive</tag>
      </tags>
  </entry>
  <entry>
    <title>Hadoop笔记</title>
    <url>/2020/07/02/Hadoop%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<h1 id="Hadoop笔记"><a href="#Hadoop笔记" class="headerlink" title="Hadoop笔记"></a>Hadoop笔记</h1><h1 id="一-Hadoop"><a href="#一-Hadoop" class="headerlink" title="一.Hadoop"></a>一.Hadoop</h1><h2 id="1-hdfs写流程"><a href="#1-hdfs写流程" class="headerlink" title="1.hdfs写流程"></a>1.hdfs写流程</h2><p><img src="https://lixiangbetter.github.io/2020/07/02/Hadoop%E7%AC%94%E8%AE%B0/20200408194104141.png" alt></p>
<ol>
<li>客户端跟namenode通信请求上传文件，namenode检查目标文件是否已存在，父目录是否存在</li>
<li>namenode返回是否可以上传</li>
<li>client请求第一个 block该传输到哪些datanode服务器上</li>
<li>namenode返回3个datanode服务器ABC</li>
<li>client请求3台dn中的一台A上传数据（本质上是一个RPC调用，建立pipeline），A收到请求会继续调用B，然后B调用C，将真个pipeline建立完成，逐级返回客户端</li>
<li>client开始往A上传第一个block（先从磁盘读取数据放到一个本地内存缓存），以packet为单位，A收到一个packet就会传给B，B传给C；A每传一个packet会放入一个应答队列等待应答</li>
<li>当一个block传输完成之后，client再次请求namenode上传第二个block的服务器。</li>
</ol>
<h2 id="2-hdfs读流程"><a href="#2-hdfs读流程" class="headerlink" title="2.hdfs读流程"></a>2.hdfs读流程</h2><p><img src="https://lixiangbetter.github.io/2020/07/02/Hadoop%E7%AC%94%E8%AE%B0/20200408194104141.png" alt></p>
<ol>
<li>client跟namenode通信查询元数据，找到文件块所在的datanode服务器</li>
<li>挑选一台datanode（就近原则，然后随机）服务器，请求建立socket流</li>
<li>datanode开始发送数据（从磁盘里面读取数据放入流，以packet为单位来做校验）</li>
<li>客户端以packet为单位接收，现在本地缓存，然后写入目标文件</li>
</ol>
<h2 id="3-hdfs的体系结构"><a href="#3-hdfs的体系结构" class="headerlink" title="3.hdfs的体系结构"></a>3.hdfs的体系结构</h2><p>hdfs有namenode、secondraynamenode、datanode组成。为n+1模式</p>
<ol>
<li>NameNode负责管理和记录整个文件系统的元数据</li>
<li>DataNode 负责管理用户的文件数据块，文件会按照固定的大小（blocksize）切成若干块后分布式存储在若干台datanode上，每一个文件块可以有多个副本，并存放在不同的datanode上，Datanode会定期向Namenode汇报自身所保存的文件block信息，而namenode则会负责保持文件的副本数量</li>
<li>HDFS的内部工作机制对客户端保持透明，客户端请求访问HDFS都是通过向namenode申请来进行</li>
<li>secondraynamenode负责合并日志</li>
</ol>
<h2 id="4-一个datanode-宕机-怎么一个流程恢复"><a href="#4-一个datanode-宕机-怎么一个流程恢复" class="headerlink" title="4.一个datanode 宕机,怎么一个流程恢复"></a>4.一个datanode 宕机,怎么一个流程恢复</h2><p>Datanode宕机了后，如果是短暂的宕机，可以事先写好脚本监控，将它启动起来。如果是长时间宕机了，那么datanode上的数据应该已经被备份到其他机器了，那这台datanode就是一台新的datanode了，删除他的所有数据文件和状态文件，重新启动。</p>
<h2 id="5-hadoop-的-namenode-宕机-怎么解决"><a href="#5-hadoop-的-namenode-宕机-怎么解决" class="headerlink" title="5.hadoop 的 namenode 宕机,怎么解决"></a>5.hadoop 的 namenode 宕机,怎么解决</h2><p>先分析宕机后的损失，宕机后直接导致client无法访问，内存中的元数据丢失，但是硬盘中的元数据应该还存在，如果只是节点挂了，重启即可，如果是机器挂了，重启机器后看节点是否能重启，不能重启就要找到原因修复了。但是最终的解决方案应该是在设计集群的初期就考虑到这个问题，做namenode的HA。</p>
<h2 id="6-namenode对元数据的管理"><a href="#6-namenode对元数据的管理" class="headerlink" title="6.namenode对元数据的管理"></a>6.namenode对元数据的管理</h2><p>namenode对数据的管理采用了三种存储形式：</p>
<ul>
<li>内存元数据(NameSystem)</li>
<li>磁盘元数据镜像文件(fsimage镜像)</li>
<li>数据操作日志文件（可通过日志运算出元数据）(edit日志文件)</li>
</ul>
<h2 id="7-元数据的checkpoint"><a href="#7-元数据的checkpoint" class="headerlink" title="7.元数据的checkpoint"></a>7.元数据的checkpoint</h2><p>每隔一段时间，会由secondary namenode将namenode上积累的所有edits和一个最新的fsimage下载到本地，并加载到内存进行merge（这个过程称为checkpoint）</p>
<p><img src="https://lixiangbetter.github.io/2020/07/02/Hadoop%E7%AC%94%E8%AE%B0/20200408195459150.png" alt></p>
<p>namenode和secondary namenode的工作目录存储结构完全相同，所以，当namenode故障退出需要重新恢复时，可以从secondary namenode的工作目录中将fsimage拷贝到namenode的工作目录，以恢复namenode的元数据</p>
<h2 id="8-yarn资源调度流程"><a href="#8-yarn资源调度流程" class="headerlink" title="8.yarn资源调度流程"></a>8.yarn资源调度流程</h2><p><img src="https://lixiangbetter.github.io/2020/07/02/Hadoop%E7%AC%94%E8%AE%B0/20200408195710128.png" alt></p>
<ol>
<li>用户向YARN 中提交应用程序， 其中包括ApplicationMaster 程序、启动ApplicationMaster 的命令、用户程序等。</li>
<li>ResourceManager 为该应用程序分配第一个Container， 并与对应的NodeManager 通信，要求它在这个Container 中启动应用程序的ApplicationMaster。</li>
<li>ApplicationMaster 首先向ResourceManager 注册， 这样用户可以直接通过ResourceManager 查看应用程序的运行状态，然后它将为各个任务申请资源，并监控它的运行状态，直到运行结束，即重复步骤4~7。</li>
<li>ApplicationMaster 采用轮询的方式通过RPC 协议向ResourceManager 申请和领取资源。</li>
<li>一旦ApplicationMaster 申请到资源后，便与对应的NodeManager 通信，要求它启动任务。</li>
<li>NodeManager 为任务设置好运行环境（包括环境变量、JAR 包、二进制程序等）后，将任务启动命令写到一个脚本中，并通过运行该脚本启动任务。</li>
<li>各个任务通过某个RPC 协议向ApplicationMaster 汇报自己的状态和进度，以让ApplicationMaster 随时掌握各个任务的运行状态，从而可以在任务失败时重新启动任务。在应用程序运行过程中，用户可随时通过RPC 向ApplicationMaster 查询应用程序的当前运行状态。</li>
<li>应用程序运行完成后，ApplicationMaster 向ResourceManager 注销并关闭自己。</li>
</ol>
<h2 id="9-hadoop中combiner和partition的作用"><a href="#9-hadoop中combiner和partition的作用" class="headerlink" title="9.hadoop中combiner和partition的作用"></a>9.hadoop中combiner和partition的作用</h2><ul>
<li>combiner是发生在map的最后一个阶段，父类就是Reducer，意义就是对每一个maptask的输出进行局部汇总，以减小网络传输量，缓解网络传输瓶颈，提高reducer的执行效率。</li>
<li>partition的主要作用将map阶段产生的所有kv对分配给不同的reducer task处理，可以将reduce阶段的处理负载进行分摊</li>
</ul>
<h2 id="10-用mapreduce怎么处理数据倾斜问题？"><a href="#10-用mapreduce怎么处理数据倾斜问题？" class="headerlink" title="10.用mapreduce怎么处理数据倾斜问题？"></a>10.用mapreduce怎么处理数据倾斜问题？</h2><p>数据倾斜：map /reduce程序执行时，reduce节点大部分执行完毕，但是有一个或者几个reduce节点运行很慢，导致整个程序的处理时间很长，这是因为某一个key的条数比其他key多很多（有时是百倍或者千倍之多），这条key所在的reduce节点所处理的数据量比其他节点就大很多，从而导致某几个节点迟迟运行不完，此称之为数据倾斜。</p>
<p>（1）局部聚合加全局聚合。</p>
<p>第一次在 map 阶段对那些导致了数据倾斜的 key 加上 1 到 n 的随机前缀，这样本来相</p>
<p>同的 key 也会被分到多个 Reducer 中进行局部聚合，数量就会大大降低。</p>
<p>第二次 mapreduce，去掉 key 的随机前缀，进行全局聚合。</p>
<p>思想：二次 mr，第一次将 key 随机散列到不同 reducer 进行处理达到负载均衡目的。第</p>
<p>二次再根据去掉 key 的随机前缀，按原 key 进行 reduce 处理。</p>
<p>这个方法进行两次 mapreduce，性能稍差。</p>
<p>（2）增加 Reducer，提升并行度</p>
<p>JobConf.setNumReduceTasks(int)</p>
<p>（3）实现自定义分区</p>
<p>根据数据分布情况，自定义散列函数，将 key 均匀分配到不同 Reducer</p>
<h2 id="11-shuffle-阶段-你怎么理解的"><a href="#11-shuffle-阶段-你怎么理解的" class="headerlink" title="11.shuffle 阶段,你怎么理解的"></a>11.shuffle 阶段,你怎么理解的</h2><p><img src="https://lixiangbetter.github.io/2020/07/02/Hadoop%E7%AC%94%E8%AE%B0/20200408221056627.png" alt>shuffle: 洗牌、发牌——（核心机制：缓存，数据分区，排序，Merge进行局部value的合并）；</p>
<p>具体来说：就是将maptask输出的处理结果数据，分发给reducetask，并在分发的过程中，对数据按key进行了分区和排序；</p>
<p>1）Map 方法之后 Reduce 方法之前这段处理过程叫 Shuffle</p>
<p>2）Map 方法之后，数据首先进入到分区方法，把数据标记好分区，然后把数据发送到 环形缓冲区；环形缓冲区默认大小 100m，环形缓冲区达到 80%时，进行溢写；溢写前对数 据进行排序，排序按照对 key 的索引进行字典顺序排序，排序的手段快排；溢写产生大量溢 写文件，需要对溢写文件进行归并排序；对溢写的文件也可以进行 Combiner 操作，前提是汇总操作，求平均值不行。最后将文件按照分区存储到磁盘，等待 Reduce 端拉取。</p>
<p>3）每个 Reduce 拉取 Map 端对应分区的数据。拉取数据后先存储到内存中，内存不够 了，再存储到磁盘。拉取完所有数据后，采用归并排序将内存和磁盘中的数据都进行排序。</p>
<p>在进入 Reduce 方法前，可以对数据进行分组操作。</p>
<h2 id="13-MapReduce优化经验"><a href="#13-MapReduce优化经验" class="headerlink" title="13.MapReduce优化经验"></a>13.MapReduce优化经验</h2><ol>
<li>设置合理的map和reduce的个数。合理设置blocksize</li>
<li>避免出现数据倾斜</li>
<li>combine函数</li>
<li>对数据进行压缩</li>
<li>小文件处理优化：事先合并成大文件，combineTextInputformat，在hdfs上用mapreduce将小文件合并SequenceFile大文件（key:文件名，value：文件内容）</li>
<li>参数优化</li>
</ol>
<h2 id="14-分别举例什么情况要使用-combiner，什么情况不使用？"><a href="#14-分别举例什么情况要使用-combiner，什么情况不使用？" class="headerlink" title="14.分别举例什么情况要使用 combiner，什么情况不使用？"></a>14.分别举例什么情况要使用 combiner，什么情况不使用？</h2><p>求平均数的时候就不需要用combiner，因为不会减少reduce执行数量。在其他的时候，可以依据情况，使用combiner，来减少map的输出数量，减少拷贝到reduce的文件，从而减轻reduce的压力，节省网络开销，提升执行效率</p>
<h2 id="15-MR运行流程解析"><a href="#15-MR运行流程解析" class="headerlink" title="15.MR运行流程解析"></a>15.MR运行流程解析</h2><ol>
<li>一个mr程序启动的时候，最先启动的是MRAppMaster，MRAppMaster启动后根据本次job的描述信息，计算出需要的maptask实例数量，然后向集群申请机器启动相应数量的maptask进程</li>
<li>maptask进程启动之后，根据给定的数据切片范围进行数据处理，主体流程为：<ol>
<li>利用客户指定的inputformat来获取RecordReader读取数据，形成输入KV对</li>
<li>将输入KV对传递给客户定义的map()方法，做逻辑运算，并将map()方法输出的KV对收集到缓存</li>
<li>将缓存中的KV对按照K分区排序后不断溢写到磁盘文件</li>
</ol>
</li>
<li>MRAppMaster监控到所有maptask进程任务完成之后，会根据客户指定的参数启动相应数量的reducetask进程，并告知reducetask进程要处理的数据范围（数据分区）</li>
<li>Reducetask进程启动之后，根据MRAppMaster告知的待处理数据所在位置，从若干台maptask运行所在机器上获取到若干个maptask输出结果文件，并在本地进行重新归并排序，然后按照相同key的KV为一个组，调用客户定义的reduce()方法进行逻辑运算，并收集运算输出的结果KV，然后调用客户指定的outputformat将结果数据输出到外部存储</li>
</ol>
<h2 id="16-简单描述一下HDFS的系统架构，怎么保证数据安全"><a href="#16-简单描述一下HDFS的系统架构，怎么保证数据安全" class="headerlink" title="16.简单描述一下HDFS的系统架构，怎么保证数据安全"></a>16.简单描述一下HDFS的系统架构，怎么保证数据安全</h2><p><img src="https://lixiangbetter.github.io/2020/07/02/Hadoop%E7%AC%94%E8%AE%B0/aHR0cHM6Ly9pbWcyMDE4LmNuYmxvZ3MuY29tL2Jsb2cvMTU3MzQ4Mi8yMDE5MDMvMTU3MzQ4Mi0yMDE5MDMwMTEzNTkzNzE1OS03ODYwNDI2MTgucG5n.jpeg" alt></p>
<p>HDFS数据安全性如何保证？</p>
<ol>
<li>存储在HDFS系统上的文件，会分割成128M大小的block存储在不同的节点上，block的副本数默认3份，也可配置成更多份；</li>
<li>第一个副本一般放置在与client（客户端）所在的同一节点上（若客户端无datanode，则随机放），第二个副本放置到与第一个副本同一机架的不同节点，第三个副本放到不同机架的datanode节点，当取用时遵循就近原则；</li>
<li>datanode以block为单位，每3s报告心跳状态，做10min内不报告心跳状态则namenode认为block已死掉，namonode会把其上面的数据备份到其他一个datanode节点上，保证数据的副本数量；</li>
<li>datanode会默认每小时把自己节点上的所有块状态信息报告给namenode；</li>
<li>采用safemode模式：datanode会周期性的报告block信息。Namenode会计算block的损坏率，当阀值&lt;0.999f时系统会进入安全模式，HDFS只读不写。 HDFS元数据采用secondaryname备份或者HA备份</li>
</ol>
<h2 id="17-在通过客户端向hdfs中写数据的时候，如果某一台机器宕机了，会怎么处理"><a href="#17-在通过客户端向hdfs中写数据的时候，如果某一台机器宕机了，会怎么处理" class="headerlink" title="17.在通过客户端向hdfs中写数据的时候，如果某一台机器宕机了，会怎么处理"></a>17.在通过客户端向hdfs中写数据的时候，如果某一台机器宕机了，会怎么处理</h2><p>在写入的时候不会重新分配datanode。 如果写入时，一个datanode挂掉，会将已经写入的数据放置到queue的顶部，并将挂掉的datanode移出pipline，将数据写入到剩余的datanode，在写入结束后， namenode会收集datanode的信息，发现此文件的replication没有达到配置的要求（default=3）,然后寻找一个datanode保存副本。</p>
<h2 id="18-Hadoop优化有哪些方面"><a href="#18-Hadoop优化有哪些方面" class="headerlink" title="18.Hadoop优化有哪些方面"></a>18.Hadoop优化有哪些方面</h2><p>0）HDFS 小文件影响</p>
<p>（1）影响 NameNode 的寿命，因为文件元数据存储在 NameNode 的内存中</p>
<p>（2）影响计算引擎的任务数量，比如每个小的文件都会生成一个 Map 任务</p>
<p>1）数据输入小文件处理：</p>
<p>（1）合并小文件：对小文件进行归档（Har）、自定义 Inputformat 将小文件存储成SequenceFile 文件。</p>
<p>（2）采用 ConbinFileInputFormat 来作为输入，解决输入端大量小文件场景。</p>
<p>（3）对于大量小文件 Job，可以开启 JVM 重用。</p>
<p>2）Map 阶段</p>
<p>（1）增大环形缓冲区大小。由 100m 扩大到 200m</p>
<p>（2）增大环形缓冲区溢写的比例。由 80%扩大到 90%</p>
<p>（3）减少对溢写文件的 merge 次数。（10 个文件，一次 20 个 merge）</p>
<p>（4）不影响实际业务的前提下，采用 Combiner 提前合并，减少 I/O。</p>
<p>3）Reduce 阶段</p>
<p>（1）合理设置 Map 和 Reduce 数：两个都不能设置太少，也不能设置太多。太少，会导致 Task 等待，延长处理时间；太多，会导致 Map、Reduce 任务间竞争资源，造成处理超时等错误。</p>
<p>（2）设置 Map、Reduce 共存：调整 slowstart.completedmaps 参数，使 Map 运行到一定程度后，Reduce 也开始运行，减少 Reduce 的等待时间。</p>
<p>（3）规避使用 Reduce，因为 Reduce 在用于连接数据集的时候将会产生大量的网络消耗。</p>
<p>（4）增加每个 Reduce 去 Map 中拿数据的并行数</p>
<p>（5）集群性能可以的前提下，增大 Reduce 端存储数据内存的大小。</p>
<p>4）IO 传输</p>
<p>（1）采用数据压缩的方式，减少网络 IO 的的时间。安装 Snappy 和 LZOP 压缩编码器。</p>
<p>（2）使用 SequenceFile 二进制文件</p>
<p>5）整体</p>
<p>（1）MapTask 默认内存大小为 1G，可以增加 MapTask 内存大小为 4-5g</p>
<p>（2）ReduceTask 默认内存大小为 1G，可以增加 ReduceTask 内存大小为 4-5g</p>
<p>（3）可以增加 MapTask 的 cpu 核数，增加 ReduceTask 的 CPU 核数</p>
<p>（4）增加每个 Container 的 CPU 核数和内存大小</p>
<p>（5）调整每个 Map Task 和 Reduce Task 最大重试次数</p>
<h2 id="19-大量数据求topN-写出mapreduce的实现思路）"><a href="#19-大量数据求topN-写出mapreduce的实现思路）" class="headerlink" title="19.大量数据求topN(写出mapreduce的实现思路）"></a>19.大量数据求topN(写出mapreduce的实现思路）</h2><p><a href="https://www.eyesmoons.com/article/31" target="_blank" rel="noopener">https://www.eyesmoons.com/article/31</a></p>
<p>​     在最初接触mapreduce时，top n问题的解决办法是将mapreduce输出（排序后）放入一个集合中，取前n个，但这种写法过于简单，内存能够加载的集合的大小是有上限的，一旦数据量大，很容易出现内存溢出。<br>​    如果要取top 5，则应该定义一个长度为6的数组，map所要做的事情就是将每条日志的那个需要排序的字段放入数组第一个元素中，调用Arrays.sort(Array[])方法可以将数组按照正序，从数字角度说是从小到大排序，比如第一条记录是9000，那么排序结果是[0,0,0,0,0,9000]，第二条日志记录是8000，排序结果是[0,0,0,0,8000,9000]，第三条日志记录是8500，排序结果是[0,0,0,8000,8500,9000]，以此类推，每次放进去一个数字如果大于数组里面最小的元素，相当于将最小的覆盖掉了，也就是说数组中元素永远是拿到日志中最大的那些个记录<br>​    ok，map将数组原封不动按照顺序输出，reduce接收到从每个map拿到的五个排好序的元素，在进行跟map一样的排序，排序后数组里面就是按照从小到大排好序的元素，将这些元素倒序输出就是最终我们要的结果了<br>​    与之前的方式做个比较，之前的map做的事情很少，在reduce中排序后拿前5条，reduce的压力是很大的，要把所有的数据都处理一遍，而一般设置reduce的个数较少，一旦数据较多，reduce就会承受不了，悲剧了。而现在的方式巧妙的将reduce的压力转移到了map，而map是集群效应的，很多台服务器来做这件事情，减少了一台机器上的负担，每个map其实只是输出了5个元素而已，如果有5个map，其实reduce才对5*5个数据进行了操作，也就不会出现内存溢出等问题了.</p>
<p>​    同样的思想，这个数据结构可以是，treemap,因为treemap的底层原理是红黑树，是有序的。</p>
<h2 id="20-列出正常工作的hadoop集群中hadoop都分别启动哪些进程以及他们的作用"><a href="#20-列出正常工作的hadoop集群中hadoop都分别启动哪些进程以及他们的作用" class="headerlink" title="20.列出正常工作的hadoop集群中hadoop都分别启动哪些进程以及他们的作用"></a>20.列出正常工作的hadoop集群中hadoop都分别启动哪些进程以及他们的作用</h2><p>1.NameNode它是hadoop中的主服务器，管理文件系统名称空间和对集群中存储的文件的访问，保存有metadate。</p>
<p>2.SecondaryNameNode它不是namenode的冗余守护进程，而是提供周期检查点和清理任务。帮助NN合并editslog，减少NN启动时间。</p>
<p>3.DataNode它负责管理连接到节点的存储（一个集群中可以有多个节点）。每个存储数据的节点运行一个datanode守护进程。</p>
<p>4.ResourceManager（JobTracker）JobTracker负责调度DataNode上的工作。每个DataNode有一个TaskTracker，它们执行实际工作。</p>
<p>5.NodeManager（TaskTracker）执行任务</p>
<p>6.DFSZKFailoverController高可用时它负责监控NN的状态，并及时的把状态信息写入ZK。它通过一个独立线程周期性的调用NN上的一个特定接口来获取NN的健康状态。FC也有选择谁作为Active NN的权利，因为最多只有两个节点，目前选择策略还比较简单（先到先得，轮换）。</p>
<p>7.JournalNode 高可用情况下存放namenode的editlog文件.</p>
<h2 id="21-Hadoop总job和Tasks之间的区别是什么？"><a href="#21-Hadoop总job和Tasks之间的区别是什么？" class="headerlink" title="21.Hadoop总job和Tasks之间的区别是什么？"></a>21.Hadoop总job和Tasks之间的区别是什么？</h2><p>Job是我们对一个完整的mapreduce程序的抽象封装</p>
<p>Task是job运行时，每一个处理阶段的具体实例，如map task，reduce task，maptask和reduce task都会有多个并发运行的实例</p>
<h2 id="22-Hadoop高可用HA模式"><a href="#22-Hadoop高可用HA模式" class="headerlink" title="22.Hadoop高可用HA模式"></a>22.Hadoop高可用HA模式</h2><p>HDFS高可用原理：</p>
<p>Hadoop HA（High Available）通过同时配置两个处于Active/Passive模式的Namenode来解决上述问题，状态分别是Active和Standby. Standby Namenode作为热备份，从而允许在机器发生故障时能够快速进行故障转移，同时在日常维护的时候使用优雅的方式进行Namenode切换。Namenode只能配置一主一备，不能多于两个Namenode。</p>
<p>主Namenode处理所有的操作请求（读写），而Standby只是作为slave，维护尽可能同步的状态，使得故障时能够快速切换到Standby。为了使Standby Namenode与Active Namenode数据保持同步，两个Namenode都与一组Journal Node进行通信。当主Namenode进行任务的namespace操作时，都会确保持久会修改日志到Journal Node节点中。Standby Namenode持续监控这些edit，当监测到变化时，将这些修改同步到自己的namespace。</p>
<p>当进行故障转移时，Standby在成为Active Namenode之前，会确保自己已经读取了Journal Node中的所有edit日志，从而保持数据状态与故障发生前一致。</p>
<p>为了确保故障转移能够快速完成，Standby Namenode需要维护最新的Block位置信息，即每个Block副本存放在集群中的哪些节点上。为了达到这一点，Datanode同时配置主备两个Namenode，并同时发送Block报告和心跳到两台Namenode。</p>
<p>确保任何时刻只有一个Namenode处于Active状态非常重要，否则可能出现数据丢失或者数据损坏。当两台Namenode都认为自己的Active Namenode时，会同时尝试写入数据（不会再去检测和同步数据）。为了防止这种脑裂现象，Journal Nodes只允许一个Namenode写入数据，内部通过维护epoch数来控制，从而安全地进行故障转移。</p>
<h2 id="23-简要描述安装配置一个hadoop集群的步骤"><a href="#23-简要描述安装配置一个hadoop集群的步骤" class="headerlink" title="23.简要描述安装配置一个hadoop集群的步骤"></a>23.简要描述安装配置一个hadoop集群的步骤</h2><ol>
<li>使用root账户登录。</li>
<li>修改IP。</li>
<li>修改Host主机名。</li>
<li>配置SSH免密码登录。</li>
<li>关闭防火墙。</li>
<li>安装JDK。</li>
<li>上传解压Hadoop安装包。</li>
<li>配置Hadoop的核心配置文件hadoop-evn.sh，core-site.xml，mapred-site.xml，hdfs-site.xml，yarn-site.xml</li>
<li>配置hadoop环境变量</li>
<li>格式化hdfs # bin/hadoop  namenode  -format</li>
<li>启动节点start-all.sh</li>
</ol>
<h2 id="24-fsimage和edit的区别"><a href="#24-fsimage和edit的区别" class="headerlink" title="24.fsimage和edit的区别"></a>24.fsimage和edit的区别</h2><p>fsimage：filesystem image 的简写，文件镜像。</p>
<p>客户端修改文件时候，先更新内存中的metadata信息,只有当对文件操作成功的时候，才会写到editlog。</p>
<p>fsimage是文件meta信息的持久化的检查点。secondary namenode会定期的将fsimage和editlog合并dump成新的fsimage</p>
<h2 id="25-yarn的三大调度策略"><a href="#25-yarn的三大调度策略" class="headerlink" title="25.yarn的三大调度策略"></a>25.yarn的三大调度策略</h2><p>FIFO Scheduler把应用按提交的顺序排成一个队列，这是一个先进先出队列，在进行资源分配的时候，先给队列中最头上的应用进行分配资源，待最头上的应用需求满足后再给下一个分配，以此类推。</p>
<p>Capacity（容量）调度器，有一个专门的队列用来运行小任务，但是为小任务专门设置一个队列会预先占用一定的集群资源，这就导致大任务的执行时间会落后于使用FIFO调度器时的时间。</p>
<p>在Fair（公平）调度器中，我们不需要预先占用一定的系统资源，Fair调度器会为所有运行的job动态的调整系统资源。当第一个大job提交时，只有这一个job在运行，此时它获得了所有集群资源；当第二个小任务提交后，Fair调度器会分配一半资源给这个小任务，让这两个任务公平的共享集群资源。</p>
<p>  需要注意的是，在下图Fair调度器中，从第二个任务提交到获得资源会有一定的延迟，因为它需要等待第一个任务释放占用的Container。小任务执行完成之后也会释放自己占用的资源，大任务又获得了全部的系统资源。最终的效果就是Fair调度器即得到了高的资源利用率又能保证小任务及时完成。</p>
<h2 id="26-hadoop的shell命令用的多吗-说出一些常用的"><a href="#26-hadoop的shell命令用的多吗-说出一些常用的" class="headerlink" title="26.hadoop的shell命令用的多吗?,说出一些常用的"></a>26.hadoop的shell命令用的多吗?,说出一些常用的</h2><p>hadoop  fs  -ls   /</p>
<p>hadoop fs  -mkdir  /aaa</p>
<p>hadoop fs -put /root/2.txt /haha/</p>
<p>hadoop fs -cat /haha/2.txt</p>
<p>hadoop fs -get /haha/2.txt </p>
<p>hadoop fs -rm -r -f /haha</p>
]]></content>
      <categories>
        <category>bigdata</category>
      </categories>
      <tags>
        <tag>hadoop</tag>
      </tags>
  </entry>
  <entry>
    <title>MySQL数据库笔记</title>
    <url>/2020/07/01/MySQL%E6%95%B0%E6%8D%AE%E5%BA%93%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<h1 id="MySQL数据库笔记"><a href="#MySQL数据库笔记" class="headerlink" title="MySQL数据库笔记"></a>MySQL数据库笔记</h1><h2 id="数据库基础知识"><a href="#数据库基础知识" class="headerlink" title="数据库基础知识"></a>数据库基础知识</h2><h3 id="为什么要使用数据库"><a href="#为什么要使用数据库" class="headerlink" title="为什么要使用数据库"></a>为什么要使用数据库</h3><p><strong>数据保存在内存</strong></p>
<p>优点： 存取速度快</p>
<p>缺点： 数据不能永久保存</p>
<p><strong>数据保存在文件</strong></p>
<p>优点： 数据永久保存</p>
<p>缺点：1）速度比内存操作慢，频繁的IO操作。2）查询数据不方便</p>
<p><strong>数据保存在数据库</strong></p>
<p>1）数据永久保存</p>
<p>2）使用SQL语句，查询方便效率高。</p>
<p>3）管理数据方便</p>
<h3 id="什么是SQL？"><a href="#什么是SQL？" class="headerlink" title="什么是SQL？"></a>什么是SQL？</h3><p>结构化查询语言(Structured Query Language)简称SQL，是一种数据库查询语言。</p>
<p>作用：用于存取数据、查询、更新和管理关系数据库系统。</p>
<h3 id="什么是MySQL"><a href="#什么是MySQL" class="headerlink" title="什么是MySQL?"></a>什么是MySQL?</h3><p>MySQL是一个关系型数据库管理系统，由瑞典MySQL AB 公司开发，属于 Oracle 旗下产品。MySQL 是最流行的关系型数据库管理系统之一，在 WEB 应用方面，MySQL是最好的 RDBMS (Relational Database Management System，关系数据库管理系统) 应用软件之一。在Java企业级开发中非常常用，因为 MySQL 是开源免费的，并且方便扩展。</p>
<h3 id="数据库三大范式是什么"><a href="#数据库三大范式是什么" class="headerlink" title="数据库三大范式是什么"></a>数据库三大范式是什么</h3><p>第一范式：每个列都不可以再拆分。</p>
<p>第二范式：在第一范式的基础上，非主键列完全依赖于主键，而不能是依赖于主键的一部分。</p>
<p>第三范式：在第二范式的基础上，非主键列只依赖于主键，不依赖于其他非主键。</p>
<p>在设计数据库结构的时候，要尽量遵守三范式，如果不遵守，必须有足够的理由。比如性能。事实上我们经常会为了性能而妥协数据库的设计。</p>
<h3 id="mysql有关权限的表都有哪几个"><a href="#mysql有关权限的表都有哪几个" class="headerlink" title="mysql有关权限的表都有哪几个"></a>mysql有关权限的表都有哪几个</h3><p>MySQL服务器通过权限表来控制用户对数据库的访问，权限表存放在mysql数据库里，由mysql_install_db脚本初始化。这些权限表分别user，db，table_priv，columns_priv和host。下面分别介绍一下这些表的结构和内容：</p>
<ul>
<li>user权限表：记录允许连接到服务器的用户帐号信息，里面的权限是全局级的。</li>
<li>db权限表：记录各个帐号在各个数据库上的操作权限。</li>
<li>table_priv权限表：记录数据表级的操作权限。</li>
<li>columns_priv权限表：记录数据列级的操作权限。</li>
<li>host权限表：配合db权限表对给定主机上数据库级操作权限作更细致的控制。这个权限表不受GRANT和REVOKE语句的影响。</li>
</ul>
<h3 id="MySQL的binlog有有几种录入格式？分别有什么区别？"><a href="#MySQL的binlog有有几种录入格式？分别有什么区别？" class="headerlink" title="MySQL的binlog有有几种录入格式？分别有什么区别？"></a>MySQL的binlog有有几种录入格式？分别有什么区别？</h3><p>有三种格式，statement，row和mixed。</p>
<ul>
<li>statement模式下，每一条会修改数据的sql都会记录在binlog中。不需要记录每一行的变化，减少了binlog日志量，节约了IO，提高性能。由于sql的执行是有上下文的，因此在保存的时候需要保存相关的信息，同时还有一些使用了函数之类的语句无法被记录复制。</li>
<li>row级别下，不记录sql语句上下文相关信息，仅保存哪条记录被修改。记录单元为每一行的改动，基本是可以全部记下来但是由于很多操作，会导致大量行的改动(比如alter table)，因此这种模式的文件保存的信息太多，日志量太大。</li>
<li>mixed，一种折中的方案，普通操作使用statement记录，当无法使用statement的时候使用row。</li>
</ul>
<p>此外，新版的MySQL中对row级别也做了一些优化，当表结构发生变化的时候，会记录语句而不是逐行记录。</p>
<h2 id="数据类型"><a href="#数据类型" class="headerlink" title="数据类型"></a>数据类型</h2><h3 id="mysql有哪些数据类型"><a href="#mysql有哪些数据类型" class="headerlink" title="mysql有哪些数据类型"></a>mysql有哪些数据类型</h3><p>1、整数类型，包括TINYINT、SMALLINT、MEDIUMINT、INT、BIGINT，分别表示1字节、2字节、3字节、4字节、8字节整数。任何整数类型都可以加上UNSIGNED属性，表示数据是无符号的，即非负整数。<br>长度：整数类型可以被指定长度，例如：INT(11)表示长度为11的INT类型。长度在大多数场景是没有意义的，它不会限制值的合法范围，只会影响显示字符的个数，而且需要和UNSIGNED ZEROFILL属性配合使用才有意义。<br>例子，假定类型设定为INT(5)，属性为UNSIGNED ZEROFILL，如果用户插入的数据为12的话，那么数据库实际存储数据为00012。</p>
<p>2、实数类型，包括FLOAT、DOUBLE、DECIMAL。<br>DECIMAL可以用于存储比BIGINT还大的整型，能存储精确的小数。<br>而FLOAT和DOUBLE是有取值范围的，并支持使用标准的浮点进行近似计算。<br>计算时FLOAT和DOUBLE相比DECIMAL效率更高一些，DECIMAL你可以理解成是用字符串进行处理。</p>
<p>3、字符串类型，包括VARCHAR、CHAR、TEXT、BLOB<br>VARCHAR用于存储可变长字符串，它比定长类型更节省空间。<br>VARCHAR使用额外1或2个字节存储字符串长度。列长度小于255字节时，使用1字节表示，否则使用2字节表示。<br>VARCHAR存储的内容超出设置的长度时，内容会被截断。<br>CHAR是定长的，根据定义的字符串长度分配足够的空间。<br>CHAR会根据需要使用空格进行填充方便比较。<br>CHAR适合存储很短的字符串，或者所有值都接近同一个长度。<br>CHAR存储的内容超出设置的长度时，内容同样会被截断。</p>
<p>使用策略：<br>对于经常变更的数据来说，CHAR比VARCHAR更好，因为CHAR不容易产生碎片。<br>对于非常短的列，CHAR比VARCHAR在存储空间上更有效率。<br>使用时要注意只分配需要的空间，更长的列排序时会消耗更多内存。<br>尽量避免使用TEXT/BLOB类型，查询时会使用临时表，导致严重的性能开销。</p>
<p>4、枚举类型（ENUM），把不重复的数据存储为一个预定义的集合。<br>有时可以使用ENUM代替常用的字符串类型。<br>ENUM存储非常紧凑，会把列表值压缩到一个或两个字节。<br>ENUM在内部存储时，其实存的是整数。<br>尽量避免使用数字作为ENUM枚举的常量，因为容易混乱。<br>排序是按照内部存储的整数</p>
<p>5、日期和时间类型，尽量使用timestamp，空间效率高于datetime，<br>用整数保存时间戳通常不方便处理。<br>如果需要存储微妙，可以使用bigint存储。<br>看到这里，这道真题是不是就比较容易回答了。</p>
<h2 id="引擎"><a href="#引擎" class="headerlink" title="引擎"></a>引擎</h2><h3 id="MySQL存储引擎MyISAM与InnoDB区别"><a href="#MySQL存储引擎MyISAM与InnoDB区别" class="headerlink" title="MySQL存储引擎MyISAM与InnoDB区别"></a>MySQL存储引擎MyISAM与InnoDB区别</h3><p>存储引擎Storage engine：MySQL中的数据、索引以及其他对象是如何存储的，是一套文件系统的实现。</p>
<p>常用的存储引擎有以下：</p>
<ul>
<li>Innodb引擎：Innodb引擎提供了对数据库ACID事务的支持。并且还提供了行级锁和外键的约束。它的设计的目标就是处理大数据容量的数据库系统。</li>
<li>MyIASM引擎(原本Mysql的默认引擎)：不提供事务的支持，也不支持行级锁和外键。</li>
<li>MEMORY引擎：所有的数据都在内存中，数据的处理速度快，但是安全性不高。</li>
</ul>
<h3 id="MyISAM索引与InnoDB索引的区别？"><a href="#MyISAM索引与InnoDB索引的区别？" class="headerlink" title="MyISAM索引与InnoDB索引的区别？"></a>MyISAM索引与InnoDB索引的区别？</h3><ul>
<li>InnoDB索引是聚簇索引，MyISAM索引是非聚簇索引。</li>
<li>InnoDB的主键索引的叶子节点存储着行数据，因此主键索引非常高效。</li>
<li>MyISAM索引的叶子节点存储的是行数据地址，需要再寻址一次才能得到数据。</li>
<li>InnoDB非主键索引的叶子节点存储的是主键和其他带索引的列数据，因此查询时做到覆盖索引会非常高效。</li>
</ul>
<h3 id="InnoDB引擎的4大特性"><a href="#InnoDB引擎的4大特性" class="headerlink" title="InnoDB引擎的4大特性"></a>InnoDB引擎的4大特性</h3><ul>
<li>插入缓冲（insert buffer)</li>
<li>二次写(double write)</li>
<li>自适应哈希索引(ahi)</li>
<li>预读(read ahead)</li>
</ul>
<h3 id="存储引擎选择"><a href="#存储引擎选择" class="headerlink" title="存储引擎选择"></a>存储引擎选择</h3><p>如果没有特别的需求，使用默认的<code>Innodb</code>即可。</p>
<p>MyISAM：以读写插入为主的应用程序，比如博客系统、新闻门户网站。</p>
<p>Innodb：更新（删除）操作频率也高，或者要保证数据的完整性；并发量高，支持事务和外键。比如OA自动化办公系统。</p>
<h2 id="索引"><a href="#索引" class="headerlink" title="索引"></a>索引</h2><h3 id="什么是索引？"><a href="#什么是索引？" class="headerlink" title="什么是索引？"></a>什么是索引？</h3><p>索引是一种特殊的文件(InnoDB数据表上的索引是表空间的一个组成部分)，它们包含着对数据表里所有记录的引用指针。</p>
<p>索引是一种数据结构。数据库索引，是数据库管理系统中一个排序的数据结构，以协助快速查询、更新数据库表中数据。索引的实现通常使用B树及其变种B+树。</p>
<p>更通俗的说，索引就相当于目录。为了方便查找书中的内容，通过对内容建立索引形成目录。索引是一个文件，它是要占据物理空间的。</p>
<h3 id="索引有哪些优缺点？"><a href="#索引有哪些优缺点？" class="headerlink" title="索引有哪些优缺点？"></a>索引有哪些优缺点？</h3><p>索引的优点</p>
<ul>
<li>可以大大加快数据的检索速度，这也是创建索引的最主要的原因。</li>
<li>通过使用索引，可以在查询的过程中，使用优化隐藏器，提高系统的性能。</li>
</ul>
<p>索引的缺点</p>
<ul>
<li>时间方面：创建索引和维护索引要耗费时间，具体地，当对表中的数据进行增加、删除和修改的时候，索引也要动态的维护，会降低增/改/删的执行效率；</li>
<li>空间方面：索引需要占物理空间。</li>
</ul>
<h3 id="索引使用场景（重点）"><a href="#索引使用场景（重点）" class="headerlink" title="索引使用场景（重点）"></a>索引使用场景（重点）</h3><p>where</p>
<p>order by</p>
<p>join</p>
<p>索引覆盖</p>
<p>如果要查询的字段都建立过索引，那么引擎会直接在索引表中查询而不会访问原始数据（否则只要有一个字段没有建立索引就会做全表扫描），这叫索引覆盖。因此我们需要尽可能的在select后只写必要的查询字段，以增加索引覆盖的几率。</p>
<p>这里值得注意的是不要想着为每个字段建立索引，因为优先使用索引的优势就在于其体积小。</p>
<h3 id="索引有哪几种类型？"><a href="#索引有哪几种类型？" class="headerlink" title="索引有哪几种类型？"></a>索引有哪几种类型？</h3><p>主键索引: 数据列不允许重复，不允许为NULL，一个表只能有一个主键。</p>
<p>唯一索引: 数据列不允许重复，允许为NULL值，一个表允许多个列创建唯一索引。</p>
<p>可以通过 ALTER TABLE table_name ADD UNIQUE (column); 创建唯一索引</p>
<p>可以通过 ALTER TABLE table_name ADD UNIQUE (column1,column2); 创建唯一组合索引</p>
<p>普通索引: 基本的索引类型，没有唯一性的限制，允许为NULL值。</p>
<p>可以通过ALTER TABLE table_name ADD INDEX index_name (column);创建普通索引</p>
<p>可以通过ALTER TABLE table_name ADD INDEX index_name(column1, column2, column3);创建组合索引</p>
<p>全文索引： 是目前搜索引擎使用的一种关键技术。</p>
<p>可以通过ALTER TABLE table_name ADD FULLTEXT (column);创建全文索引</p>
<h3 id="创建索引的原则（重中之重）"><a href="#创建索引的原则（重中之重）" class="headerlink" title="创建索引的原则（重中之重）"></a>创建索引的原则（重中之重）</h3><p>索引虽好，但也不是无限制的使用，最好符合一下几个原则</p>
<p>1） 最左前缀匹配原则，组合索引非常重要的原则，mysql会一直向右匹配直到遇到范围查询(&gt;、&lt;、between、like)就停止匹配，比如a = 1 and b = 2 and c &gt; 3 and d = 4 如果建立(a,b,c,d)顺序的索引，d是用不到索引的，如果建立(a,b,d,c)的索引则都可以用到，a,b,d的顺序可以任意调整。</p>
<p>2）较频繁作为查询条件的字段才去创建索引</p>
<p>3）更新频繁字段不适合创建索引</p>
<p>4）若是不能有效区分数据的列不适合做索引列(如性别，男女未知，最多也就三种，区分度实在太低)</p>
<p>5）尽量的扩展索引，不要新建索引。比如表中已经有a的索引，现在要加(a,b)的索引，那么只需要修改原来的索引即可。</p>
<p>6）定义有外键的数据列一定要建立索引。</p>
<p>7）对于那些查询中很少涉及的列，重复值比较多的列不要建立索引。</p>
<p>8）对于定义为text、image和bit的数据类型的列不要建立索引。</p>
<h3 id="什么是聚簇索引？何时使用聚簇索引与非聚簇索引"><a href="#什么是聚簇索引？何时使用聚簇索引与非聚簇索引" class="headerlink" title="什么是聚簇索引？何时使用聚簇索引与非聚簇索引"></a>什么是聚簇索引？何时使用聚簇索引与非聚簇索引</h3><ul>
<li>聚簇索引：将数据存储与索引放到了一块，找到索引也就找到了数据</li>
<li>非聚簇索引：将数据存储于索引分开结构，索引结构的叶子节点指向了数据的对应行，myisam通过key_buffer把索引先缓存到内存中，当需要访问数据时（通过索引访问数据），在内存中直接搜索索引，然后通过索引找到磁盘相应数据，这也就是为什么索引不在key buffer命中时，速度慢的原因</li>
</ul>
<p>澄清一个概念：innodb中，在聚簇索引之上创建的索引称之为辅助索引，辅助索引访问数据总是需要二次查找，非聚簇索引都是辅助索引，像复合索引、前缀索引、唯一索引，辅助索引叶子节点存储的不再是行的物理位置，而是主键值</p>
<p>何时使用聚簇索引与非聚簇索引</p>
<p><img src="https://lixiangbetter.github.io/2020/07/01/MySQL%E6%95%B0%E6%8D%AE%E5%BA%93%E7%AC%94%E8%AE%B0/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy8xMDE1NDQ5OS1kNTNhNWNlOWNlY2YyMmYzLnBuZw.jpeg" alt></p>
<h3 id="非聚簇索引一定会回表查询吗？"><a href="#非聚簇索引一定会回表查询吗？" class="headerlink" title="非聚簇索引一定会回表查询吗？"></a>非聚簇索引一定会回表查询吗？</h3><p>不一定，这涉及到查询语句所要求的字段是否全部命中了索引，如果全部命中了索引，那么就不必再进行回表查询。</p>
<p>举个简单的例子，假设我们在员工表的年龄上建立了索引，那么当进行select age from employee where age &lt; 20的查询时，在索引的叶子节点上，已经包含了age信息，不会再次进行回表查询。</p>
<h3 id="联合索引是什么？为什么需要注意联合索引中的顺序？"><a href="#联合索引是什么？为什么需要注意联合索引中的顺序？" class="headerlink" title="联合索引是什么？为什么需要注意联合索引中的顺序？"></a>联合索引是什么？为什么需要注意联合索引中的顺序？</h3><p>MySQL可以使用多个字段同时建立一个索引，叫做联合索引。在联合索引中，如果想要命中索引，需要按照建立索引时的字段顺序挨个使用，否则无法命中索引。</p>
<p>具体原因为:</p>
<p>MySQL使用索引时需要索引有序，假设现在建立了”name，age，school”的联合索引，那么索引的排序为: 先按照name排序，如果name相同，则按照age排序，如果age的值也相等，则按照school进行排序。</p>
<p>当进行查询时，此时索引仅仅按照name严格有序，因此必须首先使用name字段进行等值查询，之后对于匹配到的列而言，其按照age字段严格有序，此时可以使用age字段用做索引查找，以此类推。因此在建立联合索引的时候应该注意索引列的顺序，一般情况下，将查询需求频繁或者字段选择性高的列放在前面。此外可以根据特例的查询或者表结构进行单独的调整。</p>
<h2 id="事务"><a href="#事务" class="headerlink" title="事务"></a>事务</h2><h3 id="什么是数据库事务？"><a href="#什么是数据库事务？" class="headerlink" title="什么是数据库事务？"></a>什么是数据库事务？</h3><p>事务是一个不可分割的数据库操作序列，也是数据库并发控制的基本单位，其执行的结果必须使数据库从一种一致性状态变到另一种一致性状态。事务是逻辑上的一组操作，要么都执行，要么都不执行。</p>
<h3 id="事物的四大特性-ACID-介绍一下"><a href="#事物的四大特性-ACID-介绍一下" class="headerlink" title="事物的四大特性(ACID)介绍一下?"></a>事物的四大特性(ACID)介绍一下?</h3><p>原子性： 事务是最小的执行单位，不允许分割。事务的原子性确保动作要么全部完成，要么完全不起作用；<br>一致性： 执行事务前后，数据保持一致，多个事务对同一个数据读取的结果是相同的；<br>隔离性： 并发访问数据库时，一个用户的事务不被其他事务所干扰，各并发事务之间数据库是独立的；<br>持久性： 一个事务被提交之后。它对数据库中数据的改变是持久的，即使数据库发生故障也不应该对其有任何影响。</p>
<h3 id="什么是脏读？幻读？不可重复读？"><a href="#什么是脏读？幻读？不可重复读？" class="headerlink" title="什么是脏读？幻读？不可重复读？"></a>什么是脏读？幻读？不可重复读？</h3><ul>
<li>脏读(Drity Read)：某个事务已更新一份数据，另一个事务在此时读取了同一份数据，由于某些原因，前一个RollBack了操作，则后一个事务所读取的数据就会是不正确的。</li>
<li>不可重复读(Non-repeatable read):在一个事务的两次查询之中数据不一致，这可能是两次查询过程中间插入了一个事务更新的原有的数据。</li>
<li>幻读(Phantom Read):在一个事务的两次查询中数据笔数不一致，例如有一个事务查询了几列(Row)数据，而另一个事务却在此时插入了新的几列数据，先前的事务在接下来的查询中，就会发现有几列数据是它先前所没有的。</li>
</ul>
<h3 id="什么是事务的隔离级别？MySQL的默认隔离级别是什么？"><a href="#什么是事务的隔离级别？MySQL的默认隔离级别是什么？" class="headerlink" title="什么是事务的隔离级别？MySQL的默认隔离级别是什么？"></a>什么是事务的隔离级别？MySQL的默认隔离级别是什么？</h3><p>SQL 标准定义了四个隔离级别：</p>
<ul>
<li>READ-UNCOMMITTED(读取未提交)： 最低的隔离级别，允许读取尚未提交的数据变更，可能会导致脏读、幻读或不可重复读。</li>
<li>READ-COMMITTED(读取已提交)： 允许读取并发事务已经提交的数据，可以阻止脏读，但是幻读或不可重复读仍有可能发生。</li>
<li>REPEATABLE-READ(可重复读)： 对同一字段的多次读取结果都是一致的，除非数据是被本身事务自己所修改，可以阻止脏读和不可重复读，但幻读仍有可能发生。</li>
<li>SERIALIZABLE(可串行化)： 最高的隔离级别，完全服从ACID的隔离级别。所有的事务依次逐个执行，这样事务之间就完全不可能产生干扰，也就是说，该级别可以防止脏读、不可重复读以及幻读。</li>
</ul>
<h2 id="锁"><a href="#锁" class="headerlink" title="锁"></a>锁</h2><h3 id="对MySQL的锁了解吗"><a href="#对MySQL的锁了解吗" class="headerlink" title="对MySQL的锁了解吗"></a>对MySQL的锁了解吗</h3><p>当数据库有并发事务的时候，可能会产生数据的不一致，这时候需要一些机制来保证访问的次序，锁机制就是这样的一个机制。</p>
<h3 id="隔离级别与锁的关系"><a href="#隔离级别与锁的关系" class="headerlink" title="隔离级别与锁的关系"></a>隔离级别与锁的关系</h3><p>在Read Uncommitted级别下，读取数据不需要加共享锁，这样就不会跟被修改的数据上的排他锁冲突</p>
<p>在Read Committed级别下，读操作需要加共享锁，但是在语句执行完以后释放共享锁；</p>
<p>在Repeatable Read级别下，读操作需要加共享锁，但是在事务提交之前并不释放共享锁，也就是必须等待事务执行完毕以后才释放共享锁。</p>
<p>SERIALIZABLE 是限制性最强的隔离级别，因为该级别锁定整个范围的键，并一直持有锁，直到事务完成。</p>
<h3 id="按照锁的粒度分数据库锁有哪些？锁机制与InnoDB锁算法"><a href="#按照锁的粒度分数据库锁有哪些？锁机制与InnoDB锁算法" class="headerlink" title="按照锁的粒度分数据库锁有哪些？锁机制与InnoDB锁算法"></a>按照锁的粒度分数据库锁有哪些？锁机制与InnoDB锁算法</h3><p>在关系型数据库中，可以<strong>按照锁的粒度把数据库锁分</strong>为行级锁(INNODB引擎)、表级锁(MYISAM引擎)和页级锁(BDB引擎 )。</p>
<p><strong>MyISAM和InnoDB存储引擎使用的锁：</strong></p>
<ul>
<li>MyISAM采用表级锁(table-level locking)。</li>
<li>InnoDB支持行级锁(row-level locking)和表级锁，默认为行级锁</li>
</ul>
<p>行级锁，表级锁和页级锁对比</p>
<p>行级锁 行级锁是Mysql中锁定粒度最细的一种锁，表示只针对当前操作的行进行加锁。行级锁能大大减少数据库操作的冲突。其加锁粒度最小，但加锁的开销也最大。行级锁分为共享锁 和 排他锁。</p>
<p>特点：开销大，加锁慢；会出现死锁；锁定粒度最小，发生锁冲突的概率最低，并发度也最高。</p>
<p>表级锁 表级锁是MySQL中锁定粒度最大的一种锁，表示对当前操作的整张表加锁，它实现简单，资源消耗较少，被大部分MySQL引擎支持。最常使用的MYISAM与INNODB都支持表级锁定。表级锁定分为表共享读锁（共享锁）与表独占写锁（排他锁）。</p>
<p>特点：开销小，加锁快；不会出现死锁；锁定粒度大，发出锁冲突的概率最高，并发度最低。</p>
<p>页级锁 页级锁是MySQL中锁定粒度介于行级锁和表级锁中间的一种锁。表级锁速度快，但冲突多，行级冲突少，但速度慢。所以取了折衷的页级，一次锁定相邻的一组记录。</p>
<p>特点：开销和加锁时间界于表锁和行锁之间；会出现死锁；锁定粒度界于表锁和行锁之间，并发度一般</p>
<h3 id="从锁的类别上分MySQL都有哪些锁呢？像上面那样子进行锁定岂不是有点阻碍并发效率了"><a href="#从锁的类别上分MySQL都有哪些锁呢？像上面那样子进行锁定岂不是有点阻碍并发效率了" class="headerlink" title="从锁的类别上分MySQL都有哪些锁呢？像上面那样子进行锁定岂不是有点阻碍并发效率了"></a>从锁的类别上分MySQL都有哪些锁呢？像上面那样子进行锁定岂不是有点阻碍并发效率了</h3><p>从锁的类别上来讲，有共享锁和排他锁。</p>
<p>共享锁: 又叫做读锁。 当用户要进行数据的读取时，对数据加上共享锁。共享锁可以同时加上多个。</p>
<p>排他锁: 又叫做写锁。 当用户要进行数据的写入时，对数据加上排他锁。排他锁只可以加一个，他和其他的排他锁，共享锁都相斥。</p>
<p>用上面的例子来说就是用户的行为有两种，一种是来看房，多个用户一起看房是可以接受的。 一种是真正的入住一晚，在这期间，无论是想入住的还是想看房的都不可以。</p>
<p>锁的粒度取决于具体的存储引擎，InnoDB实现了行级锁，页级锁，表级锁。</p>
<p>他们的加锁开销从大到小，并发能力也是从大到小。</p>
<h3 id="MySQL中InnoDB引擎的行锁是怎么实现的？"><a href="#MySQL中InnoDB引擎的行锁是怎么实现的？" class="headerlink" title="MySQL中InnoDB引擎的行锁是怎么实现的？"></a>MySQL中InnoDB引擎的行锁是怎么实现的？</h3><p>InnoDB是基于索引来完成行锁</p>
<p>例: select * from tab_with_index where id = 1 for update;</p>
<p>for update 可以根据条件来完成行锁锁定，并且 id 是有索引键的列，如果 id 不是索引键那么InnoDB将完成表锁，并发将无从谈起</p>
<h3 id="InnoDB存储引擎的锁的算法有三种"><a href="#InnoDB存储引擎的锁的算法有三种" class="headerlink" title="InnoDB存储引擎的锁的算法有三种"></a>InnoDB存储引擎的锁的算法有三种</h3><ul>
<li>Record lock：单个行记录上的锁</li>
<li>Gap lock：间隙锁，锁定一个范围，不包括记录本身</li>
<li>Next-key lock：record+gap 锁定一个范围，包含记录本身</li>
</ul>
<p>相关知识点：</p>
<ul>
<li>innodb对于行的查询使用next-key lock</li>
<li>Next-locking keying为了解决Phantom Problem幻读问题</li>
<li>当查询的索引含有唯一属性时，将next-key lock降级为record key</li>
<li>Gap锁设计的目的是为了阻止多个事务将记录插入到同一范围内，而这会导致幻读问题的产生</li>
<li>有两种方式显式关闭gap锁：（除了外键约束和唯一性检查外，其余情况仅使用record lock） A. 将事务隔离级别设置为RC B. 将参数innodb_locks_unsafe_for_binlog设置为1</li>
</ul>
<h3 id="什么是死锁？怎么解决？"><a href="#什么是死锁？怎么解决？" class="headerlink" title="什么是死锁？怎么解决？"></a>什么是死锁？怎么解决？</h3><p>死锁是指两个或多个事务在同一资源上相互占用，并请求锁定对方的资源，从而导致恶性循环的现象。</p>
<p>常见的解决死锁的方法</p>
<p>1、如果不同程序会并发存取多个表，尽量约定以相同的顺序访问表，可以大大降低死锁机会。</p>
<p>2、在同一个事务中，尽可能做到一次锁定所需要的所有资源，减少死锁产生概率；</p>
<p>3、对于非常容易产生死锁的业务部分，可以尝试使用升级锁定颗粒度，通过表级锁定来减少死锁产生的概率；</p>
<p>如果业务处理不好可以用分布式事务锁或者使用乐观锁</p>
<h3 id="数据库的乐观锁和悲观锁是什么？怎么实现的？"><a href="#数据库的乐观锁和悲观锁是什么？怎么实现的？" class="headerlink" title="数据库的乐观锁和悲观锁是什么？怎么实现的？"></a>数据库的乐观锁和悲观锁是什么？怎么实现的？</h3><p>数据库管理系统（DBMS）中的并发控制的任务是确保在多个事务同时存取数据库中同一数据时不破坏事务的隔离性和统一性以及数据库的统一性。乐观并发控制（乐观锁）和悲观并发控制（悲观锁）是并发控制主要采用的技术手段。</p>
<p>悲观锁：假定会发生并发冲突，屏蔽一切可能违反数据完整性的操作。在查询完数据的时候就把事务锁起来，直到提交事务。实现方式：使用数据库中的锁机制</p>
<p>乐观锁：假设不会发生并发冲突，只在提交操作时检查是否违反数据完整性。在修改数据的时候把事务锁起来，通过version的方式来进行锁定。实现方式：乐一般会使用版本号机制或CAS算法实现。</p>
<p>两种锁的使用场景</p>
<p>从上面对两种锁的介绍，我们知道两种锁各有优缺点，不可认为一种好于另一种，像乐观锁适用于写比较少的情况下（多读场景），即冲突真的很少发生的时候，这样可以省去了锁的开销，加大了系统的整个吞吐量。</p>
<p>但如果是多写的情况，一般会经常产生冲突，这就会导致上层应用会不断的进行retry，这样反倒是降低了性能，所以一般多写的场景下用悲观锁就比较合适。</p>
<h2 id="视图"><a href="#视图" class="headerlink" title="视图"></a>视图</h2><h3 id="为什么要使用视图？什么是视图？"><a href="#为什么要使用视图？什么是视图？" class="headerlink" title="为什么要使用视图？什么是视图？"></a>为什么要使用视图？什么是视图？</h3><p>为了提高复杂SQL语句的复用性和表操作的安全性，MySQL数据库管理系统提供了视图特性。所谓视图，本质上是一种虚拟表，在物理上是不存在的，其内容与真实的表相似，包含一系列带有名称的列和行数据。但是，视图并不在数据库中以储存的数据值形式存在。行和列数据来自定义视图的查询所引用基本表，并且在具体引用视图时动态生成。</p>
<p>视图使开发者只关心感兴趣的某些特定数据和所负责的特定任务，只能看到视图中所定义的数据，而不是视图所引用表中的数据，从而提高了数据库中数据的安全性。</p>
<h3 id="视图有哪些特点？"><a href="#视图有哪些特点？" class="headerlink" title="视图有哪些特点？"></a>视图有哪些特点？</h3><p>视图的特点如下:</p>
<ul>
<li>视图的列可以来自不同的表，是表的抽象和在逻辑意义上建立的新关系。</li>
<li>视图是由基本表(实表)产生的表(虚表)。</li>
<li>视图的建立和删除不影响基本表。</li>
<li>对视图内容的更新(添加，删除和修改)直接影响基本表。</li>
<li>当视图来自多个基本表时，不允许添加和删除数据。</li>
</ul>
<p>视图的操作包括创建视图，查看视图，删除视图和修改视图。</p>
<h3 id="视图的使用场景有哪些？"><a href="#视图的使用场景有哪些？" class="headerlink" title="视图的使用场景有哪些？"></a>视图的使用场景有哪些？</h3><p>视图根本用途：简化sql查询，提高开发效率。如果说还有另外一个用途那就是兼容老的表结构。</p>
<p>下面是视图的常见使用场景：</p>
<ul>
<li>重用SQL语句；</li>
<li>简化复杂的SQL操作。在编写查询后，可以方便的重用它而不必知道它的基本查询细节；</li>
<li>使用表的组成部分而不是整个表；</li>
<li>保护数据。可以给用户授予表的特定部分的访问权限而不是整个表的访问权限；</li>
<li>更改数据格式和表示。视图可返回与底层表的表示和格式不同的数据。</li>
</ul>
<h3 id="视图的优点"><a href="#视图的优点" class="headerlink" title="视图的优点"></a>视图的优点</h3><p>查询简单化。视图能简化用户的操作<br>数据安全性。视图使用户能以多种角度看待同一数据，能够对机密数据提供安全保护<br>逻辑数据独立性。视图对重构数据库提供了一定程度的逻辑独立性</p>
<h3 id="视图的缺点"><a href="#视图的缺点" class="headerlink" title="视图的缺点"></a>视图的缺点</h3><ol>
<li><p>性能。数据库必须把视图的查询转化成对基本表的查询，如果这个视图是由一个复杂的多表查询所定义，那么，即使是视图的一个简单查询，数据库也把它变成一个复杂的结合体，需要花费一定的时间。</p>
</li>
<li><p>修改限制。当用户试图修改视图的某些行时，数据库必须把它转化为对基本表的某些行的修改。事实上，当从视图中插入或者删除时，情况也是这样。对于简单视图来说，这是很方便的，但是，对于比较复杂的视图，可能是不可修改的</p>
<p>这些视图有如下特征：1.有UNIQUE等集合操作符的视图。2.有GROUP BY子句的视图。3.有诸如AVG\SUM\MAX等聚合函数的视图。 4.使用DISTINCT关键字的视图。5.连接表的视图（其中有些例外）</p>
</li>
</ol>
<h3 id="什么是游标？"><a href="#什么是游标？" class="headerlink" title="什么是游标？"></a>什么是游标？</h3><p>游标是系统为用户开设的一个数据缓冲区，存放SQL语句的执行结果，每个游标区都有一个名字。用户可以通过游标逐一获取记录并赋给主变量，交由主语言进一步处理。</p>
<h2 id="存储过程与函数"><a href="#存储过程与函数" class="headerlink" title="存储过程与函数"></a>存储过程与函数</h2><h3 id="什么是存储过程？有哪些优缺点？"><a href="#什么是存储过程？有哪些优缺点？" class="headerlink" title="什么是存储过程？有哪些优缺点？"></a>什么是存储过程？有哪些优缺点？</h3><p>存储过程是一个预编译的SQL语句，优点是允许模块化的设计，就是说只需要创建一次，以后在该程序中就可以调用多次。如果某次操作需要执行多次SQL，使用存储过程比单纯SQL语句执行要快。</p>
<p>优点</p>
<p>1）存储过程是预编译过的，执行效率高。</p>
<p>2）存储过程的代码直接存放于数据库中，通过存储过程名直接调用，减少网络通讯。</p>
<p>3）安全性高，执行存储过程需要有一定权限的用户。</p>
<p>4）存储过程可以重复使用，减少数据库开发人员的工作量。</p>
<p>缺点</p>
<p>1）调试麻烦，但是用 PL/SQL Developer 调试很方便！弥补这个缺点。</p>
<p>2）移植问题，数据库端代码当然是与数据库相关的。但是如果是做工程型项目，基本不存在移植问题。</p>
<p>3）重新编译问题，因为后端代码是运行前编译的，如果带有引用关系的对象发生改变时，受影响的存储过程、包将需要重新编译（不过也可以设置成运行时刻自动编译）。</p>
<p>4）如果在一个程序系统中大量的使用存储过程，到程序交付使用的时候随着用户需求的增加会导致数据结构的变化，接着就是系统的相关问题了，最后如果用户想维护该系统可以说是很难很难、而且代价是空前的，维护起来更麻烦。</p>
<h2 id="触发器"><a href="#触发器" class="headerlink" title="触发器"></a>触发器</h2><h3 id="什么是触发器？触发器的使用场景有哪些？"><a href="#什么是触发器？触发器的使用场景有哪些？" class="headerlink" title="什么是触发器？触发器的使用场景有哪些？"></a>什么是触发器？触发器的使用场景有哪些？</h3><p>触发器是用户定义在关系表上的一类由事件驱动的特殊的存储过程。触发器是指一段代码，当触发某个事件时，自动执行这些代码。</p>
<p>使用场景</p>
<ul>
<li>可以通过数据库中的相关表实现级联更改。</li>
<li>实时监控某张表中的某个字段的更改而需要做出相应的处理。</li>
<li>例如可以生成某些业务的编号。</li>
<li>注意不要滥用，否则会造成数据库及应用程序的维护困难。</li>
</ul>
<h3 id="MySQL中都有哪些触发器？"><a href="#MySQL中都有哪些触发器？" class="headerlink" title="MySQL中都有哪些触发器？"></a>MySQL中都有哪些触发器？</h3><p>在MySQL数据库中有如下六种触发器：</p>
<ul>
<li>Before Insert</li>
<li>After Insert</li>
<li>Before Update</li>
<li>After Update</li>
<li>Before Delete</li>
<li>After Delete</li>
</ul>
<h2 id="常用SQL语句"><a href="#常用SQL语句" class="headerlink" title="常用SQL语句"></a>常用SQL语句</h2><h3 id="SQL语句主要分为哪几类"><a href="#SQL语句主要分为哪几类" class="headerlink" title="SQL语句主要分为哪几类"></a>SQL语句主要分为哪几类</h3><p>数据定义语言DDL（Data Ddefinition Language）CREATE，DROP，ALTER</p>
<p>主要为以上操作 即对逻辑结构等有操作的，其中包括表结构，视图和索引。</p>
<p>数据查询语言DQL（Data Query Language）SELECT</p>
<p>这个较为好理解 即查询操作，以select关键字。各种简单查询，连接查询等 都属于DQL。</p>
<p>数据操纵语言DML（Data Manipulation Language）INSERT，UPDATE，DELETE</p>
<p>主要为以上操作 即对数据进行操作的，对应上面所说的查询操作 DQL与DML共同构建了多数初级程序员常用的增删改查操作。而查询是较为特殊的一种 被划分到DQL中。</p>
<p>数据控制功能DCL（Data Control Language）GRANT，REVOKE，COMMIT，ROLLBACK</p>
<p>主要为以上操作 即对数据库安全性完整性等有操作的，可以简单的理解为权限控制等。</p>
<h3 id="超键、候选键、主键、外键分别是什么？"><a href="#超键、候选键、主键、外键分别是什么？" class="headerlink" title="超键、候选键、主键、外键分别是什么？"></a>超键、候选键、主键、外键分别是什么？</h3><p>超键：在关系中能唯一标识元组的属性集称为关系模式的超键。一个属性可以为作为一个超键，多个属性组合在一起也可以作为一个超键。超键包含候选键和主键。<br>候选键：是最小超键，即没有冗余元素的超键。<br>主键：数据库表中对储存数据对象予以唯一和完整标识的数据列或属性的组合。一个数据列只能有一个主键，且主键的取值不能缺失，即不能为空值（Null）。<br>外键：在一个表中存在的另一个表的主键称此表的外键。</p>
<h3 id="SQL-约束有哪几种？"><a href="#SQL-约束有哪几种？" class="headerlink" title="SQL 约束有哪几种？"></a>SQL 约束有哪几种？</h3><p>NOT NULL: 用于控制字段的内容一定不能为空（NULL）。<br>UNIQUE: 控件字段内容不能重复，一个表允许有多个 Unique 约束。<br>PRIMARY KEY: 也是用于控件字段内容不能重复，但它在一个表只允许出现一个。<br>FOREIGN KEY: 用于预防破坏表之间连接的动作，也能防止非法数据插入外键列，因为它必须是它指向的那个表中的值之一。<br>CHECK: 用于控制字段的值范围。</p>
<h3 id="六种关联查询"><a href="#六种关联查询" class="headerlink" title="六种关联查询"></a>六种关联查询</h3><ul>
<li>内连接（INNER JOIN）</li>
<li>外连接（LEFT JOIN/RIGHT JOIN）</li>
<li>联合查询（UNION与UNION ALL）</li>
<li>全连接（FULL JOIN）</li>
</ul>
<h3 id="什么是子查询"><a href="#什么是子查询" class="headerlink" title="什么是子查询"></a>什么是子查询</h3><ol>
<li>条件：一条SQL语句的查询结果做为另一条查询语句的条件或查询结果</li>
<li>嵌套：多条SQL语句嵌套使用，内部的SQL查询语句称为子查询。</li>
</ol>
<h3 id="mysql中-in-和-exists-区别"><a href="#mysql中-in-和-exists-区别" class="headerlink" title="mysql中 in 和 exists 区别"></a>mysql中 in 和 exists 区别</h3><p>mysql中的in语句是把外表和内表作hash 连接，而exists语句是对外表作loop循环，每次loop循环再对内表进行查询。一直大家都认为exists比in语句的效率要高，这种说法其实是不准确的。这个是要区分环境的。</p>
<ol>
<li>如果查询的两个表大小相当，那么用in和exists差别不大。</li>
<li>如果两个表中一个较小，一个是大表，则子查询表大的用exists，子查询表小的用in。</li>
<li>not in 和not exists：如果查询语句使用了not in，那么内外表都进行全表扫描，没有用到索引；而not extsts的子查询依然能用到表上的索引。所以无论那个表大，用not exists都比not in要快。</li>
</ol>
<h3 id="varchar与char的区别"><a href="#varchar与char的区别" class="headerlink" title="varchar与char的区别"></a>varchar与char的区别</h3><p>总之，结合性能角度（char更快）和节省磁盘空间角度（varchar更小），具体情况还需具体来设计数据库才是妥当的做法。</p>
<h3 id="varchar-50-中50的涵义"><a href="#varchar-50-中50的涵义" class="headerlink" title="varchar(50)中50的涵义"></a>varchar(50)中50的涵义</h3><p>最多存放50个字符，varchar(50)和(200)存储hello所占空间一样，但后者在排序时会消耗更多内存，因为order by col采用fixed_length计算col长度(memory引擎也一样)。在早期 MySQL 版本中， 50 代表字节数，现在代表字符数。</p>
<h3 id="int-20-中20的涵义"><a href="#int-20-中20的涵义" class="headerlink" title="int(20)中20的涵义"></a>int(20)中20的涵义</h3><p>是指显示字符的长度。20表示最大显示宽度为20，但仍占4字节存储，存储范围不变；</p>
<p>不影响内部存储，只是影响带 zerofill 定义的 int 时，前面补多少个 0，易于报表展示</p>
<h3 id="mysql为什么这么设计"><a href="#mysql为什么这么设计" class="headerlink" title="mysql为什么这么设计"></a>mysql为什么这么设计</h3><p>对大多数应用没有意义，只是规定一些工具用来显示字符的个数；int(1)和int(20)存储和计算均一样；</p>
<h3 id="mysql中int-10-和char-10-以及varchar-10-的区别"><a href="#mysql中int-10-和char-10-以及varchar-10-的区别" class="headerlink" title="mysql中int(10)和char(10)以及varchar(10)的区别"></a>mysql中int(10)和char(10)以及varchar(10)的区别</h3><ul>
<li><p>int(10)的10表示显示的数据的长度，不是存储数据的大小；chart(10)和varchar(10)的10表示存储数据的大小，即表示存储多少个字符。</p>
<p>int(10) 10位的数据长度 9999999999，占32个字节，int型4位<br>char(10) 10位固定字符串，不足补空格 最多10个字符<br>varchar(10) 10位可变字符串，不足补空格 最多10个字符</p>
</li>
<li><p>char(10)表示存储定长的10个字符，不足10个就用空格补齐，占用更多的存储空间</p>
</li>
<li><p>varchar(10)表示存储10个变长的字符，存储多少个就是多少个，空格也按一个字符存储，这一点是和char(10)的空格不同的，char(10)的空格表示占位不算一个字符</p>
</li>
</ul>
<h3 id="FLOAT和DOUBLE的区别是什么？"><a href="#FLOAT和DOUBLE的区别是什么？" class="headerlink" title="FLOAT和DOUBLE的区别是什么？"></a>FLOAT和DOUBLE的区别是什么？</h3><ul>
<li>FLOAT类型数据可以存储至多8位十进制数，并在内存中占4字节。</li>
<li>DOUBLE类型数据可以存储至多18位十进制数，并在内存中占8字节。</li>
</ul>
<h3 id="drop、delete与truncate的区别"><a href="#drop、delete与truncate的区别" class="headerlink" title="drop、delete与truncate的区别"></a>drop、delete与truncate的区别</h3><p>因此，在不再需要一张表的时候，用drop；在想删除部分数据行时候，用delete；在保留表而删除所有数据的时候用truncate。</p>
<h3 id="UNION与UNION-ALL的区别？"><a href="#UNION与UNION-ALL的区别？" class="headerlink" title="UNION与UNION ALL的区别？"></a>UNION与UNION ALL的区别？</h3><ul>
<li>如果使用UNION ALL，不会合并重复的记录行</li>
<li>效率 UNION 高于 UNION ALL</li>
</ul>
<h2 id="SQL优化"><a href="#SQL优化" class="headerlink" title="SQL优化"></a>SQL优化</h2><h3 id="如何定位及优化SQL语句的性能问题？创建的索引有没有被使用到-或者说怎么才可以知道这条语句运行很慢的原因？"><a href="#如何定位及优化SQL语句的性能问题？创建的索引有没有被使用到-或者说怎么才可以知道这条语句运行很慢的原因？" class="headerlink" title="如何定位及优化SQL语句的性能问题？创建的索引有没有被使用到?或者说怎么才可以知道这条语句运行很慢的原因？"></a>如何定位及优化SQL语句的性能问题？创建的索引有没有被使用到?或者说怎么才可以知道这条语句运行很慢的原因？</h3><p>对于低性能的SQL语句的定位，最重要也是最有效的方法就是使用执行计划，MySQL提供了explain命令来查看语句的执行计划。 我们知道，不管是哪种数据库，或者是哪种数据库引擎，在对一条SQL语句进行执行的过程中都会做很多相关的优化，对于查询语句，最重要的优化方式就是使用索引。 而执行计划，就是显示数据库引擎对于SQL语句的执行的详细情况，其中包含了是否使用索引，使用什么索引，使用的索引的相关信息等。<br>执行计划包含的信息 id 有一组数字组成。表示一个查询中各个子查询的执行顺序;</p>
<p>id相同执行顺序由上至下。<br>id不同，id值越大优先级越高，越先被执行。<br>id为null时表示一个结果集，不需要使用它查询，常出现在包含union等查询语句中。<br>select_type 每个子查询的查询类型，一些常见的查询类型。<br><strong>table</strong> 查询的数据表，当从衍生表中查数据时会显示 x 表示对应的执行计划id <strong>partitions</strong> 表分区、表创建的时候可以指定通过那个列进行表分区。 </p>
<p>type(非常重要，可以看到有没有走索引) 访问类型</p>
<ul>
<li>ALL 扫描全表数据</li>
<li>index 遍历索引</li>
<li>range 索引范围查找</li>
<li>index_subquery 在子查询中使用 ref</li>
<li>unique_subquery 在子查询中使用 eq_ref</li>
<li>ref_or_null 对Null进行索引的优化的 ref</li>
<li>fulltext 使用全文索引</li>
<li>ref 使用非唯一索引查找数据</li>
<li>eq_ref 在join查询中使用PRIMARY KEYorUNIQUE NOT NULL索引关联。</li>
</ul>
<p><strong>possible_keys</strong> 可能使用的索引，注意不一定会使用。查询涉及到的字段上若存在索引，则该索引将被列出来。当该列为 NULL时就要考虑当前的SQL是否需要优化了。</p>
<p>key 显示MySQL在查询中实际使用的索引，若没有使用索引，显示为NULL。</p>
<p>TIPS:查询中若使用了覆盖索引(覆盖索引：索引的数据覆盖了需要查询的所有数据)，则该索引仅出现在key列表中</p>
<p>key_length 索引长度</p>
<p>ref 表示上述表的连接匹配条件，即哪些列或常量被用于查找索引列上的值</p>
<p>rows 返回估算的结果集数目，并不是一个准确的值。</p>
<p>extra 的信息非常丰富，常见的有：</p>
<ul>
<li>Using index 使用覆盖索引</li>
<li>Using where 使用了用where子句来过滤结果集</li>
<li>Using filesort 使用文件排序，使用非索引列进行排序时出现，非常消耗性能，尽量优化。</li>
<li>Using temporary 使用了临时表 sql优化的目标可以参考阿里开发手册</li>
</ul>
<h3 id="SQL的生命周期？"><a href="#SQL的生命周期？" class="headerlink" title="SQL的生命周期？"></a>SQL的生命周期？</h3><ol>
<li>应用服务器与数据库服务器建立一个连接</li>
<li>数据库进程拿到请求sql</li>
<li>解析并生成执行计划，执行</li>
<li>读取数据到内存并进行逻辑处理</li>
<li>通过步骤一的连接，发送结果到客户端</li>
<li>关掉连接，释放资源</li>
</ol>
<h3 id="大表数据查询，怎么优化"><a href="#大表数据查询，怎么优化" class="headerlink" title="大表数据查询，怎么优化"></a>大表数据查询，怎么优化</h3><ol>
<li>优化shema、sql语句+索引；</li>
<li>第二加缓存，memcached, redis；</li>
<li>主从复制，读写分离；</li>
<li>垂直拆分，根据你模块的耦合度，将一个大的系统分为多个小的系统，也就是分布式系统；</li>
<li>水平切分，针对数据量大的表，这一步最麻烦，最能考验技术水平，要选择一个合理的sharding key, 为了有好的查询效率，表结构也要改动，做一定的冗余，应用也要改，sql中尽量带sharding key，将数据定位到限定的表上去查，而不是扫描全部的表；</li>
</ol>
<h3 id="超大分页怎么处理？"><a href="#超大分页怎么处理？" class="headerlink" title="超大分页怎么处理？"></a>超大分页怎么处理？</h3><p>超大的分页一般从两个方向上来解决.</p>
<ul>
<li>数据库层面,这也是我们主要集中关注的(虽然收效没那么大),类似于select * from table where age &gt; 20 limit 1000000,10这种查询其实也是有可以优化的余地的. 这条语句需要load1000000数据然后基本上全部丢弃,只取10条当然比较慢. 当时我们可以修改为select * from table where id in (select id from table where age &gt; 20 limit 1000000,10).这样虽然也load了一百万的数据,但是由于索引覆盖,要查询的所有字段都在索引中,所以速度会很快. 同时如果ID连续的好,我们还可以select * from table where id &gt; 1000000 limit 10,效率也是不错的,优化的可能性有许多种,但是核心思想都一样,就是减少load的数据.</li>
<li>从需求的角度减少这种请求…主要是不做类似的需求(直接跳转到几百万页之后的具体某一页.只允许逐页查看或者按照给定的路线走,这样可预测,可缓存)以及防止ID泄漏且连续被人恶意攻击.<br>解决超大分页,其实主要是靠缓存,可预测性的提前查到内容,缓存至redis等k-V数据库中,直接返回即可.</li>
</ul>
<h3 id="mysql-分页"><a href="#mysql-分页" class="headerlink" title="mysql 分页"></a>mysql 分页</h3><p>LIMIT 子句可以被用于强制 SELECT 语句返回指定的记录数。LIMIT 接受一个或两个数字参数。参数必须是一个整数常量。如果给定两个参数，第一个参数指定第一个返回记录行的偏移量，第二个参数指定返回记录行的最大数目。初始记录行的偏移量是 0(而不是 1)</p>
<h3 id="慢查询日志"><a href="#慢查询日志" class="headerlink" title="慢查询日志"></a>慢查询日志</h3><p>用于记录执行时间超过某个临界值的SQL日志，用于快速定位慢查询，为我们的优化做参考。</p>
<p>开启慢查询日志</p>
<p>配置项：slow_query_log</p>
<p>可以使用show variables like ‘slov_query_log’查看是否开启，如果状态值为OFF，可以使用set GLOBAL slow_query_log = on来开启，它会在datadir下产生一个xxx-slow.log的文件。</p>
<p>设置临界时间</p>
<p>配置项：long_query_time</p>
<p>查看：show VARIABLES like ‘long_query_time’，单位秒</p>
<p>设置：set long_query_time=0.5</p>
<p>实操时应该从长时间设置到短的时间，即将最慢的SQL优化掉</p>
<p>查看日志，一旦SQL超过了我们设置的临界时间就会被记录到xxx-slow.log中</p>
<h3 id="关心过业务系统里面的sql耗时吗？统计过慢查询吗？对慢查询都怎么优化过？"><a href="#关心过业务系统里面的sql耗时吗？统计过慢查询吗？对慢查询都怎么优化过？" class="headerlink" title="关心过业务系统里面的sql耗时吗？统计过慢查询吗？对慢查询都怎么优化过？"></a>关心过业务系统里面的sql耗时吗？统计过慢查询吗？对慢查询都怎么优化过？</h3><p>在业务系统中，除了使用主键进行的查询，其他的我都会在测试库上测试其耗时，慢查询的统计主要由运维在做，会定期将业务中的慢查询反馈给我们。</p>
<p>慢查询的优化首先要搞明白慢的原因是什么？ 是查询条件没有命中索引？是load了不需要的数据列？还是数据量太大？</p>
<p>所以优化也是针对这三个方向来的，</p>
<ul>
<li>首先分析语句，看看是否load了额外的数据，可能是查询了多余的行并且抛弃掉了，可能是加载了许多结果中并不需要的列，对语句进行分析以及重写。</li>
<li>分析语句的执行计划，然后获得其使用索引的情况，之后修改语句或者修改索引，使得语句可以尽可能的命中索引。</li>
<li>如果对语句的优化已经无法进行，可以考虑表中的数据量是否太大，如果是的话可以进行横向或者纵向的分表。</li>
</ul>
<h3 id="为什么要尽量设定一个主键？"><a href="#为什么要尽量设定一个主键？" class="headerlink" title="为什么要尽量设定一个主键？"></a>为什么要尽量设定一个主键？</h3><p>主键是数据库确保数据行在整张表唯一性的保障，即使业务上本张表没有主键，也建议添加一个自增长的ID列作为主键。设定了主键之后，在后续的删改查的时候可能更加快速以及确保操作数据范围安全。</p>
<h3 id="主键使用自增ID还是UUID？"><a href="#主键使用自增ID还是UUID？" class="headerlink" title="主键使用自增ID还是UUID？"></a>主键使用自增ID还是UUID？</h3><p>推荐使用自增ID，不要使用UUID。</p>
<p>因为在InnoDB存储引擎中，主键索引是作为聚簇索引存在的，也就是说，主键索引的B+树叶子节点上存储了主键索引以及全部的数据(按照顺序)，如果主键索引是自增ID，那么只需要不断向后排列即可，如果是UUID，由于到来的ID与原来的大小不确定，会造成非常多的数据插入，数据移动，然后导致产生很多的内存碎片，进而造成插入性能的下降。</p>
<p>总之，在数据量大一些的情况下，用自增主键性能会好一些。</p>
<p>关于主键是聚簇索引，如果没有主键，InnoDB会选择一个唯一键来作为聚簇索引，如果没有唯一键，会生成一个隐式的主键。</p>
<h3 id="字段为什么要求定义为not-null？"><a href="#字段为什么要求定义为not-null？" class="headerlink" title="字段为什么要求定义为not null？"></a>字段为什么要求定义为not null？</h3><p>null值会占用更多的字节，且会在程序中造成很多与预期不符的情况。</p>
<h3 id="如果要存储用户的密码散列，应该使用什么字段进行存储？"><a href="#如果要存储用户的密码散列，应该使用什么字段进行存储？" class="headerlink" title="如果要存储用户的密码散列，应该使用什么字段进行存储？"></a>如果要存储用户的密码散列，应该使用什么字段进行存储？</h3><p>密码散列，盐，用户身份证号等固定长度的字符串应该使用char而不是varchar来存储，这样可以节省空间且提高检索效率。</p>
<h3 id="优化查询过程中的数据访问"><a href="#优化查询过程中的数据访问" class="headerlink" title="优化查询过程中的数据访问"></a>优化查询过程中的数据访问</h3><ul>
<li>访问数据太多导致查询性能下降</li>
<li>确定应用程序是否在检索大量超过需要的数据，可能是太多行或列</li>
<li>确认MySQL服务器是否在分析大量不必要的数据行</li>
<li>避免犯如下SQL语句错误</li>
<li>查询不需要的数据。解决办法：使用limit解决</li>
<li>多表关联返回全部列。解决办法：指定列名</li>
<li>总是返回全部列。解决办法：避免使用SELECT *</li>
<li>重复查询相同的数据。解决办法：可以缓存数据，下次直接读取缓存</li>
<li>是否在扫描额外的记录。解决办法：</li>
<li>使用explain进行分析，如果发现查询需要扫描大量的数据，但只返回少数的行，可以通过如下技巧去优化：</li>
<li>使用索引覆盖扫描，把所有的列都放到索引中，这样存储引擎不需要回表获取对应行就可以返回结果。</li>
<li>改变数据库和表的结构，修改数据表范式</li>
<li>重写SQL语句，让优化器可以以更优的方式执行查询。</li>
</ul>
<h3 id="优化长难的查询语句"><a href="#优化长难的查询语句" class="headerlink" title="优化长难的查询语句"></a>优化长难的查询语句</h3><ul>
<li>一个复杂查询还是多个简单查询</li>
<li>MySQL内部每秒能扫描内存中上百万行数据，相比之下，响应数据给客户端就要慢得多</li>
<li>使用尽可能小的查询是好的，但是有时将一个大的查询分解为多个小的查询是很有必要的。</li>
<li>切分查询</li>
<li>将一个大的查询分为多个小的相同的查询</li>
<li>一次性删除1000万的数据要比一次删除1万，暂停一会的方案更加损耗服务器开销。</li>
<li>分解关联查询，让缓存的效率更高。</li>
<li>执行单个查询可以减少锁的竞争。</li>
<li>在应用层做关联更容易对数据库进行拆分。</li>
<li>查询效率会有大幅提升。</li>
<li>较少冗余记录的查询。</li>
</ul>
<h3 id="优化特定类型的查询语句"><a href="#优化特定类型的查询语句" class="headerlink" title="优化特定类型的查询语句"></a>优化特定类型的查询语句</h3><ul>
<li>count(<em>)会忽略所有的列，直接统计所有列数，不要使用count(列名)</em></li>
<li><em>MyISAM中，没有任何where条件的count(</em>)非常快。</li>
<li>当有where条件时，MyISAM的count统计不一定比其它引擎快。</li>
<li>可以使用explain查询近似值，用近似值替代count(*)</li>
<li>增加汇总表</li>
<li>使用缓存</li>
</ul>
<h3 id="优化关联查询"><a href="#优化关联查询" class="headerlink" title="优化关联查询"></a>优化关联查询</h3><ul>
<li>确定ON或者USING子句中是否有索引。</li>
<li>确保GROUP BY和ORDER BY只有一个表中的列，这样MySQL才有可能使用索引。</li>
</ul>
<h3 id="优化子查询"><a href="#优化子查询" class="headerlink" title="优化子查询"></a>优化子查询</h3><ul>
<li>用关联查询替代</li>
<li>优化GROUP BY和DISTINCT</li>
<li>这两种查询据可以使用索引来优化，是最有效的优化方法</li>
<li>关联查询中，使用标识列分组的效率更高</li>
<li>如果不需要ORDER BY，进行GROUP BY时加ORDER BY NULL，MySQL不会再进行文件排序。</li>
<li>WITH ROLLUP超级聚合，可以挪到应用程序处理</li>
</ul>
<h3 id="优化LIMIT分页"><a href="#优化LIMIT分页" class="headerlink" title="优化LIMIT分页"></a>优化LIMIT分页</h3><ul>
<li>LIMIT偏移量大的时候，查询效率较低</li>
<li>可以记录上次查询的最大ID，下次查询时直接根据该ID来查询</li>
</ul>
<h3 id="优化UNION查询"><a href="#优化UNION查询" class="headerlink" title="优化UNION查询"></a>优化UNION查询</h3><p>UNION ALL的效率高于UNION</p>
<h3 id="优化WHERE子句"><a href="#优化WHERE子句" class="headerlink" title="优化WHERE子句"></a>优化WHERE子句</h3><p>SQL语句优化的一些方法？</p>
<ol>
<li>对查询进行优化，应尽量避免全表扫描，首先应考虑在 where 及 order by 涉及的列上建立索引。</li>
<li>应尽量避免在 where 子句中对字段进行 null 值判断，否则将导致引擎放弃使用索引而进行全表扫描<br>– 可以在num上设置默认值0，确保表中num列没有null值</li>
<li>应尽量避免在 where 子句中使用!=或&lt;&gt;操作符，否则引擎将放弃使用索引而进行全表扫描。</li>
<li>应尽量避免在 where 子句中使用or 来连接条件，否则将导致引擎放弃使用索引而进行全表扫描</li>
<li>in 和 not in 也要慎用，否则会导致全表扫描</li>
<li>下面的查询也将导致全表扫描：select id from t where name like ‘%李%’若要提高效率，可以考虑全文检索</li>
<li>如果在 where 子句中使用参数，也会导致全表扫描。因为SQL只有在运行时才会解析局部变量，但优化程序不能将访问计划的选择推迟到运行时；它必须在编译时进行选择。然 而，如果在编译时建立访问计划，变量的值还是未知的，因而无法作为索引选择的输入项。</li>
<li>应尽量避免在 where 子句中对字段进行表达式操作，这将导致引擎放弃使用索引而进行全表扫描。</li>
<li>应尽量避免在where子句中对字段进行函数操作，这将导致引擎放弃使用索引而进行全表扫描。</li>
<li>不要在 where 子句中的“=”左边进行函数、算术运算或其他表达式运算，否则系统将可能无法正确使用索引。</li>
</ol>
<h2 id="数据库优化"><a href="#数据库优化" class="headerlink" title="数据库优化"></a>数据库优化</h2><h3 id="为什么要优化"><a href="#为什么要优化" class="headerlink" title="为什么要优化"></a>为什么要优化</h3><ul>
<li>系统的吞吐量瓶颈往往出现在数据库的访问速度上</li>
<li>随着应用程序的运行，数据库的中的数据会越来越多，处理时间会相应变慢</li>
<li>数据是存放在磁盘上的，读写速度无法和内存相比</li>
</ul>
<p>优化原则：减少系统瓶颈，减少资源占用，增加系统的反应速度。</p>
<h3 id="数据库结构优化"><a href="#数据库结构优化" class="headerlink" title="数据库结构优化"></a>数据库结构优化</h3><p>一个好的数据库设计方案对于数据库的性能往往会起到事半功倍的效果。</p>
<p>需要考虑数据冗余、查询和更新的速度、字段的数据类型是否合理等多方面的内容。</p>
<p>将字段很多的表分解成多个表</p>
<p>对于字段较多的表，如果有些字段的使用频率很低，可以将这些字段分离出来形成新表。</p>
<p>因为当一个表的数据量很大时，会由于使用频率低的字段的存在而变慢。</p>
<p>增加中间表</p>
<p>对于需要经常联合查询的表，可以建立中间表以提高查询效率。</p>
<p>通过建立中间表，将需要通过联合查询的数据插入到中间表中，然后将原来的联合查询改为对中间表的查询。</p>
<p>增加冗余字段</p>
<p>设计数据表时应尽量遵循范式理论的规约，尽可能的减少冗余字段，让数据库设计看起来精致、优雅。但是，合理的加入冗余字段可以提高查询速度。</p>
<p>表的规范化程度越高，表和表之间的关系越多，需要连接查询的情况也就越多，性能也就越差。</p>
<p>注意：</p>
<p>冗余字段的值在一个表中修改了，就要想办法在其他表中更新，否则就会导致数据不一致的问题。</p>
<h3 id="MySQL数据库cpu飙升到500-的话他怎么处理？"><a href="#MySQL数据库cpu飙升到500-的话他怎么处理？" class="headerlink" title="MySQL数据库cpu飙升到500%的话他怎么处理？"></a>MySQL数据库cpu飙升到500%的话他怎么处理？</h3><p>当 cpu 飙升到 500%时，先用操作系统命令 top 命令观察是不是 mysqld 占用导致的，如果不是，找出占用高的进程，并进行相关处理。</p>
<p>如果是 mysqld 造成的， show processlist，看看里面跑的 session 情况，是不是有消耗资源的 sql 在运行。找出消耗高的 sql，看看执行计划是否准确， index 是否缺失，或者实在是数据量太大造成。</p>
<p>一般来说，肯定要 kill 掉这些线程(同时观察 cpu 使用率是否下降)，等进行相应的调整(比如说加索引、改 sql、改内存参数)之后，再重新跑这些 SQL。</p>
<p>也有可能是每个 sql 消耗资源并不多，但是突然之间，有大量的 session 连进来导致 cpu 飙升，这种情况就需要跟应用一起来分析为何连接数会激增，再做出相应的调整，比如说限制连接数等</p>
<h3 id="大表怎么优化？某个表有近千万数据，CRUD比较慢，如何优化？分库分表了是怎么做的？分表分库了有什么问题？有用到中间件么？他们的原理知道么？"><a href="#大表怎么优化？某个表有近千万数据，CRUD比较慢，如何优化？分库分表了是怎么做的？分表分库了有什么问题？有用到中间件么？他们的原理知道么？" class="headerlink" title="大表怎么优化？某个表有近千万数据，CRUD比较慢，如何优化？分库分表了是怎么做的？分表分库了有什么问题？有用到中间件么？他们的原理知道么？"></a>大表怎么优化？某个表有近千万数据，CRUD比较慢，如何优化？分库分表了是怎么做的？分表分库了有什么问题？有用到中间件么？他们的原理知道么？</h3><p>当MySQL单表记录数过大时，数据库的CRUD性能会明显下降，一些常见的优化措施如下：</p>
<ol>
<li><p>限定数据的范围： 务必禁止不带任何限制数据范围条件的查询语句。比如：我们当用户在查询订单历史的时候，我们可以控制在一个月的范围内；</p>
</li>
<li><p>读/写分离： 经典的数据库拆分方案，主库负责写，从库负责读；</p>
</li>
<li><p>缓存： 使用MySQL的缓存，另外对重量级、更新少的数据可以考虑使用应用级别的缓存；<br>还有就是通过分库分表的方式进行优化，主要有垂直分表和水平分表；</p>
</li>
<li><p><strong>垂直分区：</strong></p>
</li>
</ol>
<p><strong>根据数据库里面数据表的相关性进行拆分。</strong> 例如，用户表中既有用户的登录信息又有用户的基本信息，可以将用户表拆分成两个单独的表，甚至放到单独的库做分库。</p>
<p><strong>简单来说垂直拆分是指数据表列的拆分，把一张列比较多的表拆分为多张表。</strong> 如下图所示，这样来说大家应该就更容易理解了。</p>
<p><strong>垂直拆分的优点：</strong> 可以使得行数据变小，在查询时减少读取的Block数，减少I/O次数。此外，垂直分区可以简化表的结构，易于维护。</p>
<p><strong>垂直拆分的缺点：</strong> 主键会出现冗余，需要管理冗余列，并会引起Join操作，可以通过在应用层进行Join来解决。此外，垂直分区会让事务变得更加复杂；</p>
<h4 id="垂直分表"><a href="#垂直分表" class="headerlink" title="垂直分表"></a>垂直分表</h4><p>把主键和一些列放在一个表，然后把主键和另外的列放在另一个表中</p>
<p>适用场景</p>
<ol>
<li>如果一个表中某些列常用，另外一些列不常用</li>
<li>可以使数据行变小，一个数据页能存储更多数据，查询时减少I/O次数</li>
</ol>
<p>缺点</p>
<ol>
<li><p>有些分表的策略基于应用层的逻辑算法，一旦逻辑算法改变，整个分表逻辑都会改变，扩展性较差</p>
</li>
<li><p>对于应用层来说，逻辑算法增加开发成本</p>
</li>
<li><p>管理冗余列，查询所有数据需要join操作</p>
</li>
<li><p><strong>水平分区</strong>：</p>
</li>
</ol>
<p>保持数据表结构不变，通过某种策略存储数据分片。这样每一片数据分散到不同的表或者库中，达到了分布式的目的。 水平拆分可以支撑非常大的数据量。</p>
<p>水平拆分是指数据表行的拆分，表的行数超过200万行时，就会变慢，这时可以把一张的表的数据拆成多张表来存放。举个例子：我们可以将用户信息表拆分成多个用户信息表，这样就可以避免单一表数据量过大对性能造成影响。</p>
<p>水平拆分可以支持非常大的数据量。需要注意的一点是:分表仅仅是解决了单一表数据过大的问题，但由于表的数据还是在同一台机器上，其实对于提升MySQL并发能力没有什么意义，所以水平拆分最好分库 。</p>
<p>水平拆分能够支持非常大的数据量存储，应用端改造也少，但 分片事务难以解决 ，跨界点Join性能较差，逻辑复杂。</p>
<p>《Java工程师修炼之道》的作者推荐 尽量不要对数据进行分片，因为拆分会带来逻辑、部署、运维的各种复杂度 ，一般的数据表在优化得当的情况下支撑千万以下的数据量是没有太大问题的。如果实在要分片，尽量选择客户端分片架构，这样可以减少一次和中间件的网络I/O。</p>
<p>水平分表：<br>表很大，分割后可以降低在查询时需要读的数据和索引的页数，同时也降低了索引的层数，提高查询次数</p>
<p>适用场景</p>
<ol>
<li>表中的数据本身就有独立性，例如表中分表记录各个地区的数据或者不同时期的数据，特别是有些数据常用，有些不常用。</li>
<li>需要把数据存放在多个介质上。</li>
</ol>
<p>水平切分的缺点</p>
<ol>
<li>给应用增加复杂度，通常查询时需要多个表名，查询所有数据都需UNION操作</li>
<li>在许多数据库应用中，这种复杂度会超过它带来的优点，查询时会增加读一个索引层的磁盘次数</li>
</ol>
<p>下面补充一下数据库分片的两种常见方案：</p>
<ol>
<li>客户端代理： 分片逻辑在应用端，封装在jar包中，通过修改或者封装JDBC层来实现。 当当网的 Sharding-JDBC 、阿里的TDDL是两种比较常用的实现。</li>
<li>中间件代理： 在应用和数据中间加了一个代理层。分片逻辑统一维护在中间件服务中。 我们现在谈的 Mycat 、360的Atlas、网易的DDB等等都是这种架构的实现。</li>
</ol>
<p><strong>分库分表后面临的问题</strong></p>
<ul>
<li><p>事务支持 分库分表后，就成了分布式事务了。如果依赖数据库本身的分布式事务管理功能去执行事务，将付出高昂的性能代价； 如果由应用程序去协助控制，形成程序逻辑上的事务，又会造成编程方面的负担。</p>
</li>
<li><p>跨库join</p>
<p>只要是进行切分，跨节点Join的问题是不可避免的。但是良好的设计和切分却可以减少此类情况的发生。解决这一问题的普遍做法是分两次查询实现。在第一次查询的结果集中找出关联数据的id,根据这些id发起第二次请求得到关联数据。 分库分表方案产品</p>
</li>
<li><p>跨节点的count,order by,group by以及聚合函数问题 这些是一类问题，因为它们都需要基于全部数据集合进行计算。多数的代理都不会自动处理合并工作。解决方案：与解决跨节点join问题的类似，分别在各个节点上得到结果后在应用程序端进行合并。和join不同的是每个结点的查询可以并行执行，因此很多时候它的速度要比单一大表快很多。但如果结果集很大，对应用程序内存的消耗是一个问题。</p>
</li>
<li><p>数据迁移，容量规划，扩容等问题 来自淘宝综合业务平台团队，它利用对2的倍数取余具有向前兼容的特性（如对4取余得1的数对2取余也是1）来分配数据，避免了行级别的数据迁移，但是依然需要进行表级别的迁移，同时对扩容规模和分表数量都有限制。总得来说，这些方案都不是十分的理想，多多少少都存在一些缺点，这也从一个侧面反映出了Sharding扩容的难度。</p>
</li>
<li><p>ID问题</p>
</li>
<li><p>一旦数据库被切分到多个物理结点上，我们将不能再依赖数据库自身的主键生成机制。一方面，某个分区数据库自生成的ID无法保证在全局上是唯一的；另一方面，应用程序在插入数据之前需要先获得ID,以便进行SQL路由. 一些常见的主键生成策略</p>
<p>UUID 使用UUID作主键是最简单的方案，但是缺点也是非常明显的。由于UUID非常的长，除占用大量存储空间外，最主要的问题是在索引上，在建立索引和基于索引进行查询时都存在性能问题。 Twitter的分布式自增ID算法Snowflake 在分布式系统中，需要生成全局UID的场合还是比较多的，twitter的snowflake解决了这种需求，实现也还是很简单的，除去配置信息，核心代码就是毫秒级时间41位 机器ID 10位 毫秒内序列12位。</p>
</li>
<li><p>跨分片的排序分页</p>
<p>一般来讲，分页时需要按照指定字段进行排序。当排序字段就是分片字段的时候，我们通过分片规则可以比较容易定位到指定的分片，而当排序字段非分片字段的时候，情况就会变得比较复杂了。为了最终结果的准确性，我们需要在不同的分片节点中将数据进行排序并返回，并将不同分片返回的结果集进行汇总和再次排序，最后再返回给用户。</p>
</li>
</ul>
<h3 id="MySQL的复制原理以及流程"><a href="#MySQL的复制原理以及流程" class="headerlink" title="MySQL的复制原理以及流程"></a>MySQL的复制原理以及流程</h3><p>主从复制：将主数据库中的DDL和DML操作通过二进制日志（BINLOG）传输到从数据库上，然后将这些日志重新执行（重做）；从而使得从数据库的数据与主数据库保持一致。</p>
<p>主从复制的作用</p>
<ol>
<li>主数据库出现问题，可以切换到从数据库。</li>
<li>可以进行数据库层面的读写分离。</li>
<li>可以在从数据库上进行日常备份。</li>
</ol>
<p>MySQL主从复制解决的问题</p>
<ul>
<li>数据分布：随意开始或停止复制，并在不同地理位置分布数据备份</li>
<li>负载均衡：降低单个服务器的压力</li>
<li>高可用和故障切换：帮助应用程序避免单点失败</li>
<li>升级测试：可以用更高版本的MySQL作为从库</li>
</ul>
<p>MySQL主从复制工作原理</p>
<ul>
<li>在主库上把数据更新记录到二进制日志</li>
<li>从库将主库的日志复制到自己的中继日志</li>
<li>从库读取中继日志的事件，将其重放到从库数据中</li>
</ul>
<p>基本原理流程，3个线程以及之间的关联</p>
<p>主：binlog线程——记录下所有改变了数据库数据的语句，放进master上的binlog中；</p>
<p>从：io线程——在使用start slave 之后，负责从master上拉取 binlog 内容，放进自己的relay log中；</p>
<p>从：sql执行线程——执行relay log中的语句；</p>
<h3 id="读写分离有哪些解决方案？"><a href="#读写分离有哪些解决方案？" class="headerlink" title="读写分离有哪些解决方案？"></a>读写分离有哪些解决方案？</h3><p>读写分离是依赖于主从复制，而主从复制又是为读写分离服务的。因为主从复制要求slave不能写只能读（如果对slave执行写操作，那么show slave status将会呈现Slave_SQL_Running=NO，此时你需要按照前面提到的手动同步一下slave）。</p>
<p>方案一</p>
<p>使用mysql-proxy代理</p>
<p>优点：直接实现读写分离和负载均衡，不用修改代码，master和slave用一样的帐号，mysql官方不建议实际生产中使用</p>
<p>缺点：降低性能， 不支持事务</p>
<p>方案二</p>
<p>使用AbstractRoutingDataSource+aop+annotation在dao层决定数据源。<br>如果采用了mybatis， 可以将读写分离放在ORM层，比如mybatis可以通过mybatis plugin拦截sql语句，所有的insert/update/delete都访问master库，所有的select 都访问salve库，这样对于dao层都是透明。 plugin实现时可以通过注解或者分析语句是读写方法来选定主从库。不过这样依然有一个问题， 也就是不支持事务， 所以我们还需要重写一下DataSourceTransactionManager， 将read-only的事务扔进读库， 其余的有读有写的扔进写库。</p>
<p>方案三</p>
<p>使用AbstractRoutingDataSource+aop+annotation在service层决定数据源，可以支持事务.</p>
<p>缺点：类内部方法通过this.xx()方式相互调用时，aop不会进行拦截，需进行特殊处理。</p>
<h3 id="备份计划，mysqldump以及xtranbackup的实现原理"><a href="#备份计划，mysqldump以及xtranbackup的实现原理" class="headerlink" title="备份计划，mysqldump以及xtranbackup的实现原理"></a>备份计划，mysqldump以及xtranbackup的实现原理</h3><p>(1)备份计划</p>
<p>视库的大小来定，一般来说 100G 内的库，可以考虑使用 mysqldump 来做，因为 mysqldump更加轻巧灵活，备份时间选在业务低峰期，可以每天进行都进行全量备份(mysqldump 备份出来的文件比较小，压缩之后更小)。</p>
<p>100G 以上的库，可以考虑用 xtranbackup 来做，备份速度明显要比 mysqldump 要快。一般是选择一周一个全备，其余每天进行增量备份，备份时间为业务低峰期。</p>
<p>(2)备份恢复时间</p>
<p>物理备份恢复快，逻辑备份恢复慢</p>
<p>这里跟机器，尤其是硬盘的速率有关系，以下列举几个仅供参考</p>
<p>20G的2分钟（mysqldump）</p>
<p>80G的30分钟(mysqldump)</p>
<p>111G的30分钟（mysqldump)</p>
<p>288G的3小时（xtra)</p>
<p>3T的4小时（xtra)</p>
<p>逻辑导入时间一般是备份时间的5倍以上</p>
<p>(3)备份恢复失败如何处理</p>
<p>首先在恢复之前就应该做足准备工作，避免恢复的时候出错。比如说备份之后的有效性检查、权限检查、空间检查等。如果万一报错，再根据报错的提示来进行相应的调整。</p>
<p>(4)mysqldump和xtrabackup实现原理</p>
<p>mysqldump</p>
<p>mysqldump 属于逻辑备份。加入–single-transaction 选项可以进行一致性备份。后台进程会先设置 session 的事务隔离级别为 RR(SET SESSION TRANSACTION ISOLATION LEVELREPEATABLE READ)，之后显式开启一个事务(START TRANSACTION /*!40100 WITH CONSISTENTSNAPSHOT */)，这样就保证了该事务里读到的数据都是事务事务时候的快照。之后再把表的数据读取出来。如果加上–master-data=1 的话，在刚开始的时候还会加一个数据库的读锁(FLUSH TABLES WITH READ LOCK),等开启事务后，再记录下数据库此时 binlog 的位置(showmaster status)，马上解锁，再读取表的数据。等所有的数据都已经导完，就可以结束事务</p>
<p>Xtrabackup:</p>
<p>xtrabackup 属于物理备份，直接拷贝表空间文件，同时不断扫描产生的 redo 日志并保存下来。最后完成 innodb 的备份后，会做一个 flush engine logs 的操作(老版本在有 bug，在5.6 上不做此操作会丢数据)，确保所有的 redo log 都已经落盘(涉及到事务的两阶段提交</p>
<p>概念，因为 xtrabackup 并不拷贝 binlog，所以必须保证所有的 redo log 都落盘，否则可能会丢最后一组提交事务的数据)。这个时间点就是 innodb 完成备份的时间点，数据文件虽然不是一致性的，但是有这段时间的 redo 就可以让数据文件达到一致性(恢复的时候做的事</p>
<p>情)。然后还需要 flush tables with read lock，把 myisam 等其他引擎的表给备份出来，备份完后解锁。这样就做到了完美的热备。</p>
<h3 id="数据表损坏的修复方式有哪些？"><a href="#数据表损坏的修复方式有哪些？" class="headerlink" title="数据表损坏的修复方式有哪些？"></a>数据表损坏的修复方式有哪些？</h3><p>使用 myisamchk 来修复，具体步骤：</p>
<p>1）修复前将mysql服务停止。<br>2）打开命令行方式，然后进入到mysql的/bin目录。<br>3）执行myisamchk –recover 数据库所在路径/*.MYI<br>使用repair table 或者 OPTIMIZE table命令来修复，REPAIR TABLE table_name 修复表 OPTIMIZE TABLE table_name 优化表 REPAIR TABLE 用于修复被破坏的表。 OPTIMIZE TABLE 用于回收闲置的数据库空间，当表上的数据行被删除时，所占据的磁盘空间并没有立即被回收，使用了OPTIMIZE TABLE命令后这些空间将被回收，并且对磁盘上的数据行进行重排（注意：是磁盘上，而非数据库）</p>
]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>mysql</tag>
      </tags>
  </entry>
  <entry>
    <title>Redis笔记</title>
    <url>/2020/06/30/Redis%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<h1 id="Redis"><a href="#Redis" class="headerlink" title="Redis"></a>Redis</h1><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><h3 id="什么是Redis"><a href="#什么是Redis" class="headerlink" title="什么是Redis"></a>什么是Redis</h3><p>Redis(Remote Dictionary Server) 是一个使用 C 语言编写的，开源的（BSD许可）高性能非关系型（NoSQL）的键值对数据库。</p>
<p>Redis 可以存储键和五种不同类型的值之间的映射。键的类型只能为字符串，值支持五种数据类型：字符串、列表、集合、散列表、有序集合。</p>
<p>与传统数据库不同的是 Redis 的数据是存在内存中的，所以读写速度非常快，因此 redis 被广泛应用于缓存方向，每秒可以处理超过 10万次读写操作，是已知性能最快的Key-Value DB。另外，Redis 也经常用来做分布式锁。除此之外，Redis 支持事务 、持久化、LUA脚本、LRU驱动事件、多种集群方案。</p>
<h3 id="Redis有哪些优缺点"><a href="#Redis有哪些优缺点" class="headerlink" title="Redis有哪些优缺点"></a>Redis有哪些优缺点</h3><p>优点</p>
<ul>
<li>读写性能优异， Redis能读的速度是110000次/s，写的速度是81000次/s。</li>
<li>支持数据持久化，支持AOF和RDB两种持久化方式。</li>
<li>支持事务，Redis的所有操作都是原子性的，同时Redis还支持对几个操作合并后的原子性执行。</li>
<li>数据结构丰富，除了支持string类型的value外还支持hash、set、zset、list等数据结构。</li>
<li>支持主从复制，主机会自动将数据同步到从机，可以进行读写分离。</li>
</ul>
<p>缺点</p>
<ul>
<li>数据库容量受到物理内存的限制，不能用作海量数据的高性能读写，因此Redis适合的场景主要局限在较小数据量的高性能操作和运算上。</li>
<li>Redis 不具备自动容错和恢复功能，主机从机的宕机都会导致前端部分读写请求失败，需要等待机器重启或者手动切换前端的IP才能恢复。</li>
<li>主机宕机，宕机前有部分数据未能及时同步到从机，切换IP后还会引入数据不一致的问题，降低了系统的可用性。</li>
<li>Redis 较难支持在线扩容，在集群容量达到上限时在线扩容会变得很复杂。为避免这一问题，运维人员在系统上线时必须确保有足够的空间，这对资源造成了很大的浪费。</li>
</ul>
<h3 id="为什么要用-Redis-为什么要用缓存"><a href="#为什么要用-Redis-为什么要用缓存" class="headerlink" title="为什么要用 Redis /为什么要用缓存"></a>为什么要用 Redis /为什么要用缓存</h3><p>主要从“高性能”和“高并发”这两点来看待这个问题。</p>
<p><strong>高性能</strong>：</p>
<p>假如用户第一次访问数据库中的某些数据。这个过程会比较慢，因为是从硬盘上读取的。将该用户访问的数据存在数缓存中，这样下一次再访问这些数据的时候就可以直接从缓存中获取了。操作缓存就是直接操作内存，所以速度相当快。如果数据库中的对应数据改变的之后，同步改变缓存中相应的数据即可！</p>
<p><strong>高并发：</strong></p>
<p>直接操作缓存能够承受的请求是远远大于直接访问数据库的，所以我们可以考虑把数据库中的部分数据转移到缓存中去，这样用户的一部分请求会直接到缓存这里而不用经过数据库。</p>
<h3 id="为什么要用-Redis-而不用-map-guava-做缓存"><a href="#为什么要用-Redis-而不用-map-guava-做缓存" class="headerlink" title="为什么要用 Redis 而不用 map/guava 做缓存?"></a>为什么要用 Redis 而不用 map/guava 做缓存?</h3><p>缓存分为本地缓存和分布式缓存。以 Java 为例，使用自带的 map 或者 guava 实现的是本地缓存，最主要的特点是轻量以及快速，生命周期随着 jvm 的销毁而结束，并且在多实例的情况下，每个实例都需要各自保存一份缓存，缓存不具有一致性。</p>
<p>使用 redis 或 memcached 之类的称为分布式缓存，在多实例的情况下，各实例共用一份缓存数据，缓存具有一致性。缺点是需要保持 redis 或 memcached服务的高可用，整个程序架构上较为复杂。</p>
<h3 id="Redis为什么这么快"><a href="#Redis为什么这么快" class="headerlink" title="Redis为什么这么快"></a>Redis为什么这么快</h3><p>1、完全基于内存，绝大部分请求是纯粹的内存操作，非常快速。数据存在内存中，类似于 HashMap，HashMap 的优势就是查找和操作的时间复杂度都是O(1)；</p>
<p>2、数据结构简单，对数据操作也简单，Redis 中的数据结构是专门进行设计的；</p>
<p>3、采用单线程，避免了不必要的上下文切换和竞争条件，也不存在多进程或者多线程导致的切换而消耗 CPU，不用去考虑各种锁的问题，不存在加锁释放锁操作，没有因为可能出现死锁而导致的性能消耗；</p>
<p>4、使用多路 I/O 复用模型，非阻塞 IO；</p>
<p>5、使用底层模型不同，它们之间底层实现方式以及与客户端之间通信的应用协议不一样，Redis 直接自己构建了 VM 机制 ，因为一般的系统调用系统函数的话，会浪费一定的时间去移动和请求；</p>
<h2 id="数据类型"><a href="#数据类型" class="headerlink" title="数据类型"></a>数据类型</h2><h3 id="Redis有哪些数据类型"><a href="#Redis有哪些数据类型" class="headerlink" title="Redis有哪些数据类型"></a>Redis有哪些数据类型</h3><p>Redis主要有5种数据类型，包括String，List，Set，Zset，Hash，满足大部分的使用要求</p>
<h3 id="Redis的应用场景"><a href="#Redis的应用场景" class="headerlink" title="Redis的应用场景"></a>Redis的应用场景</h3><p>总结一</p>
<p>计数器、缓存、会话缓存、全页缓存（FPC）、查找表、消息队列(发布/订阅功能)、分布式锁实现</p>
<p>其它</p>
<p>Set 可以实现交集、并集等操作，从而实现共同好友等功能。ZSet 可以实现有序性操作，从而实现排行榜等功能。</p>
<p>总结二</p>
<p>Redis相比其他缓存，有一个非常大的优势，就是支持多种数据类型。</p>
<p>数据类型说明string字符串，最简单的k-v存储hashhash格式，value为field和value，适合ID-Detail这样的场景。list简单的list，顺序列表，支持首位或者末尾插入数据set无序list，查找速度快，适合交集、并集、差集处理sorted set有序的set</p>
<p>其实，通过上面的数据类型的特性，基本就能想到合适的应用场景了。</p>
<p>string——适合最简单的k-v存储，类似于memcached的存储结构，短信验证码，配置信息等，就用这种类型来存储。</p>
<p>hash——一般key为ID或者唯一标示，value对应的就是详情了。如商品详情，个人信息详情，新闻详情等。</p>
<p>list——因为list是有序的，比较适合存储一些有序且数据相对固定的数据。如省市区表、字典表等。因为list是有序的，适合根据写入的时间来排序，如：最新的***，消息队列等。</p>
<p>set——可以简单的理解为ID-List的模式，如微博中一个人有哪些好友，set最牛的地方在于，可以对两个set提供交集、并集、差集操作。例如：查找两个人共同的好友等。</p>
<p>Sorted Set——是set的增强版本，增加了一个score参数，自动会根据score的值进行排序。比较适合类似于top 10等不根据插入的时间来排序的数据。</p>
<p>如上所述，虽然Redis不像关系数据库那么复杂的数据结构，但是，也能适合很多场景，比一般的缓存数据结构要多。了解每种数据结构适合的业务场景，不仅有利于提升开发效率，也能有效利用Redis的性能。</p>
<h2 id="持久化"><a href="#持久化" class="headerlink" title="持久化"></a>持久化</h2><h3 id="什么是Redis持久化？"><a href="#什么是Redis持久化？" class="headerlink" title="什么是Redis持久化？"></a>什么是Redis持久化？</h3><p>持久化就是把内存的数据写到磁盘中去，防止服务宕机了内存数据丢失。</p>
<h3 id="Redis-的持久化机制是什么？各自的优缺点？"><a href="#Redis-的持久化机制是什么？各自的优缺点？" class="headerlink" title="Redis 的持久化机制是什么？各自的优缺点？"></a>Redis 的持久化机制是什么？各自的优缺点？</h3><p>Redis 提供两种持久化机制 RDB（默认） 和 AOF 机制:</p>
<p>RDB：是Redis DataBase缩写快照</p>
<p>优点：</p>
<ul>
<li>1、只有一个文件 dump.rdb，方便持久化。</li>
<li>2、容灾性好，一个文件可以保存到安全的磁盘。</li>
<li>3、性能最大化，fork 子进程来完成写操作，让主进程继续处理命令，所以是 IO 最大化。使用单独子进程来进行持久化，主进程不会进行任何 IO 操作，保证了 redis 的高性能</li>
<li>4.相对于数据集大时，比 AOF 的启动效率更高。</li>
</ul>
<p>缺点：</p>
<ul>
<li>1、数据安全性低。RDB 是间隔一段时间进行持久化，如果持久化之间 redis 发生故障，会发生数据丢失。所以这种方式更适合数据要求不严谨的时候)</li>
</ul>
<p>AOF持久化(即Append Only File持久化)，则是将Redis执行的每次写命令记录到单独的日志文件中，当重启Redis会重新将持久化的日志中文件恢复数据。</p>
<p>当两种方式同时开启时，数据恢复Redis会优先选择AOF恢复。</p>
<p>优点：</p>
<p>1、数据安全，aof 持久化可以配置 appendfsync 属性，有 always，每进行一次 命令操作就记录到 aof 文件中一次。<br>2、通过 append 模式写文件，即使中途服务器宕机，可以通过 redis-check-aof 工具解决数据一致性问题。<br>3、AOF 机制的 rewrite 模式。AOF 文件没被 rewrite 之前（文件过大时会对命令 进行合并重写），可以删除其中的某些命令（比如误操作的 flushall）)<br>缺点：</p>
<p>1、AOF 文件比 RDB 文件大，且恢复速度慢。<br>2、数据集大的时候，比 rdb 启动效率低。</p>
<p>优缺点是什么？</p>
<ul>
<li>AOF文件比RDB更新频率高，优先使用AOF还原数据。</li>
<li>AOF比RDB更安全也更大</li>
<li>RDB性能比AOF好</li>
<li>如果两个都配了优先加载AOF</li>
</ul>
<h3 id="如何选择合适的持久化方式"><a href="#如何选择合适的持久化方式" class="headerlink" title="如何选择合适的持久化方式"></a>如何选择合适的持久化方式</h3><ul>
<li><p>一般来说， 如果想达到足以媲美PostgreSQL的数据安全性，你应该同时使用两种持久化功能。在这种情况下，当 Redis 重启的时候会优先载入AOF文件来恢复原始的数据，因为在通常情况下AOF文件保存的数据集要比RDB文件保存的数据集要完整。</p>
</li>
<li><p>如果你非常关心你的数据， 但仍然可以承受数分钟以内的数据丢失，那么你可以只使用RDB持久化。</p>
</li>
<li><p>有很多用户都只使用AOF持久化，但并不推荐这种方式，因为定时生成RDB快照（snapshot）非常便于进行数据库备份， 并且 RDB 恢复数据集的速度也要比AOF恢复的速度要快，除此之外，使用RDB还可以避免AOF程序的bug。</p>
</li>
<li><p>如果你只希望你的数据在服务器运行的时候存在，你也可以不使用任何持久化方式。</p>
</li>
</ul>
<h3 id="Redis持久化数据和缓存怎么做扩容？"><a href="#Redis持久化数据和缓存怎么做扩容？" class="headerlink" title="Redis持久化数据和缓存怎么做扩容？"></a>Redis持久化数据和缓存怎么做扩容？</h3><ul>
<li>如果Redis被当做缓存使用，使用一致性哈希实现动态扩容缩容。</li>
<li>如果Redis被当做一个持久化存储使用，必须使用固定的keys-to-nodes映射关系，节点的数量一旦确定不能变化。否则的话(即Redis节点需要动态变化的情况），必须使用可以在运行时进行数据再平衡的一套系统，而当前只有Redis集群可以做到这样。</li>
</ul>
<h2 id="过期键的删除策略"><a href="#过期键的删除策略" class="headerlink" title="过期键的删除策略"></a>过期键的删除策略</h2><h3 id="Redis的过期键的删除策略"><a href="#Redis的过期键的删除策略" class="headerlink" title="Redis的过期键的删除策略"></a>Redis的过期键的删除策略</h3><p>我们都知道，Redis是key-value数据库，我们可以设置Redis中缓存的key的过期时间。Redis的过期策略就是指当Redis中缓存的key过期了，Redis如何处理。</p>
<p>过期策略通常有以下三种：</p>
<ul>
<li>定时过期：每个设置过期时间的key都需要创建一个定时器，到过期时间就会立即清除。该策略可以立即清除过期的数据，对内存很友好；但是会占用大量的CPU资源去处理过期的数据，从而影响缓存的响应时间和吞吐量。</li>
<li>惰性过期：只有当访问一个key时，才会判断该key是否已过期，过期则清除。该策略可以最大化地节省CPU资源，却对内存非常不友好。极端情况可能出现大量的过期key没有再次被访问，从而不会被清除，占用大量内存。</li>
<li>定期过期：每隔一定的时间，会扫描一定数量的数据库的expires字典中一定数量的key，并清除其中已过期的key。该策略是前两者的一个折中方案。通过调整定时扫描的时间间隔和每次扫描的限定耗时，可以在不同情况下使得CPU和内存资源达到最优的平衡效果。<br>(expires字典会保存所有设置了过期时间的key的过期时间数据，其中，key是指向键空间中的某个键的指针，value是该键的毫秒精度的UNIX时间戳表示的过期时间。键空间是指该Redis集群中保存的所有键。)</li>
</ul>
<p>Redis中同时使用了惰性过期和定期过期两种过期策略。</p>
<h3 id="Redis-key的过期时间和永久有效分别怎么设置？"><a href="#Redis-key的过期时间和永久有效分别怎么设置？" class="headerlink" title="Redis key的过期时间和永久有效分别怎么设置？"></a>Redis key的过期时间和永久有效分别怎么设置？</h3><p>EXPIRE和PERSIST命令。</p>
<h3 id="我们知道通过expire来设置key-的过期时间，那么对过期的数据怎么处理呢"><a href="#我们知道通过expire来设置key-的过期时间，那么对过期的数据怎么处理呢" class="headerlink" title="我们知道通过expire来设置key 的过期时间，那么对过期的数据怎么处理呢?"></a>我们知道通过expire来设置key 的过期时间，那么对过期的数据怎么处理呢?</h3><p>除了缓存服务器自带的缓存失效策略之外（Redis默认的有6中策略可供选择），我们还可以根据具体的业务需求进行自定义的缓存淘汰，常见的策略有两种：</p>
<ol>
<li>定时去清理过期的缓存；</li>
<li>当有用户请求过来时，再判断这个请求所用到的缓存是否过期，过期的话就去底层系统得到新数据并更新缓存。</li>
</ol>
<h2 id="内存相关"><a href="#内存相关" class="headerlink" title="内存相关"></a>内存相关</h2><h3 id="MySQL里有2000w数据，redis中只存20w的数据，如何保证redis中的数据都是热点数据"><a href="#MySQL里有2000w数据，redis中只存20w的数据，如何保证redis中的数据都是热点数据" class="headerlink" title="MySQL里有2000w数据，redis中只存20w的数据，如何保证redis中的数据都是热点数据"></a>MySQL里有2000w数据，redis中只存20w的数据，如何保证redis中的数据都是热点数据</h3><p>redis内存数据集大小上升到一定大小的时候，就会施行数据淘汰策略。</p>
<h3 id="Redis的内存淘汰策略有哪些"><a href="#Redis的内存淘汰策略有哪些" class="headerlink" title="Redis的内存淘汰策略有哪些"></a>Redis的内存淘汰策略有哪些</h3><p>Redis的内存淘汰策略是指在Redis的用于缓存的内存不足时，怎么处理需要新写入且需要申请额外空间的数据。</p>
<p>全局的键空间选择性移除</p>
<ul>
<li>noeviction：当内存不足以容纳新写入数据时，新写入操作会报错。</li>
<li>allkeys-lru：当内存不足以容纳新写入数据时，在键空间中，移除最近最少使用的key。（这个是最常用的）</li>
<li>allkeys-random：当内存不足以容纳新写入数据时，在键空间中，随机移除某个key。</li>
</ul>
<p>设置过期时间的键空间选择性移除</p>
<ul>
<li>volatile-lru：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，移除最近最少使用的key。</li>
<li>volatile-random：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，随机移除某个key。</li>
<li>volatile-ttl：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，有更早过期时间的key优先移除。</li>
</ul>
<p>总结</p>
<p>Redis的内存淘汰策略的选取并不会影响过期的key的处理。内存淘汰策略用于处理内存不足时的需要申请额外空间的数据；过期策略用于处理过期的缓存数据。</p>
<h3 id="Redis主要消耗什么物理资源？"><a href="#Redis主要消耗什么物理资源？" class="headerlink" title="Redis主要消耗什么物理资源？"></a>Redis主要消耗什么物理资源？</h3><p>内存</p>
<h3 id="Redis的内存用完了会发生什么？"><a href="#Redis的内存用完了会发生什么？" class="headerlink" title="Redis的内存用完了会发生什么？"></a>Redis的内存用完了会发生什么？</h3><p>如果达到设置的上限，Redis的写命令会返回错误信息（但是读命令还可以正常返回。）或者你可以配置内存淘汰机制，当Redis达到内存上限时会冲刷掉旧的内容。</p>
<h3 id="Redis如何做内存优化？"><a href="#Redis如何做内存优化？" class="headerlink" title="Redis如何做内存优化？"></a>Redis如何做内存优化？</h3><p>可以好好利用Hash,list,sorted set,set等集合类型数据，因为通常情况下很多小的Key-Value可以用更紧凑的方式存放到一起。尽可能使用散列表（hashes），散列表（是说散列表里面存储的数少）使用的内存非常小，所以你应该尽可能的将你的数据模型抽象到一个散列表里面。比如你的web系统中有一个用户对象，不要为这个用户的名称，姓氏，邮箱，密码设置单独的key，而是应该把这个用户的所有信息存储到一张散列表里面。</p>
<h2 id="线程模型"><a href="#线程模型" class="headerlink" title="线程模型"></a>线程模型</h2><h3 id="Redis线程模型"><a href="#Redis线程模型" class="headerlink" title="Redis线程模型"></a>Redis线程模型</h3><p>Redis基于Reactor模式开发了网络事件处理器，这个处理器被称为文件事件处理器（file event handler）。它的组成结构为4部分：多个套接字、IO多路复用程序、文件事件分派器、事件处理器。因为文件事件分派器队列的消费是单线程的，所以Redis才叫单线程模型。</p>
<p>文件事件处理器使用 I/O 多路复用（multiplexing）程序来同时监听多个套接字， 并根据套接字目前执行的任务来为套接字关联不同的事件处理器。<br>当被监听的套接字准备好执行连接应答（accept）、读取（read）、写入（write）、关闭（close）等操作时， 与操作相对应的文件事件就会产生， 这时文件事件处理器就会调用套接字之前关联好的事件处理器来处理这些事件。<br>虽然文件事件处理器以单线程方式运行， 但通过使用 I/O 多路复用程序来监听多个套接字， 文件事件处理器既实现了高性能的网络通信模型， 又可以很好地与 redis 服务器中其他同样以单线程方式运行的模块进行对接， 这保持了 Redis 内部单线程设计的简单性。</p>
<h2 id="事务"><a href="#事务" class="headerlink" title="事务"></a>事务</h2><h3 id="什么是事务？"><a href="#什么是事务？" class="headerlink" title="什么是事务？"></a>什么是事务？</h3><p>事务是一个单独的隔离操作：事务中的所有命令都会序列化、按顺序地执行。事务在执行的过程中，不会被其他客户端发送来的命令请求所打断。</p>
<p>事务是一个原子操作：事务中的命令要么全部被执行，要么全部都不执行。</p>
<h3 id="Redis事务的概念"><a href="#Redis事务的概念" class="headerlink" title="Redis事务的概念"></a>Redis事务的概念</h3><p>Redis 事务的本质是通过MULTI、EXEC、WATCH等一组命令的集合。事务支持一次执行多个命令，一个事务中所有命令都会被序列化。在事务执行过程，会按照顺序串行化执行队列中的命令，其他客户端提交的命令请求不会插入到事务执行命令序列中。</p>
<p>总结说：redis事务就是一次性、顺序性、排他性的执行一个队列中的一系列命令。</p>
<h3 id="Redis事务的三个阶段"><a href="#Redis事务的三个阶段" class="headerlink" title="Redis事务的三个阶段"></a>Redis事务的三个阶段</h3><ol>
<li>事务开始 MULTI</li>
<li>命令入队</li>
<li>事务执行 EXEC</li>
</ol>
<p>事务执行过程中，如果服务端收到有EXEC、DISCARD、WATCH、MULTI之外的请求，将会把请求放入队列中排队</p>
<h3 id="Redis事务相关命令"><a href="#Redis事务相关命令" class="headerlink" title="Redis事务相关命令"></a>Redis事务相关命令</h3><p>Redis事务功能是通过MULTI、EXEC、DISCARD和WATCH 四个原语实现的</p>
<p>Redis会将一个事务中的所有命令序列化，然后按顺序执行。</p>
<ol>
<li>redis 不支持回滚，“Redis 在事务失败时不进行回滚，而是继续执行余下的命令”， 所以 Redis 的内部可以保持简单且快速。</li>
<li>如果在一个事务中的命令出现错误，那么所有的命令都不会执行；</li>
<li>如果在一个事务中出现运行错误，那么正确的命令会被执行。</li>
</ol>
<ul>
<li>WATCH 命令是一个乐观锁，可以为 Redis 事务提供 check-and-set （CAS）行为。 可以监控一个或多个键，一旦其中有一个键被修改（或删除），之后的事务就不会执行，监控一直持续到EXEC命令。</li>
<li>MULTI命令用于开启一个事务，它总是返回OK。 MULTI执行之后，客户端可以继续向服务器发送任意多条命令，这些命令不会立即被执行，而是被放到一个队列中，当EXEC命令被调用时，所有队列中的命令才会被执行。</li>
<li>EXEC：执行所有事务块内的命令。返回事务块内所有命令的返回值，按命令执行的先后顺序排列。 当操作被打断时，返回空值 nil 。<br>通过调用DISCARD，客户端可以清空事务队列，并放弃执行事务， 并且客户端会从事务状态中退出。</li>
<li>UNWATCH命令可以取消watch对所有key的监控。</li>
</ul>
<h3 id="事务管理（ACID）概述"><a href="#事务管理（ACID）概述" class="headerlink" title="事务管理（ACID）概述"></a>事务管理（ACID）概述</h3><p>原子性（Atomicity）<br>原子性是指事务是一个不可分割的工作单位，事务中的操作要么都发生，要么都不发生。</p>
<p>一致性（Consistency）<br>事务前后数据的完整性必须保持一致。</p>
<p>隔离性（Isolation）<br>多个事务并发执行时，一个事务的执行不应影响其他事务的执行</p>
<p>持久性（Durability）<br>持久性是指一个事务一旦被提交，它对数据库中数据的改变就是永久性的，接下来即使数据库发生故障也不应该对其有任何影响</p>
<p>Redis的事务总是具有ACID中的一致性和隔离性，其他特性是不支持的。当服务器运行在AOF持久化模式下，并且appendfsync选项的值为always时，事务也具有耐久性。</p>
<h3 id="Redis事务支持隔离性吗"><a href="#Redis事务支持隔离性吗" class="headerlink" title="Redis事务支持隔离性吗"></a>Redis事务支持隔离性吗</h3><p>Redis 是单进程程序，并且它保证在执行事务时，不会对事务进行中断，事务可以运行直到执行完所有事务队列中的命令为止。因此，<strong>Redis 的事务是总是带有隔离性的</strong>。</p>
<h3 id="Redis事务保证原子性吗，支持回滚吗"><a href="#Redis事务保证原子性吗，支持回滚吗" class="headerlink" title="Redis事务保证原子性吗，支持回滚吗"></a>Redis事务保证原子性吗，支持回滚吗</h3><p>Redis中，单条命令是原子性执行的，但<strong>事务不保证原子性，且没有回滚</strong>。事务中任意命令执行失败，其余的命令仍会被执行。</p>
<h3 id="Redis事务其他实现"><a href="#Redis事务其他实现" class="headerlink" title="Redis事务其他实现"></a>Redis事务其他实现</h3><ul>
<li>基于Lua脚本，Redis可以保证脚本内的命令一次性、按顺序地执行，<br>其同时也不提供事务运行错误的回滚，执行过程中如果部分命令运行错误，剩下的命令还是会继续运行完</li>
<li>基于中间标记变量，通过另外的标记变量来标识事务是否执行完成，读取数据时先读取该标记变量判断是否事务执行完成。但这样会需要额外写代码实现，比较繁琐</li>
</ul>
<h2 id="集群方案"><a href="#集群方案" class="headerlink" title="集群方案"></a>集群方案</h2><h3 id="哨兵模式"><a href="#哨兵模式" class="headerlink" title="哨兵模式"></a>哨兵模式</h3><p><img src="https://lixiangbetter.github.io/2020/07/01/Redis%E7%AC%94%E8%AE%B0/%E5%88%9D%E8%AF%86alluxio.png" alt></p>
<h3 id="官方Redis-Cluster-方案-服务端路由查询"><a href="#官方Redis-Cluster-方案-服务端路由查询" class="headerlink" title="官方Redis Cluster 方案(服务端路由查询)"></a>官方Redis Cluster 方案(服务端路由查询)</h3><p>redis 集群模式的工作原理能说一下么？在集群模式下，redis 的 key 是如何寻址的？分布式寻址都有哪些算法？了解一致性 hash 算法吗？</p>
<p><strong>简介</strong></p>
<p>Redis Cluster是一种服务端Sharding技术，3.0版本开始正式提供。Redis Cluster并没有使用一致性hash，而是采用slot(槽)的概念，一共分成16384个槽。将请求发送到任意节点，接收到请求的节点会将查询请求发送到正确的节点上执行</p>
<h3 id="基于客户端分配"><a href="#基于客户端分配" class="headerlink" title="基于客户端分配"></a>基于客户端分配</h3><p>简介</p>
<p>Redis Sharding是Redis Cluster出来之前，业界普遍使用的多Redis实例集群方法。其主要思想是采用哈希算法将Redis数据的key进行散列，通过hash函数，特定的key会映射到特定的Redis节点上。Java redis客户端驱动jedis，支持Redis Sharding功能，即ShardedJedis以及结合缓存池的ShardedJedisPool</p>
<h3 id="基于代理服务器分片"><a href="#基于代理服务器分片" class="headerlink" title="基于代理服务器分片"></a>基于代理服务器分片</h3><p><strong>简介</strong></p>
<p>客户端发送请求到一个代理组件，代理解析客户端的数据，并将请求转发至正确的节点，最后将结果回复给客户端</p>
<h3 id="Redis-主从架构"><a href="#Redis-主从架构" class="headerlink" title="Redis 主从架构"></a>Redis 主从架构</h3><p>redis replication -&gt; 主从架构 -&gt; 读写分离 -&gt; 水平扩容支撑读高并发</p>
<ul>
<li>redis 采用异步方式复制数据到 slave 节点，不过 redis2.8 开始，slave node 会周期性地确认自己每次复制的数据量；</li>
<li>一个 master node 是可以配置多个 slave node 的；</li>
<li>slave node 也可以连接其他的 slave node；</li>
<li>slave node 做复制的时候，不会 block master node 的正常工作；</li>
<li>slave node 在做复制的时候，也不会 block 对自己的查询操作，它会用旧的数据集来提供服务；但是复制完成的时候，需要删除旧数据集，加载新数据集，这个时候就会暂停对外服务了；</li>
<li>slave node 主要用来进行横向扩容，做读写分离，扩容的 slave node 可以提高读的吞吐量。</li>
</ul>
<p>注意，如果采用了主从架构，那么建议必须开启 master node 的持久化，不建议用 slave node 作为 master node 的数据热备，因为那样的话，如果你关掉 master 的持久化，可能在 master 宕机重启的时候数据是空的，然后可能一经过复制， slave node 的数据也丢了。</p>
<p>另外，master 的各种备份方案，也需要做。万一本地的所有文件丢失了，从备份中挑选一份 rdb 去恢复 master，这样才能确保启动的时候，是有数据的，即使采用了后续讲解的高可用机制，slave node 可以自动接管 master node，但也可能 sentinel 还没检测到 master failure，master node 就自动重启了，还是可能导致上面所有的 slave node 数据被清空。</p>
<p><strong>redis 主从复制的核心原理</strong></p>
<p>当启动一个 slave node 的时候，它会发送一个 PSYNC 命令给 master node。</p>
<p>如果这是 slave node 初次连接到 master node，那么会触发一次 full resynchronization 全量复制。此时 master 会启动一个后台线程，开始生成一份 RDB 快照文件，</p>
<p>同时还会将从客户端 client 新收到的所有写命令缓存在内存中。RDB 文件生成完毕后， master 会将这个 RDB 发送给 slave，slave 会先写入本地磁盘，然后再从本地磁盘加载到内存中，</p>
<p>接着 master 会将内存中缓存的写命令发送到 slave，slave 也会同步这些数据。</p>
<p>slave node 如果跟 master node 有网络故障，断开了连接，会自动重连，连接之后 master node 仅会复制给 slave 部分缺少的数据。</p>
<h3 id="Redis集群的主从复制模型是怎样的？"><a href="#Redis集群的主从复制模型是怎样的？" class="headerlink" title="Redis集群的主从复制模型是怎样的？"></a>Redis集群的主从复制模型是怎样的？</h3><p>为了使在部分节点失败或者大部分节点无法通信的情况下集群仍然可用，所以集群使用了主从复制模型，每个节点都会有N-1个复制品</p>
<h3 id="生产环境中的-redis-是怎么部署的？"><a href="#生产环境中的-redis-是怎么部署的？" class="headerlink" title="生产环境中的 redis 是怎么部署的？"></a>生产环境中的 redis 是怎么部署的？</h3><p>redis cluster，10 台机器，5 台机器部署了 redis 主实例，另外 5 台机器部署了 redis 的从实例，每个主实例挂了一个从实例，5 个节点对外提供读写服务，每个节点的读写高峰qps可能可以达到每秒 5 万，5 台机器最多是 25 万读写请求/s。</p>
<p>机器是什么配置？32G 内存+ 8 核 CPU + 1T 磁盘，但是分配给 redis 进程的是10g内存，一般线上生产环境，redis 的内存尽量不要超过 10g，超过 10g 可能会有问题。</p>
<p>5 台机器对外提供读写，一共有 50g 内存。</p>
<p>因为每个主实例都挂了一个从实例，所以是高可用的，任何一个主实例宕机，都会自动故障迁移，redis 从实例会自动变成主实例继续提供读写服务。</p>
<p>你往内存里写的是什么数据？每条数据的大小是多少？商品数据，每条数据是 10kb。100 条数据是 1mb，10 万条数据是 1g。常驻内存的是 200 万条商品数据，占用内存是 20g，仅仅不到总内存的 50%。目前高峰期每秒就是 3500 左右的请求量。</p>
<p>其实大型的公司，会有基础架构的 team 负责缓存集群的运维。</p>
<h3 id="说说Redis哈希槽的概念？"><a href="#说说Redis哈希槽的概念？" class="headerlink" title="说说Redis哈希槽的概念？"></a>说说Redis哈希槽的概念？</h3><p>Redis集群没有使用一致性hash,而是引入了哈希槽的概念，Redis集群有16384个哈希槽，每个key通过CRC16校验后对16384取模来决定放置哪个槽，集群的每个节点负责一部分hash槽。</p>
<h3 id="Redis集群会有写操作丢失吗？为什么？"><a href="#Redis集群会有写操作丢失吗？为什么？" class="headerlink" title="Redis集群会有写操作丢失吗？为什么？"></a>Redis集群会有写操作丢失吗？为什么？</h3><p>Redis并不能保证数据的强一致性，这意味这在实际中集群在特定的条件下可能会丢失写操作。</p>
<h3 id="Redis集群之间是如何复制的？"><a href="#Redis集群之间是如何复制的？" class="headerlink" title="Redis集群之间是如何复制的？"></a>Redis集群之间是如何复制的？</h3><p>异步复制</p>
<h3 id="Redis集群最大节点个数是多少？"><a href="#Redis集群最大节点个数是多少？" class="headerlink" title="Redis集群最大节点个数是多少？"></a>Redis集群最大节点个数是多少？</h3><p>16384个</p>
<h3 id="Redis集群如何选择数据库？"><a href="#Redis集群如何选择数据库？" class="headerlink" title="Redis集群如何选择数据库？"></a>Redis集群如何选择数据库？</h3><p>Redis集群目前无法做数据库选择，默认在0数据库。</p>
<h2 id="分区"><a href="#分区" class="headerlink" title="分区"></a>分区</h2><h3 id="Redis是单线程的，如何提高多核CPU的利用率？"><a href="#Redis是单线程的，如何提高多核CPU的利用率？" class="headerlink" title="Redis是单线程的，如何提高多核CPU的利用率？"></a>Redis是单线程的，如何提高多核CPU的利用率？</h3><p>可以在同一个服务器部署多个Redis的实例，并把他们当作不同的服务器来使用，在某些时候，无论如何一个服务器是不够的， 所以，如果你想使用多个CPU，你可以考虑一下分片（shard）。</p>
<h3 id="为什么要做Redis分区？"><a href="#为什么要做Redis分区？" class="headerlink" title="为什么要做Redis分区？"></a>为什么要做Redis分区？</h3><p>分区可以让Redis管理更大的内存，Redis将可以使用所有机器的内存。如果没有分区，你最多只能使用一台机器的内存。分区使Redis的计算能力通过简单地增加计算机得到成倍提升，Redis的网络带宽也会随着计算机和网卡的增加而成倍增长。</p>
<h3 id="你知道有哪些Redis分区实现方案？"><a href="#你知道有哪些Redis分区实现方案？" class="headerlink" title="你知道有哪些Redis分区实现方案？"></a>你知道有哪些Redis分区实现方案？</h3><ul>
<li>客户端分区就是在客户端就已经决定数据会被存储到哪个redis节点或者从哪个redis节点读取。大多数客户端已经实现了客户端分区。</li>
<li>代理分区意味着客户端将请求发送给代理，然后代理决定去哪个节点写数据或者读数据。代理根据分区规则决定请求哪些Redis实例，然后根据Redis的响应结果返回给客户端。redis和memcached的一种代理实现就是Twemproxy</li>
<li>查询路由(Query routing) 的意思是客户端随机地请求任意一个redis实例，然后由Redis将请求转发给正确的Redis节点。Redis Cluster实现了一种混合形式的查询路由，但并不是直接将请求从一个redis节点转发到另一个redis节点，而是在客户端的帮助下直接redirected到正确的redis节点。</li>
</ul>
<h3 id="Redis分区有什么缺点？"><a href="#Redis分区有什么缺点？" class="headerlink" title="Redis分区有什么缺点？"></a>Redis分区有什么缺点？</h3><ul>
<li>涉及多个key的操作通常不会被支持。例如你不能对两个集合求交集，因为他们可能被存储到不同的Redis实例（实际上这种情况也有办法，但是不能直接使用交集指令）。</li>
<li>同时操作多个key,则不能使用Redis事务.</li>
<li>分区使用的粒度是key，不能使用一个非常长的排序key存储一个数据集（The partitioning granularity is the key, so it is not possible to shard a dataset with a single huge key like a very big sorted set）</li>
<li>当使用分区的时候，数据处理会非常复杂，例如为了备份你必须从不同的Redis实例和主机同时收集RDB / AOF文件。</li>
<li>分区时动态扩容或缩容可能非常复杂。Redis集群在运行时增加或者删除Redis节点，能做到最大程度对用户透明地数据再平衡，但其他一些客户端分区或者代理分区方法则不支持这种特性。然而，有一种预分片的技术也可以较好的解决这个问题。</li>
</ul>
<h2 id="分布式问题"><a href="#分布式问题" class="headerlink" title="分布式问题"></a>分布式问题</h2><h3 id="Redis实现分布式锁"><a href="#Redis实现分布式锁" class="headerlink" title="Redis实现分布式锁"></a>Redis实现分布式锁</h3><p>Redis为单进程单线程模式，采用队列模式将并发访问变成串行访问，且多客户端对Redis的连接并不存在竞争关系Redis中可以使用SETNX命令实现分布式锁。</p>
<p>当且仅当 key 不存在，将 key 的值设为 value。 若给定的 key 已经存在，则 SETNX 不做任何动作</p>
<p>SETNX 是『SET if Not eXists』(如果不存在，则 SET)的简写。</p>
<p>返回值：设置成功，返回 1 。设置失败，返回 0 。</p>
<h3 id="如何解决-Redis-的并发竞争-Key-问题"><a href="#如何解决-Redis-的并发竞争-Key-问题" class="headerlink" title="如何解决 Redis 的并发竞争 Key 问题"></a>如何解决 Redis 的并发竞争 Key 问题</h3><p>所谓 Redis 的并发竞争 Key 的问题也就是多个系统同时对一个 key 进行操作，但是最后执行的顺序和我们期望的顺序不同，这样也就导致了结果的不同！</p>
<p>推荐一种方案：分布式锁（zookeeper 和 redis 都可以实现分布式锁）。（如果不存在 Redis 的并发竞争 Key 问题，不要使用分布式锁，这样会影响性能）</p>
<p>基于zookeeper临时有序节点可以实现的分布式锁。大致思想为：每个客户端对某个方法加锁时，在zookeeper上的与该方法对应的指定节点的目录下，生成一个唯一的瞬时有序节点。 判断是否获取锁的方式很简单，只需要判断有序节点中序号最小的一个。 当释放锁的时候，只需将这个瞬时节点删除即可。同时，其可以避免服务宕机导致的锁无法释放，而产生的死锁问题。完成业务流程后，删除对应的子节点释放锁。</p>
<h3 id="分布式Redis是前期做还是后期规模上来了再做好？为什么？"><a href="#分布式Redis是前期做还是后期规模上来了再做好？为什么？" class="headerlink" title="分布式Redis是前期做还是后期规模上来了再做好？为什么？"></a>分布式Redis是前期做还是后期规模上来了再做好？为什么？</h3><p>既然Redis是如此的轻量（单实例只使用1M内存），为防止以后的扩容，最好的办法就是一开始就启动较多实例。即便你只有一台服务器，你也可以一开始就让Redis以分布式的方式运行，使用分区，在同一台服务器上启动多个实例。</p>
<p>一开始就多设置几个Redis实例，例如32或者64个实例，对大多数用户来说这操作起来可能比较麻烦，但是从长久来看做这点牺牲是值得的。</p>
<p>这样的话，当你的数据不断增长，需要更多的Redis服务器时，你需要做的就是仅仅将Redis实例从一台服务迁移到另外一台服务器而已（而不用考虑重新分区的问题）。一旦你添加了另一台服务器，你需要将你一半的Redis实例从第一台机器迁移到第二台机器。</p>
<h3 id="什么是-RedLock"><a href="#什么是-RedLock" class="headerlink" title="什么是 RedLock"></a>什么是 RedLock</h3><p>Redis 官方站提出了一种权威的基于 Redis 实现分布式锁的方式名叫 Redlock，此种方式比原先的单节点的方法更安全。它可以保证以下特性：</p>
<ul>
<li>安全特性：互斥访问，即永远只有一个 client 能拿到锁</li>
<li>避免死锁：最终 client 都可能拿到锁，不会出现死锁的情况，即使原本锁住某资源的 client crash 了或者出现了网络分区</li>
<li>容错性：只要大部分 Redis 节点存活就可以正常提供服务</li>
</ul>
<h2 id="缓存异常"><a href="#缓存异常" class="headerlink" title="缓存异常"></a>缓存异常</h2><h3 id="缓存雪崩"><a href="#缓存雪崩" class="headerlink" title="缓存雪崩"></a>缓存雪崩</h3><p>缓存雪崩是指缓存同一时间大面积的失效，所以，后面的请求都会落到数据库上，造成数据库短时间内承受大量请求而崩掉。</p>
<p>解决方案</p>
<ul>
<li>缓存数据的过期时间设置随机，防止同一时间大量数据过期现象发生。</li>
<li>一般并发量不是特别多的时候，使用最多的解决方案是加锁排队。</li>
<li>给每一个缓存数据增加相应的缓存标记，记录缓存的是否失效，如果缓存标记失效，则更新数据缓存。</li>
</ul>
<h3 id="缓存穿透"><a href="#缓存穿透" class="headerlink" title="缓存穿透"></a>缓存穿透</h3><p>缓存穿透是指缓存和数据库中都没有的数据，导致所有的请求都落到数据库上，造成数据库短时间内承受大量请求而崩掉。</p>
<p>解决方案</p>
<ul>
<li>接口层增加校验，如用户鉴权校验，id做基础校验，id&lt;=0的直接拦截；</li>
<li>从缓存取不到的数据，在数据库中也没有取到，这时也可以将key-value对写为key-null，缓存有效时间可以设置短点，如30秒（设置太长会导致正常情况也没法使用）。这样可以防止攻击用户反复用同一个id暴力攻击</li>
<li>采用布隆过滤器，将所有可能存在的数据哈希到一个足够大的 bitmap 中，一个一定不存在的数据会被这个 bitmap 拦截掉，从而避免了对底层存储系统的查询压力</li>
</ul>
<p>Bloom-Filter一般用于在大数据量的集合中判定某元素是否存在。</p>
<h3 id="缓存击穿"><a href="#缓存击穿" class="headerlink" title="缓存击穿"></a>缓存击穿</h3><p>缓存击穿是指缓存中没有但数据库中有的数据（一般是缓存时间到期），这时由于并发用户特别多，同时读缓存没读到数据，又同时去数据库去取数据，引起数据库压力瞬间增大，造成过大压力。和缓存雪崩不同的是，缓存击穿指并发查同一条数据，缓存雪崩是不同数据都过期了，很多数据都查不到从而查数据库。</p>
<p>解决方案</p>
<ol>
<li>设置热点数据永远不过期。</li>
<li>加互斥锁，互斥锁</li>
</ol>
<h3 id="缓存预热"><a href="#缓存预热" class="headerlink" title="缓存预热"></a>缓存预热</h3><p>缓存预热就是系统上线后，将相关的缓存数据直接加载到缓存系统。这样就可以避免在用户请求的时候，先查询数据库，然后再将数据缓存的问题！用户直接查询事先被预热的缓存数据！</p>
<p>解决方案</p>
<ol>
<li><p>直接写个缓存刷新页面，上线时手工操作一下；</p>
</li>
<li><p>数据量不大，可以在项目启动的时候自动进行加载；</p>
</li>
<li><p>定时刷新缓存；</p>
</li>
</ol>
<h3 id="缓存降级"><a href="#缓存降级" class="headerlink" title="缓存降级"></a>缓存降级</h3><p>当访问量剧增、服务出现问题（如响应时间慢或不响应）或非核心服务影响到核心流程的性能时，仍然需要保证服务还是可用的，即使是有损服务。系统可以根据一些关键数据进行自动降级，也可以配置开关实现人工降级。</p>
<p>缓存降级的最终目的是保证核心服务可用，即使是有损的。而且有些服务是无法降级的（如加入购物车、结算）。</p>
<p>在进行降级之前要对系统进行梳理，看看系统是不是可以丢卒保帅；从而梳理出哪些必须誓死保护，哪些可降级；比如可以参考日志级别设置预案：</p>
<p>一般：比如有些服务偶尔因为网络抖动或者服务正在上线而超时，可以自动降级；</p>
<p>警告：有些服务在一段时间内成功率有波动（如在95~100%之间），可以自动降级或人工降级，并发送告警；</p>
<p>错误：比如可用率低于90%，或者数据库连接池被打爆了，或者访问量突然猛增到系统能承受的最大阀值，此时可以根据情况自动降级或者人工降级；</p>
<p>严重错误：比如因为特殊原因数据错误了，此时需要紧急人工降级。</p>
<p>服务降级的目的，是为了防止Redis服务故障，导致数据库跟着一起发生雪崩问题。因此，对于不重要的缓存数据，可以采取服务降级策略，例如一个比较常见的做法就是，Redis出现问题，不去数据库查询，而是直接返回默认值给用户。</p>
<h3 id="热点数据和冷数据"><a href="#热点数据和冷数据" class="headerlink" title="热点数据和冷数据"></a>热点数据和冷数据</h3><p>热点数据，缓存才有价值</p>
<p>对于冷数据而言，大部分数据可能还没有再次访问到就已经被挤出内存，不仅占用内存，而且价值不大。频繁修改的数据，看情况考虑使用缓存</p>
<p>对于热点数据，比如我们的某IM产品，生日祝福模块，当天的寿星列表，缓存以后可能读取数十万次。再举个例子，某导航产品，我们将导航信息，缓存以后可能读取数百万次。</p>
<p>数据更新前至少读取两次，缓存才有意义。这个是最基本的策略，如果缓存还没有起作用就失效了，那就没有太大价值了。</p>
<p>那存不存在，修改频率很高，但是又不得不考虑缓存的场景呢？有！比如，这个读取接口对数据库的压力很大，但是又是热点数据，这个时候就需要考虑通过缓存手段，减少数据库的压力，比如我们的某助手产品的，点赞数，收藏数，分享数等是非常典型的热点数据，但是又不断变化，此时就需要将数据同步保存到Redis缓存，减少数据库压力。</p>
<h3 id="缓存热点key"><a href="#缓存热点key" class="headerlink" title="缓存热点key"></a>缓存热点key</h3><p>缓存中的一个Key(比如一个促销商品)，在某个时间点过期的时候，恰好在这个时间点对这个Key有大量的并发请求过来，这些请求发现缓存过期一般都会从后端DB加载数据并回设到缓存，这个时候大并发的请求可能会瞬间把后端DB压垮。</p>
<p>解决方案</p>
<p>对缓存查询加锁，如果KEY不存在，就加锁，然后查DB入缓存，然后解锁；其他进程如果发现有锁就等待，然后等解锁后返回数据或者进入DB查询</p>
<h2 id="常用工具"><a href="#常用工具" class="headerlink" title="常用工具"></a>常用工具</h2><h3 id="Redis支持的Java客户端都有哪些？官方推荐用哪个？"><a href="#Redis支持的Java客户端都有哪些？官方推荐用哪个？" class="headerlink" title="Redis支持的Java客户端都有哪些？官方推荐用哪个？"></a>Redis支持的Java客户端都有哪些？官方推荐用哪个？</h3><p>Redisson、Jedis、lettuce等等，官方推荐使用Redisson。</p>
<h3 id="Redis和Redisson有什么关系？"><a href="#Redis和Redisson有什么关系？" class="headerlink" title="Redis和Redisson有什么关系？"></a>Redis和Redisson有什么关系？</h3><p>Redisson是一个高级的分布式协调Redis客服端，能帮助用户在分布式环境中轻松实现一些Java的对象 (Bloom filter, BitSet, Set, SetMultimap, ScoredSortedSet, SortedSet, Map, ConcurrentMap, List, ListMultimap, Queue, BlockingQueue, Deque, BlockingDeque, Semaphore, Lock, ReadWriteLock, AtomicLong, CountDownLatch, Publish / Subscribe, HyperLogLog)。</p>
<h3 id="Jedis与Redisson对比有什么优缺点？"><a href="#Jedis与Redisson对比有什么优缺点？" class="headerlink" title="Jedis与Redisson对比有什么优缺点？"></a>Jedis与Redisson对比有什么优缺点？</h3><p>Jedis是Redis的Java实现的客户端，其API提供了比较全面的Redis命令的支持；Redisson实现了分布式和可扩展的Java数据结构，和Jedis相比，功能较为简单，不支持字符串操作，不支持排序、事务、管道、分区等Redis特性。Redisson的宗旨是促进使用者对Redis的关注分离，从而让使用者能够将精力更集中地放在处理业务逻辑上。</p>
<h2 id="其他问题"><a href="#其他问题" class="headerlink" title="其他问题"></a>其他问题</h2><h3 id="Redis与Memcached的区别"><a href="#Redis与Memcached的区别" class="headerlink" title="Redis与Memcached的区别"></a>Redis与Memcached的区别</h3><p>两者都是非关系型内存键值数据库，现在公司一般都是用 Redis 来实现缓存，而且 Redis 自身也越来越强大了！Redis 与 Memcached 主要有以下不同：</p>
<p>(1) memcached所有的值均是简单的字符串，redis作为其替代者，支持更为丰富的数据类型</p>
<p>(2) redis的速度比memcached快很多</p>
<p>(3) redis可以持久化其数据</p>
<h3 id="如何保证缓存与数据库双写时的数据一致性？"><a href="#如何保证缓存与数据库双写时的数据一致性？" class="headerlink" title="如何保证缓存与数据库双写时的数据一致性？"></a>如何保证缓存与数据库双写时的数据一致性？</h3><p>你只要用缓存，就可能会涉及到缓存与数据库双存储双写，你只要是双写，就一定会有数据一致性的问题，那么你如何解决一致性问题？</p>
<p>一般来说，就是如果你的系统不是严格要求缓存+数据库必须一致性的话，缓存可以稍微的跟数据库偶尔有不一致的情况，最好不要做这个方案，读请求和写请求串行化，串到一个内存队列里去，这样就可以保证一定不会出现不一致的情况</p>
<p>串行化之后，就会导致系统的吞吐量会大幅度的降低，用比正常情况下多几倍的机器去支撑线上的一个请求。</p>
<p>还有一种方式就是可能会暂时产生不一致的情况，但是发生的几率特别小，就是<strong>先更新数据库，然后再删除缓存</strong>。</p>
<h3 id="Redis常见性能问题和解决方案？"><a href="#Redis常见性能问题和解决方案？" class="headerlink" title="Redis常见性能问题和解决方案？"></a>Redis常见性能问题和解决方案？</h3><ol>
<li>Master最好不要做任何持久化工作，包括内存快照和AOF日志文件，特别是不要启用内存快照做持久化。</li>
<li>如果数据比较关键，某个Slave开启AOF备份数据，策略为每秒同步一次。</li>
<li>为了主从复制的速度和连接的稳定性，Slave和Master最好在同一个局域网内。</li>
<li>尽量避免在压力较大的主库上增加从库</li>
<li>Master调用BGREWRITEAOF重写AOF文件，AOF在重写的时候会占大量的CPU和内存资源，导致服务load过高，出现短暂服务暂停现象。</li>
<li>为了Master的稳定性，主从复制不要用图状结构，用单向链表结构更稳定，即主从关系为：Master&lt;–Slave1&lt;–Slave2&lt;–Slave3…，这样的结构也方便解决单点故障问题，实现Slave对Master的替换，也即，如果Master挂了，可以立马启用Slave1做Master，其他不变。</li>
</ol>
<h3 id="Redis官方为什么不提供Windows版本？"><a href="#Redis官方为什么不提供Windows版本？" class="headerlink" title="Redis官方为什么不提供Windows版本？"></a>Redis官方为什么不提供Windows版本？</h3><p>因为目前Linux版本已经相当稳定，而且用户量很大，无需开发windows版本，反而会带来兼容性等问题。</p>
<h3 id="一个字符串类型的值能存储最大容量是多少？"><a href="#一个字符串类型的值能存储最大容量是多少？" class="headerlink" title="一个字符串类型的值能存储最大容量是多少？"></a>一个字符串类型的值能存储最大容量是多少？</h3><p>512M</p>
<h3 id="Redis如何做大量数据插入？"><a href="#Redis如何做大量数据插入？" class="headerlink" title="Redis如何做大量数据插入？"></a>Redis如何做大量数据插入？</h3><p>Redis2.6开始redis-cli支持一种新的被称之为pipe mode的新模式用于执行大量数据插入工作。</p>
<h3 id="假如Redis里面有1亿个key，其中有10w个key是以某个固定的已知的前缀开头的，如何将它们全部找出来？"><a href="#假如Redis里面有1亿个key，其中有10w个key是以某个固定的已知的前缀开头的，如何将它们全部找出来？" class="headerlink" title="假如Redis里面有1亿个key，其中有10w个key是以某个固定的已知的前缀开头的，如何将它们全部找出来？"></a>假如Redis里面有1亿个key，其中有10w个key是以某个固定的已知的前缀开头的，如何将它们全部找出来？</h3><p>使用keys指令可以扫出指定模式的key列表。<br>对方接着追问：如果这个redis正在给线上的业务提供服务，那使用keys指令会有什么问题？<br>这个时候你要回答redis关键的一个特性：redis的单线程的。keys指令会导致线程阻塞一段时间，线上服务会停顿，直到指令执行完毕，服务才能恢复。这个时候可以使用scan指令，scan指令可以无阻塞的提取出指定模式的key列表，但是会有一定的重复概率，在客户端做一次去重就可以了，但是整体所花费的时间会比直接用keys指令长。</p>
<h3 id="使用Redis做过异步队列吗，是如何实现的"><a href="#使用Redis做过异步队列吗，是如何实现的" class="headerlink" title="使用Redis做过异步队列吗，是如何实现的"></a>使用Redis做过异步队列吗，是如何实现的</h3><p>使用list类型保存数据信息，rpush生产消息，lpop消费消息，当lpop没有消息时，可以sleep一段时间，然后再检查有没有信息，如果不想sleep的话，可以使用blpop, 在没有信息的时候，会一直阻塞，直到信息的到来。redis可以通过pub/sub主题订阅模式实现一个生产者，多个消费者，当然也存在一定的缺点，当消费者下线时，生产的消息会丢失。</p>
<h3 id="Redis如何实现延时队列"><a href="#Redis如何实现延时队列" class="headerlink" title="Redis如何实现延时队列"></a>Redis如何实现延时队列</h3><p>使用sortedset，使用时间戳做score, 消息内容作为key,调用zadd来生产消息，消费者使用zrangbyscore获取n秒之前的数据做轮询处理。</p>
<h3 id="Redis回收进程如何工作的？"><a href="#Redis回收进程如何工作的？" class="headerlink" title="Redis回收进程如何工作的？"></a>Redis回收进程如何工作的？</h3><ol>
<li>一个客户端运行了新的命令，添加了新的数据。</li>
<li>Redis检查内存使用情况，如果大于maxmemory的限制， 则根据设定好的策略进行回收。</li>
<li>一个新的命令被执行，等等。</li>
<li>所以我们不断地穿越内存限制的边界，通过不断达到边界然后不断地回收回到边界以下。</li>
</ol>
<p>如果一个命令的结果导致大量内存被使用（例如很大的集合的交集保存到一个新的键），不用多久内存限制就会被这个内存使用量超越。</p>
<h3 id="Redis回收使用的是什么算法？"><a href="#Redis回收使用的是什么算法？" class="headerlink" title="Redis回收使用的是什么算法？"></a>Redis回收使用的是什么算法？</h3><p>LRU算法</p>
]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>redis</tag>
      </tags>
  </entry>
  <entry>
    <title>MyBatis笔记</title>
    <url>/2020/06/30/MyBatis%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<h1 id="MyBatis笔记"><a href="#MyBatis笔记" class="headerlink" title="MyBatis笔记"></a>MyBatis笔记</h1><h2 id="MyBatis简介"><a href="#MyBatis简介" class="headerlink" title="MyBatis简介"></a>MyBatis简介</h2><h3 id="MyBatis是什么？"><a href="#MyBatis是什么？" class="headerlink" title="MyBatis是什么？"></a>MyBatis是什么？</h3><p>MyBatis 是一款优秀的持久层框架，一个半 ORM（对象关系映射）框架，它支持定制化 SQL、存储过程以及高级映射。MyBatis 避免了几乎所有的 JDBC 代码和手动设置参数以及获取结果集。MyBatis 可以使用简单的 XML 或注解来配置和映射原生类型、接口和 Java 的 POJO（Plain Old Java Objects，普通老式 Java 对象）为数据库中的记录。</p>
<h3 id="ORM是什么"><a href="#ORM是什么" class="headerlink" title="ORM是什么"></a>ORM是什么</h3><p>ORM（Object Relational Mapping），对象关系映射，是一种为了解决关系型数据库数据与简单Java对象（POJO）的映射关系的技术。简单的说，ORM是通过使用描述对象和数据库之间映射的元数据，将程序中的对象自动持久化到关系型数据库中。</p>
<h3 id="为什么说Mybatis是半自动ORM映射工具？它与全自动的区别在哪里？"><a href="#为什么说Mybatis是半自动ORM映射工具？它与全自动的区别在哪里？" class="headerlink" title="为什么说Mybatis是半自动ORM映射工具？它与全自动的区别在哪里？"></a>为什么说Mybatis是半自动ORM映射工具？它与全自动的区别在哪里？</h3><p>Hibernate属于全自动ORM映射工具，使用Hibernate查询关联对象或者关联集合对象时，可以根据对象关系模型直接获取，所以它是全自动的。</p>
<p>而Mybatis在查询关联对象或关联集合对象时，需要手动编写sql来完成，所以，称之为半自动ORM映射工具。</p>
<h3 id="JDBC编程有哪些不足之处，MyBatis是如何解决这些问题的？"><a href="#JDBC编程有哪些不足之处，MyBatis是如何解决这些问题的？" class="headerlink" title="JDBC编程有哪些不足之处，MyBatis是如何解决这些问题的？"></a>JDBC编程有哪些不足之处，MyBatis是如何解决这些问题的？</h3><p>1、数据库链接创建、释放频繁造成系统资源浪费从而影响系统性能，如果使用数据库连接池可解决此问题。</p>
<p>解决：在mybatis-config.xml中配置数据链接池，使用连接池管理数据库连接。</p>
<p>2、Sql语句写在代码中造成代码不易维护，实际应用sql变化的可能较大，sql变动需要改变java代码。</p>
<p>解决：将Sql语句配置在XXXXmapper.xml文件中与java代码分离。</p>
<p>3、向sql语句传参数麻烦，因为sql语句的where条件不一定，可能多也可能少，占位符需要和参数一一对应。</p>
<p>解决： Mybatis自动将java对象映射至sql语句。</p>
<p>4、对结果集解析麻烦，sql变化导致解析代码变化，且解析前需要遍历，如果能将数据库记录封装成pojo对象解析比较方便。</p>
<p>解决：Mybatis自动将sql执行结果映射至java对象。</p>
<h3 id="Mybatis优缺点"><a href="#Mybatis优缺点" class="headerlink" title="Mybatis优缺点"></a>Mybatis优缺点</h3><p>优点</p>
<p>与传统的数据库访问技术相比，ORM有以下优点：</p>
<ul>
<li>基于SQL语句编程，相当灵活，不会对应用程序或者数据库的现有设计造成任何影响，SQL写在XML里，解除sql与程序代码的耦合，便于统一管理；提供XML标签，支持编写动态SQL语句，并可重用</li>
<li>与JDBC相比，减少了50%以上的代码量，消除了JDBC大量冗余的代码，不需要手动开关连接</li>
<li>很好的与各种数据库兼容（因为MyBatis使用JDBC来连接数据库，所以只要JDBC支持的数据库MyBatis都支持）</li>
<li>提供映射标签，支持对象与数据库的ORM字段关系映射；提供对象关系映射标签，支持对象关系组件维护<br>能够与Spring很好的集成</li>
</ul>
<p>缺点</p>
<ul>
<li>SQL语句的编写工作量较大，尤其当字段多、关联表多时，对开发人员编写SQL语句的功底有一定要求</li>
<li>SQL语句依赖于数据库，导致数据库移植性差，不能随意更换数据库</li>
</ul>
<h3 id="MyBatis框架适用场景"><a href="#MyBatis框架适用场景" class="headerlink" title="MyBatis框架适用场景"></a>MyBatis框架适用场景</h3><ul>
<li>MyBatis专注于SQL本身，是一个足够灵活的DAO层解决方案。</li>
<li>对性能的要求很高，或者需求变化较多的项目，如互联网项目，MyBatis将是不错的选择。</li>
</ul>
<h3 id="Hibernate-和-MyBatis-的区别"><a href="#Hibernate-和-MyBatis-的区别" class="headerlink" title="Hibernate 和 MyBatis 的区别"></a>Hibernate 和 MyBatis 的区别</h3><p>总结</p>
<p>MyBatis 是一个小巧、方便、高效、简单、直接、半自动化的持久层框架，</p>
<p>Hibernate 是一个强大、方便、高效、复杂、间接、全自动化的持久层框架。</p>
<h2 id="MyBatis的解析和运行原理"><a href="#MyBatis的解析和运行原理" class="headerlink" title="MyBatis的解析和运行原理"></a>MyBatis的解析和运行原理</h2><h3 id="MyBatis编程步骤是什么样的？"><a href="#MyBatis编程步骤是什么样的？" class="headerlink" title="MyBatis编程步骤是什么样的？"></a>MyBatis编程步骤是什么样的？</h3><p>1、 创建SqlSessionFactory</p>
<p>2、 通过SqlSessionFactory创建SqlSession</p>
<p>3、 通过sqlsession执行数据库操作</p>
<p>4、 调用session.commit()提交事务</p>
<p>5、 调用session.close()关闭会话</p>
<h3 id="请说说MyBatis的工作原理"><a href="#请说说MyBatis的工作原理" class="headerlink" title="请说说MyBatis的工作原理"></a>请说说MyBatis的工作原理</h3><p><img src="https://lixiangbetter.github.io/2020/06/30/MyBatis%E7%AC%94%E8%AE%B0/aHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL0pvdXJXb24vaW1hZ2UvbWFzdGVyL015QmF0aXMlRTYlQTElODYlRTYlOUUlQjYlRTYlODAlQkIlRTclQkIlOTMvTXlCYXRpcyVFNSVCNyVBNSVFNCVCRCU5QyVFNSU4RSU5RiVFNyU5MCU4Ni5wbmc.jpeg" alt></p>
<h3 id="MyBatis的功能架构是怎样的"><a href="#MyBatis的功能架构是怎样的" class="headerlink" title="MyBatis的功能架构是怎样的"></a>MyBatis的功能架构是怎样的</h3><p><img src="https://lixiangbetter.github.io/2020/06/30/MyBatis%E7%AC%94%E8%AE%B0/aHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL0pvdXJXb24vaW1hZ2UvbWFzdGVyL015QmF0aXMlRTYlQTElODYlRTYlOUUlQjYlRTYlODAlQkIlRTclQkIlOTMvTXlCYXRpcyVFNSU4QSU5RiVFOCU4MyVCRCVFNiU5RSVCNiVFNiU5RSU4NC5wbmc.jpeg" alt></p>
<h3 id="为什么需要预编译"><a href="#为什么需要预编译" class="headerlink" title="为什么需要预编译"></a>为什么需要预编译</h3><ol>
<li>定义：<br>SQL 预编译指的是数据库驱动在发送 SQL 语句和参数给 DBMS 之前对 SQL 语句进行编译，这样 DBMS 执行 SQL 时，就不需要重新编译。</li>
<li>为什么需要预编译<br>JDBC 中使用对象 PreparedStatement 来抽象预编译语句，使用预编译。预编译阶段可以优化 SQL 的执行。预编译之后的 SQL 多数情况下可以直接执行，DBMS 不需要再次编译，越复杂的SQL，编译的复杂度将越大，预编译阶段可以合并多次操作为一个操作。同时预编译语句对象可以重复利用。把一个 SQL 预编译后产生的 PreparedStatement 对象缓存下来，下次对于同一个SQL，可以直接使用这个缓存的 PreparedState 对象。Mybatis默认情况下，将对所有的 SQL 进行预编译。</li>
</ol>
<h3 id="Mybatis都有哪些Executor执行器？它们之间的区别是什么？"><a href="#Mybatis都有哪些Executor执行器？它们之间的区别是什么？" class="headerlink" title="Mybatis都有哪些Executor执行器？它们之间的区别是什么？"></a>Mybatis都有哪些Executor执行器？它们之间的区别是什么？</h3><p>Mybatis有三种基本的Executor执行器，SimpleExecutor、ReuseExecutor、BatchExecutor。</p>
<p>SimpleExecutor：每执行一次update或select，就开启一个Statement对象，用完立刻关闭Statement对象。</p>
<p>ReuseExecutor：执行update或select，以sql作为key查找Statement对象，存在就使用，不存在就创建，用完后，不关闭Statement对象，而是放置于Map&lt;String, Statement&gt;内，供下一次使用。简言之，就是重复使用Statement对象。</p>
<p>BatchExecutor：执行update（没有select，JDBC批处理不支持select），将所有sql都添加到批处理中（addBatch()），等待统一执行（executeBatch()），它缓存了多个Statement对象，每个Statement对象都是addBatch()完毕后，等待逐一执行executeBatch()批处理。与JDBC批处理相同。</p>
<p>作用范围：Executor的这些特点，都严格限制在SqlSession生命周期范围内。</p>
<h3 id="Mybatis中如何指定使用哪一种Executor执行器？"><a href="#Mybatis中如何指定使用哪一种Executor执行器？" class="headerlink" title="Mybatis中如何指定使用哪一种Executor执行器？"></a>Mybatis中如何指定使用哪一种Executor执行器？</h3><p>在Mybatis配置文件中，在设置（settings）可以指定默认的ExecutorType执行器类型，也可以手动给DefaultSqlSessionFactory的创建SqlSession的方法传递ExecutorType类型参数，如SqlSession openSession(ExecutorType execType)。</p>
<h3 id="Mybatis是否支持延迟加载？如果支持，它的实现原理是什么？"><a href="#Mybatis是否支持延迟加载？如果支持，它的实现原理是什么？" class="headerlink" title="Mybatis是否支持延迟加载？如果支持，它的实现原理是什么？"></a>Mybatis是否支持延迟加载？如果支持，它的实现原理是什么？</h3><p>Mybatis仅支持association关联对象和collection关联集合对象的延迟加载，association指的就是一对一，collection指的就是一对多查询。在Mybatis配置文件中，可以配置是否启用延迟加载lazyLoadingEnabled=true|false。</p>
<p>它的原理是，使用CGLIB创建目标对象的代理对象，当调用目标方法时，进入拦截器方法，比如调用a.getB().getName()，拦截器invoke()方法发现a.getB()是null值，那么就会单独发送事先保存好的查询关联B对象的sql，把B查询上来，然后调用a.setB(b)，于是a的对象b属性就有值了，接着完成a.getB().getName()方法的调用。这就是延迟加载的基本原理。</p>
<h2 id="映射器"><a href="#映射器" class="headerlink" title="映射器"></a>映射器</h2><h3 id="和-的区别"><a href="#和-的区别" class="headerlink" title="#{}和${}的区别"></a>#{}和${}的区别</h3><ul>
<li>#{}是占位符，预编译处理；${}是拼接符，字符串替换，没有预编译处理。</li>
<li>Mybatis在处理#{}时，#{}传入参数是以字符串传入，会将SQL中的#{}替换为?号，调用PreparedStatement的set方法来赋值。</li>
<li>Mybatis在处理时，是原值传入，就是把{}时，是原值传入，就是把时，是原值传入，就是把{}替换成变量的值，相当于JDBC中的Statement编译</li>
<li>变量替换后，#{} 对应的变量自动加上单引号 ‘’；变量替换后，${} 对应的变量不会加上单引号 ‘’</li>
<li>#{} 可以有效的防止SQL注入，提高系统安全性；${} 不能防止SQL 注入</li>
<li>#{} 的变量替换是在DBMS 中；${} 的变量替换是在 DBMS 外</li>
</ul>
<h3 id="模糊查询like语句该怎么写"><a href="#模糊查询like语句该怎么写" class="headerlink" title="模糊查询like语句该怎么写"></a>模糊查询like语句该怎么写</h3><p>（1）’%${question}%’ 可能引起SQL注入，不推荐</p>
<p>（2）”%”#{question}”%” 注意：因为#{…}解析成sql语句时候，会在变量外侧自动加单引号’ ‘，所以这里 % 需要使用双引号” “，不能使用单引号 ’ ‘，不然会查不到任何结果。</p>
<p>（3）CONCAT(’%’,#{question},’%’) 使用CONCAT()函数，推荐</p>
<p>（4）使用bind标签</p>
<h3 id="在mapper中如何传递多个参数"><a href="#在mapper中如何传递多个参数" class="headerlink" title="在mapper中如何传递多个参数"></a>在mapper中如何传递多个参数</h3><p><strong>方法1：顺序传参法</strong></p>
<p><strong>方法2：@Param注解传参法</strong> 推荐</p>
<p><strong>方法3：Map传参法</strong></p>
<p><strong>方法4：Java Bean传参法</strong> 推荐</p>
<h3 id="Mybatis如何执行批量操作"><a href="#Mybatis如何执行批量操作" class="headerlink" title="Mybatis如何执行批量操作"></a>Mybatis如何执行批量操作</h3><p><strong>使用foreach标签</strong></p>
<p><strong>使用ExecutorType.BATCH</strong></p>
<h3 id="如何获取生成的主键"><a href="#如何获取生成的主键" class="headerlink" title="如何获取生成的主键"></a>如何获取生成的主键</h3><p><strong>对于支持主键自增的数据库（MySQL）</strong></p>
<p><strong>不支持主键自增的数据库（Oracle）</strong></p>
<selectkey>

<h3 id="当实体类中的属性名和表中的字段名不一样-，怎么办"><a href="#当实体类中的属性名和表中的字段名不一样-，怎么办" class="headerlink" title="当实体类中的属性名和表中的字段名不一样 ，怎么办"></a>当实体类中的属性名和表中的字段名不一样 ，怎么办</h3><p>第1种： 通过在查询的SQL语句中定义字段名的别名，让字段名的别名和实体类的属性名一致。</p>
<p>第2种： 通过 resultmap 来映射字段名和实体类属性名的一一对应的关系。</p>
<h3 id="Mapper-编写有哪几种方式？"><a href="#Mapper-编写有哪几种方式？" class="headerlink" title="Mapper 编写有哪几种方式？"></a>Mapper 编写有哪几种方式？</h3><p>第三种：使用 mapper 扫描器：</p>
<p>（1）mapper.xml 文件编写：</p>
<p>mapper.xml 中的 namespace 为 mapper 接口的地址；</p>
<p>mapper 接口中的方法名和 mapper.xml 中的定义的 statement 的 id 保持一致；</p>
<p>如果将 mapper.xml 和 mapper 接口的名称保持一致则不用在 sqlMapConfig.xml中进行配置。</p>
<p>（2）定义 mapper 接口：</p>
<p>注意 mapper.xml 的文件名和 mapper 的接口名称保持一致，且放在同一个目录</p>
<p>（3）配置 mapper 扫描器</p>
<p>（4）使用扫描器后从 spring 容器中获取 mapper 的实现对象。</p>
<h3 id="什么是MyBatis的接口绑定？有哪些实现方式？"><a href="#什么是MyBatis的接口绑定？有哪些实现方式？" class="headerlink" title="什么是MyBatis的接口绑定？有哪些实现方式？"></a>什么是MyBatis的接口绑定？有哪些实现方式？</h3><h3 id="使用MyBatis的mapper接口调用时有哪些要求？"><a href="#使用MyBatis的mapper接口调用时有哪些要求？" class="headerlink" title="使用MyBatis的mapper接口调用时有哪些要求？"></a>使用MyBatis的mapper接口调用时有哪些要求？</h3><p>接口绑定，就是在MyBatis中任意定义接口，然后把接口里面的方法和SQL语句绑定，我们直接调用接口方法就可以，这样比起原来了SqlSession提供的方法我们可以有更加灵活的选择和设置。</p>
<p>接口绑定有两种实现方式</p>
<p>通过注解绑定，就是在接口的方法上面加上 @Select、@Update等注解，里面包含Sql语句来绑定；</p>
<p>通过xml里面写SQL来绑定， 在这种情况下，要指定xml映射文件里面的namespace必须为接口的全路径名。当Sql语句比较简单时候，用注解绑定， 当SQL语句比较复杂时候，用xml绑定，一般用xml绑定的比较多。</p>
<h3 id="使用MyBatis的mapper接口调用时有哪些要求？-1"><a href="#使用MyBatis的mapper接口调用时有哪些要求？-1" class="headerlink" title="使用MyBatis的mapper接口调用时有哪些要求？"></a>使用MyBatis的mapper接口调用时有哪些要求？</h3><p>1、Mapper接口方法名和mapper.xml中定义的每个sql的id相同。</p>
<p>2、Mapper接口方法的输入参数类型和mapper.xml中定义的每个sql 的parameterType的类型相同。</p>
<p>3、Mapper接口方法的输出参数类型和mapper.xml中定义的每个sql的resultType的类型相同。</p>
<p>4、Mapper.xml文件中的namespace即是mapper接口的类路径。</p>
<h3 id="最佳实践中，通常一个Xml映射文件，都会写一个Dao接口与之对应，请问，这个Dao接口的工作原理是什么？Dao接口里的方法，参数不同时，方法能重载吗"><a href="#最佳实践中，通常一个Xml映射文件，都会写一个Dao接口与之对应，请问，这个Dao接口的工作原理是什么？Dao接口里的方法，参数不同时，方法能重载吗" class="headerlink" title="最佳实践中，通常一个Xml映射文件，都会写一个Dao接口与之对应，请问，这个Dao接口的工作原理是什么？Dao接口里的方法，参数不同时，方法能重载吗"></a>最佳实践中，通常一个Xml映射文件，都会写一个Dao接口与之对应，请问，这个Dao接口的工作原理是什么？Dao接口里的方法，参数不同时，方法能重载吗</h3><p>Mapper接口是没有实现类的，当调用接口方法时，接口全限名+方法名拼接字符串作为key值，可唯一定位一个MappedStatement，举例：com.mybatis3.mappers.StudentDao.findStudentById，可以唯一找到namespace为com.mybatis3.mappers.StudentDao下面id = findStudentById的MappedStatement。在Mybatis中，每一个<select>、<insert>、<update>、<delete>标签，都会被解析为一个MappedStatement对象。</delete></update></insert></select></p>
<p>Dao接口里的方法，是不能重载的，因为是全限名+方法名的保存和寻找策略。</p>
<p>Dao接口的工作原理是JDK动态代理，Mybatis运行时会使用JDK动态代理为Dao接口生成代理proxy对象，代理对象proxy会拦截接口方法，转而执行MappedStatement所代表的sql，然后将sql执行结果返回。</p>
<h3 id="Mybatis的Xml映射文件中，不同的Xml映射文件，id是否可以重复？"><a href="#Mybatis的Xml映射文件中，不同的Xml映射文件，id是否可以重复？" class="headerlink" title="Mybatis的Xml映射文件中，不同的Xml映射文件，id是否可以重复？"></a>Mybatis的Xml映射文件中，不同的Xml映射文件，id是否可以重复？</h3><p>不同的Xml映射文件，如果配置了namespace，那么id可以重复；如果没有配置namespace，那么id不能重复；毕竟namespace不是必须的，只是最佳实践而已。</p>
<p>原因就是namespace+id是作为Map&lt;String, MappedStatement&gt;的key使用的，如果没有namespace，就剩下id，那么，id重复会导致数据互相覆盖。有了namespace，自然id就可以重复，namespace不同，namespace+id自然也就不同。</p>
<h3 id="简述Mybatis的Xml映射文件和Mybatis内部数据结构之间的映射关系？"><a href="#简述Mybatis的Xml映射文件和Mybatis内部数据结构之间的映射关系？" class="headerlink" title="简述Mybatis的Xml映射文件和Mybatis内部数据结构之间的映射关系？"></a>简述Mybatis的Xml映射文件和Mybatis内部数据结构之间的映射关系？</h3><p>Mybatis将所有Xml配置信息都封装到All-In-One重量级对象Configuration内部。在Xml映射文件中，<parametermap>标签会被解析为ParameterMap对象，其每个子元素会被解析为ParameterMapping对象。<resultmap>标签会被解析为ResultMap对象，其每个子元素会被解析为ResultMapping对象。每一个<select>、<insert>、<update>、<delete>标签均会被解析为MappedStatement对象，标签内的sql会被解析为BoundSql对象。</delete></update></insert></select></resultmap></parametermap></p>
<h3 id="Mybatis是如何将sql执行结果封装为目标对象并返回的？都有哪些映射形式？"><a href="#Mybatis是如何将sql执行结果封装为目标对象并返回的？都有哪些映射形式？" class="headerlink" title="Mybatis是如何将sql执行结果封装为目标对象并返回的？都有哪些映射形式？"></a>Mybatis是如何将sql执行结果封装为目标对象并返回的？都有哪些映射形式？</h3><p>第一种是使用<resultmap>标签，逐一定义列名和对象属性名之间的映射关系。</resultmap></p>
<p>第二种是使用sql列的别名功能，将列别名书写为对象属性名，比如T_NAME AS NAME，对象属性名一般是name，小写，但是列名不区分大小写，Mybatis会忽略列名大小写，智能找到与之对应对象属性名，你甚至可以写成T_NAME AS NaMe，Mybatis一样可以正常工作。</p>
<p>有了列名与属性名的映射关系后，Mybatis通过反射创建对象，同时使用反射给对象的属性逐一赋值并返回，那些找不到映射关系的属性，是无法完成赋值的。</p>
<h3 id="Xml映射文件中，除了常见的select-insert-updae-delete标签之外，还有哪些标签？"><a href="#Xml映射文件中，除了常见的select-insert-updae-delete标签之外，还有哪些标签？" class="headerlink" title="Xml映射文件中，除了常见的select|insert|updae|delete标签之外，还有哪些标签？"></a>Xml映射文件中，除了常见的select|insert|updae|delete标签之外，还有哪些标签？</h3><p>还有很多其他的标签，<resultmap>、<parametermap>、<sql>、<include>、<selectkey>，加上动态sql的9个标签，trim|where|set|foreach|if|choose|when|otherwise|bind等，其中<sql>为sql片段标签，通过<include>标签引入sql片段，<selectkey>为不支持自增的主键生成策略标签。</selectkey></include></sql></selectkey></include></sql></parametermap></resultmap></p>
<h3 id="Mybatis映射文件中，如果A标签通过include引用了B标签的内容，请问，B标签能否定义在A标签的后面，还是说必须定义在A标签的前面？"><a href="#Mybatis映射文件中，如果A标签通过include引用了B标签的内容，请问，B标签能否定义在A标签的后面，还是说必须定义在A标签的前面？" class="headerlink" title="Mybatis映射文件中，如果A标签通过include引用了B标签的内容，请问，B标签能否定义在A标签的后面，还是说必须定义在A标签的前面？"></a>Mybatis映射文件中，如果A标签通过include引用了B标签的内容，请问，B标签能否定义在A标签的后面，还是说必须定义在A标签的前面？</h3><p>虽然Mybatis解析Xml映射文件是按照顺序解析的，但是，被引用的B标签依然可以定义在任何地方，Mybatis都可以正确识别。</p>
<p>原理是，Mybatis解析A标签，发现A标签引用了B标签，但是B标签尚未解析到，尚不存在，此时，Mybatis会将A标签标记为未解析状态，然后继续解析余下的标签，包含B标签，待所有标签解析完毕，Mybatis会重新解析那些被标记为未解析的标签，此时再解析A标签时，B标签已经存在，A标签也就可以正常解析完成了。</p>
<h2 id="高级查询"><a href="#高级查询" class="headerlink" title="高级查询"></a>高级查询</h2><h3 id="MyBatis实现一对一，一对多有几种方式，怎么操作的？"><a href="#MyBatis实现一对一，一对多有几种方式，怎么操作的？" class="headerlink" title="MyBatis实现一对一，一对多有几种方式，怎么操作的？"></a>MyBatis实现一对一，一对多有几种方式，怎么操作的？</h3><p>有联合查询和嵌套查询。联合查询是几个表联合查询，只查询一次，通过在resultMap里面的association，collection节点配置一对一，一对多的类就可以完成</p>
<p>嵌套查询是先查一个表，根据这个表里面的结果的外键id，去再另外一个表里面查询数据，也是通过配置association，collection，但另外一个表的查询通过select节点配置。</p>
<h3 id="Mybatis是否可以映射Enum枚举类？"><a href="#Mybatis是否可以映射Enum枚举类？" class="headerlink" title="Mybatis是否可以映射Enum枚举类？"></a>Mybatis是否可以映射Enum枚举类？</h3><p>Mybatis可以映射枚举类，不单可以映射枚举类，Mybatis可以映射任何对象到表的一列上。映射方式为自定义一个TypeHandler，实现TypeHandler的setParameter()和getResult()接口方法。</p>
<p>TypeHandler有两个作用，一是完成从javaType至jdbcType的转换，二是完成jdbcType至javaType的转换，体现为setParameter()和getResult()两个方法，分别代表设置sql问号占位符参数和获取列查询结果。</p>
<h2 id="动态SQL"><a href="#动态SQL" class="headerlink" title="动态SQL"></a>动态SQL</h2><h3 id="Mybatis动态sql是做什么的？都有哪些动态sql？能简述一下动态sql的执行原理不？"><a href="#Mybatis动态sql是做什么的？都有哪些动态sql？能简述一下动态sql的执行原理不？" class="headerlink" title="Mybatis动态sql是做什么的？都有哪些动态sql？能简述一下动态sql的执行原理不？"></a>Mybatis动态sql是做什么的？都有哪些动态sql？能简述一下动态sql的执行原理不？</h3><p>Mybatis动态sql可以让我们在Xml映射文件内，以标签的形式编写动态sql，完成逻辑判断和动态拼接sql的功能，Mybatis提供了9种动态sql标签trim|where|set|foreach|if|choose|when|otherwise|bind。</p>
<p>其执行原理为，使用OGNL从sql参数对象中计算表达式的值，根据表达式的值动态拼接sql，以此来完成动态sql的功能。</p>
<h2 id="插件模块"><a href="#插件模块" class="headerlink" title="插件模块"></a>插件模块</h2><h3 id="Mybatis是如何进行分页的？分页插件的原理是什么？"><a href="#Mybatis是如何进行分页的？分页插件的原理是什么？" class="headerlink" title="Mybatis是如何进行分页的？分页插件的原理是什么？"></a>Mybatis是如何进行分页的？分页插件的原理是什么？</h3><p>Mybatis使用RowBounds对象进行分页，它是针对ResultSet结果集执行的内存分页，而非物理分页，可以在sql内直接书写带有物理分页的参数来完成物理分页功能，也可以使用分页插件来完成物理分页。</p>
<p>分页插件的基本原理是使用Mybatis提供的插件接口，实现自定义插件，在插件的拦截方法内拦截待执行的sql，然后重写sql，根据dialect方言，添加对应的物理分页语句和物理分页参数。</p>
<p>举例：select * from student，拦截sql后重写为：select t.* from (select * from student) t limit 0, 10</p>
<h3 id="简述Mybatis的插件运行原理，以及如何编写一个插件。"><a href="#简述Mybatis的插件运行原理，以及如何编写一个插件。" class="headerlink" title="简述Mybatis的插件运行原理，以及如何编写一个插件。"></a>简述Mybatis的插件运行原理，以及如何编写一个插件。</h3><p>Mybatis仅可以编写针对ParameterHandler、ResultSetHandler、StatementHandler、Executor这4种接口的插件，Mybatis使用JDK的动态代理，为需要拦截的接口生成代理对象以实现接口方法拦截功能，每当执行这4种接口对象的方法时，就会进入拦截方法，具体就是InvocationHandler的invoke()方法，当然，只会拦截那些你指定需要拦截的方法。</p>
<p>实现Mybatis的Interceptor接口并复写intercept()方法，然后在给插件编写注解，指定要拦截哪一个接口的哪些方法即可，记住，别忘了在配置文件中配置你编写的插件。</p>
<h2 id="缓存"><a href="#缓存" class="headerlink" title="缓存"></a>缓存</h2><h3 id="Mybatis的一级、二级缓存"><a href="#Mybatis的一级、二级缓存" class="headerlink" title="Mybatis的一级、二级缓存"></a>Mybatis的一级、二级缓存</h3><p>1）一级缓存: 基于 PerpetualCache 的 HashMap 本地缓存，其存储作用域为 Session，当 Session flush 或 close 之后，该 Session 中的所有 Cache 就将清空，默认打开一级缓存。</p>
<p>2）二级缓存与一级缓存其机制相同，默认也是采用 PerpetualCache，HashMap 存储，不同在于其存储作用域为 Mapper(Namespace)，并且可自定义存储源，如 Ehcache。默认不打开二级缓存，要开启二级缓存，使用二级缓存属性类需要实现Serializable序列化接口(可用来保存对象的状态),可在它的映射文件中配置<cache> ；</cache></p>
<p>3）对于缓存数据更新机制，当某一个作用域(一级缓存 Session/二级缓存Namespaces)的进行了C/U/D 操作后，默认该作用域下所有 select 中的缓存将被 clear。</p>
</selectkey>]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>mybatis</tag>
      </tags>
  </entry>
  <entry>
    <title>Spring Cloud笔记</title>
    <url>/2020/06/30/Spring-Cloud%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<h1 id="Spring-Cloud笔记"><a href="#Spring-Cloud笔记" class="headerlink" title="Spring Cloud笔记"></a>Spring Cloud笔记</h1><h2 id="为什么需要学习Spring-Cloud"><a href="#为什么需要学习Spring-Cloud" class="headerlink" title="为什么需要学习Spring Cloud"></a>为什么需要学习Spring Cloud</h2><p>不论是商业应用还是用户应用，在业务初期都很简单，我们通常会把它实现为单体结构的应用。但是，随着业务逐渐发展，产品思想会变得越来越复杂，单体结构的应用也会越来越复杂。这就会给应用带来如下的几个问题：</p>
<p>代码结构混乱：业务复杂，导致代码量很大，管理会越来越困难。同时，这也会给业务的快速迭代带来巨大挑战；<br>开发效率变低：开发人员同时开发一套代码，很难避免代码冲突。开发过程会伴随着不断解决冲突的过程，这会严重的影响开发效率；<br>排查解决问题成本高：线上业务发现 bug，修复 bug 的过程可能很简单。但是，由于只有一套代码，需要重新编译、打包、上线，成本很高。<br>由于单体结构的应用随着系统复杂度的增高，会暴露出各种各样的问题。近些年来，微服务架构逐渐取代了单体架构，且这种趋势将会越来越流行。Spring Cloud是目前最常用的微服务开发框架，已经在企业级开发中大量的应用。</p>
<h2 id="什么是Spring-Cloud"><a href="#什么是Spring-Cloud" class="headerlink" title="什么是Spring Cloud"></a>什么是Spring Cloud</h2><p>Spring Cloud是一系列框架的有序集合。它利用Spring Boot的开发便利性巧妙地简化了分布式系统基础设施的开发，如服务发现注册、配置中心、智能路由、消息总线、负载均衡、断路器、数据监控等，都可以用Spring Boot的开发风格做到一键启动和部署。Spring Cloud并没有重复制造轮子，它只是将各家公司开发的比较成熟、经得起实际考验的服务框架组合起来，通过Spring Boot风格进行再封装屏蔽掉了复杂的配置和实现原理，最终给开发者留出了一套简单易懂、易部署和易维护的分布式系统开发工具包。</p>
<h2 id="设计目标与优缺点"><a href="#设计目标与优缺点" class="headerlink" title="设计目标与优缺点"></a>设计目标与优缺点</h2><h3 id="设计目标"><a href="#设计目标" class="headerlink" title="设计目标"></a>设计目标</h3><p><strong>协调各个微服务，简化分布式系统开发</strong>。</p>
<p>优缺点<br>微服务的框架那么多比如：dubbo、Kubernetes，为什么就要使用Spring Cloud的呢？</p>
<p>优点：</p>
<ul>
<li>产出于Spring大家族，Spring在企业级开发框架中无人能敌，来头很大，可以保证后续的更新、完善</li>
<li>组件丰富，功能齐全。Spring Cloud 为微服务架构提供了非常完整的支持。例如、配置管理、服务发现、断路器、微服务网关等；</li>
<li>Spring Cloud 社区活跃度很高，教程很丰富，遇到问题很容易找到解决方案</li>
<li>服务拆分粒度更细，耦合度比较低，有利于资源重复利用，有利于提高开发效率</li>
<li>可以更精准的制定优化服务方案，提高系统的可维护性</li>
<li>减轻团队的成本，可以并行开发，不用关注其他人怎么开发，先关注自己的开发</li>
<li>微服务可以是跨平台的，可以用任何一种语言开发</li>
<li>适于互联网时代，产品迭代周期更短</li>
</ul>
<p>缺点：</p>
<ul>
<li>微服务过多，治理成本高，不利于维护系统</li>
<li>分布式系统开发的成本高（容错，分布式事务等）对团队挑战大</li>
</ul>
<p>总的来说优点大过于缺点，目前看来Spring Cloud是一套非常完善的分布式框架，目前很多企业开始用微服务、Spring Cloud的优势是显而易见的。因此对于想研究微服务架构的同学来说，学习Spring Cloud是一个不错的选择。</p>
<h2 id="Spring-Cloud发展前景"><a href="#Spring-Cloud发展前景" class="headerlink" title="Spring Cloud发展前景"></a>Spring Cloud发展前景</h2><p>Spring Cloud对于中小型互联网公司来说是一种福音，因为这类公司往往没有实力或者没有足够的资金投入去开发自己的分布式系统基础设施，使用Spring Cloud一站式解决方案能在从容应对业务发展的同时大大减少开发成本。同时，随着近几年微服务架构和Docker容器概念的火爆，也会让Spring Cloud在未来越来越“云”化的软件开发风格中立有一席之地，尤其是在五花八门的分布式解决方案中提供了标准化的、全站式的技术方案，意义可能会堪比当年Servlet规范的诞生，有效推进服务端软件系统技术水平的进步。</p>
<h2 id="整体架构"><a href="#整体架构" class="headerlink" title="整体架构"></a>整体架构</h2><p><img src="https://lixiangbetter.github.io/2020/06/30/Spring-MVC%E7%AC%94%E8%AE%B0/20200208211439106.png/Spring-Cloud%E7%AC%94%E8%AE%B0/20191226143921760.png" alt></p>
<h2 id="主要项目"><a href="#主要项目" class="headerlink" title="主要项目"></a>主要项目</h2><p>Spring Cloud的子项目，大致可分成两类，一类是对现有成熟框架”Spring Boot化”的封装和抽象，也是数量最多的项目；第二类是开发了一部分分布式系统的基础设施的实现，如Spring Cloud Stream扮演的就是kafka, ActiveMQ这样的角色。</p>
<h3 id="Spring-Cloud-Config"><a href="#Spring-Cloud-Config" class="headerlink" title="Spring Cloud Config"></a>Spring Cloud Config</h3><p>集中配置管理工具，分布式系统中统一的外部配置管理，默认使用Git来存储配置，可以支持客户端配置的刷新及加密、解密操作。</p>
<h3 id="Spring-Cloud-Netflix"><a href="#Spring-Cloud-Netflix" class="headerlink" title="Spring Cloud Netflix"></a>Spring Cloud Netflix</h3><p>Netflix OSS 开源组件集成，包括Eureka、Hystrix、Ribbon、Feign、Zuul等核心组件。</p>
<ul>
<li>Eureka：服务治理组件，包括服务端的注册中心和客户端的服务发现机制；</li>
<li>Ribbon：负载均衡的服务调用组件，具有多种负载均衡调用策略；</li>
<li>Hystrix：服务容错组件，实现了断路器模式，为依赖服务的出错和延迟提供了容错能力；</li>
<li>Feign：基于Ribbon和Hystrix的声明式服务调用组件；</li>
<li>Zuul：API网关组件，对请求提供路由及过滤功能。</li>
</ul>
<h3 id="Spring-Cloud-Bus"><a href="#Spring-Cloud-Bus" class="headerlink" title="Spring Cloud Bus"></a>Spring Cloud Bus</h3><p>用于传播集群状态变化的消息总线，使用轻量级消息代理链接分布式系统中的节点，可以用来动态刷新集群中的服务配置。</p>
<h3 id="Spring-Cloud-Consul"><a href="#Spring-Cloud-Consul" class="headerlink" title="Spring Cloud Consul"></a>Spring Cloud Consul</h3><p>基于Hashicorp Consul的服务治理组件。</p>
<h3 id="Spring-Cloud-Security"><a href="#Spring-Cloud-Security" class="headerlink" title="Spring Cloud Security"></a>Spring Cloud Security</h3><p>安全工具包，对Zuul代理中的负载均衡OAuth2客户端及登录认证进行支持。</p>
<h3 id="Spring-Cloud-Sleuth"><a href="#Spring-Cloud-Sleuth" class="headerlink" title="Spring Cloud Sleuth"></a>Spring Cloud Sleuth</h3><p>Spring Cloud应用程序的分布式请求链路跟踪，支持使用Zipkin、HTrace和基于日志（例如ELK）的跟踪。</p>
<h3 id="Spring-Cloud-Stream"><a href="#Spring-Cloud-Stream" class="headerlink" title="Spring Cloud Stream"></a>Spring Cloud Stream</h3><p>轻量级事件驱动微服务框架，可以使用简单的声明式模型来发送及接收消息，主要实现为Apache Kafka及RabbitMQ。</p>
<h3 id="Spring-Cloud-Task"><a href="#Spring-Cloud-Task" class="headerlink" title="Spring Cloud Task"></a>Spring Cloud Task</h3><p>用于快速构建短暂、有限数据处理任务的微服务框架，用于向应用中添加功能性和非功能性的特性。</p>
<h3 id="Spring-Cloud-Zookeeper"><a href="#Spring-Cloud-Zookeeper" class="headerlink" title="Spring Cloud Zookeeper"></a>Spring Cloud Zookeeper</h3><p>基于Apache Zookeeper的服务治理组件。</p>
<h3 id="Spring-Cloud-Gateway"><a href="#Spring-Cloud-Gateway" class="headerlink" title="Spring Cloud Gateway"></a>Spring Cloud Gateway</h3><p>API网关组件，对请求提供路由及过滤功能。</p>
<h3 id="Spring-Cloud-OpenFeign"><a href="#Spring-Cloud-OpenFeign" class="headerlink" title="Spring Cloud OpenFeign"></a>Spring Cloud OpenFeign</h3><p>基于Ribbon和Hystrix的声明式服务调用组件，可以动态创建基于Spring MVC注解的接口实现用于服务调用，在Spring Cloud 2.0中已经取代Feign成为了一等公民。</p>
<h3 id="Spring-Cloud和SpringBoot版本对应关系"><a href="#Spring-Cloud和SpringBoot版本对应关系" class="headerlink" title="Spring Cloud和SpringBoot版本对应关系"></a>Spring Cloud和SpringBoot版本对应关系</h3><table>
<thead>
<tr>
<th>Spring Cloud Version</th>
<th>SpringBoot Version</th>
</tr>
</thead>
<tbody><tr>
<td>Hoxton</td>
<td>2.2.x</td>
</tr>
<tr>
<td>Greenwich</td>
<td>2.1.x</td>
</tr>
<tr>
<td>Finchley</td>
<td>2.0.x</td>
</tr>
<tr>
<td>Edgware</td>
<td>1.5.x</td>
</tr>
<tr>
<td>Dalston</td>
<td>1.5.x</td>
</tr>
</tbody></table>
<h2 id="SpringBoot和SpringCloud的区别？"><a href="#SpringBoot和SpringCloud的区别？" class="headerlink" title="SpringBoot和SpringCloud的区别？"></a>SpringBoot和SpringCloud的区别？</h2><p>SpringBoot专注于快速方便的开发单个个体微服务。</p>
<p>SpringCloud是关注全局的微服务协调整理治理框架，它将SpringBoot开发的一个个单体微服务整合并管理起来，</p>
<p>为各个微服务之间提供，配置管理、服务发现、断路器、路由、微代理、事件总线、全局锁、决策竞选、分布式会话等等集成服务</p>
<p>SpringBoot可以离开SpringCloud独立使用开发项目， 但是SpringCloud离不开SpringBoot ，属于依赖的关系</p>
<p>SpringBoot专注于快速、方便的开发单个微服务个体，SpringCloud关注全局的服务治理框架。</p>
<h2 id="服务注册和发现是什么意思？Spring-Cloud-如何实现？"><a href="#服务注册和发现是什么意思？Spring-Cloud-如何实现？" class="headerlink" title="服务注册和发现是什么意思？Spring Cloud 如何实现？"></a>服务注册和发现是什么意思？Spring Cloud 如何实现？</h2><p>由于所有服务都在 Eureka 服务器上注册并通过调用 Eureka 服务器完成查找，因此无需处理服务地点的任何更改和处理。</p>
<h2 id="Spring-Cloud-和dubbo区别"><a href="#Spring-Cloud-和dubbo区别" class="headerlink" title="Spring Cloud 和dubbo区别?"></a>Spring Cloud 和dubbo区别?</h2><p>（1）服务调用方式 dubbo是RPC springcloud Rest Api</p>
<p>（2）注册中心,dubbo 是zookeeper springcloud是eureka，也可以是zookeeper</p>
<p>（3）服务网关,dubbo本身没有实现，只能通过其他第三方技术整合，springcloud有Zuul路由网关</p>
<h2 id="负载平衡的意义什么？"><a href="#负载平衡的意义什么？" class="headerlink" title="负载平衡的意义什么？"></a>负载平衡的意义什么？</h2><p>在计算中，负载平衡可以改善跨计算机，计算机集群，网络链接，中央处理单元或磁盘驱动器等多种计算资源的工作负载分布。负载平衡旨在优化资源使用，最大化吞吐量，最小化响应时间并避免任何单一资源的过载。使用多个组件进行负载平衡而不是单个组件可能会通过冗余来提高可靠性和可用性。负载平衡通常涉及专用软件或硬件，例如多层交换机或域名系统服务器进程。</p>
<h2 id="什么是-Hystrix？它如何实现容错？"><a href="#什么是-Hystrix？它如何实现容错？" class="headerlink" title="什么是 Hystrix？它如何实现容错？"></a>什么是 Hystrix？它如何实现容错？</h2><p>Hystrix 是一个延迟和容错库，旨在隔离远程系统，服务和第三方库的访问点，当出现故障是不可避免的故障时，停止级联故障并在复杂的分布式系统中实现弹性。</p>
<p>通常对于使用微服务架构开发的系统，涉及到许多微服务。这些微服务彼此协作。</p>
<h2 id="什么是-Hystrix-断路器？我们需要它吗？"><a href="#什么是-Hystrix-断路器？我们需要它吗？" class="headerlink" title="什么是 Hystrix 断路器？我们需要它吗？"></a>什么是 Hystrix 断路器？我们需要它吗？</h2><p>由于某些原因，employee-consumer 公开服务会引发异常。在这种情况下使用Hystrix 我们定义了一个回退方法。如果在公开服务中发生异常，则回退方法返回一些默认值。</p>
<h2 id="什么是-Netflix-Feign？它的优点是什么？"><a href="#什么是-Netflix-Feign？它的优点是什么？" class="headerlink" title="什么是 Netflix Feign？它的优点是什么？"></a>什么是 Netflix Feign？它的优点是什么？</h2><p>Feign 是受到 Retrofit，JAXRS-2.0 和 WebSocket 启发的 java 客户端联编程序。</p>
<p> Netflix Feign 使呼叫变得更加轻松和清洁</p>
<h2 id="什么是-Spring-Cloud-Bus？我们需要它吗？"><a href="#什么是-Spring-Cloud-Bus？我们需要它吗？" class="headerlink" title="什么是 Spring Cloud Bus？我们需要它吗？"></a>什么是 Spring Cloud Bus？我们需要它吗？</h2><p>Spring Cloud Bus 提供了跨多个实例刷新配置的功能。</p>
<h2 id="Spring-Cloud断路器的作用"><a href="#Spring-Cloud断路器的作用" class="headerlink" title="Spring Cloud断路器的作用"></a>Spring Cloud断路器的作用</h2><p>当一个服务调用另一个服务由于网络原因或自身原因出现问题，调用者就会等待被调用者的响应 当更多的服务请求到这些资源导致更多的请求等待，发生连锁效应（雪崩效应）</p>
<p>断路器有完全打开状态:一段时间内 达到一定的次数无法调用 并且多次监测没有恢复的迹象 断路器完全打开 那么下次请求就不会请求到该服务</p>
<p>半开:短时间内 有恢复迹象 断路器会将部分请求发给该服务，正常调用时 断路器关闭</p>
<p>关闭：当服务一直处于正常状态 能正常调用</p>
<h2 id="什么是Spring-Cloud-Config"><a href="#什么是Spring-Cloud-Config" class="headerlink" title="什么是Spring Cloud Config?"></a>什么是Spring Cloud Config?</h2><p>在分布式系统中，由于服务数量巨多，为了方便服务配置文件统一管理，实时更新，所以需要分布式配置中心组件。</p>
<p>使用：</p>
<p>（1）添加pom依赖</p>
<p>（2）配置文件添加相关配置</p>
<p>（3）启动类添加注解@EnableConfigServer</p>
<h2 id="什么是Spring-Cloud-Gateway"><a href="#什么是Spring-Cloud-Gateway" class="headerlink" title="什么是Spring Cloud Gateway?"></a>什么是Spring Cloud Gateway?</h2><p>Spring Cloud Gateway是Spring Cloud官方推出的第二代网关框架，取代Zuul网关。网关作为流量的，在微服务系统中有着非常作用，网关常见的功能有路由转发、权限校验、限流控制等作用。</p>
]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>spring cloud</tag>
      </tags>
  </entry>
  <entry>
    <title>Spring Boot笔记</title>
    <url>/2020/06/30/Spring-Boot%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<h1 id="Spring-Boot笔记"><a href="#Spring-Boot笔记" class="headerlink" title="Spring Boot笔记"></a>Spring Boot笔记</h1><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><h3 id="什么是-Spring-Boot？"><a href="#什么是-Spring-Boot？" class="headerlink" title="什么是 Spring Boot？"></a>什么是 Spring Boot？</h3><p>Spring Boot 是 Spring 开源组织下的子项目，是 Spring 组件一站式解决方案，主要是简化了使用 Spring 的难度，简省了繁重的配置，提供了各种启动器，开发者能快速上手。</p>
<h3 id="Spring-Boot-有哪些优点？"><a href="#Spring-Boot-有哪些优点？" class="headerlink" title="Spring Boot 有哪些优点？"></a>Spring Boot 有哪些优点？</h3><p>Spring Boot 主要有如下优点：</p>
<ul>
<li>容易上手，提升开发效率，为 Spring 开发提供一个更快、更广泛的入门体验。</li>
<li>开箱即用，远离繁琐的配置。</li>
<li>提供了一系列大型项目通用的非业务性功能，例如：内嵌服务器、安全管理、运行数据监控、运行状况检查和外部化配置等。</li>
<li>没有代码生成，也不需要XML配置。</li>
<li>避免大量的 Maven 导入和各种版本冲突。</li>
</ul>
<h3 id="Spring-Boot-的核心注解是哪个？它主要由哪几个注解组成的？"><a href="#Spring-Boot-的核心注解是哪个？它主要由哪几个注解组成的？" class="headerlink" title="Spring Boot 的核心注解是哪个？它主要由哪几个注解组成的？"></a>Spring Boot 的核心注解是哪个？它主要由哪几个注解组成的？</h3><p>启动类上面的注解是@SpringBootApplication，它也是 Spring Boot 的核心注解，主要组合包含了以下 3 个注解：</p>
<ul>
<li>@SpringBootConfiguration：组合了 @Configuration 注解，实现配置文件的功能。</li>
<li>@EnableAutoConfiguration：打开自动配置的功能，也可以关闭某个自动配置的选项，如关闭数据源自动配置功能： @SpringBootApplication(exclude = { DataSourceAutoConfiguration.class })。</li>
<li>@ComponentScan：Spring组件扫描。</li>
</ul>
<h2 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h2><h3 id="什么是-JavaConfig？"><a href="#什么是-JavaConfig？" class="headerlink" title="什么是 JavaConfig？"></a>什么是 JavaConfig？</h3><p>Spring JavaConfig 是 Spring 社区的产品，它提供了配置 Spring IoC 容器的纯Java 方法。因此它有助于避免使用 XML 配置。使用 JavaConfig 的优点在于：</p>
<p>（1）面向对象的配置。由于配置被定义为 JavaConfig 中的类，因此用户可以充分利用 Java 中的面向对象功能。一个配置类可以继承另一个，重写它的@Bean 方法等。</p>
<p>（2）减少或消除 XML 配置。基于依赖注入原则的外化配置的好处已被证明。但是，许多开发人员不希望在 XML 和 Java 之间来回切换。JavaConfig 为开发人员提供了一种纯 Java 方法来配置与 XML 配置概念相似的 Spring 容器。从技术角度来讲，只使用 JavaConfig 配置类来配置容器是可行的，但实际上很多人认为将JavaConfig 与 XML 混合匹配是理想的。</p>
<p>（3）类型安全和重构友好。JavaConfig 提供了一种类型安全的方法来配置 Spring容器。由于 Java 5.0 对泛型的支持，现在可以按类型而不是按名称检索 bean，不需要任何强制转换或基于字符串的查找。</p>
<h3 id="Spring-Boot-自动配置原理是什么？"><a href="#Spring-Boot-自动配置原理是什么？" class="headerlink" title="Spring Boot 自动配置原理是什么？"></a>Spring Boot 自动配置原理是什么？</h3><p>注解 @EnableAutoConfiguration, @Configuration, @ConditionalOnClass 就是自动配置的核心，</p>
<p>@EnableAutoConfiguration 给容器导入META-INF/spring.factories 里定义的自动配置类。</p>
<p>筛选有效的自动配置类。</p>
<p>每一个自动配置类结合对应的 xxxProperties.java 读取配置文件进行自动配置功能</p>
<h3 id="你如何理解-Spring-Boot-配置加载顺序？"><a href="#你如何理解-Spring-Boot-配置加载顺序？" class="headerlink" title="你如何理解 Spring Boot 配置加载顺序？"></a>你如何理解 Spring Boot 配置加载顺序？</h3><p>在 Spring Boot 里面，可以使用以下几种方式来加载配置。</p>
<p>1）properties文件；</p>
<p>2）YAML文件；</p>
<p>3）系统环境变量；</p>
<p>4）命令行参数；</p>
<p>等等……</p>
<h3 id="什么是-YAML？"><a href="#什么是-YAML？" class="headerlink" title="什么是 YAML？"></a>什么是 YAML？</h3><p>YAML 是一种人类可读的数据序列化语言。它通常用于配置文件。与属性文件相比，如果我们想要在配置文件中添加复杂的属性，YAML 文件就更加结构化，而且更少混淆。可以看出 YAML 具有分层配置数据。</p>
<h3 id="YAML-配置的优势在哪里"><a href="#YAML-配置的优势在哪里" class="headerlink" title="YAML 配置的优势在哪里 ?"></a>YAML 配置的优势在哪里 ?</h3><p>YAML 现在可以算是非常流行的一种配置文件格式了，无论是前端还是后端，都可以见到 YAML 配置。那么 YAML 配置和传统的 properties 配置相比到底有哪些优势呢？</p>
<ul>
<li>配置有序，在一些特殊的场景下，配置有序很关键</li>
<li>支持数组，数组中的元素可以是基本数据类型也可以是对象</li>
<li>简洁</li>
</ul>
<p>相比 properties 配置文件，YAML 还有一个缺点，就是不支持 @PropertySource 注解导入自定义的 YAML 配置。</p>
<h3 id="Spring-Boot-是否可以使用-XML-配置"><a href="#Spring-Boot-是否可以使用-XML-配置" class="headerlink" title="Spring Boot 是否可以使用 XML 配置 ?"></a>Spring Boot 是否可以使用 XML 配置 ?</h3><p>Spring Boot 推荐使用 Java 配置而非 XML 配置，但是 Spring Boot 中也可以使用 XML 配置，通过 @ImportResource 注解可以引入一个 XML 配置。</p>
<h3 id="spring-boot-核心配置文件是什么？bootstrap-properties-和-application-properties-有何区别"><a href="#spring-boot-核心配置文件是什么？bootstrap-properties-和-application-properties-有何区别" class="headerlink" title="spring boot 核心配置文件是什么？bootstrap.properties 和 application.properties 有何区别 ?"></a>spring boot 核心配置文件是什么？bootstrap.properties 和 application.properties 有何区别 ?</h3><p>单纯做 Spring Boot 开发，可能不太容易遇到 bootstrap.properties 配置文件，但是在结合 Spring Cloud 时，这个配置就会经常遇到了，特别是在需要加载一些远程配置文件的时侯。</p>
<p>spring boot 核心的两个配置文件：</p>
<ul>
<li>bootstrap (. yml 或者 . properties)：boostrap 由父 ApplicationContext 加载的，比 applicaton 优先加载，配置在应用程序上下文的引导阶段生效。一般来说我们在 Spring Cloud Config 或者 Nacos 中会用到它。且 boostrap 里面的属性不能被覆盖；</li>
<li>application (. yml 或者 . properties)： 由ApplicatonContext 加载，用于 spring boot 项目的自动化配置。</li>
</ul>
<h3 id="什么是-Spring-Profiles？"><a href="#什么是-Spring-Profiles？" class="headerlink" title="什么是 Spring Profiles？"></a>什么是 Spring Profiles？</h3><p>Spring Profiles 允许用户根据配置文件（dev，test，prod 等）来注册 bean。因此，当应用程序在开发中运行时，只有某些 bean 可以加载，而在 PRODUCTION中，某些其他 bean 可以加载。假设我们的要求是 Swagger 文档仅适用于 QA 环境，并且禁用所有其他文档。这可以使用配置文件来完成。Spring Boot 使得使用配置文件非常简单。</p>
<h3 id="如何在自定义端口上运行-Spring-Boot-应用程序？"><a href="#如何在自定义端口上运行-Spring-Boot-应用程序？" class="headerlink" title="如何在自定义端口上运行 Spring Boot 应用程序？"></a>如何在自定义端口上运行 Spring Boot 应用程序？</h3><p>为了在自定义端口上运行 Spring Boot 应用程序，您可以在application.properties 中指定端口。server.port = 8090</p>
<h2 id="安全"><a href="#安全" class="headerlink" title="安全"></a>安全</h2><h3 id="如何实现-Spring-Boot-应用程序的安全性？"><a href="#如何实现-Spring-Boot-应用程序的安全性？" class="headerlink" title="如何实现 Spring Boot 应用程序的安全性？"></a>如何实现 Spring Boot 应用程序的安全性？</h3><p>为了实现 Spring Boot 的安全性，我们使用 spring-boot-starter-security 依赖项，并且必须添加安全配置。它只需要很少的代码。配置类将必须扩展WebSecurityConfigurerAdapter 并覆盖其方法。</p>
<h3 id="比较一下-Spring-Security-和-Shiro-各自的优缺点"><a href="#比较一下-Spring-Security-和-Shiro-各自的优缺点" class="headerlink" title="比较一下 Spring Security 和 Shiro 各自的优缺点 ?"></a>比较一下 Spring Security 和 Shiro 各自的优缺点 ?</h3><p>由于 Spring Boot 官方提供了大量的非常方便的开箱即用的 Starter ，包括 Spring Security 的 Starter ，使得在 Spring Boot 中使用 Spring Security 变得更加容易，甚至只需要添加一个依赖就可以保护所有的接口，所以，如果是 Spring Boot 项目，一般选择 Spring Security 。当然这只是一个建议的组合，单纯从技术上来说，无论怎么组合，都是没有问题的。Shiro 和 Spring Security 相比，主要有如下一些特点：</p>
<ul>
<li>Spring Security 是一个重量级的安全管理框架；Shiro 则是一个轻量级的安全管理框架</li>
<li>Spring Security 概念复杂，配置繁琐；Shiro 概念简单、配置简单</li>
<li>Spring Security 功能强大；Shiro 功能简单</li>
</ul>
<h3 id="Spring-Boot-中如何解决跨域问题"><a href="#Spring-Boot-中如何解决跨域问题" class="headerlink" title="Spring Boot 中如何解决跨域问题 ?"></a>Spring Boot 中如何解决跨域问题 ?</h3><p>跨域可以在前端通过 JSONP 来解决，但是 JSONP 只可以发送 GET 请求，无法发送其他类型的请求，在 RESTful 风格的应用中，就显得非常鸡肋，因此我们推荐在后端通过 （CORS，Cross-origin resource sharing） 来解决跨域问题。这种解决方案并非 Spring Boot 特有的，在传统的 SSM 框架中，就可以通过 CORS 来解决跨域问题，只不过之前我们是在 XML 文件中配置 CORS ，现在可以通过实现WebMvcConfigurer接口然后重写addCorsMappings方法解决跨域问题。</p>
<h3 id="什么是-CSRF-攻击？"><a href="#什么是-CSRF-攻击？" class="headerlink" title="什么是 CSRF 攻击？"></a>什么是 CSRF 攻击？</h3><p>CSRF 代表跨站请求伪造。这是一种攻击，迫使最终用户在当前通过身份验证的Web 应用程序上执行不需要的操作。CSRF 攻击专门针对状态改变请求，而不是数据窃取，因为攻击者无法查看对伪造请求的响应。</p>
<h2 id="监视器"><a href="#监视器" class="headerlink" title="监视器"></a>监视器</h2><h3 id="Spring-Boot-中的监视器是什么？"><a href="#Spring-Boot-中的监视器是什么？" class="headerlink" title="Spring Boot 中的监视器是什么？"></a>Spring Boot 中的监视器是什么？</h3><p>Spring boot actuator 是 spring 启动框架中的重要功能之一。Spring boot 监视器可帮助您访问生产环境中正在运行的应用程序的当前状态。有几个指标必须在生产环境中进行检查和监控。即使一些外部应用程序可能正在使用这些服务来向相关人员触发警报消息。监视器模块公开了一组可直接作为 HTTP URL 访问的REST 端点来检查状态。</p>
<h3 id="如何在-Spring-Boot-中禁用-Actuator-端点安全性？"><a href="#如何在-Spring-Boot-中禁用-Actuator-端点安全性？" class="headerlink" title="如何在 Spring Boot 中禁用 Actuator 端点安全性？"></a>如何在 Spring Boot 中禁用 Actuator 端点安全性？</h3><p>默认情况下，所有敏感的 HTTP 端点都是安全的，只有具有 ACTUATOR 角色的用户才能访问它们。安全性是使用标准的 HttpServletRequest.isUserInRole 方法实施的。 我们可以使用来禁用安全性。只有在执行机构端点在防火墙后访问时，才建议禁用安全性。</p>
<h3 id="我们如何监视所有-Spring-Boot-微服务？"><a href="#我们如何监视所有-Spring-Boot-微服务？" class="headerlink" title="我们如何监视所有 Spring Boot 微服务？"></a>我们如何监视所有 Spring Boot 微服务？</h3><p>Spring Boot 提供监视器端点以监控各个微服务的度量。这些端点对于获取有关应用程序的信息（如它们是否已启动）以及它们的组件（如数据库等）是否正常运行很有帮助。但是，使用监视器的一个主要缺点或困难是，我们必须单独打开应用程序的知识点以了解其状态或健康状况。想象一下涉及 50 个应用程序的微服务，管理员将不得不击中所有 50 个应用程序的执行终端。为了帮助我们处理这种情况，我们将使用位于的开源项目。 它建立在 Spring Boot Actuator 之上，它提供了一个 Web UI，使我们能够可视化多个应用程序的度量。</p>
<h2 id="整合第三方项目"><a href="#整合第三方项目" class="headerlink" title="整合第三方项目"></a>整合第三方项目</h2><h3 id="什么是-WebSockets？"><a href="#什么是-WebSockets？" class="headerlink" title="什么是 WebSockets？"></a>什么是 WebSockets？</h3><p>WebSocket 是一种计算机通信协议，通过单个 TCP 连接提供全双工通信信道。</p>
<p>1、WebSocket 是双向的 -使用 WebSocket 客户端或服务器可以发起消息发送。</p>
<p>2、WebSocket 是全双工的 -客户端和服务器通信是相互独立的。</p>
<p>3、单个 TCP 连接 -初始连接使用 HTTP，然后将此连接升级到基于套接字的连接。然后这个单一连接用于所有未来的通信</p>
<p>4、Light -与 http 相比，WebSocket 消息数据交换要轻得多。</p>
<h3 id="什么是-Spring-Data"><a href="#什么是-Spring-Data" class="headerlink" title="什么是 Spring Data ?"></a>什么是 Spring Data ?</h3><p>Spring Data 是 Spring 的一个子项目。用于简化数据库访问，支持NoSQL 和 关系数据存储。其主要目标是使数据库的访问变得方便快捷。Spring Data 具有如下特点：</p>
<p>SpringData 项目支持 NoSQL 存储：</p>
<ol>
<li>MongoDB （文档数据库）</li>
<li>Neo4j（图形数据库）</li>
<li>Redis（键/值存储）</li>
<li>Hbase（列族数据库）</li>
</ol>
<p>SpringData 项目所支持的关系数据存储技术：</p>
<ol>
<li>JDBC</li>
<li>JPA</li>
</ol>
<p>Spring Data Jpa 致力于减少数据访问层 (DAO) 的开发量. 开发者唯一要做的，就是声明持久层的接口，其他都交给 Spring Data JPA 来帮你完成！Spring Data JPA 通过规范方法的名字，根据符合规范的名字来确定方法需要实现什么样的逻辑。</p>
<h3 id="什么是-Spring-Batch？"><a href="#什么是-Spring-Batch？" class="headerlink" title="什么是 Spring Batch？"></a>什么是 Spring Batch？</h3><p>Spring Boot Batch 提供可重用的函数，这些函数在处理大量记录时非常重要，包括日志/跟踪，事务管理，作业处理统计信息，作业重新启动，跳过和资源管理。它还提供了更先进的技术服务和功能，通过优化和分区技术，可以实现极高批量和高性能批处理作业。简单以及复杂的大批量批处理作业可以高度可扩展的方式利用框架处理重要大量的信息。</p>
<h3 id="什么是-FreeMarker-模板？"><a href="#什么是-FreeMarker-模板？" class="headerlink" title="什么是 FreeMarker 模板？"></a>什么是 FreeMarker 模板？</h3><p>FreeMarker 是一个基于 Java 的模板引擎，最初专注于使用 MVC 软件架构进行动态网页生成。使用 Freemarker 的主要优点是表示层和业务层的完全分离。程序员可以处理应用程序代码，而设计人员可以处理 html 页面设计。最后使用freemarker 可以将这些结合起来，给出最终的输出页面。</p>
<h3 id="如何集成-Spring-Boot-和-ActiveMQ？"><a href="#如何集成-Spring-Boot-和-ActiveMQ？" class="headerlink" title="如何集成 Spring Boot 和 ActiveMQ？"></a>如何集成 Spring Boot 和 ActiveMQ？</h3><p>对于集成 Spring Boot 和 ActiveMQ，我们使用依赖关系。 它只需要很少的配置，并且不需要样板代码。</p>
<h3 id="什么是-Apache-Kafka？"><a href="#什么是-Apache-Kafka？" class="headerlink" title="什么是 Apache Kafka？"></a>什么是 Apache Kafka？</h3><p>Apache Kafka 是一个分布式发布 - 订阅消息系统。它是一个可扩展的，容错的发布 - 订阅消息系统，它使我们能够构建分布式应用程序。这是一个 Apache 顶级项目。Kafka 适合离线和在线消息消费。</p>
<h3 id="什么是-Swagger？你用-Spring-Boot-实现了它吗？"><a href="#什么是-Swagger？你用-Spring-Boot-实现了它吗？" class="headerlink" title="什么是 Swagger？你用 Spring Boot 实现了它吗？"></a>什么是 Swagger？你用 Spring Boot 实现了它吗？</h3><p>Swagger 广泛用于可视化 API，使用 Swagger UI 为前端开发人员提供在线沙箱。Swagger 是用于生成 RESTful Web 服务的可视化表示的工具，规范和完整框架实现。它使文档能够以与服务器相同的速度更新。当通过 Swagger 正确定义时，消费者可以使用最少量的实现逻辑来理解远程服务并与其进行交互。因此，Swagger消除了调用服务时的猜测。</p>
<h3 id="前后端分离，如何维护接口文档"><a href="#前后端分离，如何维护接口文档" class="headerlink" title="前后端分离，如何维护接口文档 ?"></a>前后端分离，如何维护接口文档 ?</h3><p>前后端分离开发日益流行，大部分情况下，我们都是通过 Spring Boot 做前后端分离开发，前后端分离一定会有接口文档，不然会前后端会深深陷入到扯皮中。一个比较笨的方法就是使用 word 或者 md 来维护接口文档，但是效率太低，接口一变，所有人手上的文档都得变。在 Spring Boot 中，这个问题常见的解决方案是 Swagger ，使用 Swagger 我们可以快速生成一个接口文档网站，接口一旦发生变化，文档就会自动更新，所有开发工程师访问这一个在线网站就可以获取到最新的接口文档，非常方便。</p>
<h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><h3 id="如何重新加载-Spring-Boot-上的更改，而无需重新启动服务器？Spring-Boot项目如何热部署？"><a href="#如何重新加载-Spring-Boot-上的更改，而无需重新启动服务器？Spring-Boot项目如何热部署？" class="headerlink" title="如何重新加载 Spring Boot 上的更改，而无需重新启动服务器？Spring Boot项目如何热部署？"></a>如何重新加载 Spring Boot 上的更改，而无需重新启动服务器？Spring Boot项目如何热部署？</h3><p>这可以使用 DEV 工具来实现。通过这种依赖关系，您可以节省任何更改，嵌入式tomcat 将重新启动。Spring Boot 有一个开发工具（DevTools）模块，它有助于提高开发人员的生产力。Java 开发人员面临的一个主要挑战是将文件更改自动部署到服务器并自动重启服务器。开发人员可以重新加载 Spring Boot 上的更改，而无需重新启动服务器。这将消除每次手动部署更改的需要。Spring Boot 在发布它的第一个版本时没有这个功能。这是开发人员最需要的功能。DevTools 模块完全满足开发人员的需求。该模块将在生产环境中被禁用。它还提供 H2 数据库控制台以更好地测试应用程序。</p>
<h3 id="您使用了哪些-starter-maven-依赖项？"><a href="#您使用了哪些-starter-maven-依赖项？" class="headerlink" title="您使用了哪些 starter maven 依赖项？"></a>您使用了哪些 starter maven 依赖项？</h3><p>使用了下面的一些依赖项</p>
<p>spring-boot-starter-activemq</p>
<p>spring-boot-starter-security</p>
<p>这有助于增加更少的依赖关系，并减少版本的冲突。</p>
<h3 id="Spring-Boot-中的-starter-到底是什么"><a href="#Spring-Boot-中的-starter-到底是什么" class="headerlink" title="Spring Boot 中的 starter 到底是什么 ?"></a>Spring Boot 中的 starter 到底是什么 ?</h3><p>首先，这个 Starter 并非什么新的技术点，基本上还是基于 Spring 已有功能来实现的。首先它提供了一个自动化配置类，一般命名为 XXXAutoConfiguration ，在这个配置类中通过条件注解来决定一个配置是否生效（条件注解就是 Spring 中原本就有的），然后它还会提供一系列的默认配置，也允许开发者根据实际情况自定义相关配置，然后通过类型安全的属性注入将这些配置属性注入进来，新注入的属性会代替掉默认属性。正因为如此，很多第三方框架，我们只需要引入依赖就可以直接使用了。当然，开发者也可以自定义 Starter</p>
<h3 id="spring-boot-starter-parent-有什么用"><a href="#spring-boot-starter-parent-有什么用" class="headerlink" title="spring-boot-starter-parent 有什么用 ?"></a>spring-boot-starter-parent 有什么用 ?</h3><p>我们都知道，新创建一个 Spring Boot 项目，默认都是有 parent 的，这个 parent 就是 spring-boot-starter-parent ，spring-boot-starter-parent 主要有如下作用：</p>
<ol>
<li>定义了 Java 编译版本为 1.8 。</li>
<li>使用 UTF-8 格式编码。</li>
<li>继承自 spring-boot-dependencies，这个里边定义了依赖的版本，也正是因为继承了这个依赖，所以我们在写依赖时才不需要写版本号。</li>
<li>执行打包操作的配置。</li>
<li>自动化的资源过滤。</li>
<li>自动化的插件配置。</li>
<li>针对 application.properties 和 application.yml 的资源过滤，包括通过 profile 定义的不同环境的配置文件，例如 application-dev.properties 和 application-dev.yml。</li>
</ol>
<h3 id="Spring-Boot-打成的-jar-和普通的-jar-有什么区别"><a href="#Spring-Boot-打成的-jar-和普通的-jar-有什么区别" class="headerlink" title="Spring Boot 打成的 jar 和普通的 jar 有什么区别 ?"></a>Spring Boot 打成的 jar 和普通的 jar 有什么区别 ?</h3><p>Spring Boot 项目最终打包成的 jar 是可执行 jar ，这种 jar 可以直接通过 java -jar xxx.jar 命令来运行，这种 jar 不可以作为普通的 jar 被其他项目依赖，即使依赖了也无法使用其中的类。</p>
<p>Spring Boot 的 jar 无法被其他项目依赖，主要还是他和普通 jar 的结构不同。普通的 jar 包，解压后直接就是包名，包里就是我们的代码，而 Spring Boot 打包成的可执行 jar 解压后，在 \BOOT-INF\classes 目录下才是我们的代码，因此无法被直接引用。如果非要引用，可以在 pom.xml 文件中增加配置，将 Spring Boot 项目打包成两个 jar ，一个可执行，一个可引用。</p>
<h3 id="运行-Spring-Boot-有哪几种方式？"><a href="#运行-Spring-Boot-有哪几种方式？" class="headerlink" title="运行 Spring Boot 有哪几种方式？"></a>运行 Spring Boot 有哪几种方式？</h3><p>1）打包用命令或者放到容器中运行</p>
<p>2）用 Maven/ Gradle 插件运行</p>
<p>3）直接执行 main 方法运行</p>
<h3 id="Spring-Boot-需要独立的容器运行吗？"><a href="#Spring-Boot-需要独立的容器运行吗？" class="headerlink" title="Spring Boot 需要独立的容器运行吗？"></a>Spring Boot 需要独立的容器运行吗？</h3><p>可以不需要，内置了 Tomcat/ Jetty 等容器。</p>
<h3 id="开启-Spring-Boot-特性有哪几种方式？"><a href="#开启-Spring-Boot-特性有哪几种方式？" class="headerlink" title="开启 Spring Boot 特性有哪几种方式？"></a>开启 Spring Boot 特性有哪几种方式？</h3><p>1）继承spring-boot-starter-parent项目</p>
<p>2）导入spring-boot-dependencies项目依赖</p>
<h3 id="如何使用-Spring-Boot-实现异常处理？"><a href="#如何使用-Spring-Boot-实现异常处理？" class="headerlink" title="如何使用 Spring Boot 实现异常处理？"></a>如何使用 Spring Boot 实现异常处理？</h3><p>Spring 提供了一种使用 ControllerAdvice 处理异常的非常有用的方法。 我们通过实现一个 ControlerAdvice 类，来处理控制器类抛出的所有异常。</p>
<h3 id="如何使用-Spring-Boot-实现分页和排序？"><a href="#如何使用-Spring-Boot-实现分页和排序？" class="headerlink" title="如何使用 Spring Boot 实现分页和排序？"></a>如何使用 Spring Boot 实现分页和排序？</h3><p>使用 Spring Boot 实现分页非常简单。使用 Spring Data-JPA 可以实现将可分页的传递给存储库方法。</p>
<h3 id="微服务中如何实现-session-共享"><a href="#微服务中如何实现-session-共享" class="headerlink" title="微服务中如何实现 session 共享 ?"></a>微服务中如何实现 session 共享 ?</h3><p>在微服务中，一个完整的项目被拆分成多个不相同的独立的服务，各个服务独立部署在不同的服务器上，各自的 session 被从物理空间上隔离开了，但是经常，我们需要在不同微服务之间共享 session ，常见的方案就是 Spring Session + Redis 来实现 session 共享。将所有微服务的 session 统一保存在 Redis 上，当各个微服务对 session 有相关的读写操作时，都去操作 Redis 上的 session 。这样就实现了 session 共享，Spring Session 基于 Spring 中的代理过滤器实现，使得 session 的同步操作对开发人员而言是透明的，非常简便。</p>
<h3 id="微服务中如何实现-session-共享-1"><a href="#微服务中如何实现-session-共享-1" class="headerlink" title="微服务中如何实现 session 共享 ?"></a>微服务中如何实现 session 共享 ?</h3><p>在微服务中，一个完整的项目被拆分成多个不相同的独立的服务，各个服务独立部署在不同的服务器上，各自的 session 被从物理空间上隔离开了，但是经常，我们需要在不同微服务之间共享 session ，常见的方案就是 Spring Session + Redis 来实现 session 共享。将所有微服务的 session 统一保存在 Redis 上，当各个微服务对 session 有相关的读写操作时，都去操作 Redis 上的 session 。这样就实现了 session 共享，Spring Session 基于 Spring 中的代理过滤器实现，使得 session 的同步操作对开发人员而言是透明的，非常简便。</p>
<h3 id="Spring-Boot-中如何实现定时任务"><a href="#Spring-Boot-中如何实现定时任务" class="headerlink" title="Spring Boot 中如何实现定时任务 ?"></a>Spring Boot 中如何实现定时任务 ?</h3><p>定时任务也是一个常见的需求，Spring Boot 中对于定时任务的支持主要还是来自 Spring 框架。</p>
<p>在 Spring Boot 中使用定时任务主要有两种不同的方式，一个就是使用 Spring 中的 @Scheduled 注解，另一个则是使用第三方框架 Quartz。</p>
<p>使用 Spring 中的 @Scheduled 的方式主要通过 @Scheduled 注解来实现。</p>
<p>使用 Quartz ，则按照 Quartz 的方式，定义 Job 和 Trigger 即可。</p>
]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>springboot</tag>
      </tags>
  </entry>
  <entry>
    <title>Spring MVC笔记</title>
    <url>/2020/06/30/Spring-MVC%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<h1 id="Spring-MVC笔记"><a href="#Spring-MVC笔记" class="headerlink" title="Spring-MVC笔记"></a>Spring-MVC笔记</h1><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><h3 id="什么是Spring-MVC？简单介绍下你对Spring-MVC的理解？"><a href="#什么是Spring-MVC？简单介绍下你对Spring-MVC的理解？" class="headerlink" title="什么是Spring MVC？简单介绍下你对Spring MVC的理解？"></a>什么是Spring MVC？简单介绍下你对Spring MVC的理解？</h3><p>Spring MVC是一个基于Java的实现了MVC设计模式的请求驱动类型的轻量级Web框架，通过把模型-视图-控制器分离，将web层进行职责解耦，把复杂的web应用分成逻辑清晰的几部分，简化开发，减少出错，方便组内开发人员之间的配合。</p>
<h3 id="Spring-MVC的优点"><a href="#Spring-MVC的优点" class="headerlink" title="Spring MVC的优点"></a>Spring MVC的优点</h3><p>（1）可以支持各种视图技术,而不仅仅局限于JSP；</p>
<p>（2）与Spring框架集成（如IoC容器、AOP等）；</p>
<p>（3）清晰的角色分配：前端控制器(dispatcherServlet) , 请求到处理器映射（handlerMapping), 处理器适配器（HandlerAdapter), 视图解析器（ViewResolver）。</p>
<p>（4） 支持各种请求资源的映射策略。</p>
<h2 id="核心组件"><a href="#核心组件" class="headerlink" title="核心组件"></a>核心组件</h2><h3 id="Spring-MVC的主要组件？"><a href="#Spring-MVC的主要组件？" class="headerlink" title="Spring MVC的主要组件？"></a>Spring MVC的主要组件？</h3><p>（1）前端控制器 DispatcherServlet（不需要程序员开发）</p>
<p>作用：接收请求、响应结果，相当于转发器，有了DispatcherServlet 就减少了其它组件之间的耦合度。</p>
<p>（2）处理器映射器HandlerMapping（不需要程序员开发）</p>
<p>作用：根据请求的URL来查找Handler</p>
<p>（3）处理器适配器HandlerAdapter</p>
<p>注意：在编写Handler的时候要按照HandlerAdapter要求的规则去编写，这样适配器HandlerAdapter才可以正确的去执行Handler。</p>
<p>（4）处理器Handler（需要程序员开发）</p>
<p>（5）视图解析器 ViewResolver（不需要程序员开发）</p>
<p>作用：进行视图的解析，根据视图逻辑名解析成真正的视图（view）</p>
<p>（6）视图View（需要程序员开发jsp）</p>
<p>View是一个接口， 它的实现类支持不同的视图类型（jsp，freemarker，pdf等等）</p>
<h3 id="什么是DispatcherServlet"><a href="#什么是DispatcherServlet" class="headerlink" title="什么是DispatcherServlet"></a>什么是DispatcherServlet</h3><p>Spring的MVC框架是围绕DispatcherServlet来设计的，它用来处理所有的HTTP请求和响应。</p>
<h3 id="什么是Spring-MVC框架的控制器？"><a href="#什么是Spring-MVC框架的控制器？" class="headerlink" title="什么是Spring MVC框架的控制器？"></a>什么是Spring MVC框架的控制器？</h3><p>控制器提供一个访问应用程序的行为，此行为通常通过服务接口实现。控制器解析用户输入并将其转换为一个由视图呈现给用户的模型。Spring用一个非常抽象的方式实现了一个控制层，允许用户创建多种用途的控制器。</p>
<h3 id="Spring-MVC的控制器是不是单例模式-如果是-有什么问题-怎么解决？"><a href="#Spring-MVC的控制器是不是单例模式-如果是-有什么问题-怎么解决？" class="headerlink" title="Spring MVC的控制器是不是单例模式,如果是,有什么问题,怎么解决？"></a>Spring MVC的控制器是不是单例模式,如果是,有什么问题,怎么解决？</h3><p>答：是单例模式,所以在多线程访问的时候有线程安全问题,不要用同步,会影响性能的,解决方案是在控制器里面不能写字段。</p>
<h2 id="工作原理"><a href="#工作原理" class="headerlink" title="工作原理"></a>工作原理</h2><h3 id="请描述Spring-MVC的工作流程？描述一下-DispatcherServlet-的工作流程？"><a href="#请描述Spring-MVC的工作流程？描述一下-DispatcherServlet-的工作流程？" class="headerlink" title="请描述Spring MVC的工作流程？描述一下 DispatcherServlet 的工作流程？"></a>请描述Spring MVC的工作流程？描述一下 DispatcherServlet 的工作流程？</h3><p><img src="https://lixiangbetter.github.io/2020/06/30/Spring-MVC%E7%AC%94%E8%AE%B0/20200208211439106.png" alt></p>
<h2 id="MVC框架"><a href="#MVC框架" class="headerlink" title="MVC框架"></a>MVC框架</h2><h3 id="MVC是什么？MVC设计模式的好处有哪些"><a href="#MVC是什么？MVC设计模式的好处有哪些" class="headerlink" title="MVC是什么？MVC设计模式的好处有哪些"></a>MVC是什么？MVC设计模式的好处有哪些</h3><p>mvc是一种设计模式（设计模式就是日常开发中编写代码的一种好的方法和经验的总结）。模型（model）-视图（view）-控制器（controller），三层架构的设计模式。用于实现前端页面的展现与后端业务数据处理的分离。</p>
<p>mvc设计模式的好处</p>
<p>1.分层设计，实现了业务系统各个组件之间的解耦，有利于业务系统的可扩展性，可维护性。</p>
<p>2.有利于系统的并行开发，提升开发效率。</p>
<h2 id="常用注解"><a href="#常用注解" class="headerlink" title="常用注解"></a>常用注解</h2><h3 id="注解原理是什么"><a href="#注解原理是什么" class="headerlink" title="注解原理是什么"></a>注解原理是什么</h3><p>注解本质是一个继承了Annotation的特殊接口，其具体实现类是Java运行时生成的动态代理类。我们通过反射获取注解时，返回的是Java运行时生成的动态代理对象。通过代理对象调用自定义注解的方法，会最终调用AnnotationInvocationHandler的invoke方法。该方法会从memberValues这个Map中索引出对应的值。而memberValues的来源是Java常量池。</p>
<h3 id="Spring-MVC常用的注解有哪些？"><a href="#Spring-MVC常用的注解有哪些？" class="headerlink" title="Spring MVC常用的注解有哪些？"></a>Spring MVC常用的注解有哪些？</h3><p>@RequestMapping：用于处理请求 url 映射的注解，可用于类或方法上。用于类上，则表示类中的所有响应请求的方法都是以该地址作为父路径。</p>
<p>@RequestBody：注解实现接收http请求的json数据，将json转换为java对象。</p>
<p>@ResponseBody：注解实现将conreoller方法返回对象转化为json对象响应给客户。</p>
<h3 id="SpingMvc中的控制器的注解一般用哪个-有没有别的注解可以替代？"><a href="#SpingMvc中的控制器的注解一般用哪个-有没有别的注解可以替代？" class="headerlink" title="SpingMvc中的控制器的注解一般用哪个,有没有别的注解可以替代？"></a>SpingMvc中的控制器的注解一般用哪个,有没有别的注解可以替代？</h3><p>一般用@Controller注解,也可以使用@RestController,@RestController注解相当于@ResponseBody ＋ @Controller,表示是表现层,除此之外，一般不用别的注解代替。</p>
<h3 id="Controller注解的作用"><a href="#Controller注解的作用" class="headerlink" title="@Controller注解的作用"></a>@Controller注解的作用</h3><p>在Spring MVC 中，控制器Controller 负责处理由DispatcherServlet 分发的请求，它把用户请求的数据经过业务处理层处理之后封装成一个Model ，然后再把该Model 返回给对应的View 进行展示。</p>
<h3 id="RequestMapping注解的作用"><a href="#RequestMapping注解的作用" class="headerlink" title="@RequestMapping注解的作用"></a>@RequestMapping注解的作用</h3><p>RequestMapping注解有六个属性，下面我们把她分成三类进行说明（下面有相应示例）。</p>
<p>value， method</p>
<p>value： 指定请求的实际地址，指定的地址可以是URI Template 模式（后面将会说明）；</p>
<p>method： 指定请求的method类型， GET、POST、PUT、DELETE等；</p>
<p>consumes，produces</p>
<p>consumes： 指定处理请求的提交内容类型（Content-Type），例如application/json, text/html;</p>
<p>produces: 指定返回的内容类型，仅当request请求头中的(Accept)类型中包含该指定类型才返回；</p>
<p>params，headers</p>
<p>params： 指定request中必须包含某些参数值是，才让该方法处理。</p>
<p>headers： 指定request中必须包含某些指定的header值，才能让该方法处理请求。</p>
<h3 id="ResponseBody注解的作用"><a href="#ResponseBody注解的作用" class="headerlink" title="@ResponseBody注解的作用"></a>@ResponseBody注解的作用</h3><p>作用： 该注解用于将Controller的方法返回的对象，通过适当的HttpMessageConverter转换为指定格式后，写入到Response对象的body数据区。</p>
<p>使用时机：返回的数据不是html标签的页面，而是其他某种格式的数据时（如json、xml等）使用；</p>
<h3 id="PathVariable和-RequestParam的区别"><a href="#PathVariable和-RequestParam的区别" class="headerlink" title="@PathVariable和@RequestParam的区别"></a>@PathVariable和@RequestParam的区别</h3><p>请求路径上有个id的变量值，可以通过@PathVariable来获取 @RequestMapping(value = “/page/{id}”, method = RequestMethod.GET)</p>
<p>@RequestParam用来获得静态的URL请求入参 spring注解时action里用到。</p>
<h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><h3 id="Spring-MVC与Struts2区别"><a href="#Spring-MVC与Struts2区别" class="headerlink" title="Spring MVC与Struts2区别"></a>Spring MVC与Struts2区别</h3><h3 id="Spring-MVC怎么样设定重定向和转发的？"><a href="#Spring-MVC怎么样设定重定向和转发的？" class="headerlink" title="Spring MVC怎么样设定重定向和转发的？"></a>Spring MVC怎么样设定重定向和转发的？</h3><p>（1）转发：在返回值前面加”forward:”，譬如”forward:user.do?name=method4”</p>
<p>（2）重定向：在返回值前面加”redirect:”，譬如”redirect:<a href="http://www.baidu.com&quot;" target="_blank" rel="noopener">http://www.baidu.com&quot;</a></p>
<h3 id="Spring-MVC怎么和AJAX相互调用的？"><a href="#Spring-MVC怎么和AJAX相互调用的？" class="headerlink" title="Spring MVC怎么和AJAX相互调用的？"></a>Spring MVC怎么和AJAX相互调用的？</h3><p>通过Jackson框架就可以把Java里面的对象直接转化成Js可以识别的Json对象。具体步骤如下 ：</p>
<p>（1）加入Jackson.jar</p>
<p>（2）在配置文件中配置json的映射</p>
<p>（3）在接受Ajax方法里面可以直接返回Object,List等,但方法前面要加上@ResponseBody注解。</p>
<h3 id="如何解决POST请求中文乱码问题，GET的又如何处理呢？"><a href="#如何解决POST请求中文乱码问题，GET的又如何处理呢？" class="headerlink" title="如何解决POST请求中文乱码问题，GET的又如何处理呢？"></a>如何解决POST请求中文乱码问题，GET的又如何处理呢？</h3><p>Post:在web.xml中配置一个CharacterEncodingFilter过滤器，设置成utf-8；</p>
<p>Get:</p>
<p>①修改tomcat配置文件添加编码与工程编码一致<br>②另外一种方法对参数进行重新编码</p>
<h3 id="Spring-MVC的异常处理？"><a href="#Spring-MVC的异常处理？" class="headerlink" title="Spring MVC的异常处理？"></a>Spring MVC的异常处理？</h3><p>可以将异常抛给Spring框架，由Spring框架来处理；我们只需要配置简单的异常处理器，在异常处理器中添视图页面即可。</p>
<h3 id="如果在拦截请求中，我想拦截get方式提交的方法-怎么配置"><a href="#如果在拦截请求中，我想拦截get方式提交的方法-怎么配置" class="headerlink" title="如果在拦截请求中，我想拦截get方式提交的方法,怎么配置"></a>如果在拦截请求中，我想拦截get方式提交的方法,怎么配置</h3><p>method=RequestMethod.GET。</p>
<h3 id="怎样在方法里面得到Request-或者Session？"><a href="#怎样在方法里面得到Request-或者Session？" class="headerlink" title="怎样在方法里面得到Request,或者Session？"></a>怎样在方法里面得到Request,或者Session？</h3><p>request,Spring MVC就自动把request对象传入。</p>
<h3 id="如果想在拦截的方法里面得到从前台传入的参数-怎么得到？"><a href="#如果想在拦截的方法里面得到从前台传入的参数-怎么得到？" class="headerlink" title="如果想在拦截的方法里面得到从前台传入的参数,怎么得到？"></a>如果想在拦截的方法里面得到从前台传入的参数,怎么得到？</h3><p>直接在形参里面声明这个参数就可以,但必须名字和传过来的参数一样。</p>
<h3 id="如果前台有很多个参数传入-并且这些参数都是一个对象的-那么怎么样快速得到这个对象？"><a href="#如果前台有很多个参数传入-并且这些参数都是一个对象的-那么怎么样快速得到这个对象？" class="headerlink" title="如果前台有很多个参数传入,并且这些参数都是一个对象的,那么怎么样快速得到这个对象？"></a>如果前台有很多个参数传入,并且这些参数都是一个对象的,那么怎么样快速得到这个对象？</h3><p>直接在方法中声明这个对象,Spring MVC就自动会把属性赋值到这个对象里面。</p>
<h3 id="Spring-MVC中函数的返回值是什么？"><a href="#Spring-MVC中函数的返回值是什么？" class="headerlink" title="Spring MVC中函数的返回值是什么？"></a>Spring MVC中函数的返回值是什么？</h3><p>返回值可以有很多类型,有String, ModelAndView。ModelAndView类把视图和数据都合并的一起的，但一般用String比较好。</p>
<h3 id="Spring-MVC用什么对象从后台向前台传递数据的？"><a href="#Spring-MVC用什么对象从后台向前台传递数据的？" class="headerlink" title="Spring MVC用什么对象从后台向前台传递数据的？"></a>Spring MVC用什么对象从后台向前台传递数据的？</h3><p>通过ModelMap对象,可以在这个对象里面调用put方法,把对象加到里面,前台就可以通过el表达式拿到。</p>
<h3 id="怎么样把ModelMap里面的数据放入Session里面？"><a href="#怎么样把ModelMap里面的数据放入Session里面？" class="headerlink" title="怎么样把ModelMap里面的数据放入Session里面？"></a>怎么样把ModelMap里面的数据放入Session里面？</h3><p>可以在类上面加上@SessionAttributes注解,里面包含的字符串就是要放入session里面的key。</p>
<h3 id="Spring-MVC里面拦截器是怎么写的"><a href="#Spring-MVC里面拦截器是怎么写的" class="headerlink" title="Spring MVC里面拦截器是怎么写的"></a>Spring MVC里面拦截器是怎么写的</h3><p>有两种写法,一种是实现HandlerInterceptor接口，另外一种是继承适配器类，接着在接口方法当中，实现处理逻辑；然后在Spring MVC的配置文件中配置拦截器即可</p>
<h3 id="介绍一下-WebApplicationContext"><a href="#介绍一下-WebApplicationContext" class="headerlink" title="介绍一下 WebApplicationContext"></a>介绍一下 WebApplicationContext</h3><p>WebApplicationContext 继承了ApplicationContext 并增加了一些WEB应用必备的特有功能，它不同于一般的ApplicationContext ，因为它能处理主题，并找到被关联的servlet。</p>
]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>spring mvc</tag>
      </tags>
  </entry>
  <entry>
    <title>Spring笔记</title>
    <url>/2020/06/29/Spring%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<h1 id="Spring笔记"><a href="#Spring笔记" class="headerlink" title="Spring笔记"></a>Spring笔记</h1><h2 id="Spring概述（10）"><a href="#Spring概述（10）" class="headerlink" title="Spring概述（10）"></a>Spring概述（10）</h2><h3 id="什么是spring"><a href="#什么是spring" class="headerlink" title="什么是spring?"></a>什么是spring?</h3><p>为了降低Java开发的复杂性，Spring采取了以下4种关键策略</p>
<ul>
<li>基于POJO的轻量级和最小侵入性编程；</li>
<li>通过依赖注入和面向接口实现松耦合；</li>
<li>基于切面和惯例进行声明式编程；</li>
<li>通过切面和模板减少样板式代码。</li>
</ul>
<h3 id="Spring框架的设计目标，设计理念，和核心是什么"><a href="#Spring框架的设计目标，设计理念，和核心是什么" class="headerlink" title="Spring框架的设计目标，设计理念，和核心是什么"></a>Spring框架的设计目标，设计理念，和核心是什么</h3><p>Spring设计目标：Spring为开发者提供一个一站式轻量级应用开发平台；</p>
<p>Spring设计理念：在JavaEE开发中，支持POJO和JavaBean开发方式，使应用面向接口开发，充分支持OO（面向对象）设计方法；Spring通过IoC容器实现对象耦合关系的管理，并实现依赖反转，将对象之间的依赖关系交给IoC容器，实现解耦；</p>
<p>Spring框架的核心：IoC容器和AOP模块。通过IoC容器管理POJO对象以及他们之间的耦合关系；通过AOP以动态非侵入的方式增强服务。</p>
<p>IoC让相互协作的组件保持松散的耦合，而AOP编程允许你把遍布于应用各层的功能分离出来形成可重用的功能组件。</p>
<h3 id="Spring的优缺点是什么？"><a href="#Spring的优缺点是什么？" class="headerlink" title="Spring的优缺点是什么？"></a>Spring的优缺点是什么？</h3><p>优点</p>
<ul>
<li><p>方便解耦，简化开发</p>
<p>Spring就是一个大工厂，可以将所有对象的创建和依赖关系的维护，交给Spring管理。</p>
</li>
<li><p>AOP编程的支持</p>
<p>Spring提供面向切面编程，可以方便的实现对程序进行权限拦截、运行监控等功能。</p>
</li>
<li><p>声明式事务的支持</p>
<p>只需要通过配置就可以完成对事务的管理，而无需手动编程。</p>
</li>
<li><p>方便程序的测试</p>
<p>Spring对Junit4支持，可以通过注解方便的测试Spring程序。</p>
</li>
<li><p>方便集成各种优秀框架</p>
<p>Spring不排斥各种优秀的开源框架，其内部提供了对各种优秀框架的直接支持（如：Struts、Hibernate、MyBatis等）。</p>
</li>
<li><p>降低JavaEE API的使用难度</p>
<p>Spring对JavaEE开发中非常难用的一些API（JDBC、JavaMail、远程调用等），都提供了封装，使这些API应用难度大大降低。</p>
</li>
</ul>
<p>缺点</p>
<p>Spring明明一个很轻量级的框架，却给人感觉大而全<br>Spring依赖反射，反射影响性能<br>使用门槛升高，入门Spring需要较长时间</p>
<h3 id="Spring有哪些应用场景"><a href="#Spring有哪些应用场景" class="headerlink" title="Spring有哪些应用场景"></a>Spring有哪些应用场景</h3><p>应用场景：JavaEE企业应用开发，包括SSH、SSM等</p>
<p>Spring价值：</p>
<ul>
<li>Spring是非侵入式的框架，目标是使应用程序代码对框架依赖最小化；</li>
<li>Spring提供一个一致的编程模型，使应用直接使用POJO开发，与运行环境隔离开来；</li>
<li>Spring推动应用设计风格向面向对象和面向接口开发转变，提高了代码的重用性和可测试性；</li>
</ul>
<h3 id="Spring由哪些模块组成？"><a href="#Spring由哪些模块组成？" class="headerlink" title="Spring由哪些模块组成？"></a>Spring由哪些模块组成？</h3><p>Spring 总共大约有 20 个模块， 由 1300 多个不同的文件构成。 而这些组件被分别整合在核心容器（Core Container） 、 AOP（Aspect Oriented Programming）和设备支持（Instrmentation） 、数据访问与集成（Data Access/Integeration） 、 Web、 消息（Messaging） 、 Test等 6 个模块中。</p>
<ul>
<li>spring core：提供了框架的基本组成部分，包括控制反转（Inversion of Control，IOC）和依赖注入（Dependency Injection，DI）功能。</li>
<li>spring beans：提供了BeanFactory，是工厂模式的一个经典实现，Spring将管理对象称为Bean。</li>
<li>spring context：构建于 core 封装包基础上的 context 封装包，提供了一种框架式的对象访问方法。</li>
<li>spring jdbc：提供了一个JDBC的抽象层，消除了烦琐的JDBC编码和数据库厂商特有的错误代码解析， 用于简化JDBC。</li>
<li>spring aop：提供了面向切面的编程实现，让你可以自定义拦截器、切点等。</li>
<li>spring Web：提供了针对 Web 开发的集成特性，例如文件上传，利用 servlet listeners 进行 ioc 容器初始化和针对 Web 的 ApplicationContext。</li>
<li>spring test：主要为测试提供支持的，支持使用JUnit或TestNG对Spring组件进行单元测试和集成测试。</li>
</ul>
<h3 id="Spring-框架中都用到了哪些设计模式？"><a href="#Spring-框架中都用到了哪些设计模式？" class="headerlink" title="Spring 框架中都用到了哪些设计模式？"></a>Spring 框架中都用到了哪些设计模式？</h3><ul>
<li>工厂模式：BeanFactory就是简单工厂模式的体现，用来创建对象的实例；</li>
<li>单例模式：Bean默认为单例模式。</li>
<li>代理模式：Spring的AOP功能用到了JDK的动态代理和CGLIB字节码生成技术；</li>
<li>模板方法：用来解决代码重复的问题。比如. RestTemplate, JmsTemplate, JpaTemplate。</li>
<li>观察者模式：定义对象键一种一对多的依赖关系，当一个对象的状态发生改变时，所有依赖于它的对象都会得到通知被制动更新，如Spring中listener的实现–ApplicationListener。</li>
</ul>
<h3 id="详细讲解一下核心容器（spring-context应用上下文-模块"><a href="#详细讲解一下核心容器（spring-context应用上下文-模块" class="headerlink" title="详细讲解一下核心容器（spring context应用上下文) 模块"></a>详细讲解一下核心容器（spring context应用上下文) 模块</h3><h3 id="Spring框架中有哪些不同类型的事件"><a href="#Spring框架中有哪些不同类型的事件" class="headerlink" title="Spring框架中有哪些不同类型的事件"></a>Spring框架中有哪些不同类型的事件</h3><p>Spring 提供了以下5种标准的事件：</p>
<ul>
<li>上下文更新事件（ContextRefreshedEvent）：在调用ConfigurableApplicationContext 接口中的refresh()方法时被触发。</li>
<li>上下文开始事件（ContextStartedEvent）：当容器调用ConfigurableApplicationContext的Start()方法开始/重新开始容器时触发该事件。</li>
<li>上下文停止事件（ContextStoppedEvent）：当容器调用ConfigurableApplicationContext的Stop()方法停止容器时触发该事件。</li>
<li>上下文关闭事件（ContextClosedEvent）：当ApplicationContext被关闭时触发该事件。容器被关闭时，其管理的所有单例Bean都被销毁。</li>
<li>请求处理事件（RequestHandledEvent）：在Web应用中，当一个http请求（request）结束触发该事件。如果一个bean实现了ApplicationListener接口，当一个ApplicationEvent 被发布以后，bean会自动被通知。</li>
</ul>
<h3 id="Spring-应用程序有哪些不同组件？"><a href="#Spring-应用程序有哪些不同组件？" class="headerlink" title="Spring 应用程序有哪些不同组件？"></a>Spring 应用程序有哪些不同组件？</h3><p>Spring 应用一般有以下组件：</p>
<ul>
<li>接口 - 定义功能。</li>
<li>Bean 类 - 它包含属性，setter 和 getter 方法，函数等。</li>
<li>Bean 配置文件 - 包含类的信息以及如何配置它们。</li>
<li>Spring 面向切面编程（AOP） - 提供面向切面编程的功能。</li>
<li>用户程序 - 它使用接口。</li>
</ul>
<h3 id="使用-Spring-有哪些方式？"><a href="#使用-Spring-有哪些方式？" class="headerlink" title="使用 Spring 有哪些方式？"></a>使用 Spring 有哪些方式？</h3><p>使用 Spring 有以下方式：</p>
<ul>
<li>作为一个成熟的 Spring Web 应用程序。</li>
<li>作为第三方 Web 框架，使用 Spring Frameworks 中间层。</li>
<li>作为企业级 Java Bean，它可以包装现有的 POJO（Plain Old Java Objects）。</li>
<li>用于远程使用。</li>
</ul>
<h2 id="Spring控制反转-IOC-（13）"><a href="#Spring控制反转-IOC-（13）" class="headerlink" title="Spring控制反转(IOC)（13）"></a>Spring控制反转(IOC)（13）</h2><h3 id="什么是Spring-IOC-容器？"><a href="#什么是Spring-IOC-容器？" class="headerlink" title="什么是Spring IOC 容器？"></a>什么是Spring IOC 容器？</h3><p>Spring IOC 负责创建对象，管理对象（通过依赖注入（DI），装配对象，配置对象，并且管理这些对象的整个生命周期。</p>
<h3 id="控制反转-IoC-有什么作用"><a href="#控制反转-IoC-有什么作用" class="headerlink" title="控制反转(IoC)有什么作用"></a>控制反转(IoC)有什么作用</h3><ul>
<li>管理对象的创建和依赖关系的维护。对象的创建并不是一件简单的事，在对象关系比较复杂时，如果依赖关系需要程序猿来维护的话，那是相当头疼的</li>
<li>解耦，由容器去维护具体的对象</li>
<li>托管了类的产生过程，比如我们需要在类的产生过程中做一些处理，最直接的例子就是代理，如果有容器程序可以把这部分处理交给容器，应用程序则无需去关心类是如何完成代理的</li>
</ul>
<h3 id="IOC的优点是什么？"><a href="#IOC的优点是什么？" class="headerlink" title="IOC的优点是什么？"></a>IOC的优点是什么？</h3><ul>
<li>IOC 或 依赖注入把应用的代码量降到最低。</li>
<li>它使应用容易测试，单元测试不再需要单例和JNDI查找机制。</li>
<li>最小的代价和最小的侵入性使松散耦合得以实现。</li>
<li>IOC容器支持加载服务时的饿汉式初始化和懒加载。</li>
</ul>
<h3 id="Spring-IoC-的实现机制"><a href="#Spring-IoC-的实现机制" class="headerlink" title="Spring IoC 的实现机制"></a>Spring IoC 的实现机制</h3><p>Spring 中的 IoC 的实现原理就是工厂模式加反射机制。</p>
<h3 id="Spring-的-IoC支持哪些功能"><a href="#Spring-的-IoC支持哪些功能" class="headerlink" title="Spring 的 IoC支持哪些功能"></a>Spring 的 IoC支持哪些功能</h3><p>Spring 的 IoC 设计支持以下功能：</p>
<ul>
<li>依赖注入</li>
<li>依赖检查</li>
<li>自动装配</li>
<li>支持集合</li>
<li>指定初始化方法和销毁方法</li>
<li>支持回调某些方法（但是需要实现 Spring 接口，略有侵入）</li>
</ul>
<p>其中，最重要的就是依赖注入，从 XML 的配置上说，即 ref 标签。对应 Spring RuntimeBeanReference 对象。</p>
<p>对于 IoC 来说，最重要的就是容器。容器管理着 Bean 的生命周期，控制着 Bean 的依赖注入。</p>
<h3 id="BeanFactory-和-ApplicationContext有什么区别？"><a href="#BeanFactory-和-ApplicationContext有什么区别？" class="headerlink" title="BeanFactory 和 ApplicationContext有什么区别？"></a>BeanFactory 和 ApplicationContext有什么区别？</h3><h3 id="Spring-如何设计容器的，BeanFactory和ApplicationContext的关系详解"><a href="#Spring-如何设计容器的，BeanFactory和ApplicationContext的关系详解" class="headerlink" title="Spring 如何设计容器的，BeanFactory和ApplicationContext的关系详解"></a>Spring 如何设计容器的，BeanFactory和ApplicationContext的关系详解</h3><p>BeanFactory 简单粗暴，可以理解为就是个 HashMap，Key 是 BeanName，Value 是 Bean 实例。通常只提供注册（put），获取（get）这两个功能。我们可以称之为 <strong>“低级容器”</strong>。</p>
<p>ApplicationContext 可以称之为 “高级容器”。因为他比 BeanFactory 多了更多的功能。他继承了多个接口。因此具备了更多的功能。例如资源的获取，支持多种消息（例如 JSP tag 的支持），对 BeanFactory 多了工具级别的支持等待。所以你看他的名字，已经不是 BeanFactory 之类的工厂了，而是 “应用上下文”， 代表着整个大容器的所有功能。该接口定义了一个 refresh 方法，此方法是所有阅读 Spring 源码的人的最熟悉的方法，用于刷新整个容器，即重新加载/刷新所有的 bean。</p>
<p>小结</p>
<p>说了这么多，不知道你有没有理解Spring IoC？ 这里小结一下：IoC 在 Spring 里，只需要低级容器就可以实现，2 个步骤：</p>
<p>加载配置文件，解析成 BeanDefinition 放在 Map 里。</p>
<p>调用 getBean 的时候，从 BeanDefinition 所属的 Map 里，拿出 Class 对象进行实例化，同时，如果有依赖关系，将递归调用 getBean 方法 —— 完成依赖注入。</p>
<p>上面就是 Spring 低级容器（BeanFactory）的 IoC。</p>
<p>至于高级容器 ApplicationContext，他包含了低级容器的功能，当他执行 refresh 模板方法的时候，将刷新整个容器的 Bean。同时其作为高级容器，包含了太多的功能。一句话，他不仅仅是 IoC。他支持不同信息源头，支持 BeanFactory 工具类，支持层级容器，支持访问文件资源，支持事件发布通知，支持接口回调等等。</p>
<h3 id="ApplicationContext通常的实现是什么？"><a href="#ApplicationContext通常的实现是什么？" class="headerlink" title="ApplicationContext通常的实现是什么？"></a>ApplicationContext通常的实现是什么？</h3><p>FileSystemXmlApplicationContext ：此容器从一个XML文件中加载beans的定义，XML Bean 配置文件的全路径名必须提供给它的构造函数。</p>
<p>ClassPathXmlApplicationContext：此容器也从一个XML文件中加载beans的定义，这里，你需要正确设置classpath因为这个容器将在classpath里找bean配置。</p>
<p>WebXmlApplicationContext：此容器加载一个XML文件，此文件定义了一个WEB应用的所有bean。</p>
<h3 id="什么是Spring的依赖注入？"><a href="#什么是Spring的依赖注入？" class="headerlink" title="什么是Spring的依赖注入？"></a>什么是Spring的依赖注入？</h3><p>依赖注入：相对于IoC而言，依赖注入(DI)更加准确地描述了IoC的设计理念。所谓依赖注入（Dependency Injection），即组件之间的依赖关系由容器在应用系统运行期来决定，也就是由容器动态地将某种依赖关系的目标对象实例注入到应用系统中的各个关联的组件之中。组件不做定位查询，只提供普通的Java方法让容器去决定依赖关系。</p>
<h3 id="依赖注入的基本原则"><a href="#依赖注入的基本原则" class="headerlink" title="依赖注入的基本原则"></a>依赖注入的基本原则</h3><p>依赖注入的基本原则是：应用组件不应该负责查找资源或者其他依赖的协作对象。配置对象的工作应该由IoC容器负责，“查找资源”的逻辑应该从应用组件的代码中抽取出来，交给IoC容器负责。容器全权负责组件的装配，它会把符合依赖关系的对象通过属性（JavaBean中的setter）或者是构造器传递给需要的对象。</p>
<h3 id="依赖注入有什么优势"><a href="#依赖注入有什么优势" class="headerlink" title="依赖注入有什么优势"></a>依赖注入有什么优势</h3><p>依赖注入之所以更流行是因为它是一种更可取的方式：让容器全权负责依赖查询，受管组件只需要暴露JavaBean的setter方法或者带参数的构造器或者接口，使容器可以在初始化时组装对象的依赖关系。其与依赖查找方式相比，主要优势为：</p>
<ul>
<li>查找定位操作与应用代码完全无关。</li>
<li>不依赖于容器的API，可以很容易地在任何容器以外使用应用对象。</li>
<li>不需要特殊的接口，绝大多数对象可以做到完全不必依赖容器。</li>
</ul>
<h3 id="有哪些不同类型的依赖注入实现方式？"><a href="#有哪些不同类型的依赖注入实现方式？" class="headerlink" title="有哪些不同类型的依赖注入实现方式？"></a>有哪些不同类型的依赖注入实现方式？</h3><p>依赖注入是时下最流行的IoC实现方式，依赖注入分为接口注入（Interface Injection），Setter方法注入（Setter Injection）和构造器注入（Constructor Injection）三种方式。其中接口注入由于在灵活性和易用性比较差，现在从Spring4开始已被废弃。</p>
<p>构造器依赖注入：构造器依赖注入通过容器触发一个类的构造器来实现的，该类有一系列参数，每个参数代表一个对其他类的依赖。</p>
<p>Setter方法注入：Setter方法注入是容器通过调用无参构造器或无参static工厂 方法实例化bean之后，调用该bean的setter方法，即实现了基于setter的依赖注入。</p>
<h3 id="构造器依赖注入和-Setter方法注入的区别"><a href="#构造器依赖注入和-Setter方法注入的区别" class="headerlink" title="构造器依赖注入和 Setter方法注入的区别"></a>构造器依赖注入和 Setter方法注入的区别</h3><table>
<thead>
<tr>
<th><strong>构造函数注入</strong></th>
<th><strong>setter</strong> <strong>注入</strong></th>
</tr>
</thead>
<tbody><tr>
<td>没有部分注入</td>
<td>有部分注入</td>
</tr>
<tr>
<td>不会覆盖 setter 属性</td>
<td>会覆盖 setter 属性</td>
</tr>
<tr>
<td>任意修改都会创建一个新实例</td>
<td>任意修改不会创建一个新实例</td>
</tr>
<tr>
<td>适用于设置很多属性</td>
<td>适用于设置少量属性</td>
</tr>
</tbody></table>
<p>两种依赖方式都可以使用，构造器注入和Setter方法注入。最好的解决方案是用构造器参数实现强制依赖，setter方法实现可选依赖。</p>
<h2 id="Spring-Beans（19）"><a href="#Spring-Beans（19）" class="headerlink" title="Spring Beans（19）"></a>Spring Beans（19）</h2><h3 id="什么是Spring-beans？"><a href="#什么是Spring-beans？" class="headerlink" title="什么是Spring beans？"></a>什么是Spring beans？</h3><p>Spring beans 是那些形成Spring应用的主干的java对象。它们被Spring IOC容器初始化，装配，和管理。这些beans通过容器中配置的元数据创建。比如，以XML文件中 的形式定义。</p>
<h3 id="一个-Spring-Bean-定义-包含什么？"><a href="#一个-Spring-Bean-定义-包含什么？" class="headerlink" title="一个 Spring Bean 定义 包含什么？"></a>一个 Spring Bean 定义 包含什么？</h3><p>一个Spring Bean 的定义包含容器必知的所有配置元数据，包括如何创建一个bean，它的生命周期详情及它的依赖。</p>
<h3 id="如何给Spring-容器提供配置元数据？Spring有几种配置方式"><a href="#如何给Spring-容器提供配置元数据？Spring有几种配置方式" class="headerlink" title="如何给Spring 容器提供配置元数据？Spring有几种配置方式"></a>如何给Spring 容器提供配置元数据？Spring有几种配置方式</h3><p>这里有三种重要的方法给Spring 容器提供配置元数据。</p>
<ul>
<li>XML配置文件。</li>
<li>基于注解的配置。</li>
<li>基于java的配置。</li>
</ul>
<h3 id="Spring配置文件包含了哪些信息"><a href="#Spring配置文件包含了哪些信息" class="headerlink" title="Spring配置文件包含了哪些信息"></a>Spring配置文件包含了哪些信息</h3><p>Spring配置文件是个XML 文件，这个文件包含了类信息，描述了如何配置它们，以及如何相互调用。</p>
<h3 id="Spring基于xml注入bean的几种方式"><a href="#Spring基于xml注入bean的几种方式" class="headerlink" title="Spring基于xml注入bean的几种方式"></a>Spring基于xml注入bean的几种方式</h3><ol>
<li>Set方法注入；</li>
<li>构造器注入：①通过index设置参数的位置；②通过type设置参数类型；</li>
<li>静态工厂注入；</li>
<li>实例工厂；</li>
</ol>
<h3 id="你怎样定义类的作用域？"><a href="#你怎样定义类的作用域？" class="headerlink" title="你怎样定义类的作用域？"></a>你怎样定义类的作用域？</h3><p>当定义一个 在Spring里，我们还能给这个bean声明一个作用域。它可以通过bean 定义中的scope属性来定义。如，当Spring要在需要的时候每次生产一个新的bean实例，bean的scope属性被指定为prototype。另一方面，一个bean每次使用的时候必须返回同一个实例，这个bean的scope 属性 必须设为 singleton。</p>
<h3 id="解释Spring支持的几种bean的作用域"><a href="#解释Spring支持的几种bean的作用域" class="headerlink" title="解释Spring支持的几种bean的作用域"></a>解释Spring支持的几种bean的作用域</h3><p>Spring框架支持以下五种bean的作用域：</p>
<ul>
<li>singleton : bean在每个Spring ioc 容器中只有一个实例。</li>
<li>prototype：一个bean的定义可以有多个实例。</li>
<li>request：每次http请求都会创建一个bean，该作用域仅在基于web的Spring ApplicationContext情形下有效。</li>
<li>session：在一个HTTP Session中，一个bean定义对应一个实例。该作用域仅在基于web的Spring ApplicationContext情形下有效。</li>
<li>global-session：在一个全局的HTTP Session中，一个bean定义对应一个实例。该作用域仅在基于web的Spring ApplicationContext情形下有效。</li>
</ul>
<p>注意： 缺省的Spring bean 的作用域是Singleton。使用 prototype 作用域需要慎重的思考，因为频繁创建和销毁 bean 会带来很大的性能开销。</p>
<h3 id="Spring框架中的单例bean是线程安全的吗？"><a href="#Spring框架中的单例bean是线程安全的吗？" class="headerlink" title="Spring框架中的单例bean是线程安全的吗？"></a>Spring框架中的单例bean是线程安全的吗？</h3><p>不是，Spring框架中的单例bean不是线程安全的。</p>
<p>spring 中的 bean 默认是单例模式，spring 框架并没有对单例 bean 进行多线程的封装处理。</p>
<p>实际上大部分时候 spring bean 无状态的（比如 dao 类），所有某种程度上来说 bean 也是安全的，但如果 bean 有状态的话（比如 view model 对象），那就要开发者自己去保证线程安全了，最简单的就是改变 bean 的作用域，把“singleton”变更为“prototype”，这样请求 bean 相当于 new Bean()了，所以就可以保证线程安全了。</p>
<ul>
<li>有状态就是有数据存储功能。</li>
<li>无状态就是不会保存数据。</li>
</ul>
<h3 id="Spring如何处理线程并发问题？"><a href="#Spring如何处理线程并发问题？" class="headerlink" title="Spring如何处理线程并发问题？"></a>Spring如何处理线程并发问题？</h3><p>在一般情况下，只有无状态的Bean才可以在多线程环境下共享，在Spring中，绝大部分Bean都可以声明为singleton作用域，因为Spring对一些Bean中非线程安全状态采用ThreadLocal进行处理，解决线程安全问题。</p>
<p>ThreadLocal和线程同步机制都是为了解决多线程中相同变量的访问冲突问题。同步机制采用了“时间换空间”的方式，仅提供一份变量，不同的线程在访问前需要获取锁，没获得锁的线程则需要排队。而ThreadLocal采用了“空间换时间”的方式。</p>
<p>ThreadLocal会为每一个线程提供一个独立的变量副本，从而隔离了多个线程对数据的访问冲突。因为每一个线程都拥有自己的变量副本，从而也就没有必要对该变量进行同步了。ThreadLocal提供了线程安全的共享对象，在编写多线程代码时，可以把不安全的变量封装进ThreadLocal。</p>
<h3 id="解释Spring框架中bean的生命周期"><a href="#解释Spring框架中bean的生命周期" class="headerlink" title="解释Spring框架中bean的生命周期"></a>解释Spring框架中bean的生命周期</h3><h3 id="哪些是重要的bean生命周期方法？-你能重载它们吗？"><a href="#哪些是重要的bean生命周期方法？-你能重载它们吗？" class="headerlink" title="哪些是重要的bean生命周期方法？ 你能重载它们吗？"></a>哪些是重要的bean生命周期方法？ 你能重载它们吗？</h3><p>有两个重要的bean 生命周期方法，第一个是setup ， 它是在容器加载bean的时候被调用。第二个方法是 teardown 它是在容器卸载类的时候被调用。</p>
<p>bean 标签有两个重要的属性（init-method和destroy-method）。用它们你可以自己定制初始化和注销方法。它们也有相应的注解（@PostConstruct和@PreDestroy）。</p>
<h3 id="什么是Spring的内部bean？什么是Spring-inner-beans？"><a href="#什么是Spring的内部bean？什么是Spring-inner-beans？" class="headerlink" title="什么是Spring的内部bean？什么是Spring inner beans？"></a>什么是Spring的内部bean？什么是Spring inner beans？</h3><p>在Spring框架中，当一个bean仅被用作另一个bean的属性时，它能被声明为一个内部bean。内部bean可以用setter注入“属性”和构造方法注入“构造参数”的方式来实现，内部bean通常是匿名的，它们的Scope一般是prototype。</p>
<h3 id="在-Spring中如何注入一个java集合？"><a href="#在-Spring中如何注入一个java集合？" class="headerlink" title="在 Spring中如何注入一个java集合？"></a>在 Spring中如何注入一个java集合？</h3><p>Spring提供以下几种集合的配置元素：</p>
<p>类型用于注入一列值，允许有相同的值。</p>
<p>类型用于注入一组值，不允许有相同的值。</p>
<p>类型用于注入一组键值对，键和值都可以为任意类型。</p>
<p>类型用于注入一组键值对，键和值都只能为String类型。</p>
<h3 id="什么是bean装配？"><a href="#什么是bean装配？" class="headerlink" title="什么是bean装配？"></a>什么是bean装配？</h3><p>装配，或bean 装配是指在Spring 容器中把bean组装到一起，前提是容器需要知道bean的依赖关系，如何通过依赖注入来把它们装配到一起。</p>
<h3 id="什么是bean的自动装配？"><a href="#什么是bean的自动装配？" class="headerlink" title="什么是bean的自动装配？"></a>什么是bean的自动装配？</h3><p>在Spring框架中，在配置文件中设定bean的依赖关系是一个很好的机制，Spring 容器能够自动装配相互合作的bean，这意味着容器不需要和配置，能通过Bean工厂自动处理bean之间的协作。这意味着 Spring可以通过向Bean Factory中注入的方式自动搞定bean之间的依赖关系。自动装配可以设置在每个bean上，也可以设定在特定的bean上。</p>
<h3 id="解释不同方式的自动装配，spring-自动装配-bean-有哪些方式？"><a href="#解释不同方式的自动装配，spring-自动装配-bean-有哪些方式？" class="headerlink" title="解释不同方式的自动装配，spring 自动装配 bean 有哪些方式？"></a>解释不同方式的自动装配，spring 自动装配 bean 有哪些方式？</h3><p>在spring中，对象无需自己查找或创建与其关联的其他对象，由容器负责把需要相互协作的对象引用赋予各个对象，使用autowire来配置自动装载模式。</p>
<p>在Spring框架xml配置中共有5种自动装配：</p>
<ul>
<li>no：默认的方式是不进行自动装配的，通过手工设置ref属性来进行装配bean。</li>
<li>byName：通过bean的名称进行自动装配，如果一个bean的 property 与另一bean 的name 相同，就进行自动装配。</li>
<li>byType：通过参数的数据类型进行自动装配。</li>
<li>constructor：利用构造函数进行装配，并且构造函数的参数通过byType进行装配。</li>
<li>autodetect：自动探测，如果有构造方法，通过 construct的方式自动装配，否则使用 byType的方式自动装配。</li>
</ul>
<h3 id="使用-Autowired注解自动装配的过程是怎样的？"><a href="#使用-Autowired注解自动装配的过程是怎样的？" class="headerlink" title="使用@Autowired注解自动装配的过程是怎样的？"></a>使用@Autowired注解自动装配的过程是怎样的？</h3><p>使用@Autowired注解来自动装配指定的bean。在使用@Autowired注解之前需要在Spring配置文件进行配置，&lt;context:annotation-config /&gt;。</p>
<p>在启动spring IoC时，容器自动装载了一个AutowiredAnnotationBeanPostProcessor后置处理器，当容器扫描到@Autowied、@Resource或@Inject时，就会在IoC容器自动查找需要的bean，并装配给该对象的属性。在使用@Autowired时，首先在容器中查询对应类型的bean：</p>
<ul>
<li>如果查询结果刚好为一个，就将该bean装配给@Autowired指定的数据；</li>
<li>如果查询的结果不止一个，那么@Autowired会根据名称来查找；</li>
<li>如果上述查找的结果为空，那么会抛出异常。解决方法时，使用required=false。</li>
</ul>
<h3 id="自动装配有哪些局限性？"><a href="#自动装配有哪些局限性？" class="headerlink" title="自动装配有哪些局限性？"></a>自动装配有哪些局限性？</h3><p>自动装配的局限性是：</p>
<p><strong>重写</strong>：你仍需用 和 配置来定义依赖，意味着总要重写自动装配。</p>
<p><strong>基本数据类型</strong>：你不能自动装配简单的属性，如基本数据类型，String字符串，和类。</p>
<p><strong>模糊特性</strong>：自动装配不如显式装配精确，如果有可能，建议使用显式装配。</p>
<h3 id="你可以在Spring中注入一个null-和一个空字符串吗？"><a href="#你可以在Spring中注入一个null-和一个空字符串吗？" class="headerlink" title="你可以在Spring中注入一个null 和一个空字符串吗？"></a>你可以在Spring中注入一个null 和一个空字符串吗？</h3><p>可以。</p>
<h2 id="Spring注解（8）"><a href="#Spring注解（8）" class="headerlink" title="Spring注解（8）"></a>Spring注解（8）</h2><h3 id="什么是基于Java的Spring注解配置-给一些注解的例子"><a href="#什么是基于Java的Spring注解配置-给一些注解的例子" class="headerlink" title="什么是基于Java的Spring注解配置? 给一些注解的例子"></a>什么是基于Java的Spring注解配置? 给一些注解的例子</h3><p>基于Java的配置，允许你在少量的Java注解的帮助下，进行你的大部分Spring配置而非通过XML文件。</p>
<h3 id="怎样开启注解装配？"><a href="#怎样开启注解装配？" class="headerlink" title="怎样开启注解装配？"></a>怎样开启注解装配？</h3><p>注解装配在默认情况下是不开启的，为了使用注解装配，我们必须在Spring配置文件中配置<a href="context:annotation-config" target="_blank" rel="noopener">context:annotation-config</a>元素。</p>
<h3 id="Component-Controller-Repository-Service-有何区别？"><a href="#Component-Controller-Repository-Service-有何区别？" class="headerlink" title="@Component, @Controller, @Repository, @Service 有何区别？"></a>@Component, @Controller, @Repository, @Service 有何区别？</h3><p>@Component：这将 java 类标记为 bean。它是任何 Spring 管理组件的通用构造型。spring 的组件扫描机制现在可以将其拾取并将其拉入应用程序环境中。</p>
<p>@Controller：这将一个类标记为 Spring Web MVC 控制器。标有它的 Bean 会自动导入到 IoC 容器中。</p>
<p>@Service：此注解是组件注解的特化。它不会对 @Component 注解提供任何其他行为。您可以在服务层类中使用 @Service 而不是 @Component，因为它以更好的方式指定了意图。</p>
<p>@Repository：这个注解是具有类似用途和功能的 @Component 注解的特化。它为 DAO 提供了额外的好处。它将 DAO 导入 IoC 容器，并使未经检查的异常有资格转换为 Spring DataAccessException。</p>
<h3 id="Required-注解有什么作用"><a href="#Required-注解有什么作用" class="headerlink" title="@Required 注解有什么作用"></a>@Required 注解有什么作用</h3><p>这个注解表明bean的属性必须在配置的时候设置，通过一个bean定义的显式的属性值或通过自动装配，若@Required注解的bean属性未被设置，容器将抛出BeanInitializationException。示例：</p>
<h3 id="Autowired-注解有什么作用"><a href="#Autowired-注解有什么作用" class="headerlink" title="@Autowired 注解有什么作用"></a>@Autowired 注解有什么作用</h3><p>@Autowired默认是按照类型装配注入的，默认情况下它要求依赖对象必须存在（可以设置它required属性为false）。@Autowired 注解提供了更细粒度的控制，包括在何处以及如何完成自动装配。</p>
<h3 id="Autowired和-Resource之间的区别"><a href="#Autowired和-Resource之间的区别" class="headerlink" title="@Autowired和@Resource之间的区别"></a>@Autowired和@Resource之间的区别</h3><p>@Autowired可用于：构造函数、成员变量、Setter方法</p>
<p>@Autowired和@Resource之间的区别</p>
<ul>
<li>@Autowired默认是按照类型装配注入的，默认情况下它要求依赖对象必须存在（可以设置它required属性为false）。</li>
<li>@Resource默认是按照名称来装配注入的，只有当找不到与名称匹配的bean才会按照类型来装配注入。</li>
</ul>
<h3 id="Qualifier-注解有什么作用"><a href="#Qualifier-注解有什么作用" class="headerlink" title="@Qualifier 注解有什么作用"></a>@Qualifier 注解有什么作用</h3><p>当您创建多个相同类型的 bean 并希望仅使用属性装配其中一个 bean 时，您可以使用@Qualifier 注解和 @Autowired 通过指定应该装配哪个确切的 bean 来消除歧义。</p>
<h3 id="RequestMapping-注解有什么用？"><a href="#RequestMapping-注解有什么用？" class="headerlink" title="@RequestMapping 注解有什么用？"></a>@RequestMapping 注解有什么用？</h3><p>@RequestMapping 注解用于将特定 HTTP 请求方法映射到将处理相应请求的控制器中的特定类/方法。此注释可应用于两个级别：</p>
<ul>
<li>类级别：映射请求的 URL</li>
<li>方法级别：映射 URL 以及 HTTP 请求方法</li>
</ul>
<h2 id="Spring数据访问（14）"><a href="#Spring数据访问（14）" class="headerlink" title="Spring数据访问（14）"></a>Spring数据访问（14）</h2><h3 id="解释对象-关系映射集成模块"><a href="#解释对象-关系映射集成模块" class="headerlink" title="解释对象/关系映射集成模块"></a>解释对象/关系映射集成模块</h3><p>Spring 通过提供ORM模块，支持我们在直接JDBC之上使用一个对象/关系映射映射(ORM)工具，Spring 支持集成主流的ORM框架，如Hiberate，JDO和 iBATIS，JPA，TopLink，JDO，OJB 。Spring的事务管理同样支持以上所有ORM框架及JDBC。</p>
<h3 id="在Spring框架中如何更有效地使用JDBC？"><a href="#在Spring框架中如何更有效地使用JDBC？" class="headerlink" title="在Spring框架中如何更有效地使用JDBC？"></a>在Spring框架中如何更有效地使用JDBC？</h3><p>使用Spring JDBC 框架，资源管理和错误处理的代价都会被减轻。所以开发者只需写statements 和 queries从数据存取数据，JDBC也可以在Spring框架提供的模板类的帮助下更有效地被使用，这个模板叫JdbcTemplate</p>
<h3 id="解释JDBC抽象和DAO模块"><a href="#解释JDBC抽象和DAO模块" class="headerlink" title="解释JDBC抽象和DAO模块"></a>解释JDBC抽象和DAO模块</h3><p>通过使用JDBC抽象和DAO模块，保证数据库代码的简洁，并能避免数据库资源错误关闭导致的问题，它在各种不同的数据库的错误信息之上，提供了一个统一的异常访问层。它还利用Spring的AOP 模块给Spring应用中的对象提供事务管理服务。</p>
<h3 id="spring-DAO-有什么用？"><a href="#spring-DAO-有什么用？" class="headerlink" title="spring DAO 有什么用？"></a>spring DAO 有什么用？</h3><p>Spring DAO（数据访问对象） 使得 JDBC，Hibernate 或 JDO 这样的数据访问技术更容易以一种统一的方式工作。这使得用户容易在持久性技术之间切换。它还允许您在编写代码时，无需考虑捕获每种技术不同的异常。</p>
<h3 id="spring-JDBC-API-中存在哪些类？"><a href="#spring-JDBC-API-中存在哪些类？" class="headerlink" title="spring JDBC API 中存在哪些类？"></a>spring JDBC API 中存在哪些类？</h3><p>JdbcTemplate</p>
<p>SimpleJdbcTemplate</p>
<p>NamedParameterJdbcTemplate</p>
<p>SimpleJdbcInsert</p>
<p>SimpleJdbcCall</p>
<h3 id="JdbcTemplate是什么"><a href="#JdbcTemplate是什么" class="headerlink" title="JdbcTemplate是什么"></a>JdbcTemplate是什么</h3><p>JdbcTemplate 类提供了很多便利的方法解决诸如把数据库数据转变成基本数据类型或对象，执行写好的或可调用的数据库操作语句，提供自定义的数据错误处理。</p>
<h3 id="使用Spring通过什么方式访问Hibernate？使用-Spring-访问-Hibernate-的方法有哪些？"><a href="#使用Spring通过什么方式访问Hibernate？使用-Spring-访问-Hibernate-的方法有哪些？" class="headerlink" title="使用Spring通过什么方式访问Hibernate？使用 Spring 访问 Hibernate 的方法有哪些？"></a>使用Spring通过什么方式访问Hibernate？使用 Spring 访问 Hibernate 的方法有哪些？</h3><p>在Spring中有两种方式访问Hibernate：</p>
<ul>
<li>使用 Hibernate 模板和回调进行控制反转</li>
<li>扩展 HibernateDAOSupport 并应用 AOP 拦截器节点</li>
</ul>
<h3 id="如何通过HibernateDaoSupport将Spring和Hibernate结合起来？"><a href="#如何通过HibernateDaoSupport将Spring和Hibernate结合起来？" class="headerlink" title="如何通过HibernateDaoSupport将Spring和Hibernate结合起来？"></a>如何通过HibernateDaoSupport将Spring和Hibernate结合起来？</h3><p>用Spring的 SessionFactory 调用 LocalSessionFactory。集成过程分三步：</p>
<ul>
<li>配置the Hibernate SessionFactory</li>
<li>继承HibernateDaoSupport实现一个DAO</li>
<li>在AOP支持的事务中装配</li>
</ul>
<h3 id="Spring支持的事务管理类型，-spring-事务实现方式有哪些？"><a href="#Spring支持的事务管理类型，-spring-事务实现方式有哪些？" class="headerlink" title="Spring支持的事务管理类型， spring 事务实现方式有哪些？"></a>Spring支持的事务管理类型， spring 事务实现方式有哪些？</h3><p>Spring支持两种类型的事务管理：</p>
<p><strong>编程式事务管理</strong>：这意味你通过编程的方式管理事务，给你带来极大的灵活性，但是难维护。</p>
<p><strong>声明式事务管理</strong>：这意味着你可以将业务代码和事务管理分离，你只需用注解和XML配置来管理事务。</p>
<h3 id="Spring事务的实现方式和实现原理"><a href="#Spring事务的实现方式和实现原理" class="headerlink" title="Spring事务的实现方式和实现原理"></a>Spring事务的实现方式和实现原理</h3><p>Spring事务的本质其实就是数据库对事务的支持，没有数据库的事务支持，spring是无法提供事务功能的。真正的数据库层的事务提交和回滚是通过binlog或者redo log实现的。</p>
<h3 id="说一下Spring的事务传播行为"><a href="#说一下Spring的事务传播行为" class="headerlink" title="说一下Spring的事务传播行为"></a>说一下Spring的事务传播行为</h3><p>① PROPAGATION_REQUIRED：如果当前没有事务，就创建一个新事务，如果当前存在事务，就加入该事务，该设置是最常用的设置。</p>
<p>② PROPAGATION_SUPPORTS：支持当前事务，如果当前存在事务，就加入该事务，如果当前不存在事务，就以非事务执行。</p>
<p>③ PROPAGATION_MANDATORY：支持当前事务，如果当前存在事务，就加入该事务，如果当前不存在事务，就抛出异常。</p>
<p>④ PROPAGATION_REQUIRES_NEW：创建新事务，无论当前存不存在事务，都创建新事务。</p>
<p>⑤ PROPAGATION_NOT_SUPPORTED：以非事务方式执行操作，如果当前存在事务，就把当前事务挂起。</p>
<p>⑥ PROPAGATION_NEVER：以非事务方式执行，如果当前存在事务，则抛出异常。</p>
<p>⑦ PROPAGATION_NESTED：如果当前存在事务，则在嵌套事务内执行。如果当前没有事务，则按REQUIRED属性执行。</p>
<h3 id="说一下-spring-的事务隔离？"><a href="#说一下-spring-的事务隔离？" class="headerlink" title="说一下 spring 的事务隔离？"></a>说一下 spring 的事务隔离？</h3><p>spring 有五大隔离级别，默认值为 ISOLATION_DEFAULT（使用数据库的设置），其他四个隔离级别和数据库的隔离级别一致：</p>
<ul>
<li>ISOLATION_DEFAULT：用底层数据库的设置隔离级别，数据库设置的是什么我就用什么；</li>
<li>ISOLATION_READ_UNCOMMITTED：未提交读，最低隔离级别、事务未提交前，就可被其他事务读取（会出现幻读、脏读、不可重复读）；</li>
<li>ISOLATION_READ_COMMITTED：提交读，一个事务提交后才能被其他事务读取到（会造成幻读、不可重复读），SQL server 的默认级别；</li>
<li>ISOLATION_REPEATABLE_READ：可重复读，保证多次读取同一个数据时，其值都和事务开始时候的内容是一致，禁止读取到别的事务未提交的数据（会造成幻读），MySQL 的默认级别；</li>
<li>ISOLATION_SERIALIZABLE：序列化，代价最高最可靠的隔离级别，该隔离级别能防止脏读、不可重复读、幻读。</li>
</ul>
<p>脏读 ：表示一个事务能够读取另一个事务中还未提交的数据。比如，某个事务尝试插入记录 A，此时该事务还未提交，然后另一个事务尝试读取到了记录 A。</p>
<p>不可重复读 ：是指在一个事务内，多次读同一数据。</p>
<p>幻读 ：指同一个事务内多次查询返回的结果集不一样。比如同一个事务 A 第一次查询时候有 n 条记录，但是第二次同等条件下查询却有 n+1 条记录，这就好像产生了幻觉。发生幻读的原因也是另外一个事务新增或者删除或者修改了第一个事务结果集里面的数据，同一个记录的数据内容被修改了，所有数据行的记录就变多或者变少了。</p>
<h3 id="Spring框架的事务管理有哪些优点？"><a href="#Spring框架的事务管理有哪些优点？" class="headerlink" title="Spring框架的事务管理有哪些优点？"></a>Spring框架的事务管理有哪些优点？</h3><ul>
<li>为不同的事务API 如 JTA，JDBC，Hibernate，JPA 和JDO，提供一个不变的编程模式。</li>
<li>为编程式事务管理提供了一套简单的API而不是一些复杂的事务API</li>
<li>支持声明式事务管理。</li>
<li>和Spring各种数据访问抽象层很好得集成。</li>
</ul>
<h3 id="你更倾向用那种事务管理类型？"><a href="#你更倾向用那种事务管理类型？" class="headerlink" title="你更倾向用那种事务管理类型？"></a>你更倾向用那种事务管理类型？</h3><p>大多数Spring框架的用户选择声明式事务管理，因为它对应用代码的影响最小，因此更符合一个无侵入的轻量级容器的思想。声明式事务管理要优于编程式事务管理，虽然比编程式事务管理（这种方式允许你通过代码控制事务）少了一点灵活性。唯一不足地方是，最细粒度只能作用到方法级别，无法做到像编程式事务那样可以作用到代码块级别。</p>
<h2 id="Spring面向切面编程-AOP-（13）"><a href="#Spring面向切面编程-AOP-（13）" class="headerlink" title="Spring面向切面编程(AOP)（13）"></a>Spring面向切面编程(AOP)（13）</h2><h3 id="什么是AOP"><a href="#什么是AOP" class="headerlink" title="什么是AOP"></a>什么是AOP</h3><p>OOP(Object-Oriented Programming)面向对象编程，允许开发者定义纵向的关系，但并适用于定义横向的关系，导致了大量代码的重复，而不利于各个模块的重用。</p>
<p>AOP(Aspect-Oriented Programming)，一般称为面向切面编程，作为面向对象的一种补充，用于将那些与业务无关，但却对多个对象产生影响的公共行为和逻辑，抽取并封装为一个可重用的模块，这个模块被命名为“切面”（Aspect），减少系统中的重复代码，降低了模块间的耦合度，同时提高了系统的可维护性。可用于权限认证、日志、事务处理等。</p>
<h3 id="Spring-AOP-and-AspectJ-AOP-有什么区别？AOP-有哪些实现方式？"><a href="#Spring-AOP-and-AspectJ-AOP-有什么区别？AOP-有哪些实现方式？" class="headerlink" title="Spring AOP and AspectJ AOP 有什么区别？AOP 有哪些实现方式？"></a>Spring AOP and AspectJ AOP 有什么区别？AOP 有哪些实现方式？</h3><p>AOP实现的关键在于 代理模式，AOP代理主要分为静态代理和动态代理。静态代理的代表为AspectJ；动态代理则以Spring AOP为代表。</p>
<p>（1）AspectJ是静态代理的增强，所谓静态代理，就是AOP框架会在编译阶段生成AOP代理类，因此也称为编译时增强，他会在编译阶段将AspectJ(切面)织入到Java字节码中，运行的时候就是增强之后的AOP对象。</p>
<p>（2）Spring AOP使用的动态代理，所谓的动态代理就是说AOP框架不会去修改字节码，而是每次运行时在内存中临时为方法生成一个AOP对象，这个AOP对象包含了目标对象的全部方法，并且在特定的切点做了增强处理，并回调原对象的方法。</p>
<h3 id="JDK动态代理和CGLIB动态代理的区别"><a href="#JDK动态代理和CGLIB动态代理的区别" class="headerlink" title="JDK动态代理和CGLIB动态代理的区别"></a>JDK动态代理和CGLIB动态代理的区别</h3><p>Spring AOP中的动态代理主要有两种方式，JDK动态代理和CGLIB动态代理：</p>
<ul>
<li>JDK动态代理只提供接口的代理，不支持类的代理。核心InvocationHandler接口和Proxy类，InvocationHandler 通过invoke()方法反射来调用目标类中的代码，动态地将横切逻辑和业务编织在一起；接着，Proxy利用 InvocationHandler动态创建一个符合某一接口的的实例, 生成目标类的代理对象。</li>
<li>如果代理类没有实现 InvocationHandler 接口，那么Spring AOP会选择使用CGLIB来动态代理目标类。CGLIB（Code Generation Library），是一个代码生成的类库，可以在运行时动态的生成指定类的一个子类对象，并覆盖其中特定方法并添加增强代码，从而实现AOP。CGLIB是通过继承的方式做的动态代理，因此如果某个类被标记为final，那么它是无法使用CGLIB做动态代理的。</li>
</ul>
<p>静态代理与动态代理区别在于生成AOP代理对象的时机不同，相对来说AspectJ的静态代理方式具有更好的性能，但是AspectJ需要特定的编译器进行处理，而Spring AOP则无需特定的编译器处理。</p>
<h3 id="如何理解-Spring-中的代理？"><a href="#如何理解-Spring-中的代理？" class="headerlink" title="如何理解 Spring 中的代理？"></a>如何理解 Spring 中的代理？</h3><p>将 Advice 应用于目标对象后创建的对象称为代理。在客户端对象的情况下，目标对象和代理对象是相同的。</p>
<p>Advice + Target Object = Proxy</p>
<h3 id="解释一下Spring-AOP里面的几个名词"><a href="#解释一下Spring-AOP里面的几个名词" class="headerlink" title="解释一下Spring AOP里面的几个名词"></a>解释一下Spring AOP里面的几个名词</h3><p>（1）切面（Aspect）：切面是通知和切点的结合。通知和切点共同定义了切面的全部内容。 在Spring AOP中，切面可以使用通用类（基于模式的风格） 或者在普通类中以 @AspectJ 注解来实现。</p>
<p>（2）连接点（Join point）：指方法，在Spring AOP中，一个连接点 总是 代表一个方法的执行。 应用可能有数以千计的时机应用通知。这些时机被称为连接点。连接点是在应用执行过程中能够插入切面的一个点。这个点可以是调用方法时、抛出异常时、甚至修改一个字段时。切面代码可以利用这些点插入到应用的正常流程之中，并添加新的行为。</p>
<p>（3）通知（Advice）：在AOP术语中，切面的工作被称为通知。</p>
<p>（4）切入点（Pointcut）：切点的定义会匹配通知所要织入的一个或多个连接点。我们通常使用明确的类和方法名称，或是利用正则表达式定义所匹配的类和方法名称来指定这些切点。</p>
<p>（5）引入（Introduction）：引入允许我们向现有类添加新方法或属性。</p>
<p>（6）目标对象（Target Object）： 被一个或者多个切面（aspect）所通知（advise）的对象。它通常是一个代理对象。也有人把它叫做 被通知（adviced） 对象。 既然Spring AOP是通过运行时代理实现的，这个对象永远是一个 被代理（proxied） 对象。</p>
<p>（7）织入（Weaving）：织入是把切面应用到目标对象并创建新的代理对象的过程。在目标对象的生命周期里有多少个点可以进行织入：</p>
<ul>
<li>编译期：切面在目标类编译时被织入。AspectJ的织入编译器是以这种方式织入切面的。</li>
<li>类加载期：切面在目标类加载到JVM时被织入。需要特殊的类加载器，它可以在目标类被引入应用之前增强该目标类的字节码。AspectJ5的加载时织入就支持以这种方式织入切面。</li>
<li>运行期：切面在应用运行的某个时刻被织入。一般情况下，在织入切面时，AOP容器会为目标对象动态地创建一个代理对象。SpringAOP就是以这种方式织入切面。</li>
</ul>
<h3 id="Spring在运行时通知对象"><a href="#Spring在运行时通知对象" class="headerlink" title="Spring在运行时通知对象"></a>Spring在运行时通知对象</h3><p>通过在代理类中包裹切面，Spring在运行期把切面织入到Spring管理的bean中。代理封装了目标类，并拦截被通知方法的调用，再把调用转发给真正的目标bean。当代理拦截到方法调用时，在调用目标bean方法之前，会执行切面逻辑。</p>
<p>直到应用需要被代理的bean时，Spring才创建代理对象。如果使用的是ApplicationContext的话，在ApplicationContext从BeanFactory中加载所有bean的时候，Spring才会创建被代理的对象。因为Spring运行时才创建代理对象，所以我们不需要特殊的编译器来织入SpringAOP的切面。</p>
<h3 id="Spring只支持方法级别的连接点"><a href="#Spring只支持方法级别的连接点" class="headerlink" title="Spring只支持方法级别的连接点"></a>Spring只支持方法级别的连接点</h3><p>因为Spring基于动态代理，所以Spring只支持方法连接点。Spring缺少对字段连接点的支持，而且它不支持构造器连接点。方法之外的连接点拦截功能，我们可以利用Aspect来补充。</p>
<h3 id="在Spring-AOP-中，关注点和横切关注的区别是什么？在-spring-aop-中-concern-和-cross-cutting-concern-的不同之处"><a href="#在Spring-AOP-中，关注点和横切关注的区别是什么？在-spring-aop-中-concern-和-cross-cutting-concern-的不同之处" class="headerlink" title="在Spring AOP 中，关注点和横切关注的区别是什么？在 spring aop 中 concern 和 cross-cutting concern 的不同之处"></a>在Spring AOP 中，关注点和横切关注的区别是什么？在 spring aop 中 concern 和 cross-cutting concern 的不同之处</h3><p>关注点（concern）是应用中一个模块的行为，一个关注点可能会被定义成一个我们想实现的一个功能。</p>
<p>横切关注点（cross-cutting concern）是一个关注点，此关注点是整个应用都会使用的功能，并影响整个应用，比如日志，安全和数据传输，几乎应用的每个模块都需要的功能。因此这些都属于横切关注点。</p>
<h3 id="Spring通知有哪些类型？"><a href="#Spring通知有哪些类型？" class="headerlink" title="Spring通知有哪些类型？"></a>Spring通知有哪些类型？</h3><p>Spring切面可以应用5种类型的通知：</p>
<ul>
<li>前置通知（Before）：在目标方法被调用之前调用通知功能；</li>
<li>后置通知（After）：在目标方法完成之后调用通知，此时不会关心方法的输出是什么；</li>
<li>返回通知（After-returning ）：在目标方法成功执行之后调用通知；</li>
<li>异常通知（After-throwing）：在目标方法抛出异常后调用通知；</li>
<li>环绕通知（Around）：通知包裹了被通知的方法，在被通知的方法调用之前和调用之后执行自定义的行为。</li>
</ul>
<h3 id="什么是切面-Aspect？"><a href="#什么是切面-Aspect？" class="headerlink" title="什么是切面 Aspect？"></a>什么是切面 Aspect？</h3><p>aspect 由 pointcount 和 advice 组成，切面是通知和切点的结合。 它既包含了横切逻辑的定义, 也包括了连接点的定义. Spring AOP 就是负责实施切面的框架, 它将切面所定义的横切逻辑编织到切面所指定的连接点中.<br>AOP 的工作重心在于如何将增强编织目标对象的连接点上, 这里包含两个工作:</p>
<ul>
<li>如何通过 pointcut 和 advice 定位到特定的 joinpoint 上</li>
<li>如何在 advice 中编写切面代码.</li>
</ul>
<p>可以简单地认为, 使用 @Aspect 注解的类就是切面.</p>
<h3 id="解释基于XML-Schema方式的切面实现"><a href="#解释基于XML-Schema方式的切面实现" class="headerlink" title="解释基于XML Schema方式的切面实现"></a>解释基于XML Schema方式的切面实现</h3><p>在这种情况下，切面由常规类以及基于XML的配置实现。</p>
<h3 id="解释基于注解的切面实现"><a href="#解释基于注解的切面实现" class="headerlink" title="解释基于注解的切面实现"></a>解释基于注解的切面实现</h3><p>在这种情况下(基于@AspectJ的实现)，涉及到的切面声明的风格与带有java5标注的普通java类一致。</p>
<h3 id="有几种不同类型的自动代理？"><a href="#有几种不同类型的自动代理？" class="headerlink" title="有几种不同类型的自动代理？"></a>有几种不同类型的自动代理？</h3><p>BeanNameAutoProxyCreator</p>
<p>DefaultAdvisorAutoProxyCreator</p>
<p>Metadata autoproxying</p>
]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>spring</tag>
      </tags>
  </entry>
  <entry>
    <title>JVM笔记</title>
    <url>/2020/06/29/JVM%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<h1 id="JVM笔记"><a href="#JVM笔记" class="headerlink" title="JVM笔记"></a>JVM笔记</h1><h2 id="Java内存区域"><a href="#Java内存区域" class="headerlink" title="Java内存区域"></a>Java内存区域</h2><h3 id="说一下-JVM-的主要组成部分及其作用？"><a href="#说一下-JVM-的主要组成部分及其作用？" class="headerlink" title="说一下 JVM 的主要组成部分及其作用？"></a>说一下 JVM 的主要组成部分及其作用？</h3><p>JVM包含两个子系统和两个组件，两个子系统为Class loader(类装载)、Execution engine(执行引擎)；两个组件为Runtime data area(运行时数据区)、Native Interface(本地接口)。</p>
<p>Class loader(类装载)：根据给定的全限定名类名(如：java.lang.Object)来装载class文件到Runtime data area中的method area。</p>
<p>Execution engine（执行引擎）：执行classes中的指令。</p>
<p>Native Interface(本地接口)：与native libraries交互，是其它编程语言交互的接口。</p>
<p>Runtime data area(运行时数据区域)：这就是我们常说的JVM的内存。</p>
<p>作用 ：首先通过编译器把 Java 代码转换成字节码，类加载器（ClassLoader）再把字节码加载到内存中，将其放在运行时数据区（Runtime data area）的方法区内，而字节码文件只是 JVM 的一套指令集规范，并不能直接交给底层操作系统去执行，因此需要特定的命令解析器执行引擎（Execution Engine），将字节码翻译成底层系统指令，再交由 CPU 去执行，而这个过程中需要调用其他语言的本地库接口（Native Interface）来实现整个程序的功能。</p>
<p><strong>下面是Java程序运行机制详细说明</strong></p>
<p>Java程序运行机制步骤</p>
<ul>
<li>首先利用IDE集成开发工具编写Java源代码，源文件的后缀为.java；</li>
<li>再利用编译器(javac命令)将源代码编译成字节码文件，字节码文件的后缀名为.class；</li>
<li>运行字节码的工作是由解释器(java命令)来完成的。</li>
</ul>
<h3 id="说一下-JVM-运行时数据区"><a href="#说一下-JVM-运行时数据区" class="headerlink" title="说一下 JVM 运行时数据区"></a>说一下 JVM 运行时数据区</h3><p>不同虚拟机的运行时数据区可能略微有所不同，但都会遵从 Java 虚拟机规范， Java 虚拟机规范规定的区域分为以下 5 个部分：</p>
<ul>
<li>程序计数器（Program Counter Register）：当前线程所执行的字节码的行号指示器，字节码解析器的工作是通过改变这个计数器的值，来选取下一条需要执行的字节码指令，分支、循环、跳转、异常处理、线程恢复等基础功能，都需要依赖这个计数器来完成；</li>
<li>Java 虚拟机栈（Java Virtual Machine Stacks）：用于存储局部变量表、操作数栈、动态链接、方法出口等信息；</li>
<li>本地方法栈（Native Method Stack）：与虚拟机栈的作用是一样的，只不过虚拟机栈是服务 Java 方法的，而本地方法栈是为虚拟机调用 Native 方法服务的；</li>
<li>Java 堆（Java Heap）：Java 虚拟机中内存最大的一块，是被所有线程共享的，几乎所有的对象实例都在这里分配内存；</li>
<li>方法区（Methed Area）：用于存储已被虚拟机加载的类信息、常量、静态变量、即时编译后的代码等数据。</li>
</ul>
<h3 id="深拷贝和浅拷贝"><a href="#深拷贝和浅拷贝" class="headerlink" title="深拷贝和浅拷贝"></a>深拷贝和浅拷贝</h3><p>浅拷贝（shallowCopy）只是增加了一个指针指向已存在的内存地址，</p>
<p>深拷贝（deepCopy）是增加了一个指针并且申请了一个新的内存，使这个增加的指针指向这个新的内存，</p>
<p>使用深拷贝的情况下，释放内存的时候不会因为出现浅拷贝时释放同一个内存的错误。</p>
<p>浅复制：仅仅是指向被复制的内存地址，如果原地址发生改变，那么浅复制出来的对象也会相应的改变。</p>
<p>深复制：在计算机中开辟一块新的内存地址用于存放复制的对象。</p>
<h3 id="说一下堆栈的区别？"><a href="#说一下堆栈的区别？" class="headerlink" title="说一下堆栈的区别？"></a>说一下堆栈的区别？</h3><p>物理地址</p>
<p>堆的物理地址分配对对象是不连续的。因此性能慢些。在GC的时候也要考虑到不连续的分配，所以有各种算法。比如，标记-消除，复制，标记-压缩，分代（即新生代使用复制算法，老年代使用标记——压缩）</p>
<p>栈使用的是数据结构中的栈，先进后出的原则，物理地址分配是连续的。所以性能快。</p>
<p>内存分别</p>
<p>堆因为是不连续的，所以分配的内存是在运行期确认的，因此大小不固定。一般堆大小远远大于栈。</p>
<p>栈是连续的，所以分配的内存大小要在编译期就确认，大小是固定的。</p>
<p>存放的内容</p>
<p>堆存放的是对象的实例和数组。因此该区更关注的是数据的存储</p>
<p>栈存放：局部变量，操作数栈，返回结果。该区更关注的是程序方法的执行。</p>
<p>程序的可见度</p>
<p>堆对于整个应用程序都是共享、可见的。</p>
<p>栈只对于线程是可见的。所以也是线程私有。他的生命周期和线程相同。</p>
<h3 id="队列和栈是什么？有什么区别？"><a href="#队列和栈是什么？有什么区别？" class="headerlink" title="队列和栈是什么？有什么区别？"></a>队列和栈是什么？有什么区别？</h3><p>队列和栈都是被用来预存储数据的。</p>
<ul>
<li>操作的名称不同。队列的插入称为入队，队列的删除称为出队。栈的插入称为进栈，栈的删除称为出栈。</li>
<li>可操作的方式不同。队列是在队尾入队，队头出队，即两边都可操作。而栈的进栈和出栈都是在栈顶进行的，无法对栈底直接进行操作。</li>
<li>操作的方法不同。队列是先进先出（FIFO），即队列的修改是依先进先出的原则进行的。新来的成员总是加入队尾（不能从中间插入），每次离开的成员总是队列头上（不允许中途离队）。而栈为后进先出（LIFO）,即每次删除（出栈）的总是当前栈中最新的元素，即最后插入（进栈）的元素，而最先插入的被放在栈的底部，要到最后才能删除。</li>
</ul>
<h2 id="HotSpot虚拟机对象探秘"><a href="#HotSpot虚拟机对象探秘" class="headerlink" title="HotSpot虚拟机对象探秘"></a>HotSpot虚拟机对象探秘</h2><h3 id="对象的创建"><a href="#对象的创建" class="headerlink" title="对象的创建"></a>对象的创建</h3><table>
<thead>
<tr>
<th>使用new关键字</th>
<th>调用了构造函数</th>
</tr>
</thead>
<tbody><tr>
<td>使用Class的newInstance方法</td>
<td>调用了构造函数</td>
</tr>
<tr>
<td>使用Constructor类的newInstance方法</td>
<td>调用了构造函数</td>
</tr>
<tr>
<td>使用clone方法</td>
<td>没有调用构造函数</td>
</tr>
<tr>
<td>使用反序列化</td>
<td>没有调用构造函数</td>
</tr>
</tbody></table>
<h3 id="为对象分配内存"><a href="#为对象分配内存" class="headerlink" title="为对象分配内存"></a>为对象分配内存</h3><p><img src="https://lixiangbetter.github.io/2020/06/29/JVM%E7%AC%94%E8%AE%B0/20200103213812259.png" alt></p>
<h3 id="处理并发安全问题"><a href="#处理并发安全问题" class="headerlink" title="处理并发安全问题"></a>处理并发安全问题</h3><p><img src="https://lixiangbetter.github.io/2020/06/29/JVM%E7%AC%94%E8%AE%B0/20200103213833317.png" alt></p>
<h3 id="对象的访问定位"><a href="#对象的访问定位" class="headerlink" title="对象的访问定位"></a>对象的访问定位</h3><p><code>Java</code>程序需要通过 <code>JVM</code> 栈上的引用访问堆中的具体对象。对象的访问方式取决于 <code>JVM</code> 虚拟机的实现。目前主流的访问方式有 <strong>句柄</strong> 和 <strong>直接指针</strong> 两种方式。</p>
<h2 id="内存溢出异常"><a href="#内存溢出异常" class="headerlink" title="内存溢出异常"></a>内存溢出异常</h2><h3 id="Java会存在内存泄漏吗？请简单描述"><a href="#Java会存在内存泄漏吗？请简单描述" class="headerlink" title="Java会存在内存泄漏吗？请简单描述"></a>Java会存在内存泄漏吗？请简单描述</h3><p>内存泄漏是指不再被使用的对象或者变量一直被占据在内存中。理论上来说，Java是有GC垃圾回收机制的，也就是说，不再被使用的对象，会被GC自动回收掉，自动从内存中清除。</p>
<p>但是，即使这样，Java也还是存在着内存泄漏的情况，java导致内存泄露的原因很明确：长生命周期的对象持有短生命周期对象的引用就很可能发生内存泄露，尽管短生命周期对象已经不再需要，但是因为长生命周期对象持有它的引用而导致不能被回收，这就是java中内存泄露的发生场景。</p>
<h2 id="垃圾收集器"><a href="#垃圾收集器" class="headerlink" title="垃圾收集器"></a>垃圾收集器</h2><h3 id="简述Java垃圾回收机制"><a href="#简述Java垃圾回收机制" class="headerlink" title="简述Java垃圾回收机制"></a>简述Java垃圾回收机制</h3><p>在java中，程序员是不需要显示的去释放一个对象的内存的，而是由虚拟机自行执行。在JVM中，有一个垃圾回收线程，它是低优先级的，在正常情况下是不会执行的，只有在虚拟机空闲或者当前堆内存不足时，才会触发执行，扫面那些没有被任何引用的对象，并将它们添加到要回收的集合中，进行回收。</p>
<h3 id="GC是什么？为什么要GC"><a href="#GC是什么？为什么要GC" class="headerlink" title="GC是什么？为什么要GC"></a>GC是什么？为什么要GC</h3><p>GC 是垃圾收集的意思（Gabage Collection）,内存处理是编程人员容易出现问题的地方，忘记或者错误的内存</p>
<p>回收会导致程序或系统的不稳定甚至崩溃，Java 提供的 GC 功能可以自动监测对象是否超过作用域从而达到自动</p>
<p>回收内存的目的，Java 语言没有提供释放已分配内存的显示操作方法。</p>
<h3 id="垃圾回收的优点和原理。并考虑2种回收机制"><a href="#垃圾回收的优点和原理。并考虑2种回收机制" class="headerlink" title="垃圾回收的优点和原理。并考虑2种回收机制"></a>垃圾回收的优点和原理。并考虑2种回收机制</h3><p>java语言最显著的特点就是引入了垃圾回收机制，它使java程序员在编写程序时不再考虑内存管理的问题。</p>
<p>由于有这个垃圾回收机制，java中的对象不再有“作用域”的概念，只有引用的对象才有“作用域”。</p>
<p>垃圾回收机制有效的防止了内存泄露，可以有效的使用可使用的内存。</p>
<p>垃圾回收器通常作为一个单独的低级别的线程运行，在不可预知的情况下对内存堆中已经死亡的或很长时间没有用过的对象进行清除和回收。</p>
<p>程序员不能实时的对某个对象或所有对象调用垃圾回收器进行垃圾回收。</p>
<p>垃圾回收有分代复制垃圾回收、标记垃圾回收、增量垃圾回收。</p>
<h3 id="垃圾回收器的基本原理是什么？垃圾回收器可以马上回收内存吗？有什么办法主动通知虚拟机进行垃圾回收？"><a href="#垃圾回收器的基本原理是什么？垃圾回收器可以马上回收内存吗？有什么办法主动通知虚拟机进行垃圾回收？" class="headerlink" title="垃圾回收器的基本原理是什么？垃圾回收器可以马上回收内存吗？有什么办法主动通知虚拟机进行垃圾回收？"></a>垃圾回收器的基本原理是什么？垃圾回收器可以马上回收内存吗？有什么办法主动通知虚拟机进行垃圾回收？</h3><p>对于GC来说，当程序员创建对象时，GC就开始监控这个对象的地址、大小以及使用情况。</p>
<p>通常，GC采用有向图的方式记录和管理堆(heap)中的所有对象。通过这种方式确定哪些对象是”可达的”，哪些对象是”不可达的”。当GC确定一些对象为”不可达”时，GC就有责任回收这些内存空间。</p>
<p>可以。程序员可以手动执行System.gc()，通知GC运行，但是Java语言规范并不保证GC一定会执行。</p>
<h3 id="Java-中都有哪些引用类型？"><a href="#Java-中都有哪些引用类型？" class="headerlink" title="Java 中都有哪些引用类型？"></a>Java 中都有哪些引用类型？</h3><p>强引用：发生 gc 的时候不会被回收。<br>软引用：有用但不是必须的对象，在发生内存溢出之前会被回收。<br>弱引用：有用但不是必须的对象，在下一次GC时会被回收。<br>虚引用（幽灵引用/幻影引用）：无法通过虚引用获得对象，用 PhantomReference 实现虚引用，虚引用的用途是在 gc 时返回一个通知。</p>
<h3 id="怎么判断对象是否可以被回收？"><a href="#怎么判断对象是否可以被回收？" class="headerlink" title="怎么判断对象是否可以被回收？"></a>怎么判断对象是否可以被回收？</h3><p>垃圾收集器在做垃圾回收的时候，首先需要判定的就是哪些内存是需要被回收的，哪些对象是「存活」的，是不可以被回收的；哪些对象已经「死掉」了，需要被回收。</p>
<p>一般有两种方法来判断：</p>
<ul>
<li>引用计数器法：为每个对象创建一个引用计数，有对象引用时计数器 +1，引用被释放时计数 -1，当计数器为 0 时就可以被回收。它有一个缺点不能解决循环引用的问题；</li>
<li>可达性分析算法：从 GC Roots 开始向下搜索，搜索所走过的路径称为引用链。当一个对象到 GC Roots 没有任何引用链相连时，则证明此对象是可以被回收的。</li>
</ul>
<h3 id="在Java中，对象什么时候可以被垃圾回收"><a href="#在Java中，对象什么时候可以被垃圾回收" class="headerlink" title="在Java中，对象什么时候可以被垃圾回收"></a>在Java中，对象什么时候可以被垃圾回收</h3><p>当对象对当前使用这个对象的应用程序变得不可触及的时候，这个对象就可以被回收了。<br>垃圾回收不会发生在永久代，如果永久代满了或者是超过了临界值，会触发完全垃圾回收(Full GC)。如果你仔细查看垃圾收集器的输出信息，就会发现永久代也是被回收的。这就是为什么正确的永久代大小对避免Full GC是非常重要的原因。</p>
<h3 id="JVM中的永久代中会发生垃圾回收吗"><a href="#JVM中的永久代中会发生垃圾回收吗" class="headerlink" title="JVM中的永久代中会发生垃圾回收吗"></a>JVM中的永久代中会发生垃圾回收吗</h3><p>垃圾回收不会发生在永久代，如果永久代满了或者是超过了临界值，会触发完全垃圾回收(Full GC)。如果你仔细查看垃圾收集器的输出信息，就会发现永久代也是被回收的。这就是为什么正确的永久代大小对避免Full GC是非常重要的原因。请参考下Java8：从永久代到元数据区<br>(译者注：Java8中已经移除了永久代，新加了一个叫做元数据区的native内存区)</p>
<h3 id="说一下-JVM-有哪些垃圾回收算法？"><a href="#说一下-JVM-有哪些垃圾回收算法？" class="headerlink" title="说一下 JVM 有哪些垃圾回收算法？"></a>说一下 JVM 有哪些垃圾回收算法？</h3><ul>
<li>标记-清除算法：标记无用对象，然后进行清除回收。缺点：效率不高，无法清除垃圾碎片。</li>
<li>复制算法：按照容量划分二个大小相等的内存区域，当一块用完的时候将活着的对象复制到另一块上，然后再把已使用的内存空间一次清理掉。缺点：内存使用率不高，只有原来的一半。</li>
<li>标记-整理算法：标记无用对象，让所有存活的对象都向一端移动，然后直接清除掉端边界以外的内存。</li>
<li>分代算法：根据对象存活周期的不同将内存划分为几块，一般是新生代和老年代，新生代基本采用复制算法，老年代采用标记整理算法。</li>
</ul>
<h4 id="标记-清除算法"><a href="#标记-清除算法" class="headerlink" title="标记-清除算法"></a>标记-清除算法</h4><p>标记-清除算法（Mark-Sweep）是一种常见的基础垃圾收集算法，它将垃圾收集分为两个阶段：</p>
<ul>
<li>标记阶段：标记出可以回收的对象。</li>
<li>清除阶段：回收被标记的对象所占用的空间。</li>
</ul>
<h4 id="复制算法"><a href="#复制算法" class="headerlink" title="复制算法"></a>复制算法</h4><p><strong>优点</strong>：按顺序分配内存即可，实现简单、运行高效，不用考虑内存碎片。</p>
<h4 id="标记-整理算法"><a href="#标记-整理算法" class="headerlink" title="标记-整理算法"></a>标记-整理算法</h4><h4 id="分代收集算法"><a href="#分代收集算法" class="headerlink" title="分代收集算法"></a>分代收集算法</h4><h3 id="说一下-JVM-有哪些垃圾回收器？"><a href="#说一下-JVM-有哪些垃圾回收器？" class="headerlink" title="说一下 JVM 有哪些垃圾回收器？"></a>说一下 JVM 有哪些垃圾回收器？</h3><ul>
<li><p>Serial收集器（复制算法): 新生代单线程收集器，标记和清理都是单线程，优点是简单高效；</p>
</li>
<li><p>ParNew收集器 (复制算法): 新生代收并行集器，实际上是Serial收集器的多线程版本，在多核CPU环境下有着比Serial更好的表现；</p>
</li>
<li><p>Parallel Scavenge收集器 (复制算法): 新生代并行收集器，追求高吞吐量，高效利用 CPU。吞吐量 = 用户线程时间/(用户线程时间+GC线程时间)，高吞吐量可以高效率的利用CPU时间，尽快完成程序的运算任务，适合后台应用等对交互相应要求不高的场景；</p>
</li>
<li><p>Serial Old收集器 (标记-整理算法): 老年代单线程收集器，Serial收集器的老年代版本；</p>
</li>
<li><p>Parallel Old收集器 (标记-整理算法)： 老年代并行收集器，吞吐量优先，Parallel Scavenge收集器的老年代版本；</p>
</li>
<li><p>CMS(Concurrent Mark Sweep)收集器（标记-清除算法）： 老年代并行收集器，以获取最短回收停顿时间为目标的收集器，具有高并发、低停顿的特点，追求最短GC回收停顿时间。</p>
</li>
<li><p>G1(Garbage First)收集器 (标记-整理算法)： Java堆并行收集器，G1收集器是JDK1.7提供的一个新收集器，G1收集器基于“标记-整理”算法实现，也就是说不会产生内存碎片。此外，G1收集器不同于之前的收集器的一个重要特点是：G1回收的范围是整个Java堆(包括新生代，老年代)，而前六种收集器回收的范围仅限于新生代或老年代。</p>
</li>
</ul>
<h3 id="详细介绍一下-CMS-垃圾回收器？"><a href="#详细介绍一下-CMS-垃圾回收器？" class="headerlink" title="详细介绍一下 CMS 垃圾回收器？"></a>详细介绍一下 CMS 垃圾回收器？</h3><p>CMS 是英文 Concurrent Mark-Sweep 的简称，是以牺牲吞吐量为代价来获得最短回收停顿时间的垃圾回收器。对于要求服务器响应速度的应用上，这种垃圾回收器非常适合。在启动 JVM 的参数加上“-XX:+UseConcMarkSweepGC”来指定使用 CMS 垃圾回收器。</p>
<p>CMS 使用的是标记-清除的算法实现的，所以在 gc 的时候回产生大量的内存碎片，当剩余内存不能满足程序运行要求时，系统将会出现 Concurrent Mode Failure，临时 CMS 会采用 Serial Old 回收器进行垃圾清除，此时的性能将会被降低。</p>
<h3 id="新生代垃圾回收器和老年代垃圾回收器都有哪些？有什么区别？"><a href="#新生代垃圾回收器和老年代垃圾回收器都有哪些？有什么区别？" class="headerlink" title="新生代垃圾回收器和老年代垃圾回收器都有哪些？有什么区别？"></a>新生代垃圾回收器和老年代垃圾回收器都有哪些？有什么区别？</h3><ul>
<li>新生代回收器：Serial、ParNew、Parallel Scavenge</li>
<li>老年代回收器：Serial Old、Parallel Old、CMS</li>
<li>整堆回收器：G1</li>
</ul>
<h3 id="简述分代垃圾回收器是怎么工作的？"><a href="#简述分代垃圾回收器是怎么工作的？" class="headerlink" title="简述分代垃圾回收器是怎么工作的？"></a>简述分代垃圾回收器是怎么工作的？</h3><p>分代回收器有两个分区：老生代和新生代，新生代默认的空间占比总空间的 1/3，老生代的默认占比是 2/3。</p>
<p>新生代使用的是复制算法，新生代里有 3 个分区：Eden、To Survivor、From Survivor，它们的默认占比是 8:1:1，它的执行流程如下：</p>
<p>每次在 From Survivor 到 To Survivor 移动时都存活的对象，年龄就 +1，当年龄到达 15（默认配置是 15）时，升级为老生代。大对象也会直接进入老生代。</p>
<p>老生代当空间占用到达某个值之后就会触发全局垃圾收回，一般使用标记整理的执行算法。以上这些循环往复就构成了整个分代垃圾回收的整体执行流程。</p>
<h2 id="内存分配策略"><a href="#内存分配策略" class="headerlink" title="内存分配策略"></a>内存分配策略</h2><h3 id="简述java内存分配与回收策率以及Minor-GC和Major-GC"><a href="#简述java内存分配与回收策率以及Minor-GC和Major-GC" class="headerlink" title="简述java内存分配与回收策率以及Minor GC和Major GC"></a>简述java内存分配与回收策率以及Minor GC和Major GC</h3><p>所谓自动内存管理，最终要解决的也就是内存分配和内存回收两个问题。前面我们介绍了内存回收，这里我们再来聊聊内存分配。</p>
<p>对象的内存分配通常是在 Java 堆上分配（随着虚拟机优化技术的诞生，某些场景下也会在栈上分配，后面会详细介绍），对象主要分配在新生代的 Eden 区，如果启动了本地线程缓冲，将按照线程优先在 TLAB 上分配。少数情况下也会直接在老年代上分配。总的来说分配规则不是百分百固定的，其细节取决于哪一种垃圾收集器组合以及虚拟机相关参数有关，但是虚拟机对于内存的分配还是会遵循以下几种「普世」规则：</p>
<h4 id="对象优先在-Eden-区分配"><a href="#对象优先在-Eden-区分配" class="headerlink" title="对象优先在 Eden 区分配"></a>对象优先在 Eden 区分配</h4><p>多数情况，对象都在新生代 Eden 区分配。当 Eden 区分配没有足够的空间进行分配时，虚拟机将会发起一次 Minor GC。如果本次 GC 后还是没有足够的空间，则将启用分配担保机制在老年代中分配内存。</p>
<p>这里我们提到 Minor GC，如果你仔细观察过 GC 日常，通常我们还能从日志中发现 Major GC/Full GC。</p>
<p>Minor GC 是指发生在新生代的 GC，因为 Java 对象大多都是朝生夕死，所有 Minor GC 非常频繁，一般回收速度也非常快；<br>Major GC/Full GC 是指发生在老年代的 GC，出现了 Major GC 通常会伴随至少一次 Minor GC。Major GC 的速度通常会比 Minor GC 慢 10 倍以上。</p>
<h4 id="大对象直接进入老年代"><a href="#大对象直接进入老年代" class="headerlink" title="大对象直接进入老年代"></a>大对象直接进入老年代</h4><p>所谓大对象是指需要大量连续内存空间的对象，频繁出现大对象是致命的，会导致在内存还有不少空间的情况下提前触发 GC 以获取足够的连续空间来安置新对象。</p>
<p>前面我们介绍过新生代使用的是标记-清除算法来处理垃圾回收的，如果大对象直接在新生代分配就会导致 Eden 区和两个 Survivor 区之间发生大量的内存复制。因此对于大对象都会直接在老年代进行分配。</p>
<h4 id="长期存活对象将进入老年代"><a href="#长期存活对象将进入老年代" class="headerlink" title="长期存活对象将进入老年代"></a>长期存活对象将进入老年代</h4><p>虚拟机采用分代收集的思想来管理内存，那么内存回收时就必须判断哪些对象应该放在新生代，哪些对象应该放在老年代。因此虚拟机给每个对象定义了一个对象年龄的计数器，如果对象在 Eden 区出生，并且能够被 Survivor 容纳，将被移动到 Survivor 空间中，这时设置对象年龄为 1。对象在 Survivor 区中每「熬过」一次 Minor GC 年龄就加 1，当年龄达到一定程度（默认 15） 就会被晋升到老年代。</p>
<h2 id="虚拟机类加载机制"><a href="#虚拟机类加载机制" class="headerlink" title="虚拟机类加载机制"></a>虚拟机类加载机制</h2><h3 id="简述java类加载机制"><a href="#简述java类加载机制" class="headerlink" title="简述java类加载机制?"></a>简述java类加载机制?</h3><p>虚拟机把描述类的数据从Class文件加载到内存，并对数据进行校验，解析和初始化，最终形成可以被虚拟机直接使用的java类型。</p>
<h3 id="描述一下JVM加载Class文件的原理机制"><a href="#描述一下JVM加载Class文件的原理机制" class="headerlink" title="描述一下JVM加载Class文件的原理机制"></a>描述一下JVM加载Class文件的原理机制</h3><p>Java中的所有类，都需要由类加载器装载到JVM中才能运行。类加载器本身也是一个类，而它的工作就是把class文件从硬盘读取到内存中。在写程序的时候，我们几乎不需要关心类的加载，因为这些都是隐式装载的，除非我们有特殊的用法，像是反射，就需要显式的加载所需要的类。</p>
<p>类装载方式，有两种 ：</p>
<p>1.隐式装载， 程序在运行过程中当碰到通过new 等方式生成对象时，隐式调用类装载器加载对应的类到jvm中，</p>
<p>2.显式装载， 通过class.forname()等方法，显式加载需要的类</p>
<p>Java类的加载是动态的，它并不会一次性将所有类全部加载后再运行，而是保证程序运行的基础类(像是基类)完全加载到jvm中，至于其他类，则在需要的时候才加载。这当然就是为了节省内存开销。</p>
<h3 id="什么是类加载器，类加载器有哪些"><a href="#什么是类加载器，类加载器有哪些" class="headerlink" title="什么是类加载器，类加载器有哪些?"></a>什么是类加载器，类加载器有哪些?</h3><p>实现通过类的权限定名获取该类的二进制字节流的代码块叫做类加载器。</p>
<p>主要有一下四种类加载器:</p>
<ul>
<li>启动类加载器(Bootstrap ClassLoader)用来加载java核心类库，无法被java程序直接引用。</li>
<li>扩展类加载器(extensions class loader):它用来加载 Java 的扩展库。Java 虚拟机的实现会提供一个扩展库目录。该类加载器在此目录里面查找并加载 Java 类。</li>
<li>系统类加载器（system class loader）：它根据 Java 应用的类路径（CLASSPATH）来加载 Java 类。一般来说，Java 应用的类都是由它来完成加载的。可以通过 ClassLoader.getSystemClassLoader()来获取它。</li>
<li>用户自定义类加载器，通过继承 java.lang.ClassLoader类的方式实现。</li>
</ul>
<h3 id="说一下类装载的执行过程？"><a href="#说一下类装载的执行过程？" class="headerlink" title="说一下类装载的执行过程？"></a>说一下类装载的执行过程？</h3><p>类装载分为以下 5 个步骤：</p>
<p>加载：根据查找路径找到相应的 class 文件然后导入；<br>验证：检查加载的 class 文件的正确性；<br>准备：给类中的静态变量分配内存空间；<br>解析：虚拟机将常量池中的符号引用替换成直接引用的过程。符号引用就理解为一个标示，而在直接引用直接指向内存中的地址；<br>初始化：对静态变量和静态代码块执行初始化工作。</p>
<h3 id="什么是双亲委派模型？"><a href="#什么是双亲委派模型？" class="headerlink" title="什么是双亲委派模型？"></a>什么是双亲委派模型？</h3><p>在介绍双亲委派模型之前先说下类加载器。对于任意一个类，都需要由加载它的类加载器和这个类本身一同确立在 JVM 中的唯一性，每一个类加载器，都有一个独立的类名称空间。类加载器就是根据指定全限定名称将 class 文件加载到 JVM 内存，然后再转化为 class 对象。</p>
<p>双亲委派模型：如果一个类加载器收到了类加载的请求，它首先不会自己去加载这个类，而是把这个请求委派给父类加载器去完成，每一层的类加载器都是如此，这样所有的加载请求都会被传送到顶层的启动类加载器中，只有当父加载无法完成加载请求（它的搜索范围中没找到所需的类）时，子加载器才会尝试去加载类。</p>
<p>当一个类收到了类加载请求时，不会自己先去加载这个类，而是将其委派给父类，由父类去加载，如果此时父类不能加载，反馈给子类，由子类去完成类的加载。</p>
<h2 id="JVM调优"><a href="#JVM调优" class="headerlink" title="JVM调优"></a>JVM调优</h2><h3 id="说一下-JVM-调优的工具？"><a href="#说一下-JVM-调优的工具？" class="headerlink" title="说一下 JVM 调优的工具？"></a>说一下 JVM 调优的工具？</h3><p>JDK 自带了很多监控工具，都位于 JDK 的 bin 目录下，其中最常用的是 jconsole 和 jvisualvm 这两款视图监控工具。</p>
<ul>
<li>jconsole：用于对 JVM 中的内存、线程和类等进行监控；</li>
<li>jvisualvm：JDK 自带的全能分析工具，可以分析：内存快照、线程快照、程序死锁、监控内存的变化、gc 变化等。</li>
</ul>
<h3 id="常用的-JVM-调优的参数都有哪些？"><a href="#常用的-JVM-调优的参数都有哪些？" class="headerlink" title="常用的 JVM 调优的参数都有哪些？"></a>常用的 JVM 调优的参数都有哪些？</h3><ul>
<li>-Xms2g：初始化推大小为 2g；</li>
<li>-Xmx2g：堆最大内存为 2g；</li>
<li>-XX:NewRatio=4：设置年轻的和老年代的内存比例为 1:4；</li>
<li>-XX:SurvivorRatio=8：设置新生代 Eden 和 Survivor 比例为 8:2；</li>
<li>–XX:+UseParNewGC：指定使用 ParNew + Serial Old 垃圾回收器组合；</li>
<li>-XX:+UseParallelOldGC：指定使用 ParNew + ParNew Old 垃圾回收器组合；</li>
<li>-XX:+UseConcMarkSweepGC：指定使用 CMS + Serial Old 垃圾回收器组合；</li>
<li>-XX:+PrintGC：开启打印 gc 信息；</li>
<li>-XX:+PrintGCDetails：打印 gc 详细信息。</li>
</ul>
]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>jvm</tag>
      </tags>
  </entry>
  <entry>
    <title>并发编程笔记</title>
    <url>/2020/06/21/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<h1 id="并发编程笔记"><a href="#并发编程笔记" class="headerlink" title="并发编程笔记"></a>并发编程笔记</h1><h2 id="基础知识"><a href="#基础知识" class="headerlink" title="基础知识"></a>基础知识</h2><h3 id="并发编程的优缺点"><a href="#并发编程的优缺点" class="headerlink" title="并发编程的优缺点"></a>并发编程的优缺点</h3><h4 id="为什么要使用并发编程（并发编程的优点）"><a href="#为什么要使用并发编程（并发编程的优点）" class="headerlink" title="为什么要使用并发编程（并发编程的优点）"></a>为什么要使用并发编程（并发编程的优点）</h4><ul>
<li>充分利用多核CPU的计算能力：通过并发编程的形式可以将多核CPU的计算能力发挥到极致，性能得到提升</li>
<li>方便进行业务拆分，提升系统并发能力和性能：在特殊的业务场景下，先天的就适合于并发编程。现在的系统动不动就要求百万级甚至千万级的并发量，而多线程并发编程正是开发高并发系统的基础，利用好多线程机制可以大大提高系统整体的并发能力以及性能。面对复杂业务模型，并行程序会比串行程序更适应业务需求，而并发编程更能吻合这种业务拆分 。</li>
</ul>
<h4 id="并发编程有什么缺点"><a href="#并发编程有什么缺点" class="headerlink" title="并发编程有什么缺点"></a>并发编程有什么缺点</h4><p>并发编程的目的就是为了能提高程序的执行效率，提高程序运行速度，但是并发编程并不总是能提高程序运行速度的，而且并发编程可能会遇到很多问题，比如<strong>：内存泄漏、上下文切换、线程安全、死锁</strong>等问题。</p>
<h4 id="并发编程三要素是什么？在-Java-程序中怎么保证多线程的运行安全？"><a href="#并发编程三要素是什么？在-Java-程序中怎么保证多线程的运行安全？" class="headerlink" title="并发编程三要素是什么？在 Java 程序中怎么保证多线程的运行安全？"></a>并发编程三要素是什么？在 Java 程序中怎么保证多线程的运行安全？</h4><p>并发编程三要素（线程的安全性问题体现在）：</p>
<p>原子性：原子，即一个不可再被分割的颗粒。原子性指的是一个或多个操作要么全部执行成功要么全部执行失败。</p>
<p>可见性：一个线程对共享变量的修改,另一个线程能够立刻看到。（synchronized,volatile）</p>
<p>有序性：程序执行的顺序按照代码的先后顺序执行。（处理器可能会对指令进行重排序）</p>
<p>出现线程安全问题的原因：</p>
<ul>
<li><p>线程切换带来的原子性问题</p>
</li>
<li><p>缓存导致的可见性问题</p>
</li>
<li><p>编译优化带来的有序性问题</p>
</li>
</ul>
<p>解决办法：</p>
<ul>
<li>JDK Atomic开头的原子类、synchronized、LOCK，可以解决原子性问题</li>
<li>synchronized、volatile、LOCK，可以解决可见性问题</li>
<li>Happens-Before 规则可以解决有序性问题</li>
</ul>
<h4 id="并行和并发有什么区别？"><a href="#并行和并发有什么区别？" class="headerlink" title="并行和并发有什么区别？"></a>并行和并发有什么区别？</h4><ul>
<li>并发：多个任务在同一个 CPU 核上，按细分的时间片轮流(交替)执行，从逻辑上来看那些任务是同时执行。</li>
<li>并行：单位时间内，多个处理器或多核处理器同时处理多个任务，是真正意义上的“同时进行”。</li>
<li>串行：有n个任务，由一个线程按顺序执行。由于任务、方法都在一个线程执行所以不存在线程不安全情况，也就不存在临界区的问题。</li>
</ul>
<h4 id="什么是多线程，多线程的优劣？"><a href="#什么是多线程，多线程的优劣？" class="headerlink" title="什么是多线程，多线程的优劣？"></a>什么是多线程，多线程的优劣？</h4><p>多线程：多线程是指程序中包含多个执行流，即在一个程序中可以同时运行多个不同的线程来执行不同的任务。</p>
<p>多线程的好处：</p>
<p>可以提高 CPU 的利用率。在多线程程序中，一个线程必须等待的时候，CPU 可以运行其它的线程而不是等待，这样就大大提高了程序的效率。也就是说允许单个程序创建多个并行执行的线程来完成各自的任务。</p>
<p>多线程的劣势：</p>
<ul>
<li>线程也是程序，所以线程需要占用内存，线程越多占用内存也越多；</li>
<li>多线程需要协调和管理，所以需要 CPU 时间跟踪线程；</li>
<li>线程之间对共享资源的访问会相互影响，必须解决竞用共享资源的问题。</li>
</ul>
<h3 id="线程和进程区别"><a href="#线程和进程区别" class="headerlink" title="线程和进程区别"></a>线程和进程区别</h3><h4 id="什么是线程和进程"><a href="#什么是线程和进程" class="headerlink" title="什么是线程和进程?"></a>什么是线程和进程?</h4><p>进程</p>
<p>一个在内存中运行的应用程序。每个进程都有自己独立的一块内存空间，一个进程可以有多个线程，比如在Windows系统中，一个运行的xx.exe就是一个进程。</p>
<p>线程</p>
<p>进程中的一个执行任务（控制单元），负责当前进程中程序的执行。一个进程至少有一个线程，一个进程可以运行多个线程，多个线程可共享数据。</p>
<h4 id="进程与线程的区别"><a href="#进程与线程的区别" class="headerlink" title="进程与线程的区别"></a>进程与线程的区别</h4><p>线程具有许多传统进程所具有的特征，故又称为轻型进程(Light—Weight Process)或进程元；而把传统的进程称为重型进程(Heavy—Weight Process)，它相当于只有一个线程的任务。在引入了线程的操作系统中，通常一个进程都有若干个线程，至少包含一个线程。</p>
<p>根本区别：进程是操作系统资源分配的基本单位，而线程是处理器任务调度和执行的基本单位</p>
<p>资源开销：每个进程都有独立的代码和数据空间（程序上下文），程序之间的切换会有较大的开销；线程可以看做轻量级的进程，同一类线程共享代码和数据空间，每个线程都有自己独立的运行栈和程序计数器（PC），线程之间切换的开销小。</p>
<p>包含关系：如果一个进程内有多个线程，则执行过程不是一条线的，而是多条线（线程）共同完成的；线程是进程的一部分，所以线程也被称为轻权进程或者轻量级进程。</p>
<p>内存分配：同一进程的线程共享本进程的地址空间和资源，而进程之间的地址空间和资源是相互独立的</p>
<p>影响关系：一个进程崩溃后，在保护模式下不会对其他进程产生影响，但是一个线程崩溃整个进程都死掉。所以多进程要比多线程健壮。</p>
<p>执行过程：每个独立的进程有程序运行的入口、顺序执行序列和程序出口。但是线程不能独立执行，必须依存在应用程序中，由应用程序提供多个线程执行控制，两者均可并发执行</p>
<h4 id="什么是上下文切换"><a href="#什么是上下文切换" class="headerlink" title="什么是上下文切换?"></a>什么是上下文切换?</h4><p>多线程编程中一般线程的个数都大于 CPU 核心的个数，而一个 CPU 核心在任意时刻只能被一个线程使用，为了让这些线程都能得到有效执行，CPU 采取的策略是为每个线程分配时间片并轮转的形式。当一个线程的时间片用完的时候就会重新处于就绪状态让给其他线程使用，这个过程就属于一次上下文切换。</p>
<p>概括来说就是：当前任务在执行完 CPU 时间片切换到另一个任务之前会先保存自己的状态，以便下次再切换回这个任务时，可以再加载这个任务的状态。任务从保存到再加载的过程就是一次上下文切换。</p>
<p>上下文切换通常是计算密集型的。也就是说，它需要相当可观的处理器时间，在每秒几十上百次的切换中，每次切换都需要纳秒量级的时间。所以，上下文切换对系统来说意味着消耗大量的 CPU 时间，事实上，可能是操作系统中时间消耗最大的操作。</p>
<p>Linux 相比与其他操作系统（包括其他类 Unix 系统）有很多的优点，其中有一项就是，其上下文切换和模式切换的时间消耗非常少。</p>
<h4 id="守护线程和用户线程有什么区别呢？"><a href="#守护线程和用户线程有什么区别呢？" class="headerlink" title="守护线程和用户线程有什么区别呢？"></a>守护线程和用户线程有什么区别呢？</h4><p>守护线程和用户线程</p>
<p>用户 (User) 线程：运行在前台，执行具体的任务，如程序的主线程、连接网络的子线程等都是用户线程<br>守护 (Daemon) 线程：运行在后台，为其他前台线程服务。也可以说守护线程是 JVM 中非守护线程的 “佣人”。一旦所有用户线程都结束运行，守护线程会随 JVM 一起结束工作<br>main 函数所在的线程就是一个用户线程啊，main 函数启动的同时在 JVM 内部同时还启动了好多守护线程，比如垃圾回收线程。</p>
<p>比较明显的区别之一是用户线程结束，JVM 退出，不管这个时候有没有守护线程运行。而守护线程不会影响 JVM 的退出。</p>
<p>注意事项：</p>
<ul>
<li>setDaemon(true)必须在start()方法前执行，否则会抛出 IllegalThreadStateException 异常</li>
<li>在守护线程中产生的新线程也是守护线程</li>
<li>不是所有的任务都可以分配给守护线程来执行，比如读写操作或者计算逻辑</li>
<li>守护 (Daemon) 线程中不能依靠 finally 块的内容来确保执行关闭或清理资源的逻辑。因为我们上面也说过了一旦所有用户线程都结束运行，守护线程会随 JVM 一起结束工作，所以守护 (Daemon) 线程中的 finally 语句块可能无法被执行。</li>
</ul>
<h4 id="如何在-Windows-和-Linux-上查找哪个线程cpu利用率最高？"><a href="#如何在-Windows-和-Linux-上查找哪个线程cpu利用率最高？" class="headerlink" title="如何在 Windows 和 Linux 上查找哪个线程cpu利用率最高？"></a>如何在 Windows 和 Linux 上查找哪个线程cpu利用率最高？</h4><p>windows上面用任务管理器看，linux下可以用 top 这个工具看。</p>
<p>找出cpu耗用厉害的进程pid， 终端执行top命令，然后按下shift+p 查找出cpu利用最厉害的pid号<br>根据上面第一步拿到的pid号，top -H -p pid 。然后按下shift+p，查找出cpu利用率最厉害的线程号，比如top -H -p 1328<br>将获取到的线程号转换成16进制，去百度转换一下就行<br>使用jstack工具将进程信息打印输出，jstack pid号 &gt; /tmp/t.dat，比如jstack 31365 &gt; /tmp/t.dat<br>编辑/tmp/t.dat文件，查找线程号对应的信息</p>
<h4 id="什么是线程死锁"><a href="#什么是线程死锁" class="headerlink" title="什么是线程死锁"></a>什么是线程死锁</h4><p>死锁是指两个或两个以上的进程（线程）在执行过程中，由于竞争资源或者由于彼此通信而造成的一种阻塞的现象，若无外力作用，它们都将无法推进下去。此时称系统处于死锁状态或系统产生了死锁，这些永远在互相等待的进程（线程）称为死锁进程（线程）。</p>
<p>多个线程同时被阻塞，它们中的一个或者全部都在等待某个资源被释放。由于线程被无限期地阻塞，因此程序不可能正常终止。</p>
<h4 id="形成死锁的四个必要条件是什么"><a href="#形成死锁的四个必要条件是什么" class="headerlink" title="形成死锁的四个必要条件是什么"></a>形成死锁的四个必要条件是什么</h4><ul>
<li>互斥条件：线程(进程)对于所分配到的资源具有排它性，即一个资源只能被一个线程(进程)占用，直到被该线程(进程)释放</li>
<li>请求与保持条件：一个线程(进程)因请求被占用资源而发生阻塞时，对已获得的资源保持不放。</li>
<li>不剥夺条件：线程(进程)已获得的资源在末使用完之前不能被其他线程强行剥夺，只有自己使用完毕后才释放资源。</li>
<li>循环等待条件：当发生死锁时，所等待的线程(进程)必定会形成一个环路（类似于死循环），造成永久阻塞</li>
</ul>
<h4 id="如何避免线程死锁"><a href="#如何避免线程死锁" class="headerlink" title="如何避免线程死锁"></a>如何避免线程死锁</h4><p>我们只要破坏产生死锁的四个条件中的其中一个就可以了。</p>
<p><strong>破坏互斥条件</strong></p>
<p>没有办法破坏</p>
<p><strong>破坏请求与保持条件</strong></p>
<p>一次性申请所有的资源。</p>
<p><strong>破坏不剥夺条件</strong></p>
<p>占用部分资源的线程进一步申请其他资源时，如果申请不到，可以主动释放它占有的资源。</p>
<p><strong>破坏循环等待条件</strong></p>
<p>靠按序申请资源来预防。按某一顺序申请资源，释放资源则反序释放。破坏循环等待条件。</p>
<h3 id="创建线程的四种方式"><a href="#创建线程的四种方式" class="headerlink" title="创建线程的四种方式"></a>创建线程的四种方式</h3><h4 id="创建线程有哪几种方式？"><a href="#创建线程有哪几种方式？" class="headerlink" title="创建线程有哪几种方式？"></a>创建线程有哪几种方式？</h4><p>创建线程有四种方式：</p>
<p>继承 Thread 类；<br>实现 Runnable 接口；<br>实现 Callable 接口；<br>使用 Executors 工具类创建线程池</p>
<h4 id="说一下-runnable-和-callable-有什么区别？"><a href="#说一下-runnable-和-callable-有什么区别？" class="headerlink" title="说一下 runnable 和 callable 有什么区别？"></a>说一下 runnable 和 callable 有什么区别？</h4><p>相同点</p>
<ul>
<li>都是接口</li>
<li>都可以编写多线程程序</li>
<li>都采用Thread.start()启动线程</li>
</ul>
<p>主要区别</p>
<ul>
<li>Runnable 接口 run 方法无返回值；Callable 接口 call 方法有返回值，是个泛型，和Future、FutureTask配合可以用来获取异步执行的结果</li>
<li>Runnable 接口 run 方法只能抛出运行时异常，且无法捕获处理；Callable 接口 call 方法允许抛出异常，可以获取异常信息</li>
</ul>
<p>注：Callalbe接口支持返回执行结果，需要调用FutureTask.get()得到，此方法会阻塞主进程的继续往下执行，如果不调用不会阻塞。</p>
<h4 id="线程的-run-和-start-有什么区别？"><a href="#线程的-run-和-start-有什么区别？" class="headerlink" title="线程的 run()和 start()有什么区别？"></a>线程的 run()和 start()有什么区别？</h4><p>每个线程都是通过某个特定Thread对象所对应的方法run()来完成其操作的，run()方法称为线程体。通过调用Thread类的start()方法来启动一个线程。</p>
<p>start() 方法用于启动线程，run() 方法用于执行线程的运行时代码。run() 可以重复调用，而 start() 只能调用一次。</p>
<p>start()方法来启动一个线程，真正实现了多线程运行。调用start()方法无需等待run方法体代码执行完毕，可以直接继续执行其他的代码； 此时线程是处于就绪状态，并没有运行。 然后通过此Thread类调用方法run()来完成其运行状态， run()方法运行结束， 此线程终止。然后CPU再调度其它线程。</p>
<p>run()方法是在本线程里的，只是线程里的一个函数，而不是多线程的。 如果直接调用run()，其实就相当于是调用了一个普通函数而已，直接待用run()方法必须等待run()方法执行完毕才能执行下面的代码，所以执行路径还是只有一条，根本就没有线程的特征，所以在多线程执行时要使用start()方法而不是run()方法。</p>
<h4 id="为什么我们调用-start-方法时会执行-run-方法，为什么我们不能直接调用-run-方法？"><a href="#为什么我们调用-start-方法时会执行-run-方法，为什么我们不能直接调用-run-方法？" class="headerlink" title="为什么我们调用 start() 方法时会执行 run() 方法，为什么我们不能直接调用 run() 方法？"></a>为什么我们调用 start() 方法时会执行 run() 方法，为什么我们不能直接调用 run() 方法？</h4><p>new 一个 Thread，线程进入了新建状态。调用 start() 方法，会启动一个线程并使线程进入了就绪状态，当分配到时间片后就可以开始运行了。 start() 会执行线程的相应准备工作，然后自动执行 run() 方法的内容，这是真正的多线程工作。</p>
<p>而直接执行 run() 方法，会把 run 方法当成一个 main 线程下的普通方法去执行，并不会在某个线程中执行它，所以这并不是多线程工作。</p>
<p>调用 start 方法方可启动线程并使线程进入就绪状态，而 run 方法只是 thread 的一个普通方法调用，还是在主线程里执行。</p>
<h4 id="什么是-Callable-和-Future"><a href="#什么是-Callable-和-Future" class="headerlink" title="什么是 Callable 和 Future?"></a>什么是 Callable 和 Future?</h4><p>Callable 接口类似于 Runnable，从名字就可以看出来了，但是 Runnable 不会返回结果，并且无法抛出返回结果的异常，而 Callable 功能更强大一些，被线程执行后，可以返回值，这个返回值可以被 Future 拿到，也就是说，Future 可以拿到异步执行任务的返回值。</p>
<p>Future 接口表示异步任务，是一个可能还没有完成的异步任务的结果。所以说 Callable用于产生结果，Future 用于获取结果。</p>
<h4 id="什么是-FutureTask"><a href="#什么是-FutureTask" class="headerlink" title="什么是 FutureTask"></a>什么是 FutureTask</h4><p>FutureTask 表示一个异步运算的任务。FutureTask 里面可以传入一个 Callable 的具体实现类，可以对这个异步运算的任务的结果进行等待获取、判断是否已经完成、取消任务等操作。只有当运算完成的时候结果才能取回，如果运算尚未完成 get 方法将会阻塞。一个 FutureTask 对象可以对调用了 Callable 和 Runnable 的对象进行包装，由于 FutureTask 也是Runnable 接口的实现类，所以 FutureTask 也可以放入线程池中。</p>
<h3 id="线程的状态和基本操作"><a href="#线程的状态和基本操作" class="headerlink" title="线程的状态和基本操作"></a>线程的状态和基本操作</h3><h4 id="说说线程的生命周期及五种基本状态？"><a href="#说说线程的生命周期及五种基本状态？" class="headerlink" title="说说线程的生命周期及五种基本状态？"></a>说说线程的生命周期及五种基本状态？</h4><p><img src="https://lixiangbetter.github.io/2020/06/21/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E7%AC%94%E8%AE%B0/aHR0cHM6Ly91c2VyLWdvbGQtY2RuLnhpdHUuaW8vMjAxNy8xMi8xNS8xNjA1OWNjOTFlZThlZmIz.jpeg" alt="not found"></p>
<ol>
<li><p>新建(new)：新创建了一个线程对象。</p>
</li>
<li><p>可运行(runnable)：线程对象创建后，当调用线程对象的 start()方法，该线程处于就绪状态，等待被线程调度选中，获取cpu的使用权。</p>
</li>
<li><p>运行(running)：可运行状态(runnable)的线程获得了cpu时间片（timeslice），执行程序代码。注：就绪状态是进入到运行状态的唯一入口，也就是说，线程要想进入运行状态执行，首先必须处于就绪状态中；</p>
</li>
<li><p>阻塞(block)：处于运行状态中的线程由于某种原因，暂时放弃对 CPU的使用权，停止执行，此时进入阻塞状态，直到其进入到就绪状态，才有机会再次被 CPU 调用以进入到运行状态。</p>
<p>阻塞的情况分三种：<br>(一). 等待阻塞：运行状态中的线程执行 wait()方法，JVM会把该线程放入等待队列(waitting queue)中，使本线程进入到等待阻塞状态；<br>(二). 同步阻塞：线程在获取 synchronized 同步锁失败(因为锁被其它线程所占用)，，则JVM会把该线程放入锁池(lock pool)中，线程会进入同步阻塞状态；<br>(三). 其他阻塞: 通过调用线程的 sleep()或 join()或发出了 I/O 请求时，线程会进入到阻塞状态。当 sleep()状态超时、join()等待线程终止或者超时、或者 I/O 处理完毕时，线程重新转入就绪状态。</p>
</li>
<li><p>死亡(dead)：线程run()、main()方法执行结束，或者因异常退出了run()方法，则该线程结束生命周期。死亡的线程不可再次复生。</p>
</li>
</ol>
<h4 id="Java-中用到的线程调度算法是什么？"><a href="#Java-中用到的线程调度算法是什么？" class="headerlink" title="Java 中用到的线程调度算法是什么？"></a>Java 中用到的线程调度算法是什么？</h4><p>计算机通常只有一个 CPU，在任意时刻只能执行一条机器指令，每个线程只有获得CPU 的使用权才能执行指令。所谓多线程的并发运行，其实是指从宏观上看，各个线程轮流获得 CPU 的使用权，分别执行各自的任务。在运行池中，会有多个处于就绪状态的线程在等待 CPU，JAVA 虚拟机的一项任务就是负责线程的调度，线程调度是指按照特定机制为多个线程分配 CPU 的使用权。</p>
<p>有两种调度模型：分时调度模型和抢占式调度模型。</p>
<p>分时调度模型是指让所有的线程轮流获得 cpu 的使用权，并且平均分配每个线程占用的 CPU 的时间片这个也比较好理解。</p>
<p>Java虚拟机采用抢占式调度模型，是指优先让可运行池中优先级高的线程占用CPU，如果可运行池中的线程优先级相同，那么就随机选择一个线程，使其占用CPU。处于运行状态的线程会一直运行，直至它不得不放弃 CPU。</p>
<h4 id="线程的调度策略"><a href="#线程的调度策略" class="headerlink" title="线程的调度策略"></a>线程的调度策略</h4><p>线程调度器选择优先级最高的线程运行，但是，如果发生以下情况，就会终止线程的运行：</p>
<p>（1）线程体中调用了 yield 方法让出了对 cpu 的占用权利</p>
<p>（2）线程体中调用了 sleep 方法使线程进入睡眠状态</p>
<p>（3）线程由于 IO 操作受到阻塞</p>
<p>（4）另外一个更高优先级线程出现</p>
<p>（5）在支持时间片的系统中，该线程的时间片用完</p>
<h4 id="什么是线程调度器-Thread-Scheduler-和时间分片-Time-Slicing-？"><a href="#什么是线程调度器-Thread-Scheduler-和时间分片-Time-Slicing-？" class="headerlink" title="什么是线程调度器(Thread Scheduler)和时间分片(Time Slicing )？"></a>什么是线程调度器(Thread Scheduler)和时间分片(Time Slicing )？</h4><p>线程调度器是一个操作系统服务，它负责为 Runnable 状态的线程分配 CPU 时间。一旦我们创建一个线程并启动它，它的执行便依赖于线程调度器的实现。</p>
<p>时间分片是指将可用的 CPU 时间分配给可用的 Runnable 线程的过程。分配 CPU 时间可以基于线程优先级或者线程等待的时间。</p>
<p>线程调度并不受到 Java 虚拟机控制，所以由应用程序来控制它是更好的选择（也就是说不要让你的程序依赖于线程的优先级）。</p>
<h4 id="请说出与线程同步以及线程调度相关的方法。"><a href="#请说出与线程同步以及线程调度相关的方法。" class="headerlink" title="请说出与线程同步以及线程调度相关的方法。"></a>请说出与线程同步以及线程调度相关的方法。</h4><p>（1） wait()：使一个线程处于等待（阻塞）状态，并且释放所持有的对象的锁；</p>
<p>（2）sleep()：使一个正在运行的线程处于睡眠状态，是一个静态方法，调用此方法要处理 InterruptedException 异常；</p>
<p>（3）notify()：唤醒一个处于等待状态的线程，当然在调用此方法的时候，并不能确切的唤醒某一个等待状态的线程，而是由 JVM 确定唤醒哪个线程，而且与优先级无关；</p>
<p>（4）notityAll()：唤醒所有处于等待状态的线程，该方法并不是将对象的锁给所有线程，而是让它们竞争，只有获得锁的线程才能进入就绪状态；</p>
<h4 id="sleep-和-wait-有什么区别？"><a href="#sleep-和-wait-有什么区别？" class="headerlink" title="sleep() 和 wait() 有什么区别？"></a>sleep() 和 wait() 有什么区别？</h4><p>两者都可以暂停线程的执行</p>
<ul>
<li>类的不同：sleep() 是 Thread线程类的静态方法，wait() 是 Object类的方法。</li>
<li>是否释放锁：sleep() 不释放锁；wait() 释放锁。</li>
<li>用途不同：Wait 通常被用于线程间交互/通信，sleep 通常被用于暂停执行。</li>
<li>用法不同：wait() 方法被调用后，线程不会自动苏醒，需要别的线程调用同一个对象上的 notify() 或者 notifyAll() 方法。sleep() 方法执行完成后，线程会自动苏醒。或者可以使用wait(long timeout)超时后线程会自动苏醒。</li>
</ul>
<h4 id="你是如何调用-wait-方法的？使用-if-块还是循环？为什么？"><a href="#你是如何调用-wait-方法的？使用-if-块还是循环？为什么？" class="headerlink" title="你是如何调用 wait() 方法的？使用 if 块还是循环？为什么？"></a>你是如何调用 wait() 方法的？使用 if 块还是循环？为什么？</h4><p>处于等待状态的线程可能会收到错误警报和伪唤醒，如果不在循环中检查等待条件，程序就会在没有满足结束条件的情况下退出。</p>
<p>wait() 方法应该在循环调用，因为当线程获取到 CPU 开始执行的时候，其他条件可能还没有满足，所以在处理前，循环检测条件是否满足会更好。</p>
<h4 id="为什么线程通信的方法-wait-notify-和-notifyAll-被定义在-Object-类里？"><a href="#为什么线程通信的方法-wait-notify-和-notifyAll-被定义在-Object-类里？" class="headerlink" title="为什么线程通信的方法 wait(), notify()和 notifyAll()被定义在 Object 类里？"></a>为什么线程通信的方法 wait(), notify()和 notifyAll()被定义在 Object 类里？</h4><p>Java中，任何对象都可以作为锁，并且 wait()，notify()等方法用于等待对象的锁或者唤醒线程，在 Java 的线程中并没有可供任何对象使用的锁，所以任意对象调用方法一定定义在Object类中。</p>
<p>wait(), notify()和 notifyAll()这些方法在同步代码块中调用</p>
<p>有的人会说，既然是线程放弃对象锁，那也可以把wait()定义在Thread类里面啊，新定义的线程继承于Thread类，也不需要重新定义wait()方法的实现。然而，这样做有一个非常大的问题，一个线程完全可以持有很多锁，你一个线程放弃锁的时候，到底要放弃哪个锁？当然了，这种设计并不是不能实现，只是管理起来更加复杂。</p>
<p>综上所述，wait()、notify()和notifyAll()方法要定义在Object类中。</p>
<h4 id="为什么-wait-notify-和-notifyAll-必须在同步方法或者同步块中被调用？"><a href="#为什么-wait-notify-和-notifyAll-必须在同步方法或者同步块中被调用？" class="headerlink" title="为什么 wait(), notify()和 notifyAll()必须在同步方法或者同步块中被调用？"></a>为什么 wait(), notify()和 notifyAll()必须在同步方法或者同步块中被调用？</h4><p>当一个线程需要调用对象的 wait()方法的时候，这个线程必须拥有该对象的锁，接着它就会释放这个对象锁并进入等待状态直到其他线程调用这个对象上的 notify()方法。同样的，当一个线程需要调用对象的 notify()方法时，它会释放这个对象的锁，以便其他在等待的线程就可以得到这个对象锁。由于所有的这些方法都需要线程持有对象的锁，这样就只能通过同步来实现，所以他们只能在同步方法或者同步块中被调用。</p>
<h4 id="Thread-类中的-yield-方法有什么作用？"><a href="#Thread-类中的-yield-方法有什么作用？" class="headerlink" title="Thread 类中的 yield 方法有什么作用？"></a>Thread 类中的 yield 方法有什么作用？</h4><p>使当前线程从执行状态（运行状态）变为可执行态（就绪状态）。</p>
<p>当前线程到了就绪状态，那么接下来哪个线程会从就绪状态变成执行状态呢？可能是当前线程，也可能是其他线程，看系统的分配了。</p>
<h4 id="为什么-Thread-类的-sleep-和-yield-方法是静态的？"><a href="#为什么-Thread-类的-sleep-和-yield-方法是静态的？" class="headerlink" title="为什么 Thread 类的 sleep()和 yield ()方法是静态的？"></a>为什么 Thread 类的 sleep()和 yield ()方法是静态的？</h4><p>Thread 类的 sleep()和 yield()方法将在当前正在执行的线程上运行。所以在其他处于等待状态的线程上调用这些方法是没有意义的。这就是为什么这些方法是静态的。它们可以在当前正在执行的线程中工作，并避免程序员错误的认为可以在其他非运行线程调用这些方法。</p>
<h4 id="线程的-sleep-方法和-yield-方法有什么区别？"><a href="#线程的-sleep-方法和-yield-方法有什么区别？" class="headerlink" title="线程的 sleep()方法和 yield()方法有什么区别？"></a>线程的 sleep()方法和 yield()方法有什么区别？</h4><p>（1） sleep()方法给其他线程运行机会时不考虑线程的优先级，因此会给低优先级的线程以运行的机会；yield()方法只会给相同优先级或更高优先级的线程以运行的机会；</p>
<p>（2） 线程执行 sleep()方法后转入阻塞（blocked）状态，而执行 yield()方法后转入就绪（ready）状态；</p>
<p>（3）sleep()方法声明抛出 InterruptedException，而 yield()方法没有声明任何异常；</p>
<p>（4）sleep()方法比 yield()方法（跟操作系统 CPU 调度相关）具有更好的可移植性，通常不建议使用yield()方法来控制并发线程的执行。</p>
<h4 id="如何停止一个正在运行的线程？"><a href="#如何停止一个正在运行的线程？" class="headerlink" title="如何停止一个正在运行的线程？"></a>如何停止一个正在运行的线程？</h4><p>在java中有以下3种方法可以终止正在运行的线程：</p>
<ol>
<li>使用退出标志，使线程正常退出，也就是当run方法完成后线程终止。</li>
<li>使用stop方法强行终止，但是不推荐这个方法，因为stop和suspend及resume一样都是过期作废的方法。</li>
<li>使用interrupt方法中断线程。</li>
</ol>
<h4 id="Java-中-interrupted-和-isInterrupted-方法的区别？"><a href="#Java-中-interrupted-和-isInterrupted-方法的区别？" class="headerlink" title="Java 中 interrupted 和 isInterrupted 方法的区别？"></a>Java 中 interrupted 和 isInterrupted 方法的区别？</h4><p>interrupt：用于中断线程。调用该方法的线程的状态为将被置为”中断”状态。</p>
<p>注意：线程中断仅仅是置线程的中断状态位，不会停止线程。需要用户自己去监视线程的状态为并做处理。支持线程中断的方法（也就是线程中断后会抛出interruptedException 的方法）就是在监视线程的中断状态，一旦线程的中断状态被置为“中断状态”，就会抛出中断异常。</p>
<p>interrupted：是静态方法，查看当前中断信号是true还是false并且清除中断信号。如果一个线程被中断了，第一次调用 interrupted 则返回 true，第二次和后面的就返回 false 了。</p>
<p>isInterrupted：查看当前中断信号是true还是false</p>
<h4 id="什么是阻塞式方法？"><a href="#什么是阻塞式方法？" class="headerlink" title="什么是阻塞式方法？"></a>什么是阻塞式方法？</h4><p>阻塞式方法是指程序会一直等待该方法完成期间不做其他事情，ServerSocket 的accept()方法就是一直等待客户端连接。这里的阻塞是指调用结果返回之前，当前线程会被挂起，直到得到结果之后才会返回。此外，还有异步和非阻塞式方法在任务完成前就返回。</p>
<h4 id="Java-中你怎样唤醒一个阻塞的线程？"><a href="#Java-中你怎样唤醒一个阻塞的线程？" class="headerlink" title="Java 中你怎样唤醒一个阻塞的线程？"></a>Java 中你怎样唤醒一个阻塞的线程？</h4><p>首先 ，wait()、notify() 方法是针对对象的，调用任意对象的 wait()方法都将导致线程阻塞，阻塞的同时也将释放该对象的锁，相应地，调用任意对象的 notify()方法则将随机解除该对象阻塞的线程，但它需要重新获取该对象的锁，直到获取成功才能往下执行；</p>
<p>其次，wait、notify 方法必须在 synchronized 块或方法中被调用，并且要保证同步块或方法的锁对象与调用 wait、notify 方法的对象是同一个，如此一来在调用 wait 之前当前线程就已经成功获取某对象的锁，执行 wait 阻塞后当前线程就将之前获取的对象锁释放。</p>
<h4 id="notify-和-notifyAll-有什么区别？"><a href="#notify-和-notifyAll-有什么区别？" class="headerlink" title="notify() 和 notifyAll() 有什么区别？"></a>notify() 和 notifyAll() 有什么区别？</h4><p>如果线程调用了对象的 wait()方法，那么线程便会处于该对象的等待池中，等待池中的线程不会去竞争该对象的锁。</p>
<p>notifyAll() 会唤醒所有的线程，notify() 只会唤醒一个线程。</p>
<p>notifyAll() 调用后，会将全部线程由等待池移到锁池，然后参与锁的竞争，竞争成功则继续执行，如果不成功则留在锁池等待锁被释放后再次参与竞争。而 notify()只会唤醒一个线程，具体唤醒哪一个线程由虚拟机控制。</p>
<h4 id="如何在两个线程间共享数据？"><a href="#如何在两个线程间共享数据？" class="headerlink" title="如何在两个线程间共享数据？"></a>如何在两个线程间共享数据？</h4><p>在两个线程间共享变量即可实现共享。</p>
<p>一般来说，共享变量要求变量本身是线程安全的，然后在线程内使用的时候，如果有对共享变量的复合操作，那么也得保证复合操作的线程安全性。</p>
<h4 id="Java-如何实现多线程之间的通讯和协作？"><a href="#Java-如何实现多线程之间的通讯和协作？" class="headerlink" title="Java 如何实现多线程之间的通讯和协作？"></a>Java 如何实现多线程之间的通讯和协作？</h4><p>Java中线程通信协作的最常见的两种方式：</p>
<p>一.syncrhoized加锁的线程的Object类的wait()/notify()/notifyAll()</p>
<p>二.ReentrantLock类加锁的线程的Condition类的await()/signal()/signalAll()</p>
<p>线程间直接的数据交换：</p>
<p>三.通过管道进行线程间通信：1）字节流；2）字符流</p>
<h4 id="同步方法和同步块，哪个是更好的选择？"><a href="#同步方法和同步块，哪个是更好的选择？" class="headerlink" title="同步方法和同步块，哪个是更好的选择？"></a>同步方法和同步块，哪个是更好的选择？</h4><p>同步块是更好的选择，因为它不会锁住整个对象（当然你也可以让它锁住整个对象）。同步方法会锁住整个对象，哪怕这个类中有多个不相关联的同步块，这通常会导致他们停止执行并需要等待获得这个对象上的锁。</p>
<p>同步块更要符合开放调用的原则，只在需要锁住的代码块锁住相应的对象，这样从侧面来说也可以避免死锁。</p>
<p>请知道一条原则：同步的范围越小越好。</p>
<h4 id="什么是线程同步和线程互斥，有哪几种实现方式？"><a href="#什么是线程同步和线程互斥，有哪几种实现方式？" class="headerlink" title="什么是线程同步和线程互斥，有哪几种实现方式？"></a>什么是线程同步和线程互斥，有哪几种实现方式？</h4><p>实现线程同步的方法</p>
<p>同步代码方法：sychronized 关键字修饰的方法</p>
<p>同步代码块：sychronized 关键字修饰的代码块</p>
<p>使用特殊变量域volatile实现线程同步：volatile关键字为域变量的访问提供了一种免锁机制</p>
<p>使用重入锁实现线程同步：reentrantlock类是可冲入、互斥、实现了lock接口的锁他与sychronized方法具有相同的基本行为和语义</p>
<h4 id="在监视器-Monitor-内部，是如何做线程同步的？程序应该做哪种级别的同步？"><a href="#在监视器-Monitor-内部，是如何做线程同步的？程序应该做哪种级别的同步？" class="headerlink" title="在监视器(Monitor)内部，是如何做线程同步的？程序应该做哪种级别的同步？"></a>在监视器(Monitor)内部，是如何做线程同步的？程序应该做哪种级别的同步？</h4><p>在 java 虚拟机中，每个对象( Object 和 class )通过某种逻辑关联监视器,每个监视器和一个对象引用相关联，为了实现监视器的互斥功能，每个对象都关联着一把锁。</p>
<p>一旦方法或者代码块被 synchronized 修饰，那么这个部分就放入了监视器的监视区域，确保一次只能有一个线程执行该部分的代码，线程在获取锁之前不允许执行该部分的代码</p>
<p>另外 java 还提供了显式监视器( Lock )和隐式监视器( synchronized )两种锁方案</p>
<h4 id="如果你提交任务时，线程池队列已满，这时会发生什么"><a href="#如果你提交任务时，线程池队列已满，这时会发生什么" class="headerlink" title="如果你提交任务时，线程池队列已满，这时会发生什么"></a>如果你提交任务时，线程池队列已满，这时会发生什么</h4><p>这里区分一下：</p>
<p>（1）如果使用的是无界队列 LinkedBlockingQueue，也就是无界队列的话，没关系，继续添加任务到阻塞队列中等待执行，因为 LinkedBlockingQueue 可以近乎认为是一个无穷大的队列，可以无限存放任务</p>
<p>（2）如果使用的是有界队列比如 ArrayBlockingQueue，任务首先会被添加到ArrayBlockingQueue 中，ArrayBlockingQueue 满了，会根据maximumPoolSize 的值增加线程数量，如果增加了线程数量还是处理不过来，ArrayBlockingQueue 继续满，那么则会使用拒绝策略RejectedExecutionHandler 处理满了的任务，默认是 AbortPolicy</p>
<h4 id="什么叫线程安全？servlet-是线程安全吗"><a href="#什么叫线程安全？servlet-是线程安全吗" class="headerlink" title="什么叫线程安全？servlet 是线程安全吗?"></a>什么叫线程安全？servlet 是线程安全吗?</h4><p>线程安全是编程中的术语，指某个方法在多线程环境中被调用时，能够正确地处理多个线程之间的共享变量，使程序功能正确完成。</p>
<p>Servlet 不是线程安全的，servlet 是单实例多线程的，当多个线程同时访问同一个方法，是不能保证共享变量的线程安全性的。</p>
<p>Struts2 的 action 是多实例多线程的，是线程安全的，每个请求过来都会 new 一个新的 action 分配给这个请求，请求完成后销毁。</p>
<p>SpringMVC 的 Controller 是线程安全的吗？不是的，和 Servlet 类似的处理流程。</p>
<p>Struts2 好处是不用考虑线程安全问题；Servlet 和 SpringMVC 需要考虑线程安全问题，但是性能可以提升不用处理太多的 gc，可以使用 ThreadLocal 来处理多线程的问题。</p>
<h4 id="在-Java-程序中怎么保证多线程的运行安全？"><a href="#在-Java-程序中怎么保证多线程的运行安全？" class="headerlink" title="在 Java 程序中怎么保证多线程的运行安全？"></a>在 Java 程序中怎么保证多线程的运行安全？</h4><ul>
<li>方法一：使用安全类，比如 java.util.concurrent 下的类，使用原子类AtomicInteger</li>
<li>方法二：使用自动锁 synchronized。</li>
<li>方法三：使用手动锁 Lock。</li>
</ul>
<h4 id="你对线程优先级的理解是什么？"><a href="#你对线程优先级的理解是什么？" class="headerlink" title="你对线程优先级的理解是什么？"></a>你对线程优先级的理解是什么？</h4><p>每一个线程都是有优先级的，一般来说，高优先级的线程在运行时会具有优先权，但这依赖于线程调度的实现，这个实现是和操作系统相关的(OS dependent)。我们可以定义线程的优先级，但是这并不能保证高优先级的线程会在低优先级的线程前执行。线程优先级是一个 int 变量(从 1-10)，1 代表最低优先级，10 代表最高优先级。</p>
<p>Java 的线程优先级调度会委托给操作系统去处理，所以与具体的操作系统优先级有关，如非特别需要，一般无需设置线程优先级。</p>
<h4 id="线程类的构造方法、静态块是被哪个线程调用的"><a href="#线程类的构造方法、静态块是被哪个线程调用的" class="headerlink" title="线程类的构造方法、静态块是被哪个线程调用的"></a>线程类的构造方法、静态块是被哪个线程调用的</h4><p>这是一个非常刁钻和狡猾的问题。请记住：线程类的构造方法、静态块是被 new这个线程类所在的线程所调用的，而 run 方法里面的代码才是被线程自身所调用的。</p>
<p>如果说上面的说法让你感到困惑，那么我举个例子，假设 Thread2 中 new 了Thread1，main 函数中 new 了 Thread2，那么：</p>
<p>（1）Thread2 的构造方法、静态块是 main 线程调用的，Thread2 的 run()方法是Thread2 自己调用的</p>
<p>（2）Thread1 的构造方法、静态块是 Thread2 调用的，Thread1 的 run()方法是Thread1 自己调用的</p>
<h4 id="Java-中怎么获取一份线程-dump-文件？你如何在-Java-中获取线程堆栈？"><a href="#Java-中怎么获取一份线程-dump-文件？你如何在-Java-中获取线程堆栈？" class="headerlink" title="Java 中怎么获取一份线程 dump 文件？你如何在 Java 中获取线程堆栈？"></a>Java 中怎么获取一份线程 dump 文件？你如何在 Java 中获取线程堆栈？</h4><p>Dump文件是进程的内存镜像。可以把程序的执行状态通过调试器保存到dump文件中。</p>
<p>在 Linux 下，你可以通过命令 kill -3 PID （Java 进程的进程 ID）来获取 Java应用的 dump 文件。</p>
<p>在 Windows 下，你可以按下 Ctrl + Break 来获取。这样 JVM 就会将线程的 dump 文件打印到标准输出或错误文件中，它可能打印在控制台或者日志文件中，具体位置依赖应用的配置。</p>
<h4 id="一个线程运行时发生异常会怎样？"><a href="#一个线程运行时发生异常会怎样？" class="headerlink" title="一个线程运行时发生异常会怎样？"></a>一个线程运行时发生异常会怎样？</h4><p>如果异常没有被捕获该线程将会停止执行。Thread.UncaughtExceptionHandler是用于处理未捕获异常造成线程突然中断情况的一个内嵌接口。当一个未捕获异常将造成线程中断的时候，JVM 会使用 Thread.getUncaughtExceptionHandler()来查询线程的 UncaughtExceptionHandler 并将线程和异常作为参数传递给 handler 的 uncaughtException()方法进行处理。</p>
<h4 id="Java-线程数过多会造成什么异常？"><a href="#Java-线程数过多会造成什么异常？" class="headerlink" title="Java 线程数过多会造成什么异常？"></a>Java 线程数过多会造成什么异常？</h4><ul>
<li><p>线程的生命周期开销非常高</p>
</li>
<li><p>消耗过多的 CPU</p>
<p>资源如果可运行的线程数量多于可用处理器的数量，那么有线程将会被闲置。大量空闲的线程会占用许多内存，给垃圾回收器带来压力，而且大量的线程在竞争 CPU资源时还将产生其他性能的开销</p>
</li>
<li><p>降低稳定性JVM</p>
<p>在可创建线程的数量上存在一个限制，这个限制值将随着平台的不同而不同，并且承受着多个因素制约，包括 JVM 的启动参数、Thread 构造函数中请求栈的大小，以及底层操作系统对线程的限制等。如果破坏了这些限制，那么可能抛出OutOfMemoryError 异常。</p>
</li>
</ul>
<h2 id="并发理论"><a href="#并发理论" class="headerlink" title="并发理论"></a>并发理论</h2><h3 id="Java内存模型"><a href="#Java内存模型" class="headerlink" title="Java内存模型"></a>Java内存模型</h3><h4 id="Java中垃圾回收有什么目的？什么时候进行垃圾回收？"><a href="#Java中垃圾回收有什么目的？什么时候进行垃圾回收？" class="headerlink" title="Java中垃圾回收有什么目的？什么时候进行垃圾回收？"></a>Java中垃圾回收有什么目的？什么时候进行垃圾回收？</h4><p>垃圾回收是在内存中存在没有引用的对象或超过作用域的对象时进行的。</p>
<p>垃圾回收的目的是识别并且丢弃应用不再使用的对象来释放和重用资源。</p>
<h4 id="如果对象的引用被置为null，垃圾收集器是否会立即释放对象占用的内存？"><a href="#如果对象的引用被置为null，垃圾收集器是否会立即释放对象占用的内存？" class="headerlink" title="如果对象的引用被置为null，垃圾收集器是否会立即释放对象占用的内存？"></a>如果对象的引用被置为null，垃圾收集器是否会立即释放对象占用的内存？</h4><p>不会，在下一个垃圾回调周期中，这个对象将是被可回收的。</p>
<p>也就是说并不会立即被垃圾收集器立刻回收，而是在下一次垃圾回收时才会释放其占用的内存。</p>
<h4 id="finalize-方法什么时候被调用？析构函数-finalization-的目的是什么？"><a href="#finalize-方法什么时候被调用？析构函数-finalization-的目的是什么？" class="headerlink" title="finalize()方法什么时候被调用？析构函数(finalization)的目的是什么？"></a>finalize()方法什么时候被调用？析构函数(finalization)的目的是什么？</h4><p>1）垃圾回收器（garbage colector）决定回收某对象时，就会运行该对象的finalize()方法；<br>finalize是Object类的一个方法，该方法在Object类中的声明protected void finalize() throws Throwable { }<br>在垃圾回收器执行时会调用被回收对象的finalize()方法，可以覆盖此方法来实现对其资源的回收。注意：一旦垃圾回收器准备释放对象占用的内存，将首先调用该对象的finalize()方法，并且下一次垃圾回收动作发生时，才真正回收对象占用的内存空间</p>
<p>2）GC本来就是内存回收了，应用还需要在finalization做什么呢？ 答案是大部分时候，什么都不用做(也就是不需要重载)。只有在某些很特殊的情况下，比如你调用了一些native的方法(一般是C写的)，可以要在finaliztion里去调用C的释放函数。</p>
<h3 id="重排序与数据依赖性"><a href="#重排序与数据依赖性" class="headerlink" title="重排序与数据依赖性"></a>重排序与数据依赖性</h3><h4 id="为什么代码会重排序？"><a href="#为什么代码会重排序？" class="headerlink" title="为什么代码会重排序？"></a>为什么代码会重排序？</h4><p>在执行程序时，为了提供性能，处理器和编译器常常会对指令进行重排序，但是不能随意重排序，不是你想怎么排序就怎么排序，它需要满足以下两个条件：</p>
<p>在单线程环境下不能改变程序运行的结果；</p>
<p>存在数据依赖关系的不允许重排序</p>
<p>需要注意的是：重排序不会影响单线程环境的执行结果，但是会破坏多线程的执行语义。</p>
<h3 id="as-if-serial规则和happens-before规则的区别"><a href="#as-if-serial规则和happens-before规则的区别" class="headerlink" title="as-if-serial规则和happens-before规则的区别"></a>as-if-serial规则和happens-before规则的区别</h3><ul>
<li>as-if-serial语义保证单线程内程序的执行结果不被改变，happens-before关系保证正确同步的多线程程序的执行结果不被改变。</li>
<li>as-if-serial语义给编写单线程程序的程序员创造了一个幻境：单线程程序是按程序的顺序来执行的。happens-before关系给编写正确同步的多线程程序的程序员创造了一个幻境：正确同步的多线程程序是按happens-before指定的顺序来执行的。</li>
<li>as-if-serial语义和happens-before这么做的目的，都是为了在不改变程序执行结果的前提下，尽可能地提高程序执行的并行度。</li>
</ul>
<h2 id="并发关键字"><a href="#并发关键字" class="headerlink" title="并发关键字"></a>并发关键字</h2><h3 id="synchronized"><a href="#synchronized" class="headerlink" title="synchronized"></a>synchronized</h3><h4 id="synchronized-的作用？"><a href="#synchronized-的作用？" class="headerlink" title="synchronized 的作用？"></a>synchronized 的作用？</h4><p>在 Java 中，synchronized 关键字是用来控制线程同步的，就是在多线程的环境下，控制 synchronized 代码段不被多个线程同时执行。synchronized 可以修饰类、方法、变量。</p>
<p>另外，在 Java 早期版本中，synchronized属于重量级锁，效率低下，因为监视器锁（monitor）是依赖于底层的操作系统的 Mutex Lock 来实现的，Java 的线程是映射到操作系统的原生线程之上的。如果要挂起或者唤醒一个线程，都需要操作系统帮忙完成，而操作系统实现线程之间的切换时需要从用户态转换到内核态，这个状态之间的转换需要相对比较长的时间，时间成本相对较高，这也是为什么早期的 synchronized 效率低的原因。庆幸的是在 Java 6 之后 Java 官方对从 JVM 层面对synchronized 较大优化，所以现在的 synchronized 锁效率也优化得很不错了。JDK1.6对锁的实现引入了大量的优化，如自旋锁、适应性自旋锁、锁消除、锁粗化、偏向锁、轻量级锁等技术来减少锁操作的开销。</p>
<h4 id="说说自己是怎么使用-synchronized-关键字，在项目中用到了吗"><a href="#说说自己是怎么使用-synchronized-关键字，在项目中用到了吗" class="headerlink" title="说说自己是怎么使用 synchronized 关键字，在项目中用到了吗"></a>说说自己是怎么使用 synchronized 关键字，在项目中用到了吗</h4><p>synchronized关键字最主要的三种使用方式：</p>
<p>修饰实例方法: 作用于当前对象实例加锁，进入同步代码前要获得当前对象实例的锁<br>修饰静态方法: 也就是给当前类加锁，会作用于类的所有对象实例，因为静态成员不属于任何一个实例对象，是类成员（ static 表明这是该类的一个静态资源，不管new了多少个对象，只有一份）。所以如果一个线程A调用一个实例对象的非静态 synchronized 方法，而线程B需要调用这个实例对象所属类的静态 synchronized 方法，是允许的，不会发生互斥现象，因为访问静态 synchronized 方法占用的锁是当前类的锁，而访问非静态 synchronized 方法占用的锁是当前实例对象锁。<br>修饰代码块: 指定加锁对象，对给定对象加锁，进入同步代码库前要获得给定对象的锁。<br>总结： synchronized 关键字加到 static 静态方法和 synchronized(class)代码块上都是是给 Class 类上锁。synchronized 关键字加到实例方法上是给对象实例上锁。尽量不要使用 synchronized(String a) 因为JVM中，字符串常量池具有缓存功能！</p>
<p>uniqueInstance 采用 volatile 关键字修饰也是很有必要的， uniqueInstance = new Singleton(); 这段代码其实是分为三步执行：</p>
<ol>
<li>为 uniqueInstance 分配内存空间</li>
<li>初始化 uniqueInstance</li>
<li>将 uniqueInstance 指向分配的内存地址</li>
</ol>
<p>使用 volatile 可以禁止 JVM 的指令重排，保证在多线程环境下也能正常运行。</p>
<h4 id="说一下-synchronized-底层实现原理？"><a href="#说一下-synchronized-底层实现原理？" class="headerlink" title="说一下 synchronized 底层实现原理？"></a>说一下 synchronized 底层实现原理？</h4><p>可以看出在执行同步代码块之前之后都有一个monitor字样，其中前面的是monitorenter，后面的是离开monitorexit，不难想象一个线程也执行同步代码块，首先要获取锁，而获取锁的过程就是monitorenter ，在执行完代码块之后，要释放锁，释放锁就是执行monitorexit指令。</p>
<p>为什么会有两个monitorexit呢？</p>
<p>这个主要是防止在同步代码块中线程因异常退出，而锁没有得到释放，这必然会造成死锁（等待的线程永远获取不到锁）。因此最后一个monitorexit是保证在异常情况下，锁也可以得到释放，避免死锁。<br>仅有ACC_SYNCHRONIZED这么一个标志，该标记表明线程进入该方法时，需要monitorenter，退出该方法时需要monitorexit。</p>
<p>synchronized可重入的原理</p>
<p>重入锁是指一个线程获取到该锁之后，该线程可以继续获得该锁。底层原理维护一个计数器，当线程获取该锁时，计数器加一，再次获得该锁时继续加一，释放锁时，计数器减一，当计数器值为0时，表明该锁未被任何线程所持有，其它线程可以竞争获取锁。</p>
<h4 id="什么是自旋"><a href="#什么是自旋" class="headerlink" title="什么是自旋"></a>什么是自旋</h4><p>很多 synchronized 里面的代码只是一些很简单的代码，执行时间非常快，此时等待的线程都加锁可能是一种不太值得的操作，因为线程阻塞涉及到用户态和内核态切换的问题。既然 synchronized 里面的代码执行得非常快，不妨让等待锁的线程不要被阻塞，而是在 synchronized 的边界做忙循环，这就是自旋。如果做了多次循环发现还没有获得锁，再阻塞，这样可能是一种更好的策略。</p>
<h4 id="多线程中-synchronized-锁升级的原理是什么？"><a href="#多线程中-synchronized-锁升级的原理是什么？" class="headerlink" title="多线程中 synchronized 锁升级的原理是什么？"></a>多线程中 synchronized 锁升级的原理是什么？</h4><p>synchronized 锁升级原理：在锁对象的对象头里面有一个 thread id 字段，在第一次访问的时候 threadid 为空，jvm 让其持有偏向锁，并将 threadid 设置为其线程 id，再次进入的时候会先判断 threadid 是否与其线程 id 一致，如果一致则可以直接使用此对象，如果不一致，则升级偏向锁为轻量级锁，通过自旋循环一定次数来获取锁，执行一定次数之后，如果还没有正常获取到要使用的对象，此时就会把锁从轻量级升级为重量级锁，此过程就构成了 synchronized 锁的升级。</p>
<p>锁的升级的目的：锁升级是为了减低了锁带来的性能消耗。在 Java 6 之后优化 synchronized 的实现方式，使用了偏向锁升级为轻量级锁再升级到重量级锁的方式，从而减低了锁带来的性能消耗。</p>
<h4 id="线程-B-怎么知道线程-A-修改了变量"><a href="#线程-B-怎么知道线程-A-修改了变量" class="headerlink" title="线程 B 怎么知道线程 A 修改了变量"></a>线程 B 怎么知道线程 A 修改了变量</h4><p>（1）volatile 修饰变量</p>
<p>（2）synchronized 修饰修改变量的方法</p>
<p>（3）wait/notify</p>
<p>（4）while 轮询</p>
<h4 id="当一个线程进入一个对象的-synchronized-方法-A-之后，其它线程是否可进入此对象的-synchronized-方法-B？"><a href="#当一个线程进入一个对象的-synchronized-方法-A-之后，其它线程是否可进入此对象的-synchronized-方法-B？" class="headerlink" title="当一个线程进入一个对象的 synchronized 方法 A 之后，其它线程是否可进入此对象的 synchronized 方法 B？"></a>当一个线程进入一个对象的 synchronized 方法 A 之后，其它线程是否可进入此对象的 synchronized 方法 B？</h4><p>不能。其它线程只能访问该对象的非同步方法，同步方法则不能进入。因为非静态方法上的 synchronized 修饰符要求执行方法时要获得对象的锁，如果已经进入A 方法说明对象锁已经被取走，那么试图进入 B 方法的线程就只能在等锁池（注意不是等待池哦）中等待对象的锁。</p>
<h4 id="synchronized、volatile、CAS-比较"><a href="#synchronized、volatile、CAS-比较" class="headerlink" title="synchronized、volatile、CAS 比较"></a>synchronized、volatile、CAS 比较</h4><p>（1）synchronized 是悲观锁，属于抢占式，会引起其他线程阻塞。</p>
<p>（2）volatile 提供多线程共享变量可见性和禁止指令重排序优化。</p>
<p>（3）CAS 是基于冲突检测的乐观锁（非阻塞）</p>
<h4 id="synchronized-和-Lock-有什么区别？"><a href="#synchronized-和-Lock-有什么区别？" class="headerlink" title="synchronized 和 Lock 有什么区别？"></a>synchronized 和 Lock 有什么区别？</h4><ul>
<li>首先synchronized是Java内置关键字，在JVM层面，Lock是个Java类；</li>
<li>synchronized 可以给类、方法、代码块加锁；而 lock 只能给代码块加锁。</li>
<li>synchronized 不需要手动获取锁和释放锁，使用简单，发生异常会自动释放锁，不会造成死锁；而 lock 需要自己加锁和释放锁，如果使用不当没有 unLock()去释放锁就会造成死锁。</li>
<li>通过 Lock 可以知道有没有成功获取锁，而 synchronized 却无法办到。</li>
</ul>
<h4 id="synchronized-和-ReentrantLock-区别是什么？"><a href="#synchronized-和-ReentrantLock-区别是什么？" class="headerlink" title="synchronized 和 ReentrantLock 区别是什么？"></a>synchronized 和 ReentrantLock 区别是什么？</h4><p>synchronized 是和 if、else、for、while 一样的关键字，ReentrantLock 是类，这是二者的本质区别。既然 ReentrantLock 是类，那么它就提供了比synchronized 更多更灵活的特性，可以被继承、可以有方法、可以有各种各样的类变量</p>
<p>synchronized 早期的实现比较低效，对比 ReentrantLock，大多数场景性能都相差较大，但是在 Java 6 中对 synchronized 进行了非常多的改进。</p>
<p>相同点：两者都是可重入锁</p>
<p>两者都是可重入锁。“可重入锁”概念是：自己可以再次获取自己的内部锁。比如一个线程获得了某个对象的锁，此时这个对象锁还没有释放，当其再次想要获取这个对象的锁的时候还是可以获取的，如果不可锁重入的话，就会造成死锁。同一个线程每次获取锁，锁的计数器都自增1，所以要等到锁的计数器下降为0时才能释放锁。</p>
<p>主要区别如下：</p>
<ul>
<li>ReentrantLock 使用起来比较灵活，但是必须有释放锁的配合动作；</li>
<li>ReentrantLock 必须手动获取与释放锁，而 synchronized 不需要手动释放和开启锁；</li>
<li>ReentrantLock 只适用于代码块锁，而 synchronized 可以修饰类、方法、变量等。</li>
<li>二者的锁机制其实也是不一样的。ReentrantLock 底层调用的是 Unsafe 的park 方法加锁，synchronized 操作的应该是对象头中 mark word</li>
</ul>
<p>Java中每一个对象都可以作为锁，这是synchronized实现同步的基础：</p>
<ul>
<li>普通同步方法，锁是当前实例对象</li>
<li>静态同步方法，锁是当前类的class对象</li>
<li>同步方法块，锁是括号里面的对象</li>
</ul>
<h3 id="volatile"><a href="#volatile" class="headerlink" title="volatile"></a>volatile</h3><h4 id="volatile-关键字的作用"><a href="#volatile-关键字的作用" class="headerlink" title="volatile 关键字的作用"></a>volatile 关键字的作用</h4><p>对于可见性，Java 提供了 volatile 关键字来保证可见性和禁止指令重排。 volatile 提供 happens-before 的保证，确保一个线程的修改能对其他线程是可见的。当一个共享变量被 volatile 修饰时，它会保证修改的值会立即被更新到主存，当有其他线程需要读取时，它会去内存中读取新值。</p>
<p>从实践角度而言，volatile 的一个重要作用就是和 CAS 结合，保证了原子性，详细的可以参见 java.util.concurrent.atomic 包下的类，比如 AtomicInteger。</p>
<p>volatile 常用于多线程环境下的单次操作(单次读或者单次写)。</p>
<h4 id="Java-中能创建-volatile-数组吗？"><a href="#Java-中能创建-volatile-数组吗？" class="headerlink" title="Java 中能创建 volatile 数组吗？"></a>Java 中能创建 volatile 数组吗？</h4><p>能，Java 中可以创建 volatile 类型数组，不过只是一个指向数组的引用，而不是整个数组。意思是，如果改变引用指向的数组，将会受到 volatile 的保护，但是如果多个线程同时改变数组的元素，volatile 标示符就不能起到之前的保护作用了。</p>
<h4 id="volatile-变量和-atomic-变量有什么不同？"><a href="#volatile-变量和-atomic-变量有什么不同？" class="headerlink" title="volatile 变量和 atomic 变量有什么不同？"></a>volatile 变量和 atomic 变量有什么不同？</h4><p>volatile 变量可以确保先行关系，即写操作会发生在后续的读操作之前, 但它并不能保证原子性。例如用 volatile 修饰 count 变量，那么 count++ 操作就不是原子性的。</p>
<p>而 AtomicInteger 类提供的 atomic 方法可以让这种操作具有原子性如getAndIncrement()方法会原子性的进行增量操作把当前值加一，其它数据类型和引用变量也可以进行相似操作。</p>
<h4 id="volatile-能使得一个非原子操作变成原子操作吗？"><a href="#volatile-能使得一个非原子操作变成原子操作吗？" class="headerlink" title="volatile 能使得一个非原子操作变成原子操作吗？"></a>volatile 能使得一个非原子操作变成原子操作吗？</h4><p>关键字volatile的主要作用是使变量在多个线程间可见，但无法保证原子性，对于多个线程访问同一个实例变量需要加锁进行同步。</p>
<p>虽然volatile只能保证可见性不能保证原子性，但用volatile修饰long和double可以保证其操作原子性。</p>
<p>所以从Oracle Java Spec里面可以看到：</p>
<p>对于64位的long和double，如果没有被volatile修饰，那么对其操作可以不是原子的。在操作的时候，可以分成两步，每次对32位操作。<br>如果使用volatile修饰long和double，那么其读写都是原子操作<br>对于64位的引用地址的读写，都是原子操作<br>在实现JVM时，可以自由选择是否把读写long和double作为原子操作<br>推荐JVM实现为原子操作</p>
<h4 id="volatile-修饰符的有过什么实践？"><a href="#volatile-修饰符的有过什么实践？" class="headerlink" title="volatile 修饰符的有过什么实践？"></a>volatile 修饰符的有过什么实践？</h4><p>Double-Check单例模式</p>
<p>是否 Lazy 初始化：是</p>
<p>是否多线程安全：是</p>
<p>实现难度：较复杂</p>
<h4 id="synchronized-和-volatile-的区别是什么？"><a href="#synchronized-和-volatile-的区别是什么？" class="headerlink" title="synchronized 和 volatile 的区别是什么？"></a>synchronized 和 volatile 的区别是什么？</h4><p>synchronized 表示只有一个线程可以获取作用对象的锁，执行代码，阻塞其他线程。</p>
<p>volatile 表示变量在 CPU 的寄存器中是不确定的，必须从主存中读取。保证多线程环境下变量的可见性；禁止指令重排序。</p>
<h3 id="final"><a href="#final" class="headerlink" title="final"></a>final</h3><h4 id="什么是不可变对象，它对写并发应用有什么帮助？"><a href="#什么是不可变对象，它对写并发应用有什么帮助？" class="headerlink" title="什么是不可变对象，它对写并发应用有什么帮助？"></a>什么是不可变对象，它对写并发应用有什么帮助？</h4><p>不可变对象(Immutable Objects)即对象一旦被创建它的状态（对象的数据，也即对象属性值）就不能改变，反之即为可变对象(Mutable Objects)。</p>
<p>不可变对象保证了对象的内存可见性，对不可变对象的读取不需要进行额外的同步手段，提升了代码执行效率。</p>
<h2 id="Lock体系"><a href="#Lock体系" class="headerlink" title="Lock体系"></a>Lock体系</h2><h3 id="Lock简介与初识AQS"><a href="#Lock简介与初识AQS" class="headerlink" title="Lock简介与初识AQS"></a>Lock简介与初识AQS</h3><h4 id="Java-Concurrency-API-中的-Lock-接口-Lock-interface-是什么？对比同步它有什么优势？"><a href="#Java-Concurrency-API-中的-Lock-接口-Lock-interface-是什么？对比同步它有什么优势？" class="headerlink" title="Java Concurrency API 中的 Lock 接口(Lock interface)是什么？对比同步它有什么优势？"></a>Java Concurrency API 中的 Lock 接口(Lock interface)是什么？对比同步它有什么优势？</h4><p>Lock 接口比同步方法和同步块提供了更具扩展性的锁操作。他们允许更灵活的结构，可以具有完全不同的性质，并且可以支持多个相关类的条件对象。</p>
<p>它的优势有：</p>
<p>（1）可以使锁更公平</p>
<p>（2）可以使线程在等待锁的时候响应中断</p>
<p>（3）可以让线程尝试获取锁，并在无法获取锁的时候立即返回或者等待一段时间</p>
<p>（4）可以在不同的范围，以不同的顺序获取和释放锁</p>
<p>整体上来说 Lock 是 synchronized 的扩展版，Lock 提供了无条件的、可轮询的(tryLock 方法)、定时的(tryLock 带参方法)、可中断的(lockInterruptibly)、可多条件队列的(newCondition 方法)锁操作。另外 Lock 的实现类基本都支持非公平锁(默认)和公平锁，synchronized 只支持非公平锁，当然，在大部分情况下，非公平锁是高效的选择。</p>
<h4 id="乐观锁和悲观锁的理解及如何实现，有哪些实现方式？"><a href="#乐观锁和悲观锁的理解及如何实现，有哪些实现方式？" class="headerlink" title="乐观锁和悲观锁的理解及如何实现，有哪些实现方式？"></a>乐观锁和悲观锁的理解及如何实现，有哪些实现方式？</h4><p>悲观锁：总是假设最坏的情况，每次去拿数据的时候都认为别人会修改，所以每次在拿数据的时候都会上锁，这样别人想拿这个数据就会阻塞直到它拿到锁。传统的关系型数据库里边就用到了很多这种锁机制，比如行锁，表锁等，读锁，写锁等，都是在做操作之前先上锁。再比如 Java 里面的同步原语 synchronized 关键字的实现也是悲观锁。</p>
<p>乐观锁：顾名思义，就是很乐观，每次去拿数据的时候都认为别人不会修改，所以不会上锁，但是在更新的时候会判断一下在此期间别人有没有去更新这个数据，可以使用版本号等机制。乐观锁适用于多读的应用类型，这样可以提高吞吐量，像数据库提供的类似于 write_condition 机制，其实都是提供的乐观锁。在 Java中 java.util.concurrent.atomic 包下面的原子变量类就是使用了乐观锁的一种实现方式 CAS 实现的。</p>
<p>乐观锁的实现方式：</p>
<p>1、使用版本标识来确定读到的数据与提交时的数据是否一致。提交后修改版本标识，不一致时可以采取丢弃和再次尝试的策略。</p>
<p>2、java 中的 Compare and Swap 即 CAS ，当多个线程尝试使用 CAS 同时更新同一个变量时，只有其中一个线程能更新变量的值，而其它线程都失败，失败的线程并不会被挂起，而是被告知这次竞争中失败，并可以再次尝试。 CAS 操作中包含三个操作数 —— 需要读写的内存位置（V）、进行比较的预期原值（A）和拟写入的新值(B)。如果内存位置 V 的值与预期原值 A 相匹配，那么处理器会自动将该位置值更新为新值 B。否则处理器不做任何操作。</p>
<h4 id="什么是-CAS"><a href="#什么是-CAS" class="headerlink" title="什么是 CAS"></a>什么是 CAS</h4><p>CAS 是 compare and swap 的缩写，即我们所说的比较交换。</p>
<h4 id="CAS-的会产生什么问题？"><a href="#CAS-的会产生什么问题？" class="headerlink" title="CAS 的会产生什么问题？"></a>CAS 的会产生什么问题？</h4><p>1、ABA 问题：</p>
<p>比如说一个线程 one 从内存位置 V 中取出 A，这时候另一个线程 two 也从内存中取出 A，并且 two 进行了一些操作变成了 B，然后 two 又将 V 位置的数据变成 A，这时候线程 one 进行 CAS 操作发现内存中仍然是 A，然后 one 操作成功。尽管线程 one 的 CAS 操作成功，但可能存在潜藏的问题。从 Java1.5 开始 JDK 的 atomic包里提供了一个类 AtomicStampedReference 来解决 ABA 问题。</p>
<p>2、循环时间长开销大</p>
<p>3、只能保证一个共享变量的原子操作</p>
<h4 id="什么是死锁？"><a href="#什么是死锁？" class="headerlink" title="什么是死锁？"></a>什么是死锁？</h4><p>当线程 A 持有独占锁a，并尝试去获取独占锁 b 的同时，线程 B 持有独占锁 b，并尝试获取独占锁 a 的情况下，就会发生 AB 两个线程由于互相持有对方需要的锁，而发生的阻塞现象，我们称为死锁。</p>
<h4 id="产生死锁的条件是什么？怎么防止死锁？"><a href="#产生死锁的条件是什么？怎么防止死锁？" class="headerlink" title="产生死锁的条件是什么？怎么防止死锁？"></a>产生死锁的条件是什么？怎么防止死锁？</h4><p>产生死锁的必要条件：</p>
<p>1、互斥条件：所谓互斥就是进程在某一时间内独占资源。</p>
<p>2、请求与保持条件：一个进程因请求资源而阻塞时，对已获得的资源保持不放。</p>
<p>3、不剥夺条件：进程已获得资源，在末使用完之前，不能强行剥夺。</p>
<p>4、循环等待条件：若干进程之间形成一种头尾相接的循环等待资源关系。</p>
<p>这四个条件是死锁的必要条件，只要系统发生死锁，这些条件必然成立，而只要上述条件之 一不满足，就不会发生死锁。</p>
<p>理解了死锁的原因，尤其是产生死锁的四个必要条件，就可以最大可能地避免、预防和解除死锁。</p>
<p>防止死锁可以采用以下的方法：</p>
<ul>
<li>尽量使用 tryLock(long timeout, TimeUnit unit)的方法(ReentrantLock、ReentrantReadWriteLock)，设置超时时间，超时可以退出防止死锁。</li>
<li>尽量使用 Java. util. concurrent 并发类代替自己手写锁。</li>
<li>尽量降低锁的使用粒度，尽量不要几个功能用同一把锁。</li>
<li>尽量减少同步的代码块。</li>
</ul>
<h4 id="死锁与活锁的区别，死锁与饥饿的区别？"><a href="#死锁与活锁的区别，死锁与饥饿的区别？" class="headerlink" title="死锁与活锁的区别，死锁与饥饿的区别？"></a>死锁与活锁的区别，死锁与饥饿的区别？</h4><p>死锁：是指两个或两个以上的进程（或线程）在执行过程中，因争夺资源而造成的一种互相等待的现象，若无外力作用，它们都将无法推进下去。</p>
<p>活锁：任务或者执行者没有被阻塞，由于某些条件没有满足，导致一直重复尝试，失败，尝试，失败。</p>
<p>活锁和死锁的区别在于，处于活锁的实体是在不断的改变状态，这就是所谓的“活”， 而处于死锁的实体表现为等待；活锁有可能自行解开，死锁则不能。</p>
<p>饥饿：一个或者多个线程因为种种原因无法获得所需要的资源，导致一直无法执行的状态。</p>
<p>Java 中导致饥饿的原因：</p>
<p>1、高优先级线程吞噬所有的低优先级线程的 CPU 时间。</p>
<p>2、线程被永久堵塞在一个等待进入同步块的状态，因为其他线程总是能在它之前持续地对该同步块进行访问。</p>
<p>3、线程在等待一个本身也处于永久等待完成的对象(比如调用这个对象的 wait 方法)，因为其他线程总是被持续地获得唤醒。</p>
<h4 id="多线程锁的升级原理是什么？"><a href="#多线程锁的升级原理是什么？" class="headerlink" title="多线程锁的升级原理是什么？"></a>多线程锁的升级原理是什么？</h4><p>在Java中，锁共有4种状态，级别从低到高依次为：无状态锁，偏向锁，轻量级锁和重量级锁状态，这几个状态会随着竞争情况逐渐升级。锁可以升级但不能降级。</p>
<h3 id="AQS-AbstractQueuedSynchronizer-详解与源码分析"><a href="#AQS-AbstractQueuedSynchronizer-详解与源码分析" class="headerlink" title="AQS(AbstractQueuedSynchronizer)详解与源码分析"></a>AQS(AbstractQueuedSynchronizer)详解与源码分析</h3><h4 id="AQS-介绍"><a href="#AQS-介绍" class="headerlink" title="AQS 介绍"></a>AQS 介绍</h4><p>AQS是一个用来构建锁和同步器的框架，使用AQS能简单且高效地构造出应用广泛的大量的同步器，比如我们提到的ReentrantLock，Semaphore，其他的诸如ReentrantReadWriteLock，SynchronousQueue，FutureTask等等皆是基于AQS的。当然，我们自己也能利用AQS非常轻松容易地构造出符合我们自己需求的同步器。</p>
<h4 id="AQS-原理分析"><a href="#AQS-原理分析" class="headerlink" title="AQS 原理分析"></a>AQS 原理分析</h4><p>AQS 原理概览</p>
<p>AQS核心思想是，如果被请求的共享资源空闲，则将当前请求资源的线程设置为有效的工作线程，并且将共享资源设置为锁定状态。如果被请求的共享资源被占用，那么就需要一套线程阻塞等待以及被唤醒时锁分配的机制，这个机制AQS是用CLH队列锁实现的，即将暂时获取不到锁的线程加入到队列中。</p>
<p><img src="https://lixiangbetter.github.io/2020/06/21/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E7%AC%94%E8%AE%B0/aHR0cHM6Ly91c2VyLWdvbGQtY2RuLnhpdHUuaW8vMjAxOS8xMS8yNS8xNmVhMDQ3Njc4NGNkMzJi.jpeg" alt></p>
<p><strong>AQS 对资源的共享方式</strong></p>
<p>AQS定义两种资源共享方式</p>
<ul>
<li>Exclusive（独占）：只有一个线程能执行，如ReentrantLock。又可分为公平锁和非公平锁：<ul>
<li>公平锁：按照线程在队列中的排队顺序，先到者先拿到锁</li>
<li>非公平锁：当线程要获取锁时，无视队列顺序直接去抢锁，谁抢到就是谁的</li>
</ul>
</li>
<li>Share（共享）：多个线程可同时执行，如Semaphore/CountDownLatch。Semaphore、CountDownLatch、 CyclicBarrier、ReadWriteLock 我们都会在后面讲到。</li>
</ul>
<h3 id="ReentrantLock-重入锁-实现原理与公平锁非公平锁区别"><a href="#ReentrantLock-重入锁-实现原理与公平锁非公平锁区别" class="headerlink" title="ReentrantLock(重入锁)实现原理与公平锁非公平锁区别"></a>ReentrantLock(重入锁)实现原理与公平锁非公平锁区别</h3><h4 id="什么是可重入锁（ReentrantLock）？"><a href="#什么是可重入锁（ReentrantLock）？" class="headerlink" title="什么是可重入锁（ReentrantLock）？"></a>什么是可重入锁（ReentrantLock）？</h4><p>ReentrantLock重入锁，是实现Lock接口的一个类，也是在实际编程中使用频率很高的一个锁，支持重入性，表示能够对共享资源能够重复加锁，即当前线程获取该锁再次获取不会被阻塞。</p>
<p>ReentrantLock支持两种锁：<strong>公平锁</strong>和<strong>非公平锁</strong>。<strong>何谓公平性，是针对获取锁而言的，如果一个锁是公平的，那么锁的获取顺序就应该符合请求上的绝对时间顺序，满足FIFO</strong>。</p>
<h3 id="读写锁ReentrantReadWriteLock源码分析"><a href="#读写锁ReentrantReadWriteLock源码分析" class="headerlink" title="读写锁ReentrantReadWriteLock源码分析"></a>读写锁ReentrantReadWriteLock源码分析</h3><h4 id="ReadWriteLock-是什么"><a href="#ReadWriteLock-是什么" class="headerlink" title="ReadWriteLock 是什么"></a>ReadWriteLock 是什么</h4><p>ReentrantReadWriteLock 是 ReadWriteLock 接口的一个具体实现，实现了读写的分离，读锁是共享的，写锁是独占的，读和读之间不会互斥，读和写、写和读、写和写之间才会互斥，提升了读写的性能。</p>
<p>而读写锁有以下三个重要的特性：</p>
<p>（1）公平选择性：支持非公平（默认）和公平的锁获取方式，吞吐量还是非公平优于公平。</p>
<p>（2）重进入：读锁和写锁都支持线程重进入。</p>
<p>（3）锁降级：遵循获取写锁、获取读锁再释放写锁的次序，写锁能够降级成为读锁。</p>
<h2 id="并发容器"><a href="#并发容器" class="headerlink" title="并发容器"></a>并发容器</h2><h3 id="并发容器之ConcurrentHashMap详解-JDK1-8版本-与源码分析"><a href="#并发容器之ConcurrentHashMap详解-JDK1-8版本-与源码分析" class="headerlink" title="并发容器之ConcurrentHashMap详解(JDK1.8版本)与源码分析"></a>并发容器之ConcurrentHashMap详解(JDK1.8版本)与源码分析</h3><h4 id="什么是ConcurrentHashMap？"><a href="#什么是ConcurrentHashMap？" class="headerlink" title="什么是ConcurrentHashMap？"></a>什么是ConcurrentHashMap？</h4><p>JDK 1.6版本关键要素：</p>
<p>segment继承了ReentrantLock充当锁的角色，为每一个segment提供了线程安全的保障；</p>
<p>segment维护了哈希散列表的若干个桶，每个桶由HashEntry构成的链表。</p>
<p>JDK1.8后，ConcurrentHashMap抛弃了原有的Segment 分段锁，而采用了 CAS + synchronized 来保证并发安全性。</p>
<h4 id="Java-中-ConcurrentHashMap-的并发度是什么？"><a href="#Java-中-ConcurrentHashMap-的并发度是什么？" class="headerlink" title="Java 中 ConcurrentHashMap 的并发度是什么？"></a>Java 中 ConcurrentHashMap 的并发度是什么？</h4><p>ConcurrentHashMap 把实际 map 划分成若干部分来实现它的可扩展性和线程安全。这种划分是使用并发度获得的，它是 ConcurrentHashMap 类构造函数的一个可选参数，默认值为 16，这样在多线程情况下就能避免争用。</p>
<h4 id="什么是并发容器的实现？"><a href="#什么是并发容器的实现？" class="headerlink" title="什么是并发容器的实现？"></a>什么是并发容器的实现？</h4><p>并发容器使用了与同步容器完全不同的加锁策略来提供更高的并发性和伸缩性，例如在 ConcurrentHashMap 中采用了一种粒度更细的加锁机制，可以称为分段锁，在这种锁机制下，允许任意数量的读线程并发地访问 map，并且执行读操作的线程和写操作的线程也可以并发的访问 map，同时允许一定数量的写操作线程并发地修改 map，所以它可以在并发环境下实现更高的吞吐量。</p>
<h4 id="Java-中的同步集合与并发集合有什么区别？"><a href="#Java-中的同步集合与并发集合有什么区别？" class="headerlink" title="Java 中的同步集合与并发集合有什么区别？"></a>Java 中的同步集合与并发集合有什么区别？</h4><p>同步集合与并发集合都为多线程和并发提供了合适的线程安全的集合，不过并发集合的可扩展性更高。在 Java1.5 之前程序员们只有同步集合来用且在多线程并发的时候会导致争用，阻碍了系统的扩展性。Java5 介绍了并发集合像ConcurrentHashMap，不仅提供线程安全还用锁分离和内部分区等现代技术提高了可扩展性。</p>
<h4 id="SynchronizedMap-和-ConcurrentHashMap-有什么区别？"><a href="#SynchronizedMap-和-ConcurrentHashMap-有什么区别？" class="headerlink" title="SynchronizedMap 和 ConcurrentHashMap 有什么区别？"></a>SynchronizedMap 和 ConcurrentHashMap 有什么区别？</h4><p>SynchronizedMap 一次锁住整张表来保证线程安全，所以每次只能有一个线程来访为 map。</p>
<p>ConcurrentHashMap 使用分段锁来保证在多线程下的性能。</p>
<h3 id="并发容器之CopyOnWriteArrayList详解"><a href="#并发容器之CopyOnWriteArrayList详解" class="headerlink" title="并发容器之CopyOnWriteArrayList详解"></a>并发容器之CopyOnWriteArrayList详解</h3><h4 id="CopyOnWriteArrayList-是什么，可以用于什么应用场景？有哪些优缺点？"><a href="#CopyOnWriteArrayList-是什么，可以用于什么应用场景？有哪些优缺点？" class="headerlink" title="CopyOnWriteArrayList 是什么，可以用于什么应用场景？有哪些优缺点？"></a>CopyOnWriteArrayList 是什么，可以用于什么应用场景？有哪些优缺点？</h4><p>CopyOnWriteArrayList 的使用场景</p>
<p>通过源码分析，我们看出它的优缺点比较明显，所以使用场景也就比较明显。就是合适读多写少的场景。</p>
<h3 id="并发容器之ThreadLocal详解"><a href="#并发容器之ThreadLocal详解" class="headerlink" title="并发容器之ThreadLocal详解"></a>并发容器之ThreadLocal详解</h3><h4 id="ThreadLocal-是什么？有哪些使用场景？"><a href="#ThreadLocal-是什么？有哪些使用场景？" class="headerlink" title="ThreadLocal 是什么？有哪些使用场景？"></a>ThreadLocal 是什么？有哪些使用场景？</h4><p>ThreadLocal 是一个本地线程副本变量工具类，在每个线程中都创建了一个 ThreadLocalMap 对象，简单说 ThreadLocal 就是一种以空间换时间的做法，每个线程可以访问自己内部 ThreadLocalMap 对象内的 value。通过这种方式，避免资源在多线程间共享。</p>
<h4 id="什么是线程局部变量？"><a href="#什么是线程局部变量？" class="headerlink" title="什么是线程局部变量？"></a>什么是线程局部变量？</h4><p>线程局部变量是局限于线程内部的变量，属于线程自身所有，不在多个线程间共享。</p>
<h3 id="ThreadLocal内存泄漏分析与解决方案"><a href="#ThreadLocal内存泄漏分析与解决方案" class="headerlink" title="ThreadLocal内存泄漏分析与解决方案"></a>ThreadLocal内存泄漏分析与解决方案</h3><h4 id="ThreadLocal造成内存泄漏的原因？"><a href="#ThreadLocal造成内存泄漏的原因？" class="headerlink" title="ThreadLocal造成内存泄漏的原因？"></a>ThreadLocal造成内存泄漏的原因？</h4><p>ThreadLocalMap实现中已经考虑了这种情况，在调用 <code>set()</code>、<code>get()</code>、<code>remove()</code> 方法的时候，会清理掉 key 为 null 的记录。使用完 <code>ThreadLocal</code>方法后 最好手动调用<code>remove()</code>方法</p>
<h4 id="ThreadLocal内存泄漏解决方案？"><a href="#ThreadLocal内存泄漏解决方案？" class="headerlink" title="ThreadLocal内存泄漏解决方案？"></a>ThreadLocal内存泄漏解决方案？</h4><ul>
<li>每次使用完ThreadLocal，都调用它的remove()方法，清除数据。</li>
<li>在使用线程池的情况下，没有及时清理ThreadLocal，不仅是内存泄漏的问题，更严重的是可能导致业务逻辑出现问题。所以，使用ThreadLocal就跟加锁完要解锁一样，用完就清理。</li>
</ul>
<h3 id="并发容器之BlockingQueue详解"><a href="#并发容器之BlockingQueue详解" class="headerlink" title="并发容器之BlockingQueue详解"></a>并发容器之BlockingQueue详解</h3><h4 id="什么是阻塞队列？阻塞队列的实现原理是什么？如何使用阻塞队列来实现生产者-消费者模型？"><a href="#什么是阻塞队列？阻塞队列的实现原理是什么？如何使用阻塞队列来实现生产者-消费者模型？" class="headerlink" title="什么是阻塞队列？阻塞队列的实现原理是什么？如何使用阻塞队列来实现生产者-消费者模型？"></a>什么是阻塞队列？阻塞队列的实现原理是什么？如何使用阻塞队列来实现生产者-消费者模型？</h4><p>阻塞队列（BlockingQueue）是一个支持两个附加操作的队列。</p>
<p>这两个附加的操作是：在队列为空时，获取元素的线程会等待队列变为非空。当队列满时，存储元素的线程会等待队列可用。</p>
<p>阻塞队列常用于生产者和消费者的场景，生产者是往队列里添加元素的线程，消费者是从队列里拿元素的线程。阻塞队列就是生产者存放元素的容器，而消费者也只从容器里拿元素。</p>
<p>阻塞队列使用最经典的场景就是 socket 客户端数据的读取和解析，读取数据的线程不断将数据放入队列，然后解析线程不断从队列取数据解析。</p>
<h3 id="并发容器之ConcurrentLinkedQueue详解与源码分析"><a href="#并发容器之ConcurrentLinkedQueue详解与源码分析" class="headerlink" title="并发容器之ConcurrentLinkedQueue详解与源码分析"></a>并发容器之ConcurrentLinkedQueue详解与源码分析</h3><h3 id="并发容器之ArrayBlockingQueue与LinkedBlockingQueue详解"><a href="#并发容器之ArrayBlockingQueue与LinkedBlockingQueue详解" class="headerlink" title="并发容器之ArrayBlockingQueue与LinkedBlockingQueue详解"></a>并发容器之ArrayBlockingQueue与LinkedBlockingQueue详解</h3><h2 id="线程池"><a href="#线程池" class="headerlink" title="线程池"></a>线程池</h2><h3 id="Executors类创建四种常见线程池"><a href="#Executors类创建四种常见线程池" class="headerlink" title="Executors类创建四种常见线程池"></a>Executors类创建四种常见线程池</h3><h4 id="什么是线程池？有哪几种创建方式？"><a href="#什么是线程池？有哪几种创建方式？" class="headerlink" title="什么是线程池？有哪几种创建方式？"></a>什么是线程池？有哪几种创建方式？</h4><p>池化技术相比大家已经屡见不鲜了，线程池、数据库连接池、Http 连接池等等都是对这个思想的应用。池化技术的思想主要是为了减少每次获取资源的消耗，提高对资源的利用率。</p>
<p>（1）newSingleThreadExecutor</p>
<p>（2）newFixedThreadPool</p>
<p>（3） newCachedThreadPool</p>
<p>（4）newScheduledThreadPoo</p>
<h4 id="线程池有什么优点？"><a href="#线程池有什么优点？" class="headerlink" title="线程池有什么优点？"></a>线程池有什么优点？</h4><ul>
<li><p>降低资源消耗：重用存在的线程，减少对象创建销毁的开销。</p>
</li>
<li><p>提高响应速度。可有效的控制最大并发线程数，提高系统资源的使用率，同时避免过多资源竞争，避免堵塞。当任务到达时，任务可以不需要的等到线程创建就能立即执行。</p>
</li>
<li><p>提高线程的可管理性。线程是稀缺资源，如果无限制的创建，不仅会消耗系统资源，还会降低系统的稳定性，使用线程池可以进行统一的分配，调优和监控。</p>
</li>
<li><p>附加功能：提供定时执行、定期执行、单线程、并发数控制等功能。</p>
</li>
</ul>
<h4 id="线程池都有哪些状态？"><a href="#线程池都有哪些状态？" class="headerlink" title="线程池都有哪些状态？"></a>线程池都有哪些状态？</h4><ul>
<li>RUNNING：这是最正常的状态，接受新的任务，处理等待队列中的任务。</li>
<li>SHUTDOWN：不接受新的任务提交，但是会继续处理等待队列中的任务。</li>
<li>STOP：不接受新的任务提交，不再处理等待队列中的任务，中断正在执行任务的线程。</li>
<li>TIDYING：所有的任务都销毁了，workCount 为 0，线程池的状态在转换为 TIDYING 状态时，会执行钩子方法 terminated()。</li>
<li>TERMINATED：terminated()方法结束后，线程池的状态就会变成这个。</li>
</ul>
<h4 id="什么是-Executor-框架？为什么使用-Executor-框架？"><a href="#什么是-Executor-框架？为什么使用-Executor-框架？" class="headerlink" title="什么是 Executor 框架？为什么使用 Executor 框架？"></a>什么是 Executor 框架？为什么使用 Executor 框架？</h4><p>Executor 框架是一个根据一组执行策略调用，调度，执行和控制的异步任务的框架。</p>
<p>每次执行任务创建线程 new Thread()比较消耗性能，创建一个线程是比较耗时、耗资源的，而且无限制的创建线程会引起应用程序内存溢出。</p>
<p>所以创建一个线程池是个更好的的解决方案，因为可以限制线程的数量并且可以回收再利用这些线程。利用Executors 框架可以非常方便的创建一个线程池。</p>
<h4 id="在-Java-中-Executor-和-Executors-的区别？"><a href="#在-Java-中-Executor-和-Executors-的区别？" class="headerlink" title="在 Java 中 Executor 和 Executors 的区别？"></a>在 Java 中 Executor 和 Executors 的区别？</h4><ul>
<li><p>Executors 工具类的不同方法按照我们的需求创建了不同的线程池，来满足业务的需求。</p>
</li>
<li><p>Executor 接口对象能执行我们的线程任务。</p>
</li>
<li><p>ExecutorService 接口继承了 Executor 接口并进行了扩展，提供了更多的方法我们能获得任务执行的状态并且可以获取任务的返回值。</p>
</li>
<li><p>使用 ThreadPoolExecutor 可以创建自定义线程池。</p>
</li>
<li><p>Future 表示异步计算的结果，他提供了检查计算是否完成的方法，以等待计算的完成，并可以使用 get()方法获取计算的结果。</p>
</li>
</ul>
<h4 id="线程池中-submit-和-execute-方法有什么区别？"><a href="#线程池中-submit-和-execute-方法有什么区别？" class="headerlink" title="线程池中 submit() 和 execute() 方法有什么区别？"></a>线程池中 submit() 和 execute() 方法有什么区别？</h4><p>接收参数：execute()只能执行 Runnable 类型的任务。submit()可以执行 Runnable 和 Callable 类型的任务。</p>
<p>返回值：submit()方法可以返回持有计算结果的 Future 对象，而execute()没有</p>
<p>异常处理：submit()方便Exception处理</p>
<h4 id="什么是线程组，为什么在-Java-中不推荐使用？"><a href="#什么是线程组，为什么在-Java-中不推荐使用？" class="headerlink" title="什么是线程组，为什么在 Java 中不推荐使用？"></a>什么是线程组，为什么在 Java 中不推荐使用？</h4><p>ThreadGroup 类，可以把线程归属到某一个线程组中，线程组中可以有线程对象，也可以有线程组，组中还可以有线程，这样的组织结构有点类似于树的形式。</p>
<p>线程组和线程池是两个不同的概念，他们的作用完全不同，前者是为了方便线程的管理，后者是为了管理线程的生命周期，复用线程，减少创建销毁线程的开销。</p>
<p>为什么不推荐使用线程组？因为使用有很多的安全隐患吧，没有具体追究，如果需要使用，推荐使用线程池。</p>
<h3 id="线程池之ThreadPoolExecutor详解"><a href="#线程池之ThreadPoolExecutor详解" class="headerlink" title="线程池之ThreadPoolExecutor详解"></a>线程池之ThreadPoolExecutor详解</h3><h4 id="Executors和ThreaPoolExecutor创建线程池的区别"><a href="#Executors和ThreaPoolExecutor创建线程池的区别" class="headerlink" title="Executors和ThreaPoolExecutor创建线程池的区别"></a>Executors和ThreaPoolExecutor创建线程池的区别</h4><p>Executors 各个方法的弊端：</p>
<ul>
<li>newFixedThreadPool 和 newSingleThreadExecutor:<br>主要问题是堆积的请求处理队列可能会耗费非常大的内存，甚至 OOM。</li>
<li>newCachedThreadPool 和 newScheduledThreadPool:<br>主要问题是线程数最大数是 Integer.MAX_VALUE，可能会创建数量非常多的线程，甚至 OOM。</li>
</ul>
<h4 id="你知道怎么创建线程池吗？"><a href="#你知道怎么创建线程池吗？" class="headerlink" title="你知道怎么创建线程池吗？"></a>你知道怎么创建线程池吗？</h4><p>创建线程池的方式有多种，这里你只需要答 ThreadPoolExecutor 即可。</p>
<p>ThreadPoolExecutor() 是最原始的线程池创建，也是阿里巴巴 Java 开发手册中明确规范的创建线程池的方式。</p>
<h4 id="ThreadPoolExecutor构造函数重要参数分析"><a href="#ThreadPoolExecutor构造函数重要参数分析" class="headerlink" title="ThreadPoolExecutor构造函数重要参数分析"></a>ThreadPoolExecutor构造函数重要参数分析</h4><p>ThreadPoolExecutor 3 个最重要的参数：</p>
<ul>
<li>corePoolSize ：核心线程数，线程数定义了最小可以同时运行的线程数量。</li>
<li>maximumPoolSize ：线程池中允许存在的工作线程的最大数量</li>
<li>workQueue：当新任务来的时候会先判断当前运行的线程数量是否达到核心线程数，如果达到的话，任务就会被存放在队列中。</li>
</ul>
<p>ThreadPoolExecutor其他常见参数:</p>
<ul>
<li>keepAliveTime：线程池中的线程数量大于 corePoolSize 的时候，如果这时没有新的任务提交，核心线程外的线程不会立即销毁，而是会等待，直到等待的时间超过了 keepAliveTime才会被回收销毁；</li>
<li>unit ：keepAliveTime 参数的时间单位。</li>
<li>threadFactory：为线程池提供创建新线程的线程工厂</li>
<li>handler ：线程池任务队列超过 maxinumPoolSize 之后的拒绝策略</li>
</ul>
<h4 id="ThreadPoolExecutor饱和策略"><a href="#ThreadPoolExecutor饱和策略" class="headerlink" title="ThreadPoolExecutor饱和策略"></a>ThreadPoolExecutor饱和策略</h4><p>线程池实现原理</p>
<p><img src="https://lixiangbetter.github.io/2020/06/21/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E7%AC%94%E8%AE%B0/aHR0cHM6Ly91c2VyLWdvbGQtY2RuLnhpdHUuaW8vMjAxOS8xMS8yNS8xNmVhMDQ3NjEyOTVlNzY2.jpeg" alt></p>
<h3 id="线程池之ScheduledThreadPoolExecutor详解"><a href="#线程池之ScheduledThreadPoolExecutor详解" class="headerlink" title="线程池之ScheduledThreadPoolExecutor详解"></a>线程池之ScheduledThreadPoolExecutor详解</h3><h3 id="FutureTask详解"><a href="#FutureTask详解" class="headerlink" title="FutureTask详解"></a>FutureTask详解</h3><h2 id="原子操作类"><a href="#原子操作类" class="headerlink" title="原子操作类"></a>原子操作类</h2><h4 id="什么是原子操作？在-Java-Concurrency-API-中有哪些原子类-atomic-classes-？"><a href="#什么是原子操作？在-Java-Concurrency-API-中有哪些原子类-atomic-classes-？" class="headerlink" title="什么是原子操作？在 Java Concurrency API 中有哪些原子类(atomic classes)？"></a>什么是原子操作？在 Java Concurrency API 中有哪些原子类(atomic classes)？</h4><p>原子操作（atomic operation）意为”不可被中断的一个或一系列操作” 。</p>
<p>在 Java 中可以通过锁和循环 CAS 的方式来实现原子操作。 </p>
<p>原子类：AtomicBoolean，AtomicInteger，AtomicLong，AtomicReference</p>
<p>原子数组：AtomicIntegerArray，AtomicLongArray，AtomicReferenceArray</p>
<p>原子属性更新器：AtomicLongFieldUpdater，AtomicIntegerFieldUpdater，AtomicReferenceFieldUpdater</p>
<p>解决 ABA 问题的原子类：AtomicMarkableReference（通过引入一个 boolean来反映中间有没有变过），AtomicStampedReference（通过引入一个 int 来累加来反映中间有没有变过）</p>
<h4 id="说一下-atomic-的原理？"><a href="#说一下-atomic-的原理？" class="headerlink" title="说一下 atomic 的原理？"></a>说一下 atomic 的原理？</h4><p>Atomic包中的类基本的特性就是在多线程环境下，当有多个线程同时对单个（包括基本类型及引用类型）变量进行操作时，具有排他性，即当多个线程同时对该变量的值进行更新时，仅有一个线程能成功，而未成功的线程可以向自旋锁一样，继续尝试，一直等到执行成功。</p>
<h2 id="并发工具"><a href="#并发工具" class="headerlink" title="并发工具"></a>并发工具</h2><h3 id="并发工具之CountDownLatch与CyclicBarrier"><a href="#并发工具之CountDownLatch与CyclicBarrier" class="headerlink" title="并发工具之CountDownLatch与CyclicBarrier"></a>并发工具之CountDownLatch与CyclicBarrier</h3><h4 id="在-Java-中-CycliBarriar-和-CountdownLatch-有什么区别？"><a href="#在-Java-中-CycliBarriar-和-CountdownLatch-有什么区别？" class="headerlink" title="在 Java 中 CycliBarriar 和 CountdownLatch 有什么区别？"></a>在 Java 中 CycliBarriar 和 CountdownLatch 有什么区别？</h4><p>CountDownLatch与CyclicBarrier都是用于控制并发的工具类，都可以理解成维护的就是一个计数器，但是这两者还是各有不同侧重点的：</p>
<ul>
<li><p>CountDownLatch一般用于某个线程A等待若干个其他线程执行完任务之后，它才执行；而CyclicBarrier一般用于一组线程互相等待至某个状态，然后这一组线程再同时执行；CountDownLatch强调一个线程等多个线程完成某件事情。CyclicBarrier是多个线程互等，等大家都完成，再携手共进。</p>
</li>
<li><p>调用CountDownLatch的countDown方法后，当前线程并不会阻塞，会继续往下执行；而调用CyclicBarrier的await方法，会阻塞当前线程，直到CyclicBarrier指定的线程全部都到达了指定点的时候，才能继续往下执行；</p>
</li>
<li><p>CountDownLatch方法比较少，操作比较简单，而CyclicBarrier提供的方法更多，比如能够通过getNumberWaiting()，isBroken()这些方法获取当前多个线程的状态，并且CyclicBarrier的构造方法可以传入barrierAction，指定当所有线程都到达时执行的业务功能；</p>
</li>
<li><p>CountDownLatch是不能复用的，而CyclicLatch是可以复用的。</p>
</li>
</ul>
<h3 id="并发工具之Semaphore与Exchanger"><a href="#并发工具之Semaphore与Exchanger" class="headerlink" title="并发工具之Semaphore与Exchanger"></a>并发工具之Semaphore与Exchanger</h3><h4 id="Semaphore-有什么作用"><a href="#Semaphore-有什么作用" class="headerlink" title="Semaphore 有什么作用"></a>Semaphore 有什么作用</h4><p>Semaphore 就是一个信号量，它的作用是限制某段代码块的并发数。Semaphore有一个构造函数，可以传入一个 int 型整数 n，表示某段代码最多只有 n 个线程可以访问，如果超出了 n，那么请等待，等到某个线程执行完毕这段代码块，下一个线程再进入。由此可以看出如果 Semaphore 构造函数中传入的 int 型整数 n=1，相当于变成了一个 synchronized 了。</p>
<p>Semaphore(信号量)-允许多个线程同时访问： synchronized 和 ReentrantLock 都是一次只允许一个线程访问某个资源，Semaphore(信号量)可以指定多个线程同时访问某个资源。</p>
<h4 id="什么是线程间交换数据的工具Exchanger"><a href="#什么是线程间交换数据的工具Exchanger" class="headerlink" title="什么是线程间交换数据的工具Exchanger"></a>什么是线程间交换数据的工具Exchanger</h4><p>Exchanger是一个用于线程间协作的工具类，用于两个线程间交换数据。它提供了一个交换的同步点，在这个同步点两个线程能够交换数据。交换数据是通过exchange方法来实现的，如果一个线程先执行exchange方法，那么它会同步等待另一个线程也执行exchange方法，这个时候两个线程就都达到了同步点，两个线程就可以交换数据。</p>
<h4 id="常用的并发工具类有哪些？"><a href="#常用的并发工具类有哪些？" class="headerlink" title="常用的并发工具类有哪些？"></a>常用的并发工具类有哪些？</h4><ul>
<li>Semaphore(信号量)-允许多个线程同时访问： synchronized 和 ReentrantLock 都是一次只允许一个线程访问某个资源，Semaphore(信号量)可以指定多个线程同时访问某个资源。</li>
<li>CountDownLatch(倒计时器)： CountDownLatch是一个同步工具类，用来协调多个线程之间的同步。这个工具通常用来控制线程等待，它可以让某一个线程等待直到倒计时结束，再开始执行。</li>
<li>CyclicBarrier(循环栅栏)： CyclicBarrier 和 CountDownLatch 非常类似，它也可以实现线程间的技术等待，但是它的功能比 CountDownLatch 更加复杂和强大。主要应用场景和 CountDownLatch 类似。CyclicBarrier 的字面意思是可循环使用（Cyclic）的屏障（Barrier）。它要做的事情是，让一组线程到达一个屏障（也可以叫同步点）时被阻塞，直到最后一个线程到达屏障时，屏障才会开门，所有被屏障拦截的线程才会继续干活。CyclicBarrier默认的构造方法是 CyclicBarrier(int parties)，其参数表示屏障拦截的线程数量，每个线程调用await()方法告诉 CyclicBarrier 我已经到达了屏障，然后当前线程被阻塞。</li>
</ul>
]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>juc</tag>
      </tags>
  </entry>
  <entry>
    <title>Java异常笔记</title>
    <url>/2020/06/20/Java%E5%BC%82%E5%B8%B8%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<h1 id="Java异常笔记"><a href="#Java异常笔记" class="headerlink" title="Java异常笔记"></a>Java异常笔记</h1><h2 id="Java异常架构与异常关键字"><a href="#Java异常架构与异常关键字" class="headerlink" title="Java异常架构与异常关键字"></a>Java异常架构与异常关键字</h2><h3 id="Java异常简介"><a href="#Java异常简介" class="headerlink" title="Java异常简介"></a>Java异常简介</h3><p>Java异常是Java提供的一种识别及响应错误的一致性机制。<br>Java异常机制可以使程序中异常处理代码和正常业务代码分离，保证程序代码更加优雅，并提高程序健壮性。在有效使用异常的情况下，异常能清晰的回答what, where, why这3个问题：异常类型回答了“什么”被抛出，异常堆栈跟踪回答了“在哪”抛出，异常信息回答了“为什么”会抛出。</p>
<h3 id="Java异常架构"><a href="#Java异常架构" class="headerlink" title="Java异常架构"></a>Java异常架构</h3><p><img src="https://lixiangbetter.github.io/2020/06/20/Java%E5%BC%82%E5%B8%B8%E7%AC%94%E8%AE%B0/20200314173417278.png" alt></p>
<h4 id="1-Throwable"><a href="#1-Throwable" class="headerlink" title="1. Throwable"></a>1. Throwable</h4><p>Throwable 是 Java 语言中所有错误与异常的超类。</p>
<p>Throwable 包含两个子类：Error（错误）和 Exception（异常），它们通常用于指示发生了异常情况。</p>
<p>Throwable 包含了其线程创建时线程执行堆栈的快照，它提供了 printStackTrace() 等接口用于获取堆栈跟踪数据等信息。</p>
<h4 id="2-Error（错误）"><a href="#2-Error（错误）" class="headerlink" title="2. Error（错误）"></a>2. Error（错误）</h4><p>定义：Error 类及其子类。程序中无法处理的错误，表示运行应用程序中出现了严重的错误。</p>
<p>特点：此类错误一般表示代码运行时 JVM 出现问题。通常有 Virtual MachineError（虚拟机运行错误）、NoClassDefFoundError（类定义错误）等。比如 OutOfMemoryError：内存不足错误；StackOverflowError：栈溢出错误。此类错误发生时，JVM 将终止线程。</p>
<p>这些错误是不受检异常，非代码性错误。因此，当此类错误发生时，应用程序不应该去处理此类错误。按照Java惯例，我们是不应该实现任何新的Error子类的！</p>
<h4 id="3-Exception（异常）"><a href="#3-Exception（异常）" class="headerlink" title="3. Exception（异常）"></a>3. Exception（异常）</h4><p>程序本身可以捕获并且可以处理的异常。Exception 这种异常又分为两类：运行时异常和编译时异常。</p>
<p>运行时异常<br>定义：RuntimeException 类及其子类，表示 JVM 在运行期间可能出现的异常。</p>
<p>特点：Java 编译器不会检查它。也就是说，当程序中可能出现这类异常时，倘若既”没有通过throws声明抛出它”，也”没有用try-catch语句捕获它”，还是会编译通过。比如NullPointerException空指针异常、ArrayIndexOutBoundException数组下标越界异常、ClassCastException类型转换异常、ArithmeticExecption算术异常。此类异常属于不受检异常，一般是由程序逻辑错误引起的，在程序中可以选择捕获处理，也可以不处理。虽然 Java 编译器不会检查运行时异常，但是我们也可以通过 throws 进行声明抛出，也可以通过 try-catch 对它进行捕获处理。如果产生运行时异常，则需要通过修改代码来进行避免。例如，若会发生除数为零的情况，则需要通过代码避免该情况的发生！</p>
<p>RuntimeException 异常会由 Java 虚拟机自动抛出并自动捕获（就算我们没写异常捕获语句运行时也会抛出错误！！），此类异常的出现绝大数情况是代码本身有问题应该从逻辑上去解决并改进代码。</p>
<h5 id="编译时异常"><a href="#编译时异常" class="headerlink" title="编译时异常"></a>编译时异常</h5><p>定义: Exception 中除 RuntimeException 及其子类之外的异常。</p>
<p>特点: Java 编译器会检查它。如果程序中出现此类异常，比如 ClassNotFoundException（没有找到指定的类异常），IOException（IO流异常），要么通过throws进行声明抛出，要么通过try-catch进行捕获处理，否则不能通过编译。在程序中，通常不会自定义该类异常，而是直接使用系统提供的异常类。该异常我们必须手动在代码里添加捕获语句来处理该异常。</p>
<h4 id="4-受检异常与非受检异常"><a href="#4-受检异常与非受检异常" class="headerlink" title="4. 受检异常与非受检异常"></a>4. 受检异常与非受检异常</h4><p>Java 的所有异常可以分为受检异常（checked exception）和非受检异常（unchecked exception）。</p>
<p>受检异常<br>编译器要求必须处理的异常。正确的程序在运行过程中，经常容易出现的、符合预期的异常情况。一旦发生此类异常，就必须采用某种方式进行处理。除 RuntimeException 及其子类外，其他的 Exception 异常都属于受检异常。编译器会检查此类异常，也就是说当编译器检查到应用中的某处可能会此类异常时，将会提示你处理本异常——要么使用try-catch捕获，要么使用方法签名中用 throws 关键字抛出，否则编译不通过。</p>
<p>非受检异常<br>编译器不会进行检查并且不要求必须处理的异常，也就说当程序中出现此类异常时，即使我们没有try-catch捕获它，也没有使用throws抛出该异常，编译也会正常通过。该类异常包括运行时异常（RuntimeException极其子类）和错误（Error）。</p>
<h3 id="Java异常关键字"><a href="#Java异常关键字" class="headerlink" title="Java异常关键字"></a>Java异常关键字</h3><ul>
<li>try – 用于监听。将要被监听的代码(可能抛出异常的代码)放在try语句块之内，当try语句块内发生异常时，异常就被抛出。</li>
<li>catch – 用于捕获异常。catch用来捕获try语句块中发生的异常。</li>
<li>finally – finally语句块总是会被执行。它主要用于回收在try块里打开的物力资源(如数据库连接、网络连接和磁盘文件)。只有finally块，执行完成之后，才会回来执行try或者catch块中的return或者throw语句，如果finally中使用了return或者throw等终止方法的语句，则就不会跳回执行，直接停止。</li>
<li>throw – 用于抛出异常。</li>
<li>throws – 用在方法签名中，用于声明该方法可能抛出的异常。</li>
</ul>
<h2 id="Java异常处理"><a href="#Java异常处理" class="headerlink" title="Java异常处理"></a>Java异常处理</h2><p><img src="https://lixiangbetter.github.io/2020/06/20/Java%E5%BC%82%E5%B8%B8%E7%AC%94%E8%AE%B0/aHR0cHM6Ly91c2VyLWdvbGQtY2RuLnhpdHUuaW8vMjAxOS8xMS8xMC8xNmU1NWYyYzMyMWQ5MDlk.jpeg" alt></p>
<p>Java 通过面向对象的方法进行异常处理，一旦方法抛出异常，系统自动根据该异常对象寻找合适异常处理器（Exception Handler）来处理该异常，把各种不同的异常进行分类，并提供了良好的接口。在 Java 中，每个异常都是一个对象，它是 Throwable 类或其子类的实例。当一个方法出现异常后便抛出一个异常对象，该对象中包含有异常信息，调用这个对象的方法可以捕获到这个异常并可以对其进行处理。Java 的异常处理是通过 5 个关键词来实现的：try、catch、throw、throws 和 finally。</p>
<p>在Java应用中，异常的处理机制分为声明异常，抛出异常和捕获异常。</p>
<h3 id="声明异常"><a href="#声明异常" class="headerlink" title="声明异常"></a>声明异常</h3><p>通常，应该捕获那些知道如何处理的异常，将不知道如何处理的异常继续传递下去。传递异常可以在方法签名处使用 throws 关键字声明可能会抛出的异常。</p>
<p>注意</p>
<ul>
<li>非检查异常（Error、RuntimeException 或它们的子类）不可使用 throws 关键字来声明要抛出的异常。</li>
<li>一个方法出现编译时异常，就需要 try-catch/ throws 处理，否则会导致编译错误。</li>
</ul>
<h3 id="抛出异常"><a href="#抛出异常" class="headerlink" title="抛出异常"></a>抛出异常</h3><p>如果你觉得解决不了某些异常问题，且不需要调用者处理，那么你可以抛出异常。</p>
<p>throw关键字作用是在方法内部抛出一个<code>Throwable</code>类型的异常。任何Java代码都可以通过throw语句抛出异常。</p>
<h3 id="捕获异常"><a href="#捕获异常" class="headerlink" title="捕获异常"></a>捕获异常</h3><p>程序通常在运行之前不报错，但是运行后可能会出现某些未知的错误，但是还不想直接抛出到上一级，那么就需要通过try…catch…的形式进行异常捕获，之后根据不同的异常情况来进行相应的处理。</p>
<h3 id="如何选择异常类型"><a href="#如何选择异常类型" class="headerlink" title="如何选择异常类型"></a>如何选择异常类型</h3><p><img src="https://lixiangbetter.github.io/2020/06/20/Java%E5%BC%82%E5%B8%B8%E7%AC%94%E8%AE%B0/20200314173209267.png" alt></p>
<h3 id="常见异常处理方式"><a href="#常见异常处理方式" class="headerlink" title="常见异常处理方式"></a>常见异常处理方式</h3><h4 id="直接抛出异常"><a href="#直接抛出异常" class="headerlink" title="直接抛出异常"></a>直接抛出异常</h4><p>通常，应该捕获那些知道如何处理的异常，将不知道如何处理的异常继续传递下去。传递异常可以在方法签名处使用 <strong>throws</strong> 关键字声明可能会抛出的异常。</p>
<h4 id="封装异常再抛出"><a href="#封装异常再抛出" class="headerlink" title="封装异常再抛出"></a>封装异常再抛出</h4><p>有时我们会从 catch 中抛出一个异常，目的是为了改变异常的类型。多用于在多系统集成时，当某个子系统故障，异常类型可能有多种，可以用统一的异常类型向外暴露，不需暴露太多内部异常细节。</p>
<h4 id="捕获异常-1"><a href="#捕获异常-1" class="headerlink" title="捕获异常"></a>捕获异常</h4><p>在一个 try-catch 语句块中可以捕获多个异常类型，并对不同类型的异常做出不同的处理</p>
<p>同一个 catch 也可以捕获多种类型异常，用 | 隔开</p>
<h4 id="自定义异常"><a href="#自定义异常" class="headerlink" title="自定义异常"></a>自定义异常</h4><h4 id="try-with-resource"><a href="#try-with-resource" class="headerlink" title="try-with-resource"></a>try-with-resource</h4><p>JAVA 7 提供了更优雅的方式来实现资源的自动释放，自动释放的资源需要是实现了 AutoCloseable 接口的类。</p>
<h2 id="Java异常常见面试题"><a href="#Java异常常见面试题" class="headerlink" title="Java异常常见面试题"></a>Java异常常见面试题</h2><h3 id="1-Error-和-Exception-区别是什么？"><a href="#1-Error-和-Exception-区别是什么？" class="headerlink" title="1. Error 和 Exception 区别是什么？"></a>1. Error 和 Exception 区别是什么？</h3><p>Error 类型的错误通常为虚拟机相关错误，如系统崩溃，内存不足，堆栈溢出等，编译器不会对这类错误进行检测，JAVA 应用程序也不应对这类错误进行捕获，一旦这类错误发生，通常应用程序会被终止，仅靠应用程序本身无法恢复；</p>
<p>Exception 类的错误是可以在应用程序中进行捕获并处理的，通常遇到这种错误，应对其进行处理，使应用程序可以继续正常运行。</p>
<h3 id="2-运行时异常和一般异常-受检异常-区别是什么？"><a href="#2-运行时异常和一般异常-受检异常-区别是什么？" class="headerlink" title="2. 运行时异常和一般异常(受检异常)区别是什么？"></a>2. 运行时异常和一般异常(受检异常)区别是什么？</h3><p>运行时异常包括 RuntimeException 类及其子类，表示 JVM 在运行期间可能出现的异常。 Java 编译器不会检查运行时异常。</p>
<p>受检异常是Exception 中除 RuntimeException 及其子类之外的异常。 Java 编译器会检查受检异常。</p>
<p>RuntimeException异常和受检异常之间的区别：是否强制要求调用者必须处理此异常，如果强制要求调用者必须进行处理，那么就使用受检异常，否则就选择非受检异常(RuntimeException)。一般来讲，如果没有特殊的要求，我们建议使用RuntimeException异常。</p>
<h3 id="3-JVM-是如何处理异常的？"><a href="#3-JVM-是如何处理异常的？" class="headerlink" title="3. JVM 是如何处理异常的？"></a>3. JVM 是如何处理异常的？</h3><p>在一个方法中如果发生异常，这个方法会创建一个异常对象，并转交给 JVM，该异常对象包含异常名称，异常描述以及异常发生时应用程序的状态。创建异常对象并转交给 JVM 的过程称为抛出异常。可能有一系列的方法调用，最终才进入抛出异常的方法，这一系列方法调用的有序列表叫做调用栈。</p>
<p>JVM 会顺着调用栈去查找看是否有可以处理异常的代码，如果有，则调用异常处理代码。当 JVM 发现可以处理异常的代码时，会把发生的异常传递给它。如果 JVM 没有找到可以处理该异常的代码块，JVM 就会将该异常转交给默认的异常处理器（默认处理器为 JVM 的一部分），默认异常处理器打印出异常信息并终止应用程序。</p>
<h3 id="4-throw-和-throws-的区别是什么？"><a href="#4-throw-和-throws-的区别是什么？" class="headerlink" title="4. throw 和 throws 的区别是什么？"></a>4. throw 和 throws 的区别是什么？</h3><p>throws 关键字在方法上声明该方法要拋出的异常，或者在方法内部通过 throw 拋出异常对象。</p>
<p>throws 关键字和 throw 关键字在使用上的几点区别如下：</p>
<ul>
<li>throw 关键字用在方法内部，只能用于抛出一种异常，用来抛出方法或代码块中的异常，受查异常和非受查异常都可以被抛出。</li>
<li>throws 关键字用在方法声明上，可以抛出多个异常，用来标识该方法可能抛出的异常列表。一个方法用 throws 标识了可能抛出的异常列表，调用该方法的方法中必须包含可处理异常的代码，否则也要在方法签名中用 throws 关键字声明相应的异常。</li>
</ul>
<h3 id="5-final、finally、finalize-有什么区别？"><a href="#5-final、finally、finalize-有什么区别？" class="headerlink" title="5. final、finally、finalize 有什么区别？"></a>5. final、finally、finalize 有什么区别？</h3><ul>
<li>final可以修饰类、变量、方法，修饰类表示该类不能被继承、修饰方法表示该方法不能被重写、修饰变量表示该变量是一个常量不能被重新赋值。</li>
<li>finally一般作用在try-catch代码块中，在处理异常的时候，通常我们将一定要执行的代码方法finally代码块中，表示不管是否出现异常，该代码块都会执行，一般用来存放一些关闭资源的代码。</li>
<li>finalize是一个方法，属于Object类的一个方法，而Object类是所有类的父类，Java 中允许使用 finalize()方法在垃圾收集器将对象从内存中清除出去之前做必要的清理工作。</li>
</ul>
<h3 id="6-NoClassDefFoundError-和-ClassNotFoundException-区别？"><a href="#6-NoClassDefFoundError-和-ClassNotFoundException-区别？" class="headerlink" title="6. NoClassDefFoundError 和 ClassNotFoundException 区别？"></a>6. NoClassDefFoundError 和 ClassNotFoundException 区别？</h3><p>NoClassDefFoundError 是一个 Error 类型的异常，是由 JVM 引起的，不应该尝试捕获这个异常。</p>
<p>引起该异常的原因是 JVM 或 ClassLoader 尝试加载某类时在内存中找不到该类的定义，该动作发生在运行期间，即编译时该类存在，但是在运行时却找不到了，可能是变异后被删除了等原因导致；</p>
<p>ClassNotFoundException 是一个受查异常，需要显式地使用 try-catch 对其进行捕获和处理，或在方法签名中用 throws 关键字进行声明。当使用 Class.forName, ClassLoader.loadClass 或 ClassLoader.findSystemClass 动态加载类到内存的时候，通过传入的类路径参数没有找到该类，就会抛出该异常；另一种抛出该异常的可能原因是某个类已经由一个类加载器加载至内存中，另一个加载器又尝试去加载它。</p>
<h3 id="7-try-catch-finally-中哪个部分可以省略？"><a href="#7-try-catch-finally-中哪个部分可以省略？" class="headerlink" title="7. try-catch-finally 中哪个部分可以省略？"></a>7. try-catch-finally 中哪个部分可以省略？</h3><p>catch 可以省略</p>
<h3 id="8-try-catch-finally-中，如果-catch-中-return-了，finally-还会执行吗？"><a href="#8-try-catch-finally-中，如果-catch-中-return-了，finally-还会执行吗？" class="headerlink" title="8. try-catch-finally 中，如果 catch 中 return 了，finally 还会执行吗？"></a>8. try-catch-finally 中，如果 catch 中 return 了，finally 还会执行吗？</h3><p>答：会执行，在 return 前执行。</p>
<h3 id="9-类-ExampleA-继承-Exception，类-ExampleB-继承ExampleA。"><a href="#9-类-ExampleA-继承-Exception，类-ExampleB-继承ExampleA。" class="headerlink" title="9. 类 ExampleA 继承 Exception，类 ExampleB 继承ExampleA。"></a>9. 类 ExampleA 继承 Exception，类 ExampleB 继承ExampleA。</h3><h3 id="10-常见的-RuntimeException-有哪些？"><a href="#10-常见的-RuntimeException-有哪些？" class="headerlink" title="10. 常见的 RuntimeException 有哪些？"></a>10. 常见的 RuntimeException 有哪些？</h3><p>ClassCastException(类转换异常)<br>IndexOutOfBoundsException(数组越界)<br>NullPointerException(空指针)<br>ArrayStoreException(数据存储异常，操作数组时类型不一致)<br>还有IO操作的BufferOverflowException异常</p>
<h3 id="11-Java常见异常有哪些"><a href="#11-Java常见异常有哪些" class="headerlink" title="11. Java常见异常有哪些"></a>11. Java常见异常有哪些</h3><p>java.lang.IllegalAccessError：违法访问错误</p>
<p>java.lang.InstantiationError：实例化错误</p>
<p>java.lang.OutOfMemoryError：内存不足错误</p>
<p>java.lang.StackOverflowError：堆栈溢出错误</p>
<p>java.lang.ClassCastException：类转型异常</p>
<p>java.lang.ClassNotFoundException：找不到类异常</p>
<p>java.lang.ArithmeticException：算术条件异常</p>
<p>java.lang.ArrayIndexOutOfBoundsException：数组索引越界异常</p>
<p>java.lang.IndexOutOfBoundsException：索引越界异常</p>
<p>java.lang.InstantiationException：实例化异常</p>
<p>java.lang.NoSuchFieldException：属性不存在异常</p>
<p>java.lang.NoSuchMethodException：方法不存在异常</p>
<p>java.lang.NullPointerException：空指针异常</p>
<p>java.lang.NumberFormatException：数字格式异常</p>
<p>java.lang.StringIndexOutOfBoundsException：字符串索引越界异常</p>
<h2 id="Java异常处理最佳实践"><a href="#Java异常处理最佳实践" class="headerlink" title="Java异常处理最佳实践"></a>Java异常处理最佳实践</h2><h3 id="1-在-finally-块中清理资源或者使用-try-with-resource-语句"><a href="#1-在-finally-块中清理资源或者使用-try-with-resource-语句" class="headerlink" title="1. 在 finally 块中清理资源或者使用 try-with-resource 语句"></a>1. 在 finally 块中清理资源或者使用 try-with-resource 语句</h3><h3 id="2-优先明确的异常"><a href="#2-优先明确的异常" class="headerlink" title="2. 优先明确的异常"></a>2. 优先明确的异常</h3><h3 id="3-对异常进行文档说明"><a href="#3-对异常进行文档说明" class="headerlink" title="3. 对异常进行文档说明"></a>3. 对异常进行文档说明</h3><h3 id="4-使用描述性消息抛出异常"><a href="#4-使用描述性消息抛出异常" class="headerlink" title="4. 使用描述性消息抛出异常"></a>4. 使用描述性消息抛出异常</h3><h3 id="5-优先捕获最具体的异常"><a href="#5-优先捕获最具体的异常" class="headerlink" title="5. 优先捕获最具体的异常"></a>5. 优先捕获最具体的异常</h3><h3 id="6-不要捕获-Throwable-类"><a href="#6-不要捕获-Throwable-类" class="headerlink" title="6. 不要捕获 Throwable 类"></a>6. 不要捕获 Throwable 类</h3><h3 id="7-不要忽略异常"><a href="#7-不要忽略异常" class="headerlink" title="7. 不要忽略异常"></a>7. 不要忽略异常</h3><h3 id="8-不要记录并抛出异常"><a href="#8-不要记录并抛出异常" class="headerlink" title="8. 不要记录并抛出异常"></a>8. 不要记录并抛出异常</h3><h3 id="9-包装异常时不要抛弃原始的异常"><a href="#9-包装异常时不要抛弃原始的异常" class="headerlink" title="9. 包装异常时不要抛弃原始的异常"></a>9. 包装异常时不要抛弃原始的异常</h3><h3 id="10-不要使用异常控制程序的流程"><a href="#10-不要使用异常控制程序的流程" class="headerlink" title="10. 不要使用异常控制程序的流程"></a>10. 不要使用异常控制程序的流程</h3><h3 id="11-使用标准异常"><a href="#11-使用标准异常" class="headerlink" title="11. 使用标准异常"></a>11. 使用标准异常</h3><h3 id="12-异常会影响性能"><a href="#12-异常会影响性能" class="headerlink" title="12. 异常会影响性能"></a>12. 异常会影响性能</h3><h3 id="13-总结"><a href="#13-总结" class="headerlink" title="13. 总结"></a>13. 总结</h3><h3 id="异常处理-阿里巴巴Java开发手册"><a href="#异常处理-阿里巴巴Java开发手册" class="headerlink" title="异常处理-阿里巴巴Java开发手册"></a>异常处理-阿里巴巴Java开发手册</h3><ol>
<li>【强制】Java 类库中定义的可以通过预检查方式规避的RuntimeException异常不应该通过catch 的方式来处理，比如：NullPointerException，IndexOutOfBoundsException等等。 说明：无法通过预检查的异常除外，比如，在解析字符串形式的数字时，可能存在数字格式错误，不得不通过catch NumberFormatException来实现。 正例：if (obj != null) {…} 反例：try { obj.method(); } catch (NullPointerException e) {…}</li>
<li>【强制】异常不要用来做流程控制，条件控制。 说明：异常设计的初衷是解决程序运行中的各种意外情况，且异常的处理效率比条件判断方式要低很多。</li>
<li>【强制】catch时请分清稳定代码和非稳定代码，稳定代码指的是无论如何不会出错的代码。对于非稳定代码的catch尽可能进行区分异常类型，再做对应的异常处理。 说明：对大段代码进行try-catch，使程序无法根据不同的异常做出正确的应激反应，也不利于定位问题，这是一种不负责任的表现。 正例：用户注册的场景中，如果用户输入非法字符，或用户名称已存在，或用户输入密码过于简单，在程序上作出分门别类的判断，并提示给用户。</li>
<li>【强制】捕获异常是为了处理它，不要捕获了却什么都不处理而抛弃之，如果不想处理它，请将该异常抛给它的调用者。最外层的业务使用者，必须处理异常，将其转化为用户可以理解的内容。</li>
<li>【强制】有try块放到了事务代码中，catch异常后，如果需要回滚事务，一定要注意手动回滚事务。</li>
<li>【强制】finally块必须对资源对象、流对象进行关闭，有异常也要做try-catch。 说明：如果JDK7及以上，可以使用try-with-resources方式。</li>
<li>【强制】不要在finally块中使用return。 说明：try块中的return语句执行成功后，并不马上返回，而是继续执行finally块中的语句，如果此处存在return语句，则在此直接返回，无情丢弃掉try块中的返回点。</li>
<li>【强制】捕获异常与抛异常，必须是完全匹配，或者捕获异常是抛异常的父类。 说明：如果预期对方抛的是绣球，实际接到的是铅球，就会产生意外情况。</li>
<li>【强制】在调用RPC、二方包、或动态生成类的相关方法时，捕捉异常必须使用Throwable类来进行拦截。 说明：通过反射机制来调用方法，如果找不到方法，抛出NoSuchMethodException。什么情况会抛出NoSuchMethodError呢？二方包在类冲突时，仲裁机制可能导致引入非预期的版本使类的方法签名不匹配，或者在字节码修改框架（比如：ASM）动态创建或修改类时，修改了相应的方法签名。这些情况，即使代码编译期是正确的，但在代码运行期时，会抛出NoSuchMethodError。</li>
<li>【推荐】方法的返回值可以为null，不强制返回空集合，或者空对象等，必须添加注释充分说明什么情况下会返回null值。 说明：本手册明确防止NPE是调用者的责任。即使被调用方法返回空集合或者空对象，对调用者来说，也并非高枕无忧，必须考虑到远程调用失败、序列化失败、运行时异常等场景返回null的情况。</li>
<li>【推荐】防止NPE，是程序员的基本修养，注意NPE产生的场景： 1） 返回类型为基本数据类型，return包装数据类型的对象时，自动拆箱有可能产生NPE。 反例：public int f() { return Integer对象}， 如果为null，自动解箱抛NPE。 2） 数据库的查询结果可能为null。 3） 集合里的元素即使isNotEmpty，取出的数据元素也可能为null。 4） 远程调用返回对象时，一律要求进行空指针判断，防止NPE。 5） 对于Session中获取的数据，建议进行NPE检查，避免空指针。 6） 级联调用obj.getA().getB().getC()；一连串调用，易产生NPE。<br>正例：使用JDK8的Optional类来防止NPE问题。</li>
<li>【推荐】定义时区分unchecked / checked 异常，避免直接抛出new RuntimeException()，更不允许抛出Exception或者Throwable，应使用有业务含义的自定义异常。推荐业界已定义过的自定义异常，如：DAOException / ServiceException等。</li>
<li>【参考】对于公司外的http/api开放接口必须使用“错误码”；而应用内部推荐异常抛出；跨应用间RPC调用优先考虑使用Result方式，封装isSuccess()方法、“错误码”、“错误简短信息”。 说明：关于RPC方法返回方式使用Result方式的理由： 1）使用抛异常返回方式，调用方如果没有捕获到就会产生运行时错误。 2）如果不加栈信息，只是new自定义异常，加入自己的理解的error message，对于调用端解决问题的帮助不会太多。如果加了栈信息，在频繁调用出错的情况下，数据序列化和传输的性能损耗也是问题。</li>
<li>【参考】避免出现重复的代码（Don’t Repeat Yourself），即DRY原则。 说明：随意复制和粘贴代码，必然会导致代码的重复，在以后需要修改时，需要修改所有的副本，容易遗漏。必要时抽取共性方法，或者抽象公共类，甚至是组件化。 正例：一个类中有多个public方法，都需要进行数行相同的参数校验操作，这个时候请抽取：<br>private boolean checkParam(DTO dto) {…}</li>
</ol>
]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>exception</tag>
      </tags>
  </entry>
  <entry>
    <title>Java网络编程笔记</title>
    <url>/2020/06/20/Java%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<h1 id="Java网络编程笔记"><a href="#Java网络编程笔记" class="headerlink" title="Java网络编程笔记"></a>Java网络编程笔记</h1><h2 id="计算机网络体系结构"><a href="#计算机网络体系结构" class="headerlink" title="计算机网络体系结构"></a>计算机网络体系结构</h2><h3 id="网络协议是什么？"><a href="#网络协议是什么？" class="headerlink" title="网络协议是什么？"></a>网络协议是什么？</h3><p>在计算机网络要做到有条不紊地交换数据，就必须遵守一些事先约定好的规则，比如交换数据的格式、是否需要发送一个应答信息。这些规则被称为网络协议。</p>
<h3 id="为什么要对网络协议分层？"><a href="#为什么要对网络协议分层？" class="headerlink" title="为什么要对网络协议分层？"></a>为什么要对网络协议分层？</h3><ul>
<li>简化问题难度和复杂度。由于各层之间独立，我们可以分割大问题为小问题。</li>
<li>灵活性好。当其中一层的技术变化时，只要层间接口关系保持不变，其他层不受影响。</li>
<li>易于实现和维护。</li>
<li>促进标准化工作。分开后，每层功能可以相对简单地被描述。</li>
</ul>
<p><img src="https://lixiangbetter.github.io/2020/06/20/Java%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%E7%AC%94%E8%AE%B0/20200316173310511.png" alt></p>
<h2 id="TCP-IP-协议族"><a href="#TCP-IP-协议族" class="headerlink" title="TCP/IP 协议族"></a>TCP/IP 协议族</h2><h3 id="应用层"><a href="#应用层" class="headerlink" title="应用层"></a>应用层</h3><p>应用层( application-layer ）的任务是通过应用进程间的交互来完成特定网络应用。应用层协议定义的是应用进程（进程：主机中正在运行的程序）间的通信和交互的规则。</p>
<p>对于不同的网络应用需要不同的应用层协议。在互联网中应用层协议很多，如域名系统 DNS，支持万维网应用的 HTTP 协议，支持电子邮件的 SMTP 协议等等。</p>
<h3 id="运输层"><a href="#运输层" class="headerlink" title="运输层"></a>运输层</h3><p>运输层(transport layer)的主要任务就是负责向两台主机进程之间的通信提供通用的数据传输服务。应用进程利用该服务传送应用层报文。</p>
<p>运输层主要使用一下两种协议</p>
<p>传输控制协议-TCP：提供面向连接的，可靠的数据传输服务。<br>用户数据协议-UDP：提供无连接的，尽最大努力的数据传输服务（不保证数据传输的可靠性）。</p>
<h3 id="网络层"><a href="#网络层" class="headerlink" title="网络层"></a>网络层</h3><p>网络层的任务就是选择合适的网间路由和交换结点，确保计算机通信的数据及时传送。在发送数据时，网络层把运输层产生的报文段或用户数据报封装成分组和包进行传送。在 TCP/IP 体系结构中，由于网络层使用 IP 协议，因此分组也叫 IP 数据报 ，简称数据报。</p>
<p>互联网是由大量的异构（heterogeneous）网络通过路由器（router）相互连接起来的。互联网使用的网络层协议是无连接的网际协议（Intert Prococol）和许多路由选择协议，因此互联网的网络层也叫做网际层或 IP 层。</p>
<h3 id="数据链路层"><a href="#数据链路层" class="headerlink" title="数据链路层"></a>数据链路层</h3><p>数据链路层(data link layer)通常简称为链路层。两台主机之间的数据传输，总是在一段一段的链路上传送的，这就需要使用专门的链路层的协议。</p>
<p>在两个相邻节点之间传送数据时，数据链路层将网络层交下来的 IP 数据报组装成帧，在两个相邻节点间的链路上传送帧。每一帧包括数据和必要的控制信息（如同步信息，地址信息，差错控制等）。</p>
<p>在接收数据时，控制信息使接收端能够知道一个帧从哪个比特开始和到哪个比特结束。</p>
<p><img src="https://lixiangbetter.github.io/2020/06/20/Java%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%E7%AC%94%E8%AE%B0/aHR0cHM6Ly91c2VyLWdvbGQtY2RuLnhpdHUuaW8vMjAxOS81LzkvMTZhOWM5Y2Q1MjNlMDU5OQ.jpeg" alt></p>
<h3 id="物理层"><a href="#物理层" class="headerlink" title="物理层"></a>物理层</h3><p>在物理层上所传送的数据单位是比特。 物理层(physical layer)的作用是实现相邻计算机节点之间比特流的透明传送，尽可能屏蔽掉具体传输介质和物理设备的差异。使其上面的数据链路层不必考虑网络的具体传输介质是什么。“透明传送比特流”表示经实际电路传送后的比特流没有发生变化，对传送的比特流来说，这个电路好像是看不见的。</p>
<h3 id="TCP-IP-协议族-1"><a href="#TCP-IP-协议族-1" class="headerlink" title="TCP/IP 协议族"></a>TCP/IP 协议族</h3><p><img src="https://lixiangbetter.github.io/2020/06/20/Java%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%E7%AC%94%E8%AE%B0/aHR0cHM6Ly91c2VyLWdvbGQtY2RuLnhpdHUuaW8vMjAxOS80LzcvMTY5ZjY5NjZjMjRhZjM0NQ.jpeg" alt></p>
<h2 id="TCP的三次握手四次挥手"><a href="#TCP的三次握手四次挥手" class="headerlink" title="TCP的三次握手四次挥手"></a>TCP的三次握手四次挥手</h2><h3 id="三次握手"><a href="#三次握手" class="headerlink" title="三次握手"></a>三次握手</h3><p>我让信使运输一份信件给对方，对方收到了，那么他就知道了我的发件能力和他的收件能力是可以的。</p>
<p>于是他给我回信，我若收到了，我便知我的发件能力和他的收件能力是可以的，并且他的发件能力和我的收件能力是可以。</p>
<p>然而此时他还不知道他的发件能力和我的收件能力到底可不可以，于是我最后回馈一次，他若收到了，他便清楚了他的发件能力和我的收件能力是可以的。</p>
<p><img src="https://lixiangbetter.github.io/2020/06/20/Java%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%E7%AC%94%E8%AE%B0/aHR0cHM6Ly91c2VyLWdvbGQtY2RuLnhpdHUuaW8vMjAyMC8xLzcvMTZmN2UwM2IxZWE1MDdlOA.jpeg" alt></p>
<h3 id="四次挥手"><a href="#四次挥手" class="headerlink" title="四次挥手"></a>四次挥手</h3><p><img src="https://lixiangbetter.github.io/2020/06/20/Java%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%E7%AC%94%E8%AE%B0/aHR0cHM6Ly91c2VyLWdvbGQtY2RuLnhpdHUuaW8vMjAyMC8xLzcvMTZmN2UwM2IyMWEwN2YwYw.jpeg" alt></p>
<p>第一次挥手：当客户端的数据都传输完成后，客户端向服务端发出连接释放报文(当然数据没发完时也可以发送连接释放报文并停止发送数据)，需要注意的是客户端发出FIN报文段后只是不能发数据了，但是还可以正常收数据；<br>第二次挥手：服务端收到客户端发的FIN报文后给客户端回复确认报文，此时服务端处于关闭等待状态，而不是立马给客户端发FIN报文，这个状态还要持续一段时间，因为服务端可能还有数据没发完。<br>第三次挥手：服务端将最后数据(比如50个字节)发送完毕后就向客户端发出连接释放报文，<br>第四次挥手：客户端收到服务端发的FIN报文后，向服务端发出确认报文，注意客户端发出确认报文后不是立马释放TCP连接，而是要经过2MSL(最长报文段寿命的2倍时长)后才释放TCP连接。而服务端一旦收到客户端发出的确认报文就会立马释放TCP连接，所以服务端结束TCP连接的时间要比客户端早一些。</p>
<h2 id="常见面试题"><a href="#常见面试题" class="headerlink" title="常见面试题"></a>常见面试题</h2><h3 id="为什么TCP连接的时候是3次？2次不可以吗？"><a href="#为什么TCP连接的时候是3次？2次不可以吗？" class="headerlink" title="为什么TCP连接的时候是3次？2次不可以吗？"></a>为什么TCP连接的时候是3次？2次不可以吗？</h3><p>因为需要考虑连接时丢包的问题，如果只握手2次，第二次握手时如果服务端发给客户端的确认报文段丢失，此时服务端已经准备好了收发数(可以理解服务端已经连接成功)据，而客户端一直没收到服务端的确认报文，所以客户端就不知道服务端是否已经准备好了(可以理解为客户端未连接成功)，这种情况下客户端不会给服务端发数据，也会忽略服务端发过来的数据。</p>
<p>如果是三次握手，即便发生丢包也不会有问题，比如如果第三次握手客户端发的确认ack报文丢失，服务端在一段时间内没有收到确认ack报文的话就会重新进行第二次握手，也就是服务端会重发SYN报文段，客户端收到重发的报文段后会再次给服务端发送确认ack报文。</p>
<h3 id="为什么TCP连接的时候是3次，关闭的时候却是4次？"><a href="#为什么TCP连接的时候是3次，关闭的时候却是4次？" class="headerlink" title="为什么TCP连接的时候是3次，关闭的时候却是4次？"></a>为什么TCP连接的时候是3次，关闭的时候却是4次？</h3><p>因为只有在客户端和服务端都没有数据要发送的时候才能断开TCP。而客户端发出FIN报文时只能保证客户端没有数据发了，服务端还有没有数据发客户端是不知道的。而服务端收到客户端的FIN报文后只能先回复客户端一个确认报文来告诉客户端我服务端已经收到你的FIN报文了，但我服务端还有一些数据没发完，等这些数据发完了服务端才能给客户端发FIN报文(所以不能一次性将确认报文和FIN报文发给客户端，就是这里多出来了一次)。</p>
<h3 id="为什么客户端发出第四次挥手的确认报文后要等2MSL的时间才能释放TCP连接？"><a href="#为什么客户端发出第四次挥手的确认报文后要等2MSL的时间才能释放TCP连接？" class="headerlink" title="为什么客户端发出第四次挥手的确认报文后要等2MSL的时间才能释放TCP连接？"></a>为什么客户端发出第四次挥手的确认报文后要等2MSL的时间才能释放TCP连接？</h3><p>这里同样是要考虑丢包的问题，如果第四次挥手的报文丢失，服务端没收到确认ack报文就会重发第三次挥手的报文，这样报文一去一回最长时间就是2MSL，所以需要等这么长时间来确认服务端确实已经收到了。</p>
<h3 id="如果已经建立了连接，但是客户端突然出现故障了怎么办？"><a href="#如果已经建立了连接，但是客户端突然出现故障了怎么办？" class="headerlink" title="如果已经建立了连接，但是客户端突然出现故障了怎么办？"></a>如果已经建立了连接，但是客户端突然出现故障了怎么办？</h3><p>TCP设有一个保活计时器，客户端如果出现故障，服务器不能一直等下去，白白浪费资源。服务器每收到一次客户端的请求后都会重新复位这个计时器，时间通常是设置为2小时，若两小时还没有收到客户端的任何数据，服务器就会发送一个探测报文段，以后每隔75秒钟发送一次。若一连发送10个探测报文仍然没反应，服务器就认为客户端出了故障，接着就关闭连接。</p>
<h3 id="什么是HTTP，HTTP-与-HTTPS-的区别"><a href="#什么是HTTP，HTTP-与-HTTPS-的区别" class="headerlink" title="什么是HTTP，HTTP 与 HTTPS 的区别"></a>什么是HTTP，HTTP 与 HTTPS 的区别</h3><p>HTTP 是一个在计算机世界里专门在两点之间传输文字、图片、音频、视频等超文本数据的约定和规范</p>
<p><strong>HTTPS是添加了加密和认证机制的 HTTP</strong></p>
<h3 id="常用HTTP状态码"><a href="#常用HTTP状态码" class="headerlink" title="常用HTTP状态码"></a>常用HTTP状态码</h3><ul>
<li>1XX    Informational（信息性状态码） 接受的请求正在处理</li>
<li>2XX    Success（成功状态码） 请求正常处理完毕</li>
<li>3XX    Redirection（重定向状态码） 需要进行附加操作以完成请求</li>
<li>4XX    Client Error（客户端错误状态码） 服务器无法处理请求</li>
<li>5XX    Server Error（服务器错误状态码） 服务器处理请求出错</li>
</ul>
<h3 id="GET和POST区别"><a href="#GET和POST区别" class="headerlink" title="GET和POST区别"></a>GET和POST区别</h3><ul>
<li>GET：从服务器上获取数据，也就是所谓的查，仅仅是获取服务器资源，不进行修改。</li>
<li>POST：向服务器提交数据，这就涉及到了数据的更新，也就是更改服务器的数据。</li>
<li>PUT：英文含义是放置，也就是向服务器新添加数据，就是所谓的增。</li>
<li>DELETE：从字面意思也能看出，这种方式就是删除服务器数据的过程。</li>
</ul>
<ol>
<li>Get是不安全的，因为在传输过程，数据被放在请求的URL中；Post的所有操作对用户来说都是不可见的。 但是这种做法也不时绝对的，大部分人的做法也是按照上面的说法来的，但是也可以在get请求加上 request body，给 post请求带上 URL 参数。</li>
<li>Get请求提交的url中的数据最多只能是2048字节，这个限制是浏览器或者服务器给添加的，http协议并没有对url长度进行限制，目的是为了保证服务器和浏览器能够正常运行，防止有人恶意发送请求。Post请求则没有大小限制。</li>
<li>Get限制Form表单的数据集的值必须为ASCII字符；而Post支持整个ISO10646字符集。</li>
<li>Get执行效率却比Post方法好。Get是form提交的默认方法。</li>
</ol>
<p>GET产生一个TCP数据包；POST产生两个TCP数据包。</p>
<p>对于GET方式的请求，浏览器会把http header和data一并发送出去，服务器响应200（返回数据）；</p>
<p>而对于POST，浏览器先发送header，服务器响应100 continue，浏览器再发送data，服务器响应200 ok（返回数据）。</p>
<h3 id="什么是对称加密与非对称加密"><a href="#什么是对称加密与非对称加密" class="headerlink" title="什么是对称加密与非对称加密"></a>什么是对称加密与非对称加密</h3><p>对称密钥加密是指加密和解密使用同一个密钥的方式，这种方式存在的最大问题就是密钥发送问题，即如何安全地将密钥发给对方；</p>
<p>而非对称加密是指使用一对非对称密钥，即公钥和私钥，公钥可以随意发布，但私钥只有自己知道。发送密文的一方使用对方的公钥进行加密处理，对方接收到加密信息后，使用自己的私钥进行解密。<br>由于非对称加密的方式不需要发送用来解密的私钥，所以可以保证安全性；但是和对称加密比起来，非常的慢</p>
<h3 id="什么是HTTP2"><a href="#什么是HTTP2" class="headerlink" title="什么是HTTP2"></a>什么是HTTP2</h3><p>HTTP2 可以提高了网页的性能。</p>
<p>在 HTTP1 中浏览器限制了同一个域名下的请求数量（Chrome 下一般是六个），当在请求很多资源的时候，由于队头阻塞当浏览器达到最大请求数量时，剩余的资源需等待当前的六个请求完成后才能发起请求。</p>
<p>HTTP2 中引入了多路复用的技术，这个技术可以只通过一个 TCP 连接就可以传输所有的请求数据。多路复用可以绕过浏览器限制同一个域名下的请求数量的问题，进而提高了网页的性能。</p>
<h3 id="Session、Cookie和Token的主要区别"><a href="#Session、Cookie和Token的主要区别" class="headerlink" title="Session、Cookie和Token的主要区别"></a>Session、Cookie和Token的主要区别</h3><p>什么是cookie</p>
<p>cookie是由Web服务器保存在用户浏览器上的小文件（key-value格式），包含用户相关的信息。</p>
<p>什么是session</p>
<p>session是依赖Cookie实现的。session是服务器端对象</p>
<p>session 是浏览器和服务器会话过程中，服务器分配的一块储存空间。服务器默认为浏览器在cookie中设置 sessionid，浏览器在向服务器请求过程中传输 cookie 包含 sessionid ，服务器根据 sessionid 获取出会话中存储的信息，然后确定会话的身份信息。</p>
<p>cookie与session区别</p>
<ul>
<li>存储位置与安全性：cookie客户端，安全性较差，session服务器，安全性相对更高；</li>
<li>存储空间：单个cookie保存的数据不能超过4K，很多浏览器都限制一个站点最多保存20个cookie，session无此限制</li>
<li>占用服务器资源：session一定时间内保存在服务器上，当访问增多，占用服务器性能，考虑到服务器性能方面，应当使用cookie。</li>
</ul>
<p>什么是Token</p>
<p>Token的引入：Token是在客户端频繁向服务端请求数据，服务端频繁的去数据库查询用户名和密码并进行对比，判断用户名和密码正确与否，并作出相应提示，在这样的背景下，Token便应运而生。</p>
<p>Token的定义：Token是服务端生成的一串字符串，以作客户端进行请求的一个令牌，当第一次登录后，服务器生成一个Token便将此Token返回给客户端，以后客户端只需带上这个Token前来请求数据即可，无需再次带上用户名和密码。</p>
<p>使用Token的目的：Token的目的是为了减轻服务器的压力，减少频繁的查询数据库，使服务器更加健壮。</p>
<p>Token 是在服务端产生的。如果前端使用用户名/密码向服务端请求认证，服务端认证成功，那么在服务端会返回 Token 给前端。前端可以在每次请求的时候带上 Token 证明自己的合法地位</p>
<p><strong>session与token区别</strong></p>
<ul>
<li>session机制存在服务器压力增大，CSRF跨站伪造请求攻击，扩展性不强等问题；</li>
<li>session存储在服务器端，token存储在客户端</li>
<li>token提供认证和授权功能，作为身份认证，token安全性比session好；</li>
<li>session这种会话存储方式方式只适用于客户端代码和服务端代码运行在同一台服务器上，token适用于项目级的前后端分离（前后端代码运行在不同的服务器下）</li>
</ul>
<h3 id="Servlet是线程安全的吗"><a href="#Servlet是线程安全的吗" class="headerlink" title="Servlet是线程安全的吗"></a>Servlet是线程安全的吗</h3><p><strong>Servlet不是线程安全的，多线程并发的读写会导致数据不同步的问题。</strong></p>
<h3 id="Servlet接口中有哪些方法及Servlet生命周期探秘"><a href="#Servlet接口中有哪些方法及Servlet生命周期探秘" class="headerlink" title="Servlet接口中有哪些方法及Servlet生命周期探秘"></a>Servlet接口中有哪些方法及Servlet生命周期探秘</h3><p><strong>Servlet</strong>主要负责接收用户请求<strong>HttpServletRequest</strong>，在<strong>doGet()</strong>，<strong>doPost()\</strong>中做相应的处理，并将回应*<em>HttpServletResponse*</em>反馈给用户。Servlet可以设置初始化参数，供Servlet内部使用。</p>
<p>Servlet接口定义了5个方法，其中前三个方法与Servlet生命周期相关：</p>
<ul>
<li>void init(ServletConfig config) throws ServletException</li>
<li>void service(ServletRequest req, ServletResponse resp) throws ServletException, java.io.IOException</li>
<li>void destory()</li>
<li>java.lang.String getServletInfo()</li>
<li>ServletConfig getServletConfig()</li>
</ul>
<p><strong>生命周期：</strong></p>
<p>Web容器加载Servlet并将其实例化后，Servlet生命周期开始，容器运行其init()方法进行Servlet的初始化；</p>
<p>请求到达时调用Servlet的service()方法，service()方法会根据需要调用与请求对应的doGet或doPost等方法；</p>
<p>当服务器关闭或项目被卸载时服务器会将Servlet实例销毁，此时会调用Servlet的destroy()方法。</p>
<p>init方法和destory方法只会执行一次，service方法客户端每次请求Servlet都会执行。</p>
<h3 id="如果客户端禁止-cookie-能实现-session-还能用吗？"><a href="#如果客户端禁止-cookie-能实现-session-还能用吗？" class="headerlink" title="如果客户端禁止 cookie 能实现 session 还能用吗？"></a>如果客户端禁止 cookie 能实现 session 还能用吗？</h3><p>Cookie 与 Session，一般认为是两个独立的东西，Session采用的是在服务器端保持状态的方案，而Cookie采用的是在客户端保持状态的方案。</p>
<p>但为什么禁用Cookie就不能得到Session呢？因为Session是用Session ID来确定当前对话所对应的服务器Session，而Session ID是通过Cookie来传递的，禁用Cookie相当于失去了Session ID，也就得不到Session了。</p>
<p>假定用户关闭Cookie的情况下使用Session，其实现途径有以下几种：</p>
<ul>
<li>手动通过URL传值、隐藏表单传递Session ID。</li>
<li>用文件、数据库等形式保存Session ID，在跨页过程中手动调用。</li>
</ul>
]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>网络编程</tag>
      </tags>
  </entry>
  <entry>
    <title>Java基础知识笔记</title>
    <url>/2020/06/20/Java%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<h1 id="Java基础知识笔记"><a href="#Java基础知识笔记" class="headerlink" title="Java基础知识笔记"></a>Java基础知识笔记</h1><h2 id="Java概述"><a href="#Java概述" class="headerlink" title="Java概述"></a>Java概述</h2><h3 id="何为编程"><a href="#何为编程" class="headerlink" title="何为编程"></a>何为编程</h3><p>编程就是让计算机为解决某个问题而使用某种程序设计语言编写程序代码，并最终得到结果的过程。</p>
<h3 id="什么是Java"><a href="#什么是Java" class="headerlink" title="什么是Java"></a>什么是Java</h3><p>Java 面向对象编程语言，吸收C++语言优点，还摒弃了C++难以理解的多继承、指针等概念，因此Java语言具有功能强大和简单易用两个特征。</p>
<h3 id="jdk1-5之后的三大版本"><a href="#jdk1-5之后的三大版本" class="headerlink" title="jdk1.5之后的三大版本"></a>jdk1.5之后的三大版本</h3><ul>
<li>Java SE（J2SE，Java 2 Platform Standard Edition，标准版）<br>Java SE 以前称为 J2SE。它允许开发和部署在桌面、服务器、嵌入式环境和实时环境中使用的 Java 应用程序。Java SE 为Java EE和Java ME提供基础。</li>
<li>Java EE（J2EE，Java 2 Platform Enterprise Edition，企业版）<br>Java EE 以前称为 J2EE。企业版本帮助开发和部署可移植、健壮、可伸缩且安全的服务器端Java 应用程序。Java EE 是在 Java SE 的基础上构建的，它提供 Web 服务、组件模型、管理和通信 API，可以用来实现企业级的面向服务体系结构（service-oriented architecture，SOA）和 Web2.0应用程序。2018年2月，Eclipse 宣布正式将 JavaEE 更名为 JakartaEE</li>
<li>Java ME（J2ME，Java 2 Platform Micro Edition，微型版）<br>Java ME 以前称为 J2ME。Java ME 为在移动设备和嵌入式设备（比如手机、PDA、电视机顶盒和打印机）上运行的应用程序提供一个健壮且灵活的环境。Java ME 包括灵活的用户界面、健壮的安全模型、许多内置的网络协议以及对可以动态下载的连网和离线应用程序的丰富支持。基于 Java ME 规范的应用程序只需编写一次，就可以用于许多设备，而且可以利用每个设备的本机功能。</li>
</ul>
<h3 id="JVM、JRE和JDK的关系"><a href="#JVM、JRE和JDK的关系" class="headerlink" title="JVM、JRE和JDK的关系"></a>JVM、JRE和JDK的关系</h3><p><img src="https://lixiangbetter.github.io/2020/06/20/Java%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E7%AC%94%E8%AE%B0/aHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL0pvdXJXb24vaW1hZ2UvbWFzdGVyL0phdmElRTclQUUlODAlRTQlQkIlOEIvSlZNJkpSRSZKREslRTUlODUlQjMlRTclQjMlQkIlRTUlOUIlQkUucG5n.jpeg" alt="not found"></p>
<h3 id="什么是跨平台性？原理是什么"><a href="#什么是跨平台性？原理是什么" class="headerlink" title="什么是跨平台性？原理是什么"></a>什么是跨平台性？原理是什么</h3><p>实现原理：Java程序是通过java虚拟机在系统平台上运行的，只要该系统可以安装相应的java虚拟机，该系统就可以运行java程序。</p>
<h3 id="Java语言有哪些特点简单易学（Java语言的语法与C语言和C-语言很接近）"><a href="#Java语言有哪些特点简单易学（Java语言的语法与C语言和C-语言很接近）" class="headerlink" title="Java语言有哪些特点简单易学（Java语言的语法与C语言和C++语言很接近）"></a>Java语言有哪些特点简单易学（Java语言的语法与C语言和C++语言很接近）</h3><p>面向对象（封装，继承，多态）</p>
<p>平台无关性（Java虚拟机实现平台无关性）</p>
<p>支持网络编程并且很方便（Java语言诞生本身就是为简化网络编程设计的）</p>
<p>支持多线程（多线程机制使应用程序在同一时间并行执行多项任）</p>
<p>健壮性（Java语言的强类型机制、异常处理、垃圾的自动收集等）</p>
<p>安全性</p>
<h3 id="什么是字节码？采用字节码的最大好处是什么"><a href="#什么是字节码？采用字节码的最大好处是什么" class="headerlink" title="什么是字节码？采用字节码的最大好处是什么"></a>什么是字节码？采用字节码的最大好处是什么</h3><p><strong>字节码</strong>：Java源代码经过虚拟机编译器编译后产生的文件（即扩展为.class的文件），它不面向任何特定的处理器，只面向虚拟机。</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">Java源代码----&gt;编译器----&gt;jvm可执行的Java字节码(即虚拟指令)----&gt;jvm----&gt;jvm中解释器-----&gt;机器可执行的二进制机器码----&gt;程序运行。</span><br></pre></td></tr></table></figure>

<h3 id="什么是Java程序的主类？应用程序和小程序的主类有何不同？"><a href="#什么是Java程序的主类？应用程序和小程序的主类有何不同？" class="headerlink" title="什么是Java程序的主类？应用程序和小程序的主类有何不同？"></a>什么是Java程序的主类？应用程序和小程序的主类有何不同？</h3><p>一个主类。包含main()方法，不一定public；Java小程序中，主类继承自系统类JApplet或Applet，必须为public；主类是Java程序执行的入口点。</p>
<h3 id="Java和C-的区别"><a href="#Java和C-的区别" class="headerlink" title="Java和C++的区别"></a>Java和C++的区别</h3><ul>
<li>都是面向对象的语言，都支持封装、继承和多态</li>
<li>Java不提供指针来直接访问内存，程序内存更加安全</li>
<li>Java的类是单继承的，C++支持多重继承；虽然Java的类不可以多继承，但是接口可以多继承。</li>
<li>Java有自动内存管理机制，不需要程序员手动释放无用内存</li>
</ul>
<h3 id="Oracle-JDK-和-OpenJDK-的对比"><a href="#Oracle-JDK-和-OpenJDK-的对比" class="headerlink" title="Oracle JDK 和 OpenJDK 的对比"></a>Oracle JDK 和 OpenJDK 的对比</h3><ul>
<li>Oracle JDK每三年发布一次，OpenJDK每三个月发布一次；</li>
<li>OpenJDK 完全开源，而Oracle JDK是OpenJDK的一个实现，不是完全开源的；</li>
<li>Oracle JDK 比 OpenJDK 更稳定。O</li>
<li>在响应性和JVM性能方面，Oracle JDK与OpenJDK相比提供了更好的性能；</li>
<li>Oracle JDK不会为即将发布的版本提供长期支持，用户每次都必须通过更新到最新版本获得支持来获取最新版本；</li>
<li>Oracle JDK根据二进制代码许可协议获得许可，而OpenJDK根据GPL v2许可获得许可。</li>
</ul>
<h2 id="基础语法"><a href="#基础语法" class="headerlink" title="基础语法"></a>基础语法</h2><h3 id="数据类型"><a href="#数据类型" class="headerlink" title="数据类型"></a>数据类型</h3><h4 id="Java有哪些数据类型"><a href="#Java有哪些数据类型" class="headerlink" title="Java有哪些数据类型"></a>Java有哪些数据类型</h4><p>定义：Java语言是强类型语言，对于每一种数据都定义了明确的具体的数据类型，在内存中分配了不同大小的内存空间。</p>
<p>分类</p>
<ul>
<li>基本数据类型<ul>
<li>数值型<ul>
<li>整数类型(byte,short,int,long)</li>
<li>浮点类型(float,double)</li>
</ul>
</li>
<li>字符型(char)</li>
<li>布尔型(boolean)</li>
</ul>
</li>
<li>引用数据类型<ul>
<li>类(class)</li>
<li>接口(interface)</li>
<li>数组([])</li>
</ul>
</li>
</ul>
<h4 id="switch-是否能作用在-byte-上，是否能作用在-long-上，是否能作用在-String-上"><a href="#switch-是否能作用在-byte-上，是否能作用在-long-上，是否能作用在-String-上" class="headerlink" title="switch 是否能作用在 byte 上，是否能作用在 long 上，是否能作用在 String 上"></a>switch 是否能作用在 byte 上，是否能作用在 long 上，是否能作用在 String 上</h4><p>长整型（long）在目前所有的版本中都是不可以的。</p>
<h4 id="用最有效率的方法计算-2-乘以-8"><a href="#用最有效率的方法计算-2-乘以-8" class="headerlink" title="用最有效率的方法计算 2 乘以 8"></a>用最有效率的方法计算 2 乘以 8</h4><p>2 &lt;&lt; 3</p>
<h4 id="Math-round-11-5-等于多少？Math-round-11-5-等于多少"><a href="#Math-round-11-5-等于多少？Math-round-11-5-等于多少" class="headerlink" title="Math.round(11.5) 等于多少？Math.round(-11.5)等于多少"></a>Math.round(11.5) 等于多少？Math.round(-11.5)等于多少</h4><p>四舍五入的原理是在参数上加 0.5 然后进行下取整。</p>
<h4 id="float-f-3-4-是否正确"><a href="#float-f-3-4-是否正确" class="headerlink" title="float f=3.4;是否正确"></a>float f=3.4;是否正确</h4><p>不正确。精度损失；强制类型转换float f =(float)3.4; 或者写成 float f =3.4F；</p>
<h4 id="short-s1-1-s1-s1-1-有错吗-short-s1-1-s1-1-有错吗"><a href="#short-s1-1-s1-s1-1-有错吗-short-s1-1-s1-1-有错吗" class="headerlink" title="short s1 = 1; s1 = s1 + 1;有错吗?short s1 = 1; s1 += 1;有错吗"></a>short s1 = 1; s1 = s1 + 1;有错吗?short s1 = 1; s1 += 1;有错吗</h4><p>对于 short s1 = 1; s1 = s1 + 1;由于 1 是 int 类型，因此 s1+1 运算结果也是 int型，需要强制转换类型才能赋值给 short 型。</p>
<p>而 short s1 = 1; s1 += 1;可以正确编译，因为 s1+= 1;相当于 s1 = (short(s1 + 1);其中有隐含的强制类型转换。</p>
<h3 id="编码"><a href="#编码" class="headerlink" title="编码"></a>编码</h3><h4 id="Java语言采用何种编码方案？有何特点？"><a href="#Java语言采用何种编码方案？有何特点？" class="headerlink" title="Java语言采用何种编码方案？有何特点？"></a>Java语言采用何种编码方案？有何特点？</h4><p>Java语言采用Unicode编码标准，Unicode（标准码），它为每个字符制订了一个唯一的数值，因此在任何的语言，平台，程序都可以放心的使用。</p>
<h3 id="注释"><a href="#注释" class="headerlink" title="注释"></a>注释</h3><h4 id="什么Java注释"><a href="#什么Java注释" class="headerlink" title="什么Java注释"></a>什么Java注释</h4><p><strong>定义</strong>：用于解释说明程序的文字</p>
<h3 id="访问修饰符"><a href="#访问修饰符" class="headerlink" title="访问修饰符"></a>访问修饰符</h3><p><img src="https://lixiangbetter.github.io/2020/06/20/Java%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E7%AC%94%E8%AE%B0/aHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL0pvdXJXb24vaW1hZ2UvbWFzdGVyL0phdmElRTUlOUYlQkElRTclQTElODAlRTglQUYlQUQlRTYlQjMlOTUvSmF2YSVFOCVBRSVCRiVFOSU5NyVBRSVFNCVCRiVBRSVFOSVBNSVCMCVFNyVBQyVBNi5wbmc.jpeg" alt="not found"></p>
<h3 id="运算符"><a href="#运算符" class="headerlink" title="运算符"></a>运算符</h3><h4 id="amp-和-amp-amp-的区别"><a href="#amp-和-amp-amp-的区别" class="headerlink" title="&amp;和&amp;&amp;的区别"></a>&amp;和&amp;&amp;的区别</h4><p>&amp;运算符有两种用法：(1)按位与；(2)逻辑与。</p>
<p>&amp;&amp;运算符是短路与运算。</p>
<h3 id="关键字"><a href="#关键字" class="headerlink" title="关键字"></a>关键字</h3><h4 id="Java-有没有-goto"><a href="#Java-有没有-goto" class="headerlink" title="Java 有没有 goto"></a>Java 有没有 goto</h4><p>goto 是 Java 中的保留字，在目前版本的 Java 中没有使用。</p>
<h4 id="final-有什么用？"><a href="#final-有什么用？" class="headerlink" title="final 有什么用？"></a>final 有什么用？</h4><ul>
<li>被final修饰的类不可以被继承</li>
<li>被final修饰的方法不可以被重写</li>
<li>被final修饰的变量不可以被改变，被final修饰不可变的是变量的引用，而不是引用指向的内容，引用指向的内容是可以改变的</li>
</ul>
<h4 id="final-finally-finalize区别"><a href="#final-finally-finalize区别" class="headerlink" title="final finally finalize区别"></a>final finally finalize区别</h4><ul>
<li>final可以修饰类、变量、方法</li>
<li>finally处理异常的时候，将一定要执行的代码方法finally代码块</li>
<li>finalize是一个方法，属于Object类的一个方法，而Object类是所有类的父类，该方法一般由垃圾回收器来调<br>用，当我们调用System.gc() 方法的时候，由垃圾回收器调用finalize()，回收垃圾，一个对象是否可回收的<br>最后判断。</li>
</ul>
<h4 id="this关键字的用法"><a href="#this关键字的用法" class="headerlink" title="this关键字的用法"></a>this关键字的用法</h4><p>1.普通的直接引用，this相当于是指向当前对象本身。</p>
<p>2.形参与成员名字重名，用this来区分：</p>
<p>3.引用本类的构造函数</p>
<h4 id="super关键字的用法"><a href="#super关键字的用法" class="headerlink" title="super关键字的用法"></a>super关键字的用法</h4><p>1.普通的直接引用</p>
<p>2.子类成员变量或方法与父类中同名时，用super进行区分</p>
<p>3.引用父类构造函数</p>
<h4 id="this与super的区别"><a href="#this与super的区别" class="headerlink" title="this与super的区别"></a>this与super的区别</h4><ul>
<li>super:　它引用当前对象的直接父类中的成员</li>
<li>this：它代表当前对象名</li>
<li>super()在子类中调用父类的构造方法，this()在本类内调用本类的其它构造方</li>
<li>super()和this()均需放在构造方法内第一行。</li>
<li>尽管可以用this调用一个构造器，但却不能调用两个。</li>
<li>this和super不能同时出现在一个构造函数里面</li>
<li>this()和super()都指的是对象，所以，均不可以在static环境中使用。包括：static变量,static方法，static语句块。</li>
<li>从本质上讲，this是一个指向本对象的指针, 然而super是一个Java关键字。</li>
</ul>
<h4 id="static存在的主要意义"><a href="#static存在的主要意义" class="headerlink" title="static存在的主要意义"></a>static存在的主要意义</h4><p>static的主要意义是在于创建独立于具体对象的域变量或者方法。以致于即使没有创建对象，也能使用属性和调用方法。为什么说static块可以用来优化程序性能，是因为它的特性:<strong>只会在类加载的时候执行一次</strong>。将一些只需要进行一次的初始化操作都放在static代码块中进行。</p>
<h4 id="static的独特之处"><a href="#static的独特之处" class="headerlink" title="static的独特之处"></a>static的独特之处</h4><p>1、被static修饰的变量或者方法是独立于该类的任何对象，也就是说，这些变量和方法不属于任何一个实例对象，而是被类的实例对象所共享。</p>
<p>2、在该类被第一次加载的时候，就会去加载被static修饰的部分，而且只在类第一次使用时加载并进行初始化</p>
<p>3、static变量值在类加载的时候分配空间，以后创建类对象的时候不会重新分配。</p>
<p>4、被static修饰的变量或者方法是优先于对象存在的，也就是说当一个类加载完毕之后，即便没有创建对象，也可以去访问。</p>
<h4 id="static应用场景"><a href="#static应用场景" class="headerlink" title="static应用场景"></a>static应用场景</h4><p>因为static是被类的实例对象所共享，因此如果<strong>某个成员变量是被所有对象所共享的，那么这个成员变量就应该定义为静态变量</strong>。</p>
<p>1、修饰成员变量 2、修饰成员方法 3、静态代码块 4、修饰类【只能修饰内部类也就是静态内部类】 5、静态导包</p>
<h4 id="static注意事项"><a href="#static注意事项" class="headerlink" title="static注意事项"></a>static注意事项</h4><p>1、静态只能访问静态。 2、非静态既可以访问非静态的，也可以访问静态的。</p>
<h3 id="流程控制语句"><a href="#流程控制语句" class="headerlink" title="流程控制语句"></a>流程控制语句</h3><h4 id="break-continue-return-的区别及作用"><a href="#break-continue-return-的区别及作用" class="headerlink" title="break ,continue ,return 的区别及作用"></a>break ,continue ,return 的区别及作用</h4><p>break 结束当前的循环体</p>
<p>continue 结束正在执行的循环 进入下一个循环条件</p>
<p>return 结束当前的方法 直接返回</p>
<h4 id="在-Java-中，如何跳出当前的多重嵌套循环"><a href="#在-Java-中，如何跳出当前的多重嵌套循环" class="headerlink" title="在 Java 中，如何跳出当前的多重嵌套循环"></a>在 Java 中，如何跳出当前的多重嵌套循环</h4><p>标号+break 语句，即可跳出外层循环</p>
<h2 id="面向对象"><a href="#面向对象" class="headerlink" title="面向对象"></a>面向对象</h2><h3 id="面向对象概述"><a href="#面向对象概述" class="headerlink" title="面向对象概述"></a>面向对象概述</h3><h4 id="面向对象和面向过程的区别"><a href="#面向对象和面向过程的区别" class="headerlink" title="面向对象和面向过程的区别"></a>面向对象和面向过程的区别</h4><p>面向过程：</p>
<p>优点：性能比面向对象高，因为类调用时需要实例化，开销比较大，比较消耗资源;比如单片机、嵌入式开发、Linux/Unix等一般采用面向过程开发，性能是最重要的因素。</p>
<p>缺点：没有面向对象易维护、易复用、易扩展</p>
<p>面向对象：</p>
<p>优点：易维护、易复用、易扩展，由于面向对象有封装、继承、多态性的特性，可以设计出低耦合的系统，使系统更加灵活、更加易于维护</p>
<p>缺点：性能比面向过程低</p>
<p>面向对象的底层其实还是面向过程，把面向过程抽象成类，然后封装，方便我们使用的就是面向对象了。</p>
<h3 id="面向对象三大特性"><a href="#面向对象三大特性" class="headerlink" title="面向对象三大特性"></a>面向对象三大特性</h3><h4 id="面向对象的特征有哪些方面"><a href="#面向对象的特征有哪些方面" class="headerlink" title="面向对象的特征有哪些方面"></a>面向对象的特征有哪些方面</h4><p>抽象：抽象是将一类对象的共同特征总结出来构造类的过程，包括数据抽象和行为抽象两方面。抽象只关注对象有哪些属性和行为，并不关注这些行为的细节是什么。</p>
<p>其中Java 面向对象编程三大特性：封装 继承 多态</p>
<p>封装：隐藏对象的属性和实现细节，仅对外提供公共访问方式，将变化隔离，便于使用，提高复用性和安全性。</p>
<p>继承：继承是使用已存在的类的定义作为基础建立新类的技术，新类的定义可以增加新的数据或新的功能，也可以用父类的功能，但不能选择性地继承父类。通过使用继承可以提高代码复用性。继承是多态的前提。</p>
<p>关于继承如下 3 点请记住：</p>
<p>子类拥有父类非 private 的属性和方法。</p>
<p>子类可以拥有自己属性和方法，即子类可以对父类进行扩展。</p>
<p>子类可以用自己的方式实现父类的方法。</p>
<p>多态性：父类或接口定义的引用变量可以指向子类或具体实现类的实例对象。提高了程序的拓展性。</p>
<p>在Java中有两种形式可以实现多态：继承（多个子类对同一方法的重写）和接口（实现接口并覆盖接口中同一方法）。</p>
<p>方法重载（overload）实现的是编译时的多态性（也称为前绑定），而方法重写（override）实现的是运行时的多态性（也称为后绑定）。</p>
<p>一个引用变量到底会指向哪个类的实例对象，该引用变量发出的方法调用到底是哪个类中实现的方法，必须在由程序运行期间才能决定。运行时的多态是面向对象最精髓的东西，要实现多态需要做两件事：</p>
<p>方法重写（子类继承父类并重写父类中已有的或抽象的方法）；<br>对象转型（用父类型引用子类型对象，这样同样的引用调用同样的方法就会根据子类对象的不同而表现出不同的行为）。</p>
<h4 id="什么是多态机制？Java语言是如何实现多态的？"><a href="#什么是多态机制？Java语言是如何实现多态的？" class="headerlink" title="什么是多态机制？Java语言是如何实现多态的？"></a>什么是多态机制？Java语言是如何实现多态的？</h4><p>所谓多态就是指程序中定义的引用变量所指向的具体类型和通过该引用变量发出的方法调用在编程时并不确定，而是在程序运行期间才确定，即一个引用变量到底会指向哪个类的实例对象，该引用变量发出的方法调用到底是哪个类中实现的方法，必须在由程序运行期间才能决定。</p>
<p>多态的实现</p>
<p>Java实现多态有三个必要条件：继承、重写、向上转型。</p>
<p>继承：在多态中必须存在有继承关系的子类和父类。</p>
<p>重写：子类对父类中某些方法进行重新定义，在调用这些方法时就会调用子类的方法。</p>
<p>向上转型：在多态中需要将子类的对象赋给父类引用，只有这样该引用才能够具备技能调用父类的方法和子类的方法。</p>
<p>只有满足了上述三个条件，我们才能够在同一个继承结构中使用统一的逻辑实现代码处理不同的对象，从而达到执行不同的行为。</p>
<p>对于Java而言，它多态的实现机制遵循一个原则：当超类对象引用变量引用子类对象时，被引用对象的类型而不是引用变量的类型决定了调用谁的成员方法，但是这个被调用的方法必须是在超类中定义过的，也就是说被子类覆盖的方法。</p>
<h4 id="多态的实现方式"><a href="#多态的实现方式" class="headerlink" title="多态的实现方式"></a>多态的实现方式</h4><ul>
<li>重写/重载</li>
<li>接口</li>
<li>抽象类和抽象方法</li>
</ul>
<h4 id="面向对象五大基本原则是什么（可选）"><a href="#面向对象五大基本原则是什么（可选）" class="headerlink" title="面向对象五大基本原则是什么（可选）"></a>面向对象五大基本原则是什么（可选）</h4><ul>
<li>单一职责原则SRP(Single Responsibility Principle)<br>类的功能要单一</li>
<li>开放封闭原则OCP(Open－Close Principle)<br>增加功能热烈欢迎，想要修改，哼，一万个不乐意。</li>
<li>里式替换原则LSP(the Liskov Substitution Principle LSP)<br>子类可以替换父类出现在父类能够出现的任何地方。比如你能代表你爸去你姥姥家干活。哈哈~~</li>
<li>依赖倒置原则DIP(the Dependency Inversion Principle DIP)<br>高层次的模块不应该依赖于低层次的模块，他们都应该依赖于抽象。抽象不应该依赖于具体实现，具体实现应该依赖于抽象。</li>
<li>接口分离原则ISP(the Interface Segregation Principle ISP)<br>设计时采用多个与特定客户类有关的接口比采用一个通用的接口要好。就比如一个手机拥有打电话，看视频，玩游戏等功能，把这几个功能拆分成不同的接口，比在一个接口里要好的多。</li>
</ul>
<h3 id="类与接口"><a href="#类与接口" class="headerlink" title="类与接口"></a>类与接口</h3><h4 id="抽象类和接口的对比"><a href="#抽象类和接口的对比" class="headerlink" title="抽象类和接口的对比"></a>抽象类和接口的对比</h4><p>抽象类是用来捕捉子类的通用特性的。接口是抽象方法的集合。</p>
<p>从设计层面来说，抽象类是对类的抽象，是一种模板设计，接口是行为的抽象，是一种行为的规范。</p>
<h4 id="普通类和抽象类有哪些区别？"><a href="#普通类和抽象类有哪些区别？" class="headerlink" title="普通类和抽象类有哪些区别？"></a>普通类和抽象类有哪些区别？</h4><ul>
<li>普通类不能包含抽象方法，抽象类可以包含抽象方法。</li>
<li>抽象类不能直接实例化，普通类可以直接实例化。</li>
</ul>
<h4 id="抽象类能使用-final-修饰吗？"><a href="#抽象类能使用-final-修饰吗？" class="headerlink" title="抽象类能使用 final 修饰吗？"></a>抽象类能使用 final 修饰吗？</h4><p>final不可继承</p>
<h4 id="创建一个对象用什么关键字？对象实例与对象引用有何不同？"><a href="#创建一个对象用什么关键字？对象实例与对象引用有何不同？" class="headerlink" title="创建一个对象用什么关键字？对象实例与对象引用有何不同？"></a>创建一个对象用什么关键字？对象实例与对象引用有何不同？</h4><p>new，new创建对象实例（对象实例在堆内存中），对象引用指向对象实例（对象引用存放在栈内存中）。</p>
<h3 id="变量与方法"><a href="#变量与方法" class="headerlink" title="变量与方法"></a>变量与方法</h3><h4 id="成员变量与局部变量的区别有哪些"><a href="#成员变量与局部变量的区别有哪些" class="headerlink" title="成员变量与局部变量的区别有哪些"></a>成员变量与局部变量的区别有哪些</h4><p>变量：在程序执行的过程中，在某个范围内其值可以发生改变的量。从本质上讲，变量其实是内存中的一小块区域</p>
<p>成员变量：方法外部，类内部定义的变量</p>
<p>局部变量：类的方法中的变量。</p>
<p>成员变量和局部变量的区别</p>
<p>作用域</p>
<p>成员变量：整个类<br>局部变量：某个范围。(一般指的就是方法,语句体内)</p>
<p>存储位置</p>
<p>成员变量：随着对象的创建而存在，随着对象的消失而消失，存储在堆内存中。<br>局部变量：在方法被调用，或者语句被执行的时候存在，存储在栈内存中。当方法调用完，或者语句结束后，就自动释放。</p>
<p>生命周期</p>
<p>成员变量：随着对象的创建而存在，随着对象的消失而消失<br>局部变量：当方法调用完，或者语句结束后，就自动释放。</p>
<p>初始值</p>
<p>成员变量：有</p>
<p>局部变量：无，使用前必须赋值。</p>
<p>使用原则</p>
<p>在使用变量时需要遵循的原则为：就近原则<br>首先在局部范围找，有就使用；接着在成员位置找。</p>
<h4 id="在Java中定义一个不做事且没有参数的构造方法的作用"><a href="#在Java中定义一个不做事且没有参数的构造方法的作用" class="headerlink" title="在Java中定义一个不做事且没有参数的构造方法的作用"></a>在Java中定义一个不做事且没有参数的构造方法的作用</h4><p>Java程序在执行子类的构造方法之前，如果没有用super()来调用父类特定的构造方法，则会调用父类中“没有参数的构造方法”。</p>
<h4 id="在调用子类构造方法之前会先调用父类没有参数的构造方法，其目的是？"><a href="#在调用子类构造方法之前会先调用父类没有参数的构造方法，其目的是？" class="headerlink" title="在调用子类构造方法之前会先调用父类没有参数的构造方法，其目的是？"></a>在调用子类构造方法之前会先调用父类没有参数的构造方法，其目的是？</h4><p>帮助子类做初始化工作。</p>
<h4 id="一个类的构造方法的作用是什么？若一个类没有声明构造方法，改程序能正确执行吗？为什么？"><a href="#一个类的构造方法的作用是什么？若一个类没有声明构造方法，改程序能正确执行吗？为什么？" class="headerlink" title="一个类的构造方法的作用是什么？若一个类没有声明构造方法，改程序能正确执行吗？为什么？"></a>一个类的构造方法的作用是什么？若一个类没有声明构造方法，改程序能正确执行吗？为什么？</h4><p>对类对象的初始化工作；可以执行</p>
<h4 id="构造方法有哪些特性？"><a href="#构造方法有哪些特性？" class="headerlink" title="构造方法有哪些特性？"></a>构造方法有哪些特性？</h4><p>名字与类名相同；</p>
<p>没有返回值，但不能用void声明构造函数；</p>
<p>生成类的对象时自动执行，无需调用。</p>
<h4 id="静态变量和实例变量区别"><a href="#静态变量和实例变量区别" class="headerlink" title="静态变量和实例变量区别"></a>静态变量和实例变量区别</h4><p>静态变量：属于类的，在内存中只会有一份</p>
<p>实例变量： 实例变量是属于实例对象的，在内存中，创建几次对象，就有几份成员变量。</p>
<h4 id="静态变量与普通变量区别"><a href="#静态变量与普通变量区别" class="headerlink" title="静态变量与普通变量区别"></a>静态变量与普通变量区别</h4><p>同上</p>
<h4 id="静态方法和实例方法有何不同？"><a href="#静态方法和实例方法有何不同？" class="headerlink" title="静态方法和实例方法有何不同？"></a>静态方法和实例方法有何不同？</h4><p>静态方法和实例方法的区别主要体现在两个方面：</p>
<p>调用静态方法：”类名.方法名”、”对象名.方法名” 实例方法只有后面这种方式</p>
<p>静态方法只允许访问静态成员；实例方法则无此限制</p>
<h4 id="在一个静态方法内调用一个非静态成员为什么是非法的？"><a href="#在一个静态方法内调用一个非静态成员为什么是非法的？" class="headerlink" title="在一个静态方法内调用一个非静态成员为什么是非法的？"></a>在一个静态方法内调用一个非静态成员为什么是非法的？</h4><p>由于静态方法可以不通过对象进行调用</p>
<h4 id="什么是方法的返回值？返回值的作用是什么？"><a href="#什么是方法的返回值？返回值的作用是什么？" class="headerlink" title="什么是方法的返回值？返回值的作用是什么？"></a>什么是方法的返回值？返回值的作用是什么？</h4><p>返回值的作用:接收出结果，使得它可以用于其他的操作。</p>
<h3 id="内部类"><a href="#内部类" class="headerlink" title="内部类"></a>内部类</h3><h4 id="什么是内部类？"><a href="#什么是内部类？" class="headerlink" title="什么是内部类？"></a>什么是内部类？</h4><p>一个类的定义放在另外一个类的定义内部；内部类本身就是类的一个属性，与其他属性定义方式一致。</p>
<h4 id="内部类的分类有哪些"><a href="#内部类的分类有哪些" class="headerlink" title="内部类的分类有哪些"></a>内部类的分类有哪些</h4><h5 id="成员内部类、局部内部类、匿名内部类和静态内部类。"><a href="#成员内部类、局部内部类、匿名内部类和静态内部类。" class="headerlink" title="成员内部类、局部内部类、匿名内部类和静态内部类。"></a><strong>成员内部类、局部内部类、匿名内部类和静态内部类</strong>。</h5><p>匿名内部类以下特点：</p>
<ul>
<li>匿名内部类必须继承一个抽象类或者实现一个接口。</li>
<li>匿名内部类不能定义任何静态成员和静态方法。</li>
<li>当所在的方法的形参需要被匿名内部类使用时，必须声明为 final。</li>
<li>匿名内部类不能是抽象的，它必须要实现继承的类或者实现的接口的所有抽象方法。</li>
</ul>
<h4 id="内部类的优点"><a href="#内部类的优点" class="headerlink" title="内部类的优点"></a>内部类的优点</h4><p>因为它有以下优点：</p>
<ul>
<li>一个内部类对象可以访问创建它的外部类对象的内容，包括私有数据！</li>
<li>内部类不为同一包的其他类所见，具有很好的封装性；</li>
<li>内部类有效实现了“多重继承”，优化 java 单继承的缺陷。</li>
<li>匿名内部类可以很方便的定义回调。</li>
</ul>
<h4 id="内部类有哪些应用场景"><a href="#内部类有哪些应用场景" class="headerlink" title="内部类有哪些应用场景"></a>内部类有哪些应用场景</h4><ol>
<li>一些多算法场合</li>
<li>解决一些非面向对象的语句块。</li>
<li>适当使用内部类，使得代码更加灵活和富有扩展性。</li>
<li>当某个类除了它的外部类，不再被其他的类使用时。</li>
</ol>
<h4 id="局部内部类和匿名内部类访问局部变量的时候，为什么变量必须要加上final？"><a href="#局部内部类和匿名内部类访问局部变量的时候，为什么变量必须要加上final？" class="headerlink" title="局部内部类和匿名内部类访问局部变量的时候，为什么变量必须要加上final？"></a>局部内部类和匿名内部类访问局部变量的时候，为什么变量必须要加上final？</h4><p>因为<strong>生命周期不一致</strong>，加了final，可以确保局部内部类使用的变量与外层的局部变量区分开，解决了这个问题。</p>
<h3 id="重写与重载"><a href="#重写与重载" class="headerlink" title="重写与重载"></a>重写与重载</h3><h4 id="构造器（constructor）是否可被重写（override）"><a href="#构造器（constructor）是否可被重写（override）" class="headerlink" title="构造器（constructor）是否可被重写（override）"></a>构造器（constructor）是否可被重写（override）</h4><p>构造器不能被继承，因此不能被重写，但可以被重载。</p>
<h4 id="重载（Overload）和重写（Override）的区别。重载的方法能否根据返回类型进行区分？"><a href="#重载（Overload）和重写（Override）的区别。重载的方法能否根据返回类型进行区分？" class="headerlink" title="重载（Overload）和重写（Override）的区别。重载的方法能否根据返回类型进行区分？"></a>重载（Overload）和重写（Override）的区别。重载的方法能否根据返回类型进行区分？</h4><p>方法的重载和重写都是实现多态的方式，区别在于前者实现的是编译时的多态性，而后者实现的是运行时的多态性。</p>
<p>重载：发生在同一个类中，方法名相同参数列表不同（参数类型不同、个数不同、顺序不同），与方法返回值和访问修饰符无关，即重载的方法不能根据返回类型进行区分</p>
<p>重写：发生在父子类中，方法名、参数列表必须相同，返回值小于等于父类，抛出的异常小于等于父类，访问修饰符大于等于父类（里氏代换原则）；如果父类方法访问修饰符为private则子类中就不是重写。</p>
<h3 id="对象相等判断"><a href="#对象相等判断" class="headerlink" title="对象相等判断"></a>对象相等判断</h3><h4 id="和-equals-的区别是什么"><a href="#和-equals-的区别是什么" class="headerlink" title="== 和 equals 的区别是什么"></a>== 和 equals 的区别是什么</h4><p>== : (基本数据类型 == 比较的是值，引用数据类型 == 比较的是内存地址)</p>
<p>equals() : 它的作用也是判断两个对象是否相等。但它一般有两种使用情况：</p>
<p>情况1：类没有覆盖 equals() 方法。则通过 equals() 比较该类的两个对象时，等价于通过“==”比较这两个对象。</p>
<p>情况2：类覆盖了 equals() 方法。一般，我们都覆盖 equals() 方法来两个对象的内容相等；若它们的内容相等，则返回 true (即，认为这两个对象相等)。</p>
<h4 id="hashCode-与-equals-重要"><a href="#hashCode-与-equals-重要" class="headerlink" title="hashCode 与 equals (重要)"></a>hashCode 与 equals (重要)</h4><p><strong>hashCode()与equals()的相关规定</strong></p>
<p>如果两个对象相等，则hashcode一定也是相同的</p>
<p>两个对象相等，对两个对象分别调用equals方法都返回true</p>
<p>两个对象有相同的hashcode值，它们也不一定是相等的</p>
<p><strong>因此，equals 方法被覆盖过，则 hashCode 方法也必须被覆盖</strong></p>
<h4 id="对象的相等与指向他们的引用相等，两者有什么不同？"><a href="#对象的相等与指向他们的引用相等，两者有什么不同？" class="headerlink" title="对象的相等与指向他们的引用相等，两者有什么不同？"></a>对象的相等与指向他们的引用相等，两者有什么不同？</h4><p>对象的相等 比的是内存中存放的内容是否相等而 引用相等 比较的是他们指向的内存地址是否相等。</p>
<h3 id="值传递"><a href="#值传递" class="headerlink" title="值传递"></a>值传递</h3><h4 id="当一个对象被当作参数传递到一个方法后，此方法可改变这个对象的属性，并可返回变化后的结果，那么这里到底是值传递还是引用传递"><a href="#当一个对象被当作参数传递到一个方法后，此方法可改变这个对象的属性，并可返回变化后的结果，那么这里到底是值传递还是引用传递" class="headerlink" title="当一个对象被当作参数传递到一个方法后，此方法可改变这个对象的属性，并可返回变化后的结果，那么这里到底是值传递还是引用传递"></a>当一个对象被当作参数传递到一个方法后，此方法可改变这个对象的属性，并可返回变化后的结果，那么这里到底是值传递还是引用传递</h4><p>是值传递。Java 语言的方法调用只支持参数的值传递。当一个对象实例作为一个参数被传递到方法中时，参数的值就是对该对象的引用。对象的属性可以在被调用过程中被改变，但对对象引用的改变是不会影响到调用者的</p>
<h4 id="为什么-Java-中只有值传递"><a href="#为什么-Java-中只有值传递" class="headerlink" title="为什么 Java 中只有值传递"></a>为什么 Java 中只有值传递</h4><p>按值调用(call by value)表示方法接收的是调用者提供的值，而按引用调用（call by reference)表示方法接收的是调用者提供的变量地址。一个方法可以修改传递引用所对应的变量值，而不能修改传递值调用所对应的变量值。</p>
<p><strong>Java程序设计语言总是采用按值调用。也就是说，方法得到的是所有参数值的一个拷贝</strong></p>
<h4 id="值传递和引用传递有什么区别"><a href="#值传递和引用传递有什么区别" class="headerlink" title="值传递和引用传递有什么区别"></a>值传递和引用传递有什么区别</h4><p>值传递：指的是在方法调用时，传递的参数是按值的拷贝传递，传递的是值的拷贝，也就是说传递后就互不相关了。</p>
<p>引用传递：指的是在方法调用时，传递的参数是按引用进行传递，其实传递的引用的地址，也就是变量所对应的内存空间的地址。传递的是值的引用，也就是说传递前和传递后都指向同一个引用（也就是同一个内存空间）。</p>
<h3 id="Java包"><a href="#Java包" class="headerlink" title="Java包"></a>Java包</h3><h4 id="JDK-中常用的包有哪些"><a href="#JDK-中常用的包有哪些" class="headerlink" title="JDK 中常用的包有哪些"></a>JDK 中常用的包有哪些</h4><ul>
<li>java.lang：这个是系统的基础类；</li>
<li>java.io：这里面是所有输入输出有关的类，比如文件操作等；</li>
<li>java.nio：为了完善 io 包中的功能，提高 io 包中性能而写的一个新包；</li>
<li>java.net：这里面是与网络有关的类；</li>
<li>java.util：这个是系统辅助类，特别是集合类；</li>
<li>java.sql：这个是数据库操作的类。</li>
</ul>
<h4 id="import-java和javax有什么区别"><a href="#import-java和javax有什么区别" class="headerlink" title="import java和javax有什么区别"></a>import java和javax有什么区别</h4><p>刚开始的时候 JavaAPI 所必需的包是 java 开头的包，javax 当时只是扩展 API 包来说使用。然而随着时间的推移，javax 逐渐的扩展成为 Java API 的组成部分。</p>
<h2 id="IO流"><a href="#IO流" class="headerlink" title="IO流"></a>IO流</h2><h3 id="java-中-IO-流分为几种"><a href="#java-中-IO-流分为几种" class="headerlink" title="java 中 IO 流分为几种?"></a>java 中 IO 流分为几种?</h3><ul>
<li>按照流的流向分，可以分为输入流和输出流；</li>
<li>按照操作单元划分，可以划分为字节流和字符流；</li>
<li>按照流的角色划分为节点流和处理流。</li>
</ul>
<h3 id="BIO-NIO-AIO-有什么区别"><a href="#BIO-NIO-AIO-有什么区别" class="headerlink" title="BIO,NIO,AIO 有什么区别?"></a>BIO,NIO,AIO 有什么区别?</h3><p>BIO：Block IO 同步阻塞式 IO，就是我们平常使用的传统 IO，它的特点是模式简单使用方便，并发处理能力低。<br>NIO：Non IO 同步非阻塞 IO，是传统 IO 的升级，客户端和服务器端通过 Channel（通道）通讯，实现了多路复用。<br>AIO：Asynchronous IO 是 NIO 的升级，也叫 NIO2，实现了异步非堵塞 IO ，异步 IO 的操作基于事件和回调机制。</p>
<h3 id="Files的常用方法都有哪些？"><a href="#Files的常用方法都有哪些？" class="headerlink" title="Files的常用方法都有哪些？"></a>Files的常用方法都有哪些？</h3><p>Files. exists()：检测文件路径是否存在。<br>Files. createFile()：创建文件。<br>Files. createDirectory()：创建文件夹。<br>Files. delete()：删除一个文件或目录。<br>Files. copy()：复制文件。<br>Files. move()：移动文件。<br>Files. size()：查看文件个数。<br>Files. read()：读取文件。<br>Files. write()：写入文件。</p>
<h2 id="反射"><a href="#反射" class="headerlink" title="反射"></a>反射</h2><h3 id="什么是反射机制？"><a href="#什么是反射机制？" class="headerlink" title="什么是反射机制？"></a>什么是反射机制？</h3><p>JAVA反射机制是在运行状态中，对于任意一个类，都能够知道这个类的所有属性和方法；对于任意一个对象，都能够调用它的任意一个方法和属性；</p>
<h3 id="反射机制优缺点"><a href="#反射机制优缺点" class="headerlink" title="反射机制优缺点"></a>反射机制优缺点</h3><ul>
<li><strong>优点：</strong> 运行期类型的判断，动态加载类，提高代码灵活度。</li>
<li><strong>缺点：</strong> 性能瓶颈：反射相当于一系列解释操作，通知 JVM 要做的事情，性能比直接的java代码要慢很多。</li>
</ul>
<h3 id="反射机制的应用场景有哪些？"><a href="#反射机制的应用场景有哪些？" class="headerlink" title="反射机制的应用场景有哪些？"></a>反射机制的应用场景有哪些？</h3><p>Spring／Hibernate 等框架也大量使用到了反射机制、Class.forName()</p>
<h3 id="Java获取反射的三种方法"><a href="#Java获取反射的三种方法" class="headerlink" title="Java获取反射的三种方法"></a>Java获取反射的三种方法</h3><p>1.通过new对象实现反射机制 2.通过路径实现反射机制 3.通过类名实现反射机制</p>
<h2 id="常用API"><a href="#常用API" class="headerlink" title="常用API"></a>常用API</h2><h3 id="String相关"><a href="#String相关" class="headerlink" title="String相关"></a>String相关</h3><h4 id="字符型常量和字符串常量的区别"><a href="#字符型常量和字符串常量的区别" class="headerlink" title="字符型常量和字符串常量的区别"></a>字符型常量和字符串常量的区别</h4><p>形式上: 单引号一个字符 双引号若干个字符<br>含义上: 字符常量相当于一个整形值(ASCII值),可以参加表达式运算 字符串常量代表一个地址值(该字符串在内存中存放位置)<br>占内存大小：字符常量一个字节 字符串常量占若干个字节(至少一个字符结束标志)</p>
<h4 id="什么是字符串常量池？"><a href="#什么是字符串常量池？" class="headerlink" title="什么是字符串常量池？"></a>什么是字符串常量池？</h4><p>字符串常量池位于堆内存中，专门用来存储字符串常量，可以提高内存的使用率，避免开辟多块空间存储相同的字符串，在创建字符串时 JVM 会首先检查字符串常量池，如果该字符串已经存在池中，则返回它的引用，如果不存在，则实例化一个字符串放到池中，并返回其引用。</p>
<h4 id="String-是最基本的数据类型吗"><a href="#String-是最基本的数据类型吗" class="headerlink" title="String 是最基本的数据类型吗"></a>String 是最基本的数据类型吗</h4><p>不是。Java 中的基本数据类型只有 8 个 ：byte、short、int、long、float、double、char、boolean；除了基本类型（primitive type），剩下的都是引用类型（referencetype）</p>
<h4 id="String有哪些特性"><a href="#String有哪些特性" class="headerlink" title="String有哪些特性"></a>String有哪些特性</h4><ul>
<li>不变性：String 是只读字符串，是一个典型的 immutable 对象，对它进行任何操作，其实都是创建一个新的对象，再把引用指向该对象。不变模式的主要作用在于当一个对象需要被多线程共享并频繁访问时，可以保证数据的一致性。</li>
<li>常量池优化：String 对象创建之后，会在字符串常量池中进行缓存，如果下次创建同样的对象时，会直接返回缓存的引用。</li>
<li>final：使用 final 来定义 String 类，表示 String 类不能被继承，提高了系统的安全性。</li>
</ul>
<h4 id="String为什么是不可变的吗？"><a href="#String为什么是不可变的吗？" class="headerlink" title="String为什么是不可变的吗？"></a>String为什么是不可变的吗？</h4><p>String类利用了final修饰的char类型数组存储字符</p>
<h4 id="String真的是不可变的吗？"><a href="#String真的是不可变的吗？" class="headerlink" title="String真的是不可变的吗？"></a>String真的是不可变的吗？</h4><p>不可变</p>
<p><strong>1) String不可变但不代表引用不可以变</strong></p>
<p><strong>2) 通过反射是可以修改所谓的“不可变”对象</strong></p>
<h4 id="是否可以继承-String-类"><a href="#是否可以继承-String-类" class="headerlink" title="是否可以继承 String 类"></a>是否可以继承 String 类</h4><p>String 类是 final 类，不可以被继承。</p>
<h4 id="String-str-”i”与-String-str-new-String-“i”-一样吗？"><a href="#String-str-”i”与-String-str-new-String-“i”-一样吗？" class="headerlink" title="String str=”i”与 String str=new String(“i”)一样吗？"></a>String str=”i”与 String str=new String(“i”)一样吗？</h4><p>不一样，因为内存的分配方式不一样。String str=”i”的方式，java 虚拟机会将其分配到常量池中；而 String str=new String(“i”) 则会被分到堆内存中。</p>
<h4 id="String-s-new-String-“xyz”-创建了几个字符串对象"><a href="#String-s-new-String-“xyz”-创建了几个字符串对象" class="headerlink" title="String s = new String(“xyz”);创建了几个字符串对象"></a>String s = new String(“xyz”);创建了几个字符串对象</h4><p>两个对象，一个是静态区的”xyz”，一个是用new创建在堆上的对象。</p>
<h4 id="如何将字符串反转？"><a href="#如何将字符串反转？" class="headerlink" title="如何将字符串反转？"></a>如何将字符串反转？</h4><p>使用 StringBuilder 或者 stringBuffer 的 reverse() 方法。</p>
<h4 id="数组有没有-length-方法？String-有没有-length-方法"><a href="#数组有没有-length-方法？String-有没有-length-方法" class="headerlink" title="数组有没有 length()方法？String 有没有 length()方法"></a>数组有没有 length()方法？String 有没有 length()方法</h4><p>数组没有 length()方法 ，有 length 的属性。String 有 length()方法</p>
<h4 id="String-类的常用方法都有那些？"><a href="#String-类的常用方法都有那些？" class="headerlink" title="String 类的常用方法都有那些？"></a>String 类的常用方法都有那些？</h4><p>indexOf()：返回指定字符的索引。<br>charAt()：返回指定索引处的字符。<br>replace()：字符串替换。<br>trim()：去除字符串两端空白。<br>split()：分割字符串，返回一个分割后的字符串数组。<br>getBytes()：返回字符串的 byte 类型数组。<br>length()：返回字符串长度。<br>toLowerCase()：将字符串转成小写字母。<br>toUpperCase()：将字符串转成大写字符。<br>substring()：截取字符串。<br>equals()：字符串比较。</p>
<h4 id="在使用-HashMap-的时候，用-String-做-key-有什么好处？"><a href="#在使用-HashMap-的时候，用-String-做-key-有什么好处？" class="headerlink" title="在使用 HashMap 的时候，用 String 做 key 有什么好处？"></a>在使用 HashMap 的时候，用 String 做 key 有什么好处？</h4><p>HashMap 内部实现是通过 key 的 hashcode 来确定 value 的存储位置，因为字符串是不可变的，所以当创建字符串时，它的 hashcode 被缓存下来，不需要再次计算，所以相比于其他对象更快。</p>
<h4 id="String和StringBuffer、StringBuilder的区别是什么？String为什么是不可变的"><a href="#String和StringBuffer、StringBuilder的区别是什么？String为什么是不可变的" class="headerlink" title="String和StringBuffer、StringBuilder的区别是什么？String为什么是不可变的"></a>String和StringBuffer、StringBuilder的区别是什么？String为什么是不可变的</h4><p>可变性</p>
<p>string对象是不可变的。StringBuilder与StringBuffer都继承自AbstractStringBuilder类，在AbstractStringBuilder中也是使用字符数组保存字符串，char[] value，都是可变的。</p>
<p>线程安全性</p>
<p>String常量，线程安全。StringBuffer对方法加了同步锁或者对调用的方法加了同步锁，所以是线程安全的。StringBuilder并没有对方法进行加同步锁，所以是非线程安全的。</p>
<p>性能</p>
<p>每次对String 类型进行改变的时候，都会生成一个新的String对象，然后将指针指向新的String 对象。StringBuffer每次都会对StringBuffer对象本身进行操作，而不是生成新的对象并改变对象引用。相同情况下使用StirngBuilder 相比使用StringBuffer 仅能获得10%~15% 左右的性能提升，但却要冒多线程不安全的风险。</p>
<p><strong>对于三者使用的总结</strong></p>
<p>如果要操作少量的数据用 = String</p>
<p>单线程操作字符串缓冲区 下操作大量数据 = StringBuilder</p>
<p>多线程操作字符串缓冲区 下操作大量数据 = StringBuffer</p>
<h3 id="Date相关"><a href="#Date相关" class="headerlink" title="Date相关"></a>Date相关</h3><h3 id="包装类相关"><a href="#包装类相关" class="headerlink" title="包装类相关"></a>包装类相关</h3><h4 id="自动装箱与拆箱"><a href="#自动装箱与拆箱" class="headerlink" title="自动装箱与拆箱"></a>自动装箱与拆箱</h4><p><strong>装箱</strong>：将基本类型用它们对应的引用类型包装起来；</p>
<p><strong>拆箱</strong>：将包装类型转换为基本数据类型；</p>
<h4 id="int-和-Integer-有什么区别"><a href="#int-和-Integer-有什么区别" class="headerlink" title="int 和 Integer 有什么区别"></a>int 和 Integer 有什么区别</h4><p>Java 是一个近乎纯洁的面向对象编程语言，但是为了编程的方便还是引入了基本数据类型，但是为了能够将这些基本数据类型当成对象操作，Java 为每一个基本数据类型都引入了对应的包装类型（wrapper class），int 的包装类就是 Integer，从 Java 5 开始引入了自动装箱/拆箱机制，使得二者可以相互转换。</p>
<p>Java 为每个原始类型提供了包装类型：</p>
<p>原始类型: boolean，char，byte，short，int，long，float，double</p>
<p>包装类型：Boolean，Character，Byte，Short，Integer，Long，Float，Double</p>
]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>base</tag>
      </tags>
  </entry>
  <entry>
    <title>java集合笔记</title>
    <url>/2020/06/20/java%E9%9B%86%E5%90%88%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<h1 id="java集合笔记"><a href="#java集合笔记" class="headerlink" title="java集合笔记"></a>java集合笔记</h1><h2 id="集合概述"><a href="#集合概述" class="headerlink" title="集合概述"></a>集合概述</h2><h4 id="什么是集合"><a href="#什么是集合" class="headerlink" title="什么是集合"></a>什么是集合</h4><p>集合框架：用于存储数据的容器。</p>
<p>集合框架是为表示和操作集合而规定的一种统一的标准的体系结构。<br>任何集合框架都包含三大块内容：对外的接口、接口的实现和对集合运算的算法。</p>
<p>接口：表示集合的抽象数据类型。接口允许我们操作集合时不必关注具体实现，从而达到“多态”。在面向对象编程语言中，接口通常用来形成规范。</p>
<p>实现：集合接口的具体实现，是重用性很高的数据结构。</p>
<p>算法：在一个实现了某个集合框架中的接口的对象身上完成某种有用的计算的方法，例如查找、排序等。这些算法通常是多态的，因为相同的方法可以在同一个接口被多个类实现时有不同的表现。事实上，算法是可复用的函数。<br>它减少了程序设计的辛劳。</p>
<p>集合框架通过提供有用的数据结构和算法使你能集中注意力于你的程序的重要部分上，而不是为了让程序能正常运转而将注意力于低层设计上。<br>通过这些在无关API之间的简易的互用性，使你免除了为改编对象或转换代码以便联合这些API而去写大量的代码。 它提高了程序速度和质量。</p>
<h4 id="集合的特点"><a href="#集合的特点" class="headerlink" title="集合的特点"></a>集合的特点</h4><p>集合的特点主要有如下两点：</p>
<ul>
<li>对象封装数据，对象多了也需要存储。集合用于存储对象。</li>
<li>对象的个数确定可以使用数组，对象的个数不确定的可以用集合。因为集合是可变长度的。</li>
</ul>
<h4 id="集合和数组的区别"><a href="#集合和数组的区别" class="headerlink" title="集合和数组的区别"></a>集合和数组的区别</h4><ul>
<li>数组是固定长度的；集合可变长度的。</li>
<li>数组可以存储基本数据类型，也可以存储引用数据类型；集合只能存储引用数据类型。</li>
<li>数组存储的元素必须是同一个数据类型；集合存储的对象可以是不同数据类型。</li>
</ul>
<h4 id="使用集合框架的好处"><a href="#使用集合框架的好处" class="headerlink" title="使用集合框架的好处"></a>使用集合框架的好处</h4><ul>
<li>容量自增长；</li>
<li>提供了高性能的数据结构和算法，使编码更轻松，提高了程序速度和质量；</li>
<li>允许不同 API 之间的互操作，API之间可以来回传递集合；</li>
<li>可以方便地扩展或改写集合，提高代码复用性和可操作性。</li>
<li>通过使用JDK自带的集合类，可以降低代码维护和学习新API成本。</li>
</ul>
<h4 id="常用的集合类有哪些？"><a href="#常用的集合类有哪些？" class="headerlink" title="常用的集合类有哪些？"></a>常用的集合类有哪些？</h4><p>Map接口和Collection接口是所有集合框架的父接口：</p>
<p>Collection接口的子接口包括：Set接口和List接口<br>Map接口的实现类主要有：HashMap、TreeMap、Hashtable、ConcurrentHashMap以及Properties等<br>Set接口的实现类主要有：HashSet、TreeSet、LinkedHashSet等<br>List接口的实现类主要有：ArrayList、LinkedList、Stack以及Vector等</p>
<h4 id="List，Set，Map三者的区别？List、Set、Map-是否继承自-Collection-接口？List、Map、Set-三个接口存取元素时，各有什么特点？"><a href="#List，Set，Map三者的区别？List、Set、Map-是否继承自-Collection-接口？List、Map、Set-三个接口存取元素时，各有什么特点？" class="headerlink" title="List，Set，Map三者的区别？List、Set、Map 是否继承自 Collection 接口？List、Map、Set 三个接口存取元素时，各有什么特点？"></a>List，Set，Map三者的区别？List、Set、Map 是否继承自 Collection 接口？List、Map、Set 三个接口存取元素时，各有什么特点？</h4><p><img src="https://lixiangbetter.github.io/2020/06/20/java%E9%9B%86%E5%90%88%E7%AC%94%E8%AE%B0/aHR0cHM6Ly9pbWcyMDE4LmNuYmxvZ3MuY29tL290aGVyLzE0MDgxODMvMjAxOTExLzE0MDgxODMtMjAxOTExMTkxODQxNDk1NTktMTU3MTU5NTY2OC5qcGc.jpeg" alt="https://lixiangbetter.github.io/2020/06/20/java%E9%9B%86%E5%90%88%E7%AC%94%E8%AE%B0/aHR0cHM6Ly9pbWcyMDE4LmNuYmxvZ3MuY29tL290aGVyLzE0MDgxODMvMjAxOTExLzE0MDgxODMtMjAxOTExMTkxODQxNDk1NTktMTU3MTU5NTY2OC5qcGc.jpeg"></p>
<h4 id="集合框架底层数据结构"><a href="#集合框架底层数据结构" class="headerlink" title="集合框架底层数据结构"></a>集合框架底层数据结构</h4><p>Collection</p>
<ol>
<li>List</li>
</ol>
<ul>
<li>Arraylist： Object数组</li>
<li>Vector： Object数组</li>
<li>LinkedList： 双向循环链表</li>
</ul>
<ol start="2">
<li>Set</li>
</ol>
<ul>
<li>HashSet（无序，唯一）：底层采用 HashMap 来保存元素</li>
<li>LinkedHashSet： LinkedHashSet 继承于HashSet，并且其内部是通过 LinkedHashMap 来实现的。</li>
<li>TreeSet（有序，唯一）： 红黑树(自平衡的排序二叉树。)</li>
</ul>
<p>Map</p>
<ul>
<li>HashMap： JDK1.8之前HashMap由数组+链表组成的；JDK1.8以后链表+红黑树；</li>
<li>LinkedHashMap：LinkedHashMap 继承自 HashMap，所以它的底层仍然是基于拉链式散列结构即由数组和链表或红黑树组成。另外，LinkedHashMap 在上面结构的基础上，增加了一条双向链表，使得上面的结构可以保持键值对的插入顺序。同时通过对链表进行相应的操作，实现了访问顺序相关逻辑。</li>
<li>HashTable： 数组+链表组成的，数组是 HashMap 的主体，链表则是主要为了解决哈希冲突而存在</li>
<li>TreeMap： 红黑树（自平衡的排序二叉树）</li>
</ul>
<h4 id="哪些集合类是线程安全的？"><a href="#哪些集合类是线程安全的？" class="headerlink" title="哪些集合类是线程安全的？"></a>哪些集合类是线程安全的？</h4><ul>
<li>vector：就比arraylist多了个同步化机制（线程安全），因为效率较低，现在已经不太建议使用。在web应用中，特别是前台页面，往往效率（页面响应速度）是优先考虑的。</li>
<li>statck：堆栈类，先进后出。</li>
<li>hashtable：就比hashmap多了个线程安全。</li>
<li>enumeration：枚举，相当于迭代器。</li>
</ul>
<h4 id="Java集合的快速失败机制-“fail-fast”？"><a href="#Java集合的快速失败机制-“fail-fast”？" class="headerlink" title="Java集合的快速失败机制 “fail-fast”？"></a>Java集合的快速失败机制 “fail-fast”？</h4><p>是java集合的一种错误检测机制，当多个线程对集合进行结构上的改变的操作时，有可能会产生 fail-fast 机制。</p>
<p>例如：假设存在两个线程（线程1、线程2），线程1通过Iterator在遍历集合A中的元素，在某个时候线程2修改了集合A的结构（是结构上面的修改，而不是简单的修改集合元素的内容），那么这个时候程序就会抛出 ConcurrentModificationException 异常，从而产生fail-fast机制。</p>
<p>原因：迭代器在遍历时直接访问集合中的内容，并且在遍历过程中使用一个 modCount 变量。集合在被遍历期间如果内容发生变化，就会改变modCount的值。每当迭代器使用hashNext()/next()遍历下一个元素之前，都会检测modCount变量是否为expectedmodCount值，是的话就返回遍历；否则抛出异常，终止遍历。</p>
<p>解决办法：</p>
<p>在遍历过程中，所有涉及到改变modCount值得地方全部加上synchronized。</p>
<p>使用CopyOnWriteArrayList来替换ArrayList</p>
<h4 id="怎么确保一个集合不能被修改？"><a href="#怎么确保一个集合不能被修改？" class="headerlink" title="怎么确保一个集合不能被修改？"></a>怎么确保一个集合不能被修改？</h4><p>可以使用 Collections. unmodifiableCollection(Collection c) 方法来创建一个只读集合，这样改变集合的任何操作都会抛出 Java. lang. UnsupportedOperationException 异常。</p>
<h2 id="Collection接口"><a href="#Collection接口" class="headerlink" title="Collection接口"></a>Collection接口</h2><h3 id="List接口"><a href="#List接口" class="headerlink" title="List接口"></a>List接口</h3><h4 id="迭代器-Iterator-是什么？"><a href="#迭代器-Iterator-是什么？" class="headerlink" title="迭代器 Iterator 是什么？"></a>迭代器 Iterator 是什么？</h4><p>Iterator 接口提供遍历任何 Collection 的接口。迭代器取代了 Java 集合框架中的 Enumeration，迭代器允许调用者在迭代过程中移除元素。</p>
<h4 id="Iterator-怎么使用？有什么特点？"><a href="#Iterator-怎么使用？有什么特点？" class="headerlink" title="Iterator 怎么使用？有什么特点？"></a>Iterator 怎么使用？有什么特点？</h4><p>Iterator 的特点是只能单向遍历，但是更加安全，因为它可以确保，在当前遍历的集合元素被更改的时候，就会抛出 ConcurrentModificationException 异常。</p>
<h4 id="如何边遍历边移除-Collection-中的元素？"><a href="#如何边遍历边移除-Collection-中的元素？" class="headerlink" title="如何边遍历边移除 Collection 中的元素？"></a>如何边遍历边移除 Collection 中的元素？</h4><p>边遍历边修改 Collection 的唯一正确方式是使用 Iterator.remove() 方法</p>
<h4 id="Iterator-和-ListIterator-有什么区别？"><a href="#Iterator-和-ListIterator-有什么区别？" class="headerlink" title="Iterator 和 ListIterator 有什么区别？"></a>Iterator 和 ListIterator 有什么区别？</h4><ul>
<li>Iterator 可以遍历 Set 和 List 集合，而 ListIterator 只能遍历 List。</li>
<li>Iterator 只能单向遍历，而 ListIterator 可以双向遍历（向前/后遍历）。</li>
<li>ListIterator 实现 Iterator 接口，然后添加了一些额外的功能，比如添加一个元素、替换一个元素、获取前面或后面元素的索引位置。</li>
</ul>
<h4 id="遍历一个-List-有哪些不同的方式？每种方法的实现原理是什么？Java-中-List-遍历的最佳实践是什么？"><a href="#遍历一个-List-有哪些不同的方式？每种方法的实现原理是什么？Java-中-List-遍历的最佳实践是什么？" class="headerlink" title="遍历一个 List 有哪些不同的方式？每种方法的实现原理是什么？Java 中 List 遍历的最佳实践是什么？"></a>遍历一个 List 有哪些不同的方式？每种方法的实现原理是什么？Java 中 List 遍历的最佳实践是什么？</h4><ol>
<li>for 循环遍历，基于计数器。在集合外部维护一个计数器，然后依次读取每一个位置的元素，当读取到最后一个元素后停止。</li>
<li>迭代器遍历，Iterator。Iterator 是面向对象的一个设计模式，目的是屏蔽不同数据集合的特点，统一遍历集合的接口。Java 在 Collections 中支持了 Iterator 模式。</li>
<li>foreach 循环遍历。foreach 内部也是采用了 Iterator 的方式实现，使用时不需要显式声明 Iterator 或计数器。优点是代码简洁，不易出错；缺点是只能做简单的遍历，不能在遍历过程中操作数据集合，例如删除、替换。</li>
</ol>
<p>推荐的做法就是，支持 Random Access 的列表可用 for 循环遍历，否则建议用 Iterator 或 foreach 遍历。</p>
<h4 id="说一下-ArrayList-的优缺点"><a href="#说一下-ArrayList-的优缺点" class="headerlink" title="说一下 ArrayList 的优缺点"></a>说一下 ArrayList 的优缺点</h4><p>优点：随机访问快</p>
<p>缺点：插入删除需复制，耗费性能</p>
<h4 id="如何实现数组和-List-之间的转换？"><a href="#如何实现数组和-List-之间的转换？" class="headerlink" title="如何实现数组和 List 之间的转换？"></a>如何实现数组和 List 之间的转换？</h4><ul>
<li>数组转 List：使用 Arrays. asList(array) 进行转换。</li>
<li>List 转数组：使用 List 自带的 toArray() 方法。</li>
</ul>
<h4 id="ArrayList-和-LinkedList-的区别是什么？"><a href="#ArrayList-和-LinkedList-的区别是什么？" class="headerlink" title="ArrayList 和 LinkedList 的区别是什么？"></a>ArrayList 和 LinkedList 的区别是什么？</h4><p>数据结构实现：ArrayList 动态数组，而 LinkedList 双向链表<br>随机访问效率：ArrayList 更好<br>增加和删除效率：LinkedList 更好<br>内存空间占用：LinkedList 比 ArrayList 更占内存，因为 LinkedList 的节点除了存储数据，还存储了两个引用<br>线程安全：都不保证线程安全；<br>综合来说，在需要频繁读取集合中的元素时，更推荐使用 ArrayList，而在插入和删除操作较多时，更推荐使用 LinkedList。</p>
<h4 id="ArrayList-和-Vector-的区别是什么？"><a href="#ArrayList-和-Vector-的区别是什么？" class="headerlink" title="ArrayList 和 Vector 的区别是什么？"></a>ArrayList 和 Vector 的区别是什么？</h4><ul>
<li>线程安全：Vector 使用了 Synchronized 来实现线程同步，是线程安全的，而 ArrayList 是非线程安全的。</li>
<li>性能：ArrayList 在性能方面要优于 Vector。</li>
<li>扩容：ArrayList 和 Vector 都会根据实际的需要动态的调整容量，只不过在 Vector 扩容每次会增加 1 倍，而 ArrayList 只会增加 50%。</li>
</ul>
<h4 id="插入数据时，ArrayList、LinkedList、Vector谁速度较快？阐述-ArrayList、Vector、LinkedList-的存储性能和特性？"><a href="#插入数据时，ArrayList、LinkedList、Vector谁速度较快？阐述-ArrayList、Vector、LinkedList-的存储性能和特性？" class="headerlink" title="插入数据时，ArrayList、LinkedList、Vector谁速度较快？阐述 ArrayList、Vector、LinkedList 的存储性能和特性？"></a>插入数据时，ArrayList、LinkedList、Vector谁速度较快？阐述 ArrayList、Vector、LinkedList 的存储性能和特性？</h4><p>ArrayList、Vector 底层数组方式存储数据。</p>
<p>LinkedList 双向链表，LinkedList 插入速度较快。</p>
<h4 id="多线程场景下如何使用-ArrayList？"><a href="#多线程场景下如何使用-ArrayList？" class="headerlink" title="多线程场景下如何使用 ArrayList？"></a>多线程场景下如何使用 ArrayList？</h4><p>ArrayList 不是线程安全的，如果遇到多线程场景，可以通过 Collections 的 synchronizedList 方法将其转换成线程安全的容器后再使用</p>
<h4 id="为什么-ArrayList-的-elementData-加上-transient-修饰？"><a href="#为什么-ArrayList-的-elementData-加上-transient-修饰？" class="headerlink" title="为什么 ArrayList 的 elementData 加上 transient 修饰？"></a>为什么 ArrayList 的 elementData 加上 transient 修饰？</h4><p>每次序列化时，先调用 defaultWriteObject() 方法序列化 ArrayList 中的非 transient 元素，然后遍历 elementData，只序列化已存入的元素，这样既加快了序列化的速度，又减小了序列化之后的文件大小。</p>
<h4 id="List-和-Set-的区别"><a href="#List-和-Set-的区别" class="headerlink" title="List 和 Set 的区别"></a>List 和 Set 的区别</h4><p>list: 有序 元素可重复 多个null 有索引 for和iterator 检索低效 插入删除高效 </p>
<p>set: 无序 不可重复 一个null iterator 查找高效 插入删除低效</p>
<h3 id="Set接口"><a href="#Set接口" class="headerlink" title="Set接口"></a>Set接口</h3><h4 id="说一下-HashSet-的实现原理？"><a href="#说一下-HashSet-的实现原理？" class="headerlink" title="说一下 HashSet 的实现原理？"></a>说一下 HashSet 的实现原理？</h4><p>HashSet: 底层HashMap hashmap的value统一为PRESENT 底层调用hashmap的方法 hashset不允许重复</p>
<h4 id="HashSet如何检查重复？HashSet是如何保证数据不可重复的？"><a href="#HashSet如何检查重复？HashSet是如何保证数据不可重复的？" class="headerlink" title="HashSet如何检查重复？HashSet是如何保证数据不可重复的？"></a>HashSet如何检查重复？HashSet是如何保证数据不可重复的？</h4><p>检查重复，不仅比较hash值，还要结合equals方法</p>
<p>值作为hashmap的key，所以不会重复</p>
<h3 id="Queue"><a href="#Queue" class="headerlink" title="Queue"></a>Queue</h3><h4 id="BlockingQueue是什么？"><a href="#BlockingQueue是什么？" class="headerlink" title="BlockingQueue是什么？"></a>BlockingQueue是什么？</h4><p>阻塞队列  在进行检索或移除一个元素的时候，它会等待队列变为非空；当在添加一个元素时，它会等待队列中的可用空间</p>
<h4 id="在-Queue-中-poll-和-remove-有什么区别？"><a href="#在-Queue-中-poll-和-remove-有什么区别？" class="headerlink" title="在 Queue 中 poll()和 remove()有什么区别？"></a>在 Queue 中 poll()和 remove()有什么区别？</h4><p>相同点：返回第一元素，并删除</p>
<p>不同点：没有元素，poll返回null remove抛出异常NoSuchElementException</p>
<h2 id="Map接口"><a href="#Map接口" class="headerlink" title="Map接口"></a>Map接口</h2><h4 id="说一下-HashMap-的实现原理？"><a href="#说一下-HashMap-的实现原理？" class="headerlink" title="说一下 HashMap 的实现原理？"></a>说一下 HashMap 的实现原理？</h4><p>概述： HashMap是基于哈希表的Map接口的非同步实现</p>
<p>数组和链表的结合体</p>
<p>1.用key的hashcode作hash计算下标</p>
<p>2.(1)key相同，覆盖原始值；(2)key不同（出现冲突），key-value放入链表</p>
<p>Jdk 1.8中对HashMap的实现做了优化，当链表中的节点数据超过八个之后，该链表会转为红黑树来提高查询效率，从原来的O(n)到O(logn)</p>
<h4 id="HashMap在JDK1-7和JDK1-8中有哪些不同？HashMap的底层实现"><a href="#HashMap在JDK1-7和JDK1-8中有哪些不同？HashMap的底层实现" class="headerlink" title="HashMap在JDK1.7和JDK1.8中有哪些不同？HashMap的底层实现"></a>HashMap在JDK1.7和JDK1.8中有哪些不同？HashMap的底层实现</h4><p>JDK1.8之前: 数据+链表； 之后：链表长度大于阈值（默认为8），链表转为红黑树</p>
<h4 id="HashMap的put方法的具体流程？"><a href="#HashMap的put方法的具体流程？" class="headerlink" title="HashMap的put方法的具体流程？"></a>HashMap的put方法的具体流程？</h4><p>①.判断键值对数组table是否为空或为null，否则执行resize()进行扩容；</p>
<p>②.根据键值key计算hash值得到插入的数组索引i，如果table[i]==null，直接新建节点添加，转向⑥，如果table[i]不为空，转向③；</p>
<p>③.判断table[i]的首个元素是否和key一样，如果相同直接覆盖value，否则转向④，这里的相同指的是hashCode以及equals；</p>
<p>④.判断table[i] 是否为treeNode，即table[i] 是否是红黑树，如果是红黑树，则直接在树中插入键值对，否则转向⑤；</p>
<p>⑤.遍历table[i]，判断链表长度是否大于8，大于8的话把链表转换为红黑树，在红黑树中执行插入操作，否则进行链表的插入操作；遍历过程中若发现key已经存在直接覆盖value即可；</p>
<p>⑥.插入成功后，判断实际存在的键值对数量size是否超多了最大容量threshold，如果超过，进行扩容。</p>
<h4 id="HashMap的扩容操作是怎么实现的？"><a href="#HashMap的扩容操作是怎么实现的？" class="headerlink" title="HashMap的扩容操作是怎么实现的？"></a>HashMap的扩容操作是怎么实现的？</h4><p>①.在jdk1.8中，resize方法是在hashmap中的键值对大于阀值时或者初始化时，就调用resize方法进行扩容；</p>
<p>②.每次扩展的时候，都是扩展2倍；</p>
<p>③.扩展后Node对象的位置要么在原位置，要么移动到原偏移量两倍的位置。</p>
<h4 id="HashMap是怎么解决哈希冲突的？"><a href="#HashMap是怎么解决哈希冲突的？" class="headerlink" title="HashMap是怎么解决哈希冲突的？"></a>HashMap是怎么解决哈希冲突的？</h4><p>什么是哈希：<strong>就是把任意长度的输入通过散列算法，变换成固定长度的输出，该输出就是散列值（哈希值）</strong></p>
<p>基本特性：<strong>根据同一散列函数计算出的散列值如果不同，那么输入值肯定也不同。但是，根据同一散列函数计算出的散列值如果相同，输入值不一定相同</strong></p>
<p>什么是哈希冲突：<strong>当两个不同的输入值，根据同一散列函数计算出相同的散列值的现象，我们就把它叫做碰撞（哈希碰撞）</strong></p>
<p>HashMap的数据结构：<strong>数组的特点是：寻址容易，插入和删除困难；链表的特点是：寻址困难，但插入和删除容易</strong></p>
<p>hash()函数：<strong>与自己右移16位进行异或运算（高低位异或）</strong></p>
<h4 id="能否使用任何类作为-Map-的-key？"><a href="#能否使用任何类作为-Map-的-key？" class="headerlink" title="能否使用任何类作为 Map 的 key？"></a>能否使用任何类作为 Map 的 key？</h4><p>可以，考虑一下几点：</p>
<p>1.重写了 equals() 方法，也应该重写 hashCode() 方法。</p>
<p>2.遵循与 equals() 和 hashCode() 相关的规则。</p>
<p>3.用户自定义 Key 类最佳实践是使之为不可变的</p>
<h4 id="为什么HashMap中String、Integer这样的包装类适合作为Key？"><a href="#为什么HashMap中String、Integer这样的包装类适合作为Key？" class="headerlink" title="为什么HashMap中String、Integer这样的包装类适合作为Key？"></a>为什么HashMap中String、Integer这样的包装类适合作为Key？</h4><p>1.都是final类型，即不可变性，保证key的不可更改性，不会存在获取hash值不同的情况</p>
<p>2.内部已重写了<code>equals()</code>、<code>hashCode()</code>等方法，遵守了HashMap内部的规范</p>
<h4 id="如果使用Object作为HashMap的Key，应该怎么办呢？"><a href="#如果使用Object作为HashMap的Key，应该怎么办呢？" class="headerlink" title="如果使用Object作为HashMap的Key，应该怎么办呢？"></a>如果使用Object作为HashMap的Key，应该怎么办呢？</h4><p>重写<code>hashCode()</code>和<code>equals()</code>方法</p>
<h4 id="HashMap为什么不直接使用hashCode-处理后的哈希值直接作为table的下标？"><a href="#HashMap为什么不直接使用hashCode-处理后的哈希值直接作为table的下标？" class="headerlink" title="HashMap为什么不直接使用hashCode()处理后的哈希值直接作为table的下标？"></a>HashMap为什么不直接使用hashCode()处理后的哈希值直接作为table的下标？</h4><p><code>hashCode()</code>方法返回的是int整数类型，其范围为-(2 ^ 31)~(2 ^ 31 - 1)，约有40亿个映射空间；哈希值可能不在数组大小范围内，进而无法匹配存储位置</p>
<h4 id="HashMap-的长度为什么是2的幂次方"><a href="#HashMap-的长度为什么是2的幂次方" class="headerlink" title="HashMap 的长度为什么是2的幂次方"></a>HashMap 的长度为什么是2的幂次方</h4><p> hash%length==hash&amp;(length-1)的前提是 length 是2的 n 次方；</p>
<h4 id="HashMap-与-HashTable-有什么区别？"><a href="#HashMap-与-HashTable-有什么区别？" class="headerlink" title="HashMap 与 HashTable 有什么区别？"></a>HashMap 与 HashTable 有什么区别？</h4><p>1.线程安全 hashtable用synchronized修饰</p>
<p>2.效率 hashmap效率高</p>
<p>3.对null key的支持 hashmap可以 hashtable报错</p>
<p>4.Hashtable 默认大小11，之后扩充，容量为原来2n+1。HashMap 默认大小16。扩充，原来的2倍</p>
<h4 id="如何决定使用-HashMap-还是-TreeMap？"><a href="#如何决定使用-HashMap-还是-TreeMap？" class="headerlink" title="如何决定使用 HashMap 还是 TreeMap？"></a>如何决定使用 HashMap 还是 TreeMap？</h4><p>对于在Map中插入、删除和定位元素，HashMap最好。然而，对一个有序的key集合进行遍历，TreeMap更好</p>
<h4 id="HashMap-和-ConcurrentHashMap-的区别"><a href="#HashMap-和-ConcurrentHashMap-的区别" class="headerlink" title="HashMap 和 ConcurrentHashMap 的区别"></a>HashMap 和 ConcurrentHashMap 的区别</h4><p>1.JDK1.8之后ConcurrentHashMap启用了一种全新的方式实现,利用CAS算法。</p>
<p>2.hashmap允许null</p>
<h4 id="ConcurrentHashMap-和-Hashtable-的区别？"><a href="#ConcurrentHashMap-和-Hashtable-的区别？" class="headerlink" title="ConcurrentHashMap 和 Hashtable 的区别？"></a>ConcurrentHashMap 和 Hashtable 的区别？</h4><p>1.底层数据结构，ConcurrentHashMap：数组+链表/红黑二叉树；Hashtable：数组+链表</p>
<p>2.<strong>实现线程安全的方式（重要）</strong>：① 在JDK1.7的时候，ConcurrentHashMap（分段锁） 对整个桶数组进行了分割分段(Segment)，每一把锁只锁容器其中一部分数据，多线程访问容器里不同数据段的数据，就不会存在锁竞争，提高并发访问率。（默认分配16个Segment，比Hashtable效率提高16倍。） 到了 JDK1.8 的时候已经摒弃了Segment的概念，而是直接用 Node 数组+链表+红黑树的数据结构来实现，并发控制使用 synchronized 和 CAS 来操作。（JDK1.6以后 对 synchronized锁做了很多优化） 整个看起来就像是优化过且线程安全的 HashMap，虽然在JDK1.8中还能看到 Segment 的数据结构，但是已经简化了属性，只是为了兼容旧版本；② Hashtable(同一把锁) :使用 synchronized 来保证线程安全，效率非常低下。当一个线程访问同步方法时，其他线程也访问同步方法，可能会进入阻塞或轮询状态，如使用 put 添加元素，另一个线程不能使用 put 添加元素，也不能使用 get，竞争会越来越激烈效率越低。</p>
<h4 id="ConcurrentHashMap-底层具体实现知道吗？实现原理是什么？"><a href="#ConcurrentHashMap-底层具体实现知道吗？实现原理是什么？" class="headerlink" title="ConcurrentHashMap 底层具体实现知道吗？实现原理是什么？"></a>ConcurrentHashMap 底层具体实现知道吗？实现原理是什么？</h4><p>JDK1.7: ConcurrentHashMap采用Segment + HashEntry的方式进行实现</p>
<p>JDK1.8: synchronized只锁定当前链表或红黑二叉树的首节点</p>
<h4 id="Array-和-ArrayList-有何区别？"><a href="#Array-和-ArrayList-有何区别？" class="headerlink" title="Array 和 ArrayList 有何区别？"></a>Array 和 ArrayList 有何区别？</h4><p>Array 存储基本数据类型和对象，ArrayList 只能存储对象。<br>Array 是指定固定大小的，而 ArrayList 大小是自动扩展的。<br>Array 内置方法没有 ArrayList 多，比如 addAll、removeAll、iteration 等方法只有 ArrayList 有。</p>
<h4 id="如何实现-Array-和-List-之间的转换？"><a href="#如何实现-Array-和-List-之间的转换？" class="headerlink" title="如何实现 Array 和 List 之间的转换？"></a>如何实现 Array 和 List 之间的转换？</h4><ul>
<li>Array 转 List： Arrays. asList(array) ；</li>
<li>List 转 Array：List 的 toArray() 方法。</li>
</ul>
<h4 id="comparable-和-comparator的区别？"><a href="#comparable-和-comparator的区别？" class="headerlink" title="comparable 和 comparator的区别？"></a>comparable 和 comparator的区别？</h4><ul>
<li>comparable接口实际上是出自java.lang包，它有一个 compareTo(Object obj)方法用来排序</li>
<li>comparator接口实际上是出自 java.util 包，它有一个compare(Object obj1, Object obj2)方法用来排序</li>
</ul>
<h4 id="Collection-和-Collections-有什么区别？"><a href="#Collection-和-Collections-有什么区别？" class="headerlink" title="Collection 和 Collections 有什么区别？"></a>Collection 和 Collections 有什么区别？</h4><ul>
<li>java.util.Collection 是一个集合接口（集合类的一个顶级接口）。它提供了对集合对象进行基本操作的通用接口方法。Collection接口在Java 类库中有很多具体的实现。Collection接口的意义是为各种具体的集合提供了最大化的统一操作方式，其直接继承接口有List与Set。</li>
<li>Collections则是集合类的一个工具类/帮助类，其中提供了一系列静态方法，用于对集合中元素进行排序、搜索以及线程安全等各种操作。</li>
</ul>
<h4 id="TreeMap-和-TreeSet-在排序时如何比较元素？Collections-工具类中的-sort-方法如何比较元素？"><a href="#TreeMap-和-TreeSet-在排序时如何比较元素？Collections-工具类中的-sort-方法如何比较元素？" class="headerlink" title="TreeMap 和 TreeSet 在排序时如何比较元素？Collections 工具类中的 sort()方法如何比较元素？"></a>TreeMap 和 TreeSet 在排序时如何比较元素？Collections 工具类中的 sort()方法如何比较元素？</h4><ol>
<li>TreeSet 要求存放的对象所属的类必须实现 Comparable 接口，该接口提供了比较元素的 compareTo()方法，当插入元素时会回调该方法比较元素的大小。TreeMap 要求存放的键值对映射的键必须实现 Comparable 接口从而根据键对元素进 行排 序。</li>
<li>Collections 工具类的 sort 方法有两种重载的形式，</li>
</ol>
<p>第一种要求传入的待排序容器中存放的对象比较实现 Comparable 接口以实现元素的比较；</p>
<p>第二种不强制性的要求容器中的元素必须可比较，但是要求传入第二个参数，参数是Comparator 接口的子类型（需要重写 compare 方法实现元素的比较），相当于一个临时定义的排序规则，其实就是通过接口注入比较元素大小的算法，也是对回调模式的应用（Java 中对函数式编程的支持）。</p>
]]></content>
      <categories>
        <category>java</category>
      </categories>
      <tags>
        <tag>Collections</tag>
      </tags>
  </entry>
  <entry>
    <title>初识alluxio</title>
    <url>/2020/01/12/%E5%88%9D%E8%AF%86alluxio/</url>
    <content><![CDATA[<p>Alluxio<br>    Tachyon 前身的名字  超光速粒子<br>    以内存为中心的分布式文件系统<br>        HDFS、S3….<br>    介于计算层和存储层之间<br>        计算层：Spark、Flink、MapReduce<br>    存储层在内存中的一个Cache系统<br>    Spark/Alluxio：AMPLab<br>    2012/12 0.1.0<br>    将计算和存储分离    移动计算优于移动数据</p>
<p>能够为我们带来什么？？？</p>
<pre><code>Flink能否替代Spark成为第三代/新一代执行引擎？
Hadoop真的凉了吗？那我还有必须学习Hadoop吗？
Flume吞吐量多少？Spark Application放多少资源？
如何保证数据不丢失

自动动手测试一下

时效性的要求是越来越高的

基于内存  Memory is King   Spark Flink

两面性</code></pre><p>1） 2 Spark Application 需要共享数据，必须通过写XX操作<br>2）基于JVM对数据进行缓存<br>    Spark Application = 1 Driver + N executor<br>3）2 Spark Application操作相同的数据<br>    HDFS ==&gt; WC ==&gt; SINK<br>    HDFS ==&gt; XXX ==&gt; SINK</p>
<p>Alluxio不是Apache的顶级项目<br>    <a href="https://www.alluxio.io/" target="_blank" rel="noopener">https://www.alluxio.io/</a><br>    <a href="https://github.com/Alluxio/alluxio" target="_blank" rel="noopener">https://github.com/Alluxio/alluxio</a></p>
<p>特点：<br>1）原生的API和文件系统的非常类似<br>2）兼容性   Hadoop   Spark  Flink<br>3）列式<br>4）底层文件系统是可插拔的<br>5）Web UI<br>6）Command line interaction<br>    hadoop/hdfs fs -ls …<br>    alluxio fs ….</p>
<p>Spark 两个不同角度的应用进行实战<br>    Spark 离线<br>    Spark 实时</p>
<p>Alluxio部署<br>    1）下载<br>    2）解压到app<br>    3）配置到系统环境变量<br>    4）conf/<br>        alluxio-site.properties<br>        masters<br>        workers<br>    5）格式化<br>    6）启动<br>    7）hadoop000:19999 可以看到Alluxio的Web UI</p>
<p>Alluxio常用的命令行参数<br>    alluxio fs<br>        ls lsr mkdir cat<br>        copyFromLocal copyToLocal mv<br>        pin<br>        count  location</p>
<p>Alluxio和HDFS整合</p>
<p>Alluxio和MapReduce整合</p>
<p>hadoop jar hadoop-mapreduce-examples-2.6.0-cdh5.15.1.jar wordcount -libjars /home/hadoop/app/alluxio-1.8.1/client/alluxio-1.8.1-client.jar alluxio://hadoop000:19998/alluxio/wc/input/hello.txt alluxio://hadoop000:19998/alluxio/wc/output</p>
<p>Alluxio和Spark整合</p>
<p>做了这几个与Alluxio的整合，业务逻辑根本没有发生变化，只是:</p>
<p>1) 环境上变化<br>2) hdfs ==&gt; alluxio</p>
]]></content>
      <categories>
        <category>bigdata</category>
      </categories>
      <tags>
        <tag>alluxio</tag>
      </tags>
  </entry>
  <entry>
    <title>Spark优化笔记</title>
    <url>/2020/01/12/Spark%E4%BC%98%E5%8C%96%E7%AC%94%E8%AE%B0/</url>
    <content><![CDATA[<h5 id="优化杂谈"><a href="#优化杂谈" class="headerlink" title="优化杂谈"></a>优化杂谈</h5><p>优化点一：资源<br>    spark作业在运行的时候能占用多少资源：cpu、memory<br>    分配”足够多“的资源，在一定范围内，增加资源 和 性能提升 成正比的<br>    Spark on YARN 作业跑在规划好的YARN的队列中</p>
<figure class="highlight plain"><table><tr><td class="code"><pre><span class="line">./bin/spark-submit --class org.apache.spark.examples.SparkPi \</span><br><span class="line">    --master yarn \</span><br><span class="line">    --deploy-mode cluster \</span><br><span class="line">    --driver-memory 4g \    # Driver的内存</span><br><span class="line">    --executor-memory 2g \  # 每个Executor的内存</span><br><span class="line">    --executor-cores 1 \    # Executor的cpu core的数量</span><br><span class="line">    --queue thequeue \      # 运行在YARN的哪个队列上</span><br><span class="line">    --num-executors 3 \     # Executor的数量 </span><br><span class="line">    examples/jars/spark-examples*.jar \</span><br><span class="line">    10</span><br></pre></td></tr></table></figure>

<pre><code>送你们一句话：尽量将你的作业使用的资源调整到最大

YARN: pkspark  400G 100C
    50exe ==&gt; 
        executor-memory = 8G
        executor-cores  = 2C

num-executors + :    task的并行度  num*cores    
    4exe 2core = 8task
    8exe 2core = 16task
    100task 

executor-cores + : task的并行度

executor-memory + :
    能cache的数据多 ==&gt; 写入disk的次数会降低
    shuffle   IO
    JVM   GC

思考：Spark ETL HBase 运行在YARN之上</code></pre><p>调优之算子的选择<br>    map<br>        def map[U: ClassTag](f: T =&gt; U): RDD[U]</p>
<pre><code>mapPartitions
    def mapPartitions[U: ClassTag](
          f: Iterator[T] =&gt; Iterator[U],
          preservesPartitioning: Boolean = false): RDD[U]

transforamtion:转换算子

RDD = 2Partitions (2 * 1w = 2w)
    map  2w
    mapPartitions  2 </code></pre><p>QA：转换算子能生成Job吗？</p>
<pre><code>foreach 
    def foreach(f: T =&gt; Unit)

foreachPartitions
    def foreachPartition(f: Iterator[T] =&gt; Unit)

Action算子

送你们一句话：如果涉及到写数据库操作，
    建议采用带Partitions的，但是由于mapPartitions是一个transforamtion算子，所以建议采用foreachPartitions

    OOM
    使用之前：
        评估你要处理的RDD的数据量
        每个partition的数据量
        整个作业使用到的资源</code></pre><p>生产或者面试：Spark自定义排序</p>
<p>class 和 case class在使用层面有什么区别？？？</p>
<p>Spark Streaming对接Kafka数据<br>    对于Kafka来说，我们的Spark Streaming应用程序其实就是一个消费者</p>
<pre><code>1） Spark Streaming挂了，那么就没有办法去消费Kafka中的数据了，Kafka中的数据就会有积压
2） 高峰期的时候，由于你作业的资源并没有很好的设置，在某些批次中，很可能数据比较大

batch时间到了，那么Spark Streaming就会处理这个批次中的数据
假设：batch time 10s  就会出现10s你根本处理不过来整个批次的数据
后续批次的作业就会产生挤压，那么时效性就没有办法保证

==&gt; Kafka的限速
假设限速是100


10秒一个批次
    topic 是1个分区：10 * 1 * 100 = 1000
    topic 是3个分区：10 * 3 * 100 = 3000

要提升数据处理的吞吐量：提升Kafka的分区数    </code></pre><p>Spark Streaming对接Kafka数据进行处理时，能否保证仅处理一次的语义<br>    至少一次：可能数据消费重复<br>    至多一次：可能数据有丢失<br>    仅仅一次：不会有数据的丢失，也不会重复消费   ✅</p>
<pre><code>能？ 怎么做？
不能做到？还能用吗？</code></pre><p>广播<br>    join： shuffle/reduce join   mapjoin</p>
<p>val o = xxxx   // 20M  算子的外部变量<br>rdd.map(x =&gt; {</p>
<pre><code>//....
o</code></pre><p>})    </p>
<p>每个task都会获得一份变量o的副本</p>
<p>20executor  500task ==&gt; 500 * 20M = 10G</p>
<p>如果使用了广播变量：<br>    每个executor保存一个变量o的副本</p>
<pre><code>20 * 20m = 400M </code></pre>]]></content>
      <categories>
        <category>bigdata</category>
      </categories>
      <tags>
        <tag>spark</tag>
      </tags>
  </entry>
  <entry>
    <title>OLTP和OLAP</title>
    <url>/2020/01/05/OLTP%E5%92%8COLAP/</url>
    <content><![CDATA[<p>1 OLTP和OLAP</p>
<p>online transaction processing，联机事务处理。业务类系统主要供基层人员使用，进行一线业务操作，通常被称为联机事务处理。</p>
<p>online analytical processing，联机分析处理。数据分析的目标是探索并挖掘数据的价值，作为企业高层进行决策的参考。</p>
<p>从功能层面上来看，OLTP负责基本业务的正常运转，业务数据积累所产生的价值信息被OLAP所呈现，根据OLAP所产生的价值信息不断优化基本业务。</p>
<p>2 OLTP</p>
<p>OLTP负责基本业务的正常运转，因此使用基本的关系型数据库就可以了。比如Mysql。</p>
<p>3 OLAP</p>
<p>基本业务生成的数据越来越多，目前流行的是分布式的处理方案，即sql on hadoop。比如百度的关系数据仓储Palo。</p>
<p>MPP架构的数据仓储是典型的OLAP应用。</p>
<p>4 MPP</p>
<p>massively parallel processing，大规模并行处理。比如非共享数据库集群。</p>
<p>图示<br><img src="https://lixiangbetter.github.io/2020/01/05/OLTP%E5%92%8COLAP/2018072122222525.png" alt="not found"></p>
<p>图片转载：<a href="https://blog.csdn.net/qq_33414271/article/details/81149966" target="_blank" rel="noopener">https://blog.csdn.net/qq_33414271/article/details/81149966</a></p>
]]></content>
      <categories>
        <category>bigdata</category>
      </categories>
      <tags>
        <tag>olap</tag>
      </tags>
  </entry>
  <entry>
    <title>浅谈四层和七层负载均衡</title>
    <url>/2020/01/05/%E6%B5%85%E8%B0%88%E5%9B%9B%E5%B1%82%E5%92%8C%E4%B8%83%E5%B1%82%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1/</url>
    <content><![CDATA[<h4 id="浅谈四层和七层负载"><a href="#浅谈四层和七层负载" class="headerlink" title="浅谈四层和七层负载"></a>浅谈四层和七层负载</h4><p>关于负载均衡，经常听到四层负载均衡和七层负载均衡的说法，他们之间有什么关系和区别呢，今天就简单总结概括下。</p>
<h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><blockquote>
<p>用一句话来说，<strong>四层负载均衡就是工作在计算机网络OSI七层分层的第四层（传输层）的，七层负载军和则是工作在第七层（应用层）的</strong>。</p>
</blockquote>
<p>也就是说，四层负载均衡是<strong>基于IP+端口</strong>的负载均衡，七层负载均衡是<strong>基于URL</strong>等应用层信息的负载均衡。</p>
<p>同理，还有基于MAC地址的二层负载均衡和基于IP地址的三层负载均衡。</p>
<blockquote>
<ul>
<li><strong>二层负载均衡(mac)</strong><br> 一般是用<code>虚拟mac地址</code>方式,外部对虚拟MAC地址请求,负载均衡接收后分配后端实际的MAC地址响应。</li>
<li><strong>三层负载均衡(ip)</strong><br> 一般采用<code>虚拟IP地址</code>方式,外部对虚拟的ip地址请求,负载均衡接收后分配后端实际的IP地址响应。</li>
<li><strong>四层负载均衡(tcp)</strong><br> 用<code>虚拟ip+port</code>接收请求,再转发到对应的真实机器。</li>
<li><strong>七层负载均衡(http)</strong><br> 用<code>虚拟的url或主机名</code>接收请求,再转向相应的处理服务器。</li>
</ul>
</blockquote>
<p>在实际应用中,比较常见的就是四层负载及七层负载。这里也重点说下这两种负载。</p>
<p>  所谓的四到七层负载均衡，就是在对后台的服务器进行负载均衡时，<strong>依据四层的信息或七层的信息来决定怎么样转发流量</strong>。 比如四层的负载均衡，就是通过发布三层的IP地址（VIP），然后加四层的端口号，来决定哪些流量需要做负载均衡，对需要处理的流量进行NAT处理，转发至后台服务器，并记录下这个TCP或者UDP的流量是由哪台服务器处理的，后续这个连接的所有流量都同样转发到同一台服务器处理。七层的负载均衡，就是在四层的基础上（没有四层是绝对不可能有七层的），再考虑应用层的特征，比如同一个Web服务器的负载均衡，除了根据VIP加80端口辨别是否需要处理的流量，还可根据七层的URL、浏览器类别、语言来决定是否要进行负载均衡。举个例子，如果你的Web服务器分成两组，一组是中文语言的，一组是英文语言的，那么七层负载均衡就可以当用户来访问你的域名时，自动辨别用户语言，然后选择对应的语言服务器组进行负载均衡处理。</p>
<hr>
<h3 id="具体区别"><a href="#具体区别" class="headerlink" title="具体区别"></a>具体区别</h3><p>负载均衡器通常称为<strong>四层交换机</strong>或<strong>七层交换机</strong>。那么四层和七层两者到底区别在哪里？</p>
<h5 id="1-技术原理区别"><a href="#1-技术原理区别" class="headerlink" title="1. 技术原理区别"></a>1. 技术原理区别</h5><ul>
<li>所谓<strong>四层负载均衡</strong>，也就是主要通过报文中的目标地址和端口，再加上负载均衡设备设置的服务器选择方式，决定最终选择的内部服务器。</li>
</ul>
<p>  以常见的TCP为例，负载均衡设备在接收到第一个来自客户端的SYN 请求时，即通过上述方式选择一个最佳的服务器，并对报文中目标IP地址进行修改(改为后端服务器IP），直接转发给该服务器。TCP的连接建立，即<strong>三次握手是客户端和服务器直接建立的，负载均衡设备只是起到一个类似路由器的转发动作</strong>。在某些部署情况下，为保证服务器回包可以正确返回给负载均衡设备，在转发报文的同时可能还会对报文原来的源地址进行修改。<br><img src="/Users/lx/Documents/myblog/source/_posts/%E6%B5%85%E8%B0%88%E5%9B%9B%E5%B1%82%E5%92%8C%E4%B8%83%E5%B1%82%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1/1038472-618c1fc22f893b96.jpg" alt="not found"></p>
<p>四层和七层交换机原理</p>
<ul>
<li>所谓<strong>七层负载均衡</strong>，也称为“内容交换”，也就是主要通过报文中的真正有意义的应用层内容，再加上负载均衡设备设置的服务器选择方式，决定最终选择的内部服务器。</li>
</ul>
<p>  以常见的TCP为例，负载均衡设备如果要根据真正的应用层内容再选择服务器，只能先代理最终的服务器和客户端建立连接(三次握手)后，才可能接受到客户端发送的真正应用层内容的报文，然后再根据该报文中的特定字段，再加上负载均衡设备设置的服务器选择方式，决定最终选择的内部服务器。<strong>负载均衡设备在这种情况下，更类似于一个代理服务器</strong>。负载均衡和前端的客户端以及后端的服务器会分别建立TCP连接。所以从这个技术原理上来看，七层负载均衡明显的对负载均衡设备的要求更高，处理七层的能力也必然会低于四层模式的部署方式。</p>
<h5 id="2-应用场景区别"><a href="#2-应用场景区别" class="headerlink" title="2.应用场景区别"></a>2.应用场景区别</h5><p>  七层因为可以代理任意修改和处理用户的请求，所以可以使整个应用更加智能化和安全，代价就是设计和配置会更复杂。所以是否有必要使用七层负载均衡是一个需要权衡的问题。</p>
<p>  现在的7层负载均衡，主要还是着重于应用HTTP协议，所以其应用范围主要是众多的网站或者内部信息平台等基于B/S开发的系统。 4层负载均衡则对应其他TCP应用，例如基于C/S开发的ERP等系统。</p>
<p>原文链接：<a href="https://www.jianshu.com/p/04518b017c90" target="_blank" rel="noopener">https://www.jianshu.com/p/04518b017c90</a></p>
]]></content>
      <categories>
        <category>linux</category>
      </categories>
      <tags>
        <tag>负载均衡</tag>
      </tags>
  </entry>
  <entry>
    <title>storm创建DRPC远程客户端</title>
    <url>/2019/12/11/storm%E5%88%9B%E5%BB%BAdrpc%E8%BF%9C%E7%A8%8B%E5%AE%A2%E6%88%B7%E7%AB%AF/</url>
    <content><![CDATA[<h4 id="创建DRPC远程客户端"><a href="#创建DRPC远程客户端" class="headerlink" title="创建DRPC远程客户端"></a>创建DRPC远程客户端</h4><p>DRPCClient创建方法如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line"><span class="comment">//conf map, drpc server, port no, timeout for the call</span></span><br><span class="line"><span class="keyword">new</span> DRPCClient(conf, <span class="string">"192.168.0.217"</span>, <span class="number">3772</span>, <span class="number">5000</span>);</span><br></pre></td></tr></table></figure>

<p>conf如下：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Config conf = <span class="keyword">new</span> Config();</span><br><span class="line">conf.setDebug(<span class="keyword">false</span>);</span><br></pre></td></tr></table></figure>

<p>这将产生下列这个错误：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">java.lang.NullPointerException</span><br><span class="line">java.lang.RuntimeException: java.lang.NullPointerException</span><br><span class="line">at backtype.storm.security.auth.AuthUtils.GetTransportPlugin(AuthUtils.java:<span class="number">230</span>)</span><br><span class="line">at backtype.storm.security.auth.ThriftClient.reconnect(ThriftClient.java:<span class="number">91</span>)</span><br></pre></td></tr></table></figure>

<p>如何添加下列这句话：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">conf.put(<span class="string">"storm.thrift.transport"</span>, <span class="string">"backtype.storm.security.auth.SimpleTransportPlugin"</span>);</span><br></pre></td></tr></table></figure>

<p>这将继续报这个错误：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Don<span class="string">'t know how to convert null to int</span></span><br><span class="line"><span class="string">java.lang.IllegalArgumentException: Don'</span>t know how to convert <span class="keyword">null</span> to <span class="keyword">int</span></span><br><span class="line">at backtype.storm.utils.Utils.getInt(Utils.java:<span class="number">420</span>)</span><br><span class="line">at backtype.storm.security.auth.ThriftClient.reconnect(ThriftClient.java:<span class="number">100</span>)</span><br></pre></td></tr></table></figure>

<p>在storm0.10之后就已经做了改进，使用map来传递配置参数。</p>
<p>正确做法：</p>
<figure class="highlight java"><table><tr><td class="code"><pre><span class="line">Config conf = <span class="keyword">new</span> Config();</span><br><span class="line">Map defaultConfig = Utils.readDefaultConfig();</span><br><span class="line"></span><br><span class="line">defaultConfig.put(<span class="string">"storm.thrift.transport"</span>,<span class="string">"org.apache.storm.security.auth.SimpleTransportPlugin"</span>);</span><br><span class="line">defaultConfig.put(Config.STORM_EXHIBITOR_RETRY_TIMES, <span class="number">3</span>);</span><br><span class="line">defaultConfig.put(Config.STORM_EXHIBITOR_RETRY_INTERVAL, <span class="number">10</span>);</span><br><span class="line">defaultConfig.put(Config.STORM_EXHIBITOR_RETRY_INTERVAL_CEILING, <span class="number">20</span>);</span><br><span class="line">defaultConfig.put(Config.DRPC_MAX_BUFFER_SIZE, <span class="number">1048576</span>);</span><br><span class="line"></span><br><span class="line">conf.putAll(defaultConfig);</span><br><span class="line"></span><br><span class="line">DRPCClient drpcClient = <span class="keyword">new</span> DRPCClient(conf,<span class="string">"localhost"</span>, <span class="number">3772</span>,<span class="number">5000</span>);</span><br></pre></td></tr></table></figure>]]></content>
      <categories>
        <category>bigdata</category>
      </categories>
      <tags>
        <tag>storm</tag>
      </tags>
  </entry>
  <entry>
    <title>研一前的暑假，深度学习初体验</title>
    <url>/2019/08/10/%E7%A0%94%E4%B8%80%E5%89%8D%E7%9A%84%E6%9A%91%E5%81%87%EF%BC%8C%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%88%9D%E4%BD%93%E9%AA%8C/</url>
    <content><![CDATA[<h1 id="选导师"><a href="#选导师" class="headerlink" title="选导师"></a>选导师</h1><p>&nbsp;&nbsp;&nbsp;&nbsp;在五月二十七号左右，再次来到一所新的城市，即将在这里度过我研究生的三年，之所以来这么早，是因为希望研究生阶段，能够跟着一个研究方向，自己比较感兴趣的导师。找的第一个导师比较偏学术一些，而我又希望自己在研究生阶段能够有一定的项目经历，所以在经过思考后，我和老师表明了我的想法，老师也表示理解，和我推荐了其他的导师。在找导师的过程中，发现比较热门的导师的特点有以下几点。在校刚来的几位导师中，比较年轻的导师无论能力还是学术都比较强，所有也很快就定下了人选。在来之前，自己也大致确立了方向，大数据。学校有个大数据院，看了相关的介绍，这一方向在校长的带领下，是发展的非常不错的。但是对于我看似来的早，实际上并不早的人来说，是轮不到我了。。此处省略一万字吧。最终找到了我现在的导师，导师管理的公司主要是做图像识别。也就属于当前的计算机视觉方向，被分配在公司的算法组，所以也就不得不走向了研究深度学习的方向。</p>
<h1 id="做事情"><a href="#做事情" class="headerlink" title="做事情"></a>做事情</h1><p>&nbsp;&nbsp;&nbsp;&nbsp;在简单的熟悉了公司之后，由于初来公司，并没有分配什么很具体的工作，所以自己在没事的时候，我选择继续考研之后做的事情，那就是学习springboot. 由于在公司里，没有很具体的工作，每天自己看视频自学，一个视频教程边看自己边跟着实践，也很快就完成了。 就这样，看了springboot企业微信点餐，springcloud升级企业微信点餐两个教程。就在这个时候，公司有准备让我写一些简单的接口。这时候随着对微服务的了解之后，我发现我对这个概念非常感兴趣。所以我不断去深入了解这一块的知识。</p>
<h1 id="深度学习相关"><a href="#深度学习相关" class="headerlink" title="深度学习相关"></a>深度学习相关</h1><p>&nbsp;&nbsp;&nbsp;&nbsp;在机器学习中，我们通常使用梯度下降来更新模型参数从而求解。损失函数关于模型参数的梯度指向一个可以降低损失函数值的方向，我们不断地沿着梯度的方向更新模型从而最小化损失函数。</p>
<h2 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;线性回归输出是一个连续值，因此适用于回归问题。回归问题在实际中很常见，如预测房屋价格、气温、销售额等连续值的问题。与回归问题不同，分类问题中模型的最终输出是一个离散值。我们所说的图像分类、垃圾邮件识别、疾病检测等输出为离散值的问题都属于分类问题的范畴。softmax回归则适用于分类问题。</p>
<h3 id="线性回归的基本要素"><a href="#线性回归的基本要素" class="headerlink" title="线性回归的基本要素"></a>线性回归的基本要素</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;我们以一个简单的房屋价格预测作为例子来解释线性回归的基本要素。这个应用的目标是预测一栋房子的售出价格（元）。我们知道这个价格取决于很多因素，如房屋状况、地段、市场行情等。为了简单起见，这里我们假设价格只取决于房屋状况的两个因素，即面积（平方米）和房龄（年）。接下来我们希望探索价格与这两个因素的具体关系。</p>
<h3 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;设房屋的面积为x1，房龄为x2，售出价格为y。我们需要建立基于输入x1和x2来计算输出y的表达式，也就是模型（model）。顾名思义，线性回归假设输出与各个输入之间是线性关系：<br>&nbsp;&nbsp;&nbsp;&nbsp;y^=x1w1+x2w2+b,<br>&nbsp;&nbsp;&nbsp;&nbsp;其中w1和w2是权重（weight），b是偏差（bias），且均为标量。它们是线性回归模型的参数（parameter）。模型输出ˆy是线性回归对真实价格y的预测或估计。我们通常允许它们之间有一定误差。</p>
<h3 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;接下来我们需要通过数据来寻找特定的模型参数值，使模型在数据上的误差尽可能小。这个过程叫作模型训练（model training）。下面我们介绍模型训练所涉及的3个要素。</p>
<h4 id="训练数据"><a href="#训练数据" class="headerlink" title="训练数据"></a>训练数据</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;我们通常收集一系列的真实数据，例如多栋房屋的真实售出价格和它们对应的面积和房龄。我们希望在这个数据上面寻找模型参数来使模型的预测价格与真实价格的误差最小。在机器学习术语里，该数据集被称为训练数据集（training data set）或训练集（training set），一栋房屋被称为一个样本（sample），其真实售出价格叫作标签（label），用来预测标签的两个因素叫作特征（feature）。特征用来表征样本的特点。<br>&nbsp;&nbsp;&nbsp;&nbsp;假设我们采集的样本数为n，索引为i的样本的特征为x(i)1和x(i)2，标签为y(i)。对于索引为i的房屋，线性回归模型的房屋价格预测表达式为<br>&nbsp;&nbsp;&nbsp;&nbsp;y(i)=x1(i)w1+x2(i)w2+b.</p>
<h4 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;在模型训练中，我们需要衡量价格预测值与真实值之间的误差。通常我们会选取一个非负数作为误差，且数值越小表示误差越小。一个常用的选择是平方函数。<br>    给定训练数据集，这个误差只与模型参数相关，因此我们将它记为以模型参数为参数的函数。在机器学习里，将衡量误差的函数称为损失函数（loss function）。<br>&nbsp;&nbsp;&nbsp;&nbsp;通常，我们用训练数据集中所有样本误差的平均来衡量模型预测的质量.<br>&nbsp;&nbsp;&nbsp;&nbsp;在模型训练中，我们希望找出一组模型参数，记为w∗1,w∗2,b∗，来使训练样本平均损失最小.</p>
<h4 id="优化算法"><a href="#优化算法" class="headerlink" title="优化算法"></a>优化算法</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;当模型和损失函数形式较为简单时，上面的误差最小化问题的解可以直接用公式表达出来。这类解叫作解析解（analytical solution）。本节使用的线性回归和平方误差刚好属于这个范畴。然而，大多数深度学习模型并没有解析解，只能通过优化算法有限次迭代模型参数来尽可能降低损失函数的值。这类解叫作数值解（numerical solution）。<br>&nbsp;&nbsp;&nbsp;&nbsp;在求数值解的优化算法中，小批量随机梯度下降（mini-batch stochastic gradient descent）在深度学习中被广泛使用。它的算法很简单：先选取一组模型参数的初始值，如随机选取；接下来对参数进行多次迭代，使每次迭代都可能降低损失函数的值。在每次迭代中，先随机均匀采样一个由固定数目训练数据样本所组成的小批量（mini-batch）β，然后求小批量中数据样本的平均损失有关模型参数的导数（梯度），最后用此结果与预先设定的一个正数的乘积作为模型参数在本次迭代的减小量。<br>&nbsp;&nbsp;&nbsp;&nbsp;在迭代的过程中，β代表每个小批量中的样本个数（批量大小，batch size），η称作学习率（learning rate）并取正数。需要强调的是，这里的批量大小和学习率的值是人为设定的，并不是通过模型训练学出的，因此叫作超参数（hyperparameter）。我们通常所说的“调参”指的正是调节超参数，例如通过反复试错来找到超参数合适的值。</p>
<h3 id="模型预测"><a href="#模型预测" class="headerlink" title="模型预测"></a>模型预测</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;模型训练完成后，我们将模型参数w1, w2, b在优化算法停止时的值分别记作w<sub>1</sub>, w<sub>2</sub>, b。注意，这里我们得到的并不一定是最小化损失函数的最优解w*<sub>1</sub>, w*<sub>2</sub>，b*，而是对最优解的一个近似。然后，我们就可以使用学出的线性回归模型x1w1+ x2w2 + b来估算训练数据集以外任意一栋面积（平方米）为x1、房龄（年）为x2的房屋的价格了。这里的估算也叫作模型预测、模型推断或模型测试。</p>
]]></content>
      <categories>
        <category>计算机视觉</category>
        <category>机器学习</category>
      </categories>
      <tags>
        <tag>深度学习</tag>
      </tags>
  </entry>
  <entry>
    <title>Hello World</title>
    <url>/2019/03/11/hello-world/</url>
    <content><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>
]]></content>
  </entry>
</search>
