<?xml version="1.0" encoding="utf-8"?>
<search> 
  
  
    
    <entry>
      <title>Storm学习</title>
      <link href="/2020/10/22/Storm%E5%AD%A6%E4%B9%A0/"/>
      <url>/2020/10/22/Storm%E5%AD%A6%E4%B9%A0/</url>
      
        <content type="html"><![CDATA[<h1 id="Storm学习"><a href="#Storm学习" class="headerlink" title="Storm学习"></a>Storm学习</h1><h2 id="1-导学"><a href="#1-导学" class="headerlink" title="1.导学"></a>1.导学</h2><p>VMware Fusion<br>    Mac上搭建：为了给大家演示如何使用我们的OOTB环境<br>    Hadoop环境：虚拟机，我是远程登录<br>    Mac<br>        那么就不需要使用我们的OOTB环境<br>        VMware Fusion+OOTB</p><pre><code>Window：VMware</code></pre><p>hadoop/hadoop<br>root用户的密码是什么？<br>修改配置文件，是需要root权限的，怎么办？<br>    sudo command</p><p>只有一个地方需要修改：ip地址<br>/etc/hosts<br>    192.168.199.128 hadoop000<br>    192.168.199.128 localhost</p><h2 id="2-初识实时流处理Storm"><a href="#2-初识实时流处理Storm" class="headerlink" title="2.初识实时流处理Storm"></a>2.初识实时流处理Storm</h2><p>Apache Storm is a free and open source distributed realtime computation system<br>    免费<br>    开源<br>    分布式<br>    实时计算系统</p><p>Storm makes it easy to reliably process unbounded streams of data<br>    unbounded: 无界<br>    bounded: Hadoop/Spark SQL   离线(input …. output)</p><p>Storm has many use cases:<br>    realtime analytics,<br>    online machine learning,<br>    continuous computation,<br>    distributed RPC,<br>    ETL, and more. </p><p>Storm特点<br>    fast：a million tuples processed per second per node<br>    scalable,<br>    fault-tolerant,<br>    guarantees your data will be processed<br>    and is easy to set up and operate.</p><p>小结：Strom能实现高频数据和大规模数据的实时处理</p><p>Storm产生于BackType(被Twitter收购)公司</p><p>#…#</p><p>需求：大数据的实时处理</p><p>自己来实现实时系统，要考虑的因素：</p><p>1) 健壮性<br>2) 扩展性/分布式<br>3) 如何使得数据不丢失，不重复<br>4) 高性能、低延时</p><p>Storm开源<br>    2011.9<br>    Apache<br>    Clojure Java</p><p>Storm技术网站</p><p>1) 官网： storm.apache.org<br>2) GitHub: github.com/apache/storm<br>3) wiki: <a href="https://en.wikipedia.org/wiki/Storm_(event_processor)" target="_blank" rel="noopener">https://en.wikipedia.org/wiki/Storm_(event_processor)</a></p><p>Storm vs Hadoop<br>    数据源/处理领域<br>    处理过程<br>        Hadoop: Map  Reduce<br>        Storm:  Spout  Bolt<br>    进程是否结束<br>    处理速度<br>    使用场景    </p><p>发展趋势<br>    1) 社区的发展、活跃度<br>    2) 企业的需求<br>    3) 大数据相关的大会， Storm主题的数量上升<br>    4) 互联网  JStorm</p><h2 id="3-Storm核心概念"><a href="#3-Storm核心概念" class="headerlink" title="3.Storm核心概念"></a>3.Storm核心概念</h2><p>核心概念<br>    Topologies<br>        拓扑，将整个流程串起来<br>    Streams<br>        流，数据流，水流<br>    Spouts<br>        产生数据/水的东西<br>    Bolts<br>        处理数据/水的东西    水壶/水桶<br>    Tuple<br>        数据/水    </p><p>制约中国互联网发展的最大瓶颈是什么？ 后厂村路<br>13号线：回龙观==&gt;龙泽==&gt;西二旗</p><p>Storm核心概念总结<br>    Topology： 计算拓扑，由spout和bolt组成的<br>    Stream：消息流，抽象概念，没有边界的tuple构成<br>    Tuple：消息/数据  传递的基本单元<br>    Spout：消息流的源头，Topology的消息生产者<br>    Bolt：消息处理单元，可以做过滤、聚合、查询/写数据库的操作</p><h2 id="4-Storm编程"><a href="#4-Storm编程" class="headerlink" title="4.Storm编程"></a>4.Storm编程</h2><p>搭建开发环境<br>    jdk: 1.8<br>        windows: exe<br>        linux/mac(dmg): tar …..   把jdk指定到系统环境变量(~/.bash_profile)<br>            export JAVA_HOME=/Library/Java/JavaVirtualMachines/jdk1.8.0_121.jdk/Contents/Home<br>            export PATH=$JAVA_HOME/bin:$PATH</p><pre><code>        source ~/.bash_profile        echo $JAVA_HOME        java -versionIDEA: Maven: 3.3+    windows/linux/mac 下载安装包    tar .... -C ~/app    把maven指定到系统环境变量(~/.bash_profile)    export MAVEN_HOME=/Users/rocky/app/apache-maven-3.3.9    export PATH=$MAVEN_HOME/bin:$PATH    source ~/.bash_profile    echo $JAVA_HOME    mvn -v    调整maven依赖下载的jar所在位置： $MAVEN_HOME/conf/setting.xml    &lt;localRepository&gt;/Users/rocky/maven_repos&lt;/localRepository&gt;在pom.xml中添加storm的maven依赖    &lt;dependency&gt;      &lt;groupId&gt;org.apache.storm&lt;/groupId&gt;      &lt;artifactId&gt;storm-core&lt;/artifactId&gt;      &lt;version&gt;1.1.1&lt;/version&gt;    &lt;/dependency&gt;</code></pre><p>ISpout<br>    概述<br>        核心接口(interface)，负责将数据发送到topology中去处理<br>        Storm会跟踪Spout发出去的tuple的DAG<br>        ack/fail<br>        tuple: message id<br>        ack/fail/nextTuple是在同一个线程中执行的，所以不用考虑线程安全方面</p><pre><code>核心方法    open： 初始化操作    close： 资源释放操作    nextTuple： 发送数据   core api    ack： tuple处理成功，storm会反馈给spout一个成功消息    fail：tuple处理失败，storm会发送一个消息给spout，处理失败实现类    public abstract class BaseRichSpout extends BaseComponent implements IRichSpout {    public interface IRichSpout extends ISpout, IComponent     DRPCSpout    ShellSpout</code></pre><p>IComponent接口<br>    概述：<br>        public interface IComponent extends Serializable<br>        为topology中所有可能的组件提供公用的方法</p><pre><code>    void declareOutputFields(OutputFieldsDeclarer declarer);    用于声明当前Spout/Bolt发送的tuple的名称    使用OutputFieldsDeclarer配合使用实现类：public abstract class BaseComponent implements IComponent</code></pre><p>IBolt接口<br>    概述<br>        职责：接收tuple处理，并进行相应的处理(filter/join/….)<br>        hold住tuple再处理<br>        IBolt会在一个运行的机器上创建，使用Java序列化它，然后提交到主节点(nimbus)上去执行<br>        nimbus会启动worker来反序列化，调用prepare方法，然后才开始处理tuple处理</p><pre><code>方法    prepare：初始化    execute：处理一个tuple暑假，tuple对象中包含了元数据信息    cleanup：shutdown之前的资源清理操作实现类：    public abstract class BaseRichBolt extends BaseComponent implements IRichBolt {    public interface IRichBolt extends IBolt, IComponent     RichShellBolt</code></pre><p>求和案例<br>    需求：1 + 2 + 3 + ….   = ???<br>    实现方案：<br>        Spout发送数字作为input<br>        使用Bolt来处理业务逻辑：求和<br>        将结果输出到控制台<br>    拓扑设计： DataSourceSpout  –&gt; SumBolt    </p><p>词频统计<br>    需求：读取指定目录的数据，并实现单词计数功能<br>    实现方案：<br>        Spout来读取指定目录的数据，作为后续Bolt处理的input<br>        使用一个Bolt把input的数据，切割开，我们按照逗号进行分割<br>        使用一个Bolt来进行最终的单词的次数统计操作<br>        并输出<br>    拓扑设计： DataSourceSpout ==&gt; SplitBolt ==&gt; CountBolt    </p><p>Storm编程注意事项</p><p>1) Exception in thread “main” java.lang.IllegalArgumentException: Spout has already been declared for id DataSourceSpout<br>2) org.apache.storm.generated.InvalidTopologyException: null<br>3) topology的名称不是重复： local似乎没问题， 等我们到集群测试的时候再来验证这个问题</p><h2 id="5-Storm周边框架使用"><a href="#5-Storm周边框架使用" class="headerlink" title="5.Storm周边框架使用"></a>5.Storm周边框架使用</h2><p>环境前置说明：<br>    通过我们的客户端(终端，CRT，XShell)<br>    ssh hadoop@hadoop000<br>    ssh <a href="mailto:hadoop@192.168.199.102" target="_blank" rel="noopener">hadoop@192.168.199.102</a></p><pre><code>远程服务器的用户名是hadoop，密码也是hadoop有没有提供root权限，sudo commandhadoop000(192.168.199.102)是远程服务器的hostname如果你想在本地通过ssh hadoop@hadoop000远程登录，那么你本地的hosts肯定要添加ip和hostname的映射192.168.199.102  hadoop000</code></pre><p>JDK的安装<br>    将所有的软件都安装到~/app<br>        tar -zxvf jdk-8u91-linux-x64.tar.gz -C ~/app/</p><pre><code>建议将jdk的bin目录配置到系统环境变量中： ~/.bash_profile    export JAVA_HOME=/home/hadoop/app/jdk1.8.0_91    export PATH=$JAVA_HOME/bin:$PATH让系统环境变量生效    source ~/.bash_profile验证    java -version</code></pre><p>ZooKeeper安装<br>    下载ZK的安装包：<a href="http://archive.cloudera.com/cdh5/cdh/5/" target="_blank" rel="noopener">http://archive.cloudera.com/cdh5/cdh/5/</a><br>    解压：tar -zxvf zookeeper-3.4.5-cdh5.7.0.tar.gz -C ~/app/<br>    建议ZK_HOME/bin添加到系统环境变量: ~/.bash_profile<br>        export ZK_HOME=/home/hadoop/app/zookeeper-3.4.5-cdh5.7.0<br>        export PATH=$ZK_HOME/bin:$PATH<br>    让系统环境变量生效<br>        source ~/.bash_profile<br>    修改ZK的配置： $ZK_HOME/conf/zoo.cfg<br>        dataDir=/home/hadoop/app/tmp/zookeeper<br>    启动zk: $ZK_HOME/bin/<br>        zkServer.sh start<br>    验证: jps<br>        多了一个QuorumPeerMain进程，就表示zk启动成功了</p><p>ELK:<br>    <a href="http://www.elastic.co" target="_blank" rel="noopener">www.elastic.co</a></p><p>Kafka概述<br>    和消息系统类似</p><pre><code>消息中间件：生产者和消费者妈妈：生产者你：消费者馒头：数据流、消息    正常情况下： 生产一个  消费一个    其他情况：          一直生产，你吃到某一个馒头时，你卡主(机器故障)， 馒头就丢失了        一直生产，做馒头速度快，你吃来不及，馒头也就丢失了    拿个碗/篮子，馒头做好以后先放到篮子里，你要吃的时候去篮子里面取出来吃篮子/框： Kafka    当篮子满了，馒头就装不下了，咋办？     多准备几个篮子 === Kafka的扩容</code></pre><p>Kafka架构<br>    producer：生产者，就是生产馒头(老妈)<br>    consumer：消费者，就是吃馒头的(你)<br>    broker：篮子<br>    topic：主题，给馒头带一个标签，topica的馒头是给你吃的，topicb的馒头是给你弟弟吃</p><p>单节点单broker的部署及使用</p><p>$KAFKA_HOME/config/server.properties<br>broker.id=0<br>listeners<br>host.name<br>log.dirs<br>zookeeper.connect</p><p>启动Kafka<br>kafka-server-start.sh<br>USAGE: /home/hadoop/app/kafka_2.11-0.9.0.0/bin/kafka-server-start.sh [-daemon] server.properties [–override property=value]*</p><p>kafka-server-start.sh $KAFKA_HOME/config/server.properties</p><p>创建topic: zk<br>kafka-topics.sh –create –zookeeper hadoop000:2181 –replication-factor 1 –partitions 1 –topic hello_topic</p><p>查看所有topic<br>kafka-topics.sh –list –zookeeper hadoop000:2181</p><p>发送消息: broker<br>kafka-console-producer.sh –broker-list hadoop000:9092 –topic hello_topic</p><p>消费消息: zk<br>kafka-console-consumer.sh –zookeeper hadoop000:2181 –topic hello_topic –from-beginning</p><p>–from-beginning的使用</p><p>查看所有topic的详细信息：kafka-topics.sh –describe –zookeeper hadoop000:2181<br>查看指定topic的详细信息：kafka-topics.sh –describe –zookeeper hadoop000:2181 –topic hello_topic</p><p>单节点多broker<br>server-1.properties<br>    log.dirs=/home/hadoop/app/tmp/kafka-logs-1<br>    listeners=PLAINTEXT://:9093<br>    broker.id=1</p><p>server-2.properties<br>    log.dirs=/home/hadoop/app/tmp/kafka-logs-2<br>    listeners=PLAINTEXT://:9094<br>    broker.id=2</p><p>server-3.properties<br>    log.dirs=/home/hadoop/app/tmp/kafka-logs-3<br>    listeners=PLAINTEXT://:9095<br>    broker.id=3</p><p>kafka-server-start.sh -daemon $KAFKA_HOME/config/server-1.properties &amp;<br>kafka-server-start.sh -daemon $KAFKA_HOME/config/server-2.properties &amp;<br>kafka-server-start.sh -daemon $KAFKA_HOME/config/server-3.properties &amp;</p><p>kafka-topics.sh –create –zookeeper hadoop000:2181 –replication-factor 3 –partitions 1 –topic my-replicated-topic</p><p>kafka-console-producer.sh –broker-list hadoop000:9093,hadoop000:9094,hadoop000:9095 –topic my-replicated-topic<br>kafka-console-consumer.sh –zookeeper hadoop000:2181 –topic my-replicated-topic</p><p>kafka-topics.sh –describe –zookeeper hadoop000:2181 –topic my-replicated-topic</p><h2 id="6-Storm架构及部署"><a href="#6-Storm架构及部署" class="headerlink" title="6.Storm架构及部署"></a>6.Storm架构及部署</h2><p>Storm架构<br>    类似于Hadoop的架构，主从(Master/Slave)<br>    Nimbus: 主<br>        集群的主节点，负责任务(task)的指派和分发、资源的分配<br>    Supervisor: 从<br>        可以启动多个Worker，具体几个呢？可以通过配置来指定<br>        一个Topo可以运行在多个Worker之上，也可以通过配置来指定<br>        集群的从节点，(负责干活的)，负责执行任务的具体部分<br>        启动和停止自己管理的Worker进程<br>    无状态，在他们上面的信息(元数据)会存储在ZK中<br>    Worker: 运行具体组件逻辑(Spout/Bolt)的进程<br>    =====================分割线===================<br>    task：<br>        Spout和Bolt<br>        Worker中每一个Spout和Bolt的线程称为一个Task<br>    executor： spout和bolt可能会共享一个线程</p><p>Storm部署的前置条件<br>    jdk7+<br>    python2.6.6+</p><p>我们课程使用的Storm版本是：1.1.1</p><p>Storm部署<br>    下载<br>    解压到<del>/app<br>    添加到系统环境变量:</del>/.bash_profile<br>        export STORM_HOME=/home/hadoop/app/apache-storm-1.1.1<br>        export PATH=$STORM_HOME/bin:$PATH<br>    使其生效: source ~/.bash_profile</p><pre><code>目录结构    bin    examples    conf    lib</code></pre><p>Storm启动<br>    $STORM_HOME/bin/storm   如何使用  执行storm就能看到很多详细的命令<br>        dev-zookeeper  启动zk<br>            storm dev-zookeeper  前台启动<br>            nohup sh storm dev-zookeeper &amp;<br>            jps ： dev_zookeeper<br>        nimbus  启动主节点<br>            nohup sh storm nimbus &amp;<br>        supervisor 启动从节点<br>            nohup sh storm supervisor &amp;<br>        ui  启动UI界面<br>            nohup sh storm ui &amp;<br>        logviewer 启动日志查看服务<br>            nohup sh storm logviewer &amp;</p><p>注意事项<br>    1) 为什么是4个slot<br>    2) 为什么有2个Nimbus</p><p>Storm如何运行我们自己开发的应用程序呢?<br>    Syntax: storm jar topology-jar-path class args0 args1 args2</p><p>storm jar /home/hadoop/lib/storm-1.0.jar com.imooc.bigdata.ClusterSumStormTopology</p><p>问题: 3个executor，那么页面就看到spout1个和bolt1个，那么还有一个去哪了？</p><p>如何修改将跑在本地的storm app改成运行在集群上的<br>StormSubmitter.submitTopology(topoName,new Config(), builder.createTopology());</p><p>storm 其他命令的使用</p><pre><code>list        Syntax: storm list        List the running topologies and their statuses.如何停止作业    kill        Syntax: storm kill topology-name [-w wait-time-secs]如何停止集群    hadoop： stop-all.sh    kill -9 pid,pid....</code></pre><p>Storm集群的部署规划<br>    hadoop000   192.168.199.102<br>    hadoop001   192.168.199.247<br>    hadoop002   192.168.199.138 </p><pre><code>每台机器的host映射：/etc/hosts    192.168.199.102 hadoop000    192.168.199.247 hadoop001    192.168.199.138 hadoop002hadoop000: zk nimbus  supervisorhadoop001: zk           supervisorhadoop002: zk         supervisor</code></pre><p>安装包的分发: 从hadoop000机器做为出发点<br>    scp  xxxx  hadoop@hadoop001:<del>/software<br>    scp  xxxx  hadoop@hadoop002:</del>/software</p><p>jdk的安装<br>    解压<br>    配置到系统环境变量<br>    验证</p><p>ZK分布式环境的安装<br>server.1=hadoop000:2888:3888<br>server.2=hadoop001:2888:3888<br>server.3=hadoop002:2888:3888</p><p>hadoop000的dataDir目录: myid的值1<br>hadoop001的dataDir目录: myid的值2<br>hadoop002的dataDir目录: myid的值3</p><p>在每个节点上启动zk: zkServer.sh start<br>在每个节点上查看当前机器zk的状态: zkServer.sh status</p><p>Storm集群<br>    $STORM_HOME/conf/storm.yaml<br>        storm.zookeeper.servers:<br>             - “hadoop000”<br>             - “hadoop001”<br>             - “hadoop002”</p><pre><code>storm.local.dir: &quot;/home/hadoop/app/tmp/storm&quot;supervisor.slots.ports:     - 6700     - 6701     - 6702     - 6703</code></pre><p>启动<br>    hadoop000: nimbus  supervisor(ui,logviewer)<br>    hadoop001: supervisor(logviewer)<br>    hadoop002: supervisor(logviewer)</p><p>nimbus  启动主节点<br>    nohup sh storm nimbus &amp;<br>supervisor 启动从节点<br>    nohup sh storm supervisor &amp;<br>ui  启动UI界面<br>    nohup sh storm ui &amp;<br>logviewer 启动日志查看服务<br>    nohup sh storm logviewer &amp;</p><p>启动完所有的进程之后，查看<br>[hadoop@hadoop000 bin]$ jps<br>7424 QuorumPeerMain<br>8164 Supervisor<br>7769 nimbus<br>8380 logviewer<br>7949 core</p><p>[hadoop@hadoop001 bin]$ jps<br>3142 logviewer<br>2760 QuorumPeerMain<br>2971 Supervisor</p><p>[hadoop@hadoop002 bin]$ jps<br>3106 logviewer<br>2925 Supervisor<br>2719 QuorumPeerMain</p><p>storm jar /home/hadoop/lib/storm-1.0.jar com.imooc.bigdata.ClusterSumStormTopology</p><p>目录树<br>    storm.local.dir<br>        nimbus/inbox:stormjar-….jar<br>        supervisor/stormdist<br>            ClusterSumStormTopology-1-1511599690<br>            │   │       ├── stormcode.ser<br>            │   │       ├── stormconf.ser<br>            │   │       └── stormjar.jar</p><h2 id="7-并行度"><a href="#7-并行度" class="headerlink" title="7.并行度"></a>7.并行度</h2><p>并行度<br>    一个worker进程执行的是一个topo的子集<br>    一个worker进程会启动1..n个executor线程来执行一个topo的component<br>    一个运行的topo就是由集群中多台物理机上的多个worker进程组成</p><pre><code>executor是一个被worker进程启动的单独线程，每个executor只会运行1个topo的一个componenttask是最终运行spout或者bolt代码的最小执行单元默认：    一个supervisor节点最多启动4个worker进程  ?    每一个topo默认占用一个worker进程         ?    每个worker进程会启动一个executor        ?    每个executor启动一个task                ?</code></pre><p>Total slots:4<br>Executors: 3 ????  spout + bolt = 2  why 3?<br>    acker 导致的</p><h2 id="8-分组策略"><a href="#8-分组策略" class="headerlink" title="8.分组策略"></a>8.分组策略</h2><p>A stream grouping defines how that stream should be partitioned among the bolt’s tasks</p><p>storm jar /home/had/lib/storm-1.0.jar com.imooc.bigdata.ClusterSumShuffleGroupingStormTopology</p><h2 id="9-Storm可靠性"><a href="#9-Storm可靠性" class="headerlink" title="9.Storm可靠性"></a>9.Storm可靠性</h2>]]></content>
      
      
      <categories>
          
          <category> bigdata </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Storm </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>hadoop基础</title>
      <link href="/2020/10/21/hadoop%E5%9F%BA%E7%A1%80/"/>
      <url>/2020/10/21/hadoop%E5%9F%BA%E7%A1%80/</url>
      
        <content type="html"><![CDATA[<h1 id="hadoop基础"><a href="#hadoop基础" class="headerlink" title="hadoop基础"></a>hadoop基础</h1><h2 id="1-大数据概述"><a href="#1-大数据概述" class="headerlink" title="1.大数据概述"></a>1.大数据概述</h2><p>大数据带来的技术变革<br>    技术驱动：数据量大<br>        存储：文件存储 ==&gt; 分布式存储<br>        计算：单机    ==&gt; 分布式计算<br>        网络：万兆<br>        DB： RDBMS  ==&gt; NoSQL(HBase/Redis….)</p><p>大数据技术概念<br>    单机：CPU Memory Disk<br>    分布式并行计算/处理</p><p>船的选择<br>    廉价：<br>    中高价值：</p><p>运输过程拆开<br>    货物搬到船上： 数据采集   数据存储<br>    处理：小于多少的石头扔了   精细化的筛选 </p><pre><code>数据采集：Flume Sqoop数据存储：Hadoop数据处理、分析、挖掘：Hadoop、Spark、Flink....可视化：</code></pre><h2 id="2-初识Hadoop"><a href="#2-初识Hadoop" class="headerlink" title="2.初识Hadoop"></a>2.初识Hadoop</h2><p>Nutch、Hadoop：Doug Cutting<br>Spring：</p><p>学习一个新的框架，我的风格是直接查看该项目的官网地址<br>Hadoop<br>Hive</p><p>Apache社区的顶级项目：xxxx.apache.org<br>    hadoop.apache.org<br>    hive.apache.org<br>    hbase.apache.org<br>    spark.apache.org<br>    flink.apache.org<br>    storm.apache.org</p><p>reliable, scalable, distributed computing.</p><p>单机存储<br>单机计算</p><p>Hadoop：提供分布式的存储（一个文件被拆分成很多个块，并且以副本的方式存储在各个节点中）和计算<br>    是一个分布式的系统基础架构：用户可以在不了解分布式底层细节的情况下进行使用。</p><p>分布式文件系统：HDFS实现将文件分布式存储在很多的服务器上<br>分布式计算框架：MapReduce实现在很多机器上分布式并行计算<br>分布式资源调度框架：YARN实现集群资源管理以及作业的调度</p><p>文件、块、副本<br>    文件：test.log  200M<br>    块(block)：默认的blocksize是128M， 2个块 = 1个128M + 1个72M<br>    副本：HDFS默认3副本</p><pre><code>node1：blk1  blk2  X  node2：blk2node3：blk1  blk2node4：node5：blk1</code></pre><p>去IoE</p><p>常用的Hadoop发行版<br>    Apache<br>        优点：纯开源<br>        缺点：不同版本/不同框架之间整合 jar冲突… 吐血</p><pre><code>CDH：https://www.cloudera.com/   60-70%    优点：cm(cloudera manager) 通过页面一键安装各种框架、升级、impala    缺点：cm不开源、与社区版本有些许出入Hortonworks：HDP  企业发布自己的数据平台可以直接基于页面框架进行改造    优点：原装Hadoop、纯开源、支持tez    缺点：企业级安全不开源MapR</code></pre><h2 id="3-分布式文件系统HDFS"><a href="#3-分布式文件系统HDFS" class="headerlink" title="3.分布式文件系统HDFS"></a>3.分布式文件系统HDFS</h2><p>HDFS概述<br>    1) 分布式<br>    2）commodity hardware<br>    3）fault-tolerant 容错<br>    4) high throughput<br>    5) large data sets</p><pre><code>HDFS是一个分布式的文件系统文件系统：Linux、Windows、Mac....    目录结构: C    /    存放的是文件或者文件夹    对外提供服务：创建、修改、删除、查看、移动等等普通文件系统 vs  分布式文件系统    单机    分布式文件系统能够横跨N个机器</code></pre><p>HDFS前提和设计目标<br>    Hardware Failure  硬件错误<br>        每个机器只存储文件的部分数据，blocksize=128M<br>        block存放在不同的机器上的，由于容错，HDFS默认采用3副本机制<br>    Streaming Data Access  流式数据访问<br>        The emphasis is on high throughput of data access<br>        rather than low latency of data access.<br>    Large Data Sets  大规模数据集<br>    Moving Computation is Cheaper than Moving Data    移动计算比移动数据更划算</p><p>HDFS的架构 *****</p><pre><code>1) NameNode(master) and DataNodes(slave)2) master/slave的架构3) NN:     the file system namespace        /home/hadoop/software                    /app    regulates access to files by clients4）DN：storage5）HDFS exposes a file system namespace and allows user data to be stored in files.6）a file is split into one or more blocks    blocksize: 128M     150M拆成2个block7）blocks are stored in a set of DataNodes    为什么？ 容错！！！8）NameNode executes file system namespace operations：CRUD9）determines the mapping of blocks to DataNodes    a.txt  150M   blocksize=128M    a.txt 拆分成2个block   一个是block1：128M  另一个是block2：22M    block1存放在哪个DN？block2存放在哪个DN？    a.txt        block1：128M, 192.168.199.1        block2：22M,  192.168.199.2    get a.txt    这个过程对于用户来说是不感知的10）通常情况下：1个Node部署一个组件</code></pre><p>课程环境介绍：<br>本课程录制的系统是Mac，所以我采用的linux客户端是mac自带的shell<br>如果你们是win：xshell、crt<br>服务器/linux地址：192.168.199.233<br>连接到linux环境<br>    登陆：ssh <a href="mailto:hadoop@192.168.199.233" target="_blank" rel="noopener">hadoop@192.168.199.233</a><br>    登陆成功以后：[hadoop@hadoop000 ~]$<br>    linux机器：用户名hadoop、密码123456、hostname是hadoop000<br>    创建课程中所需要的目录（合适的文件存放在合适的目录）<br>        [hadoop@hadoop000 ~]$ mkdir software  存放课程所使用的软件安装包<br>        [hadoop@hadoop000 ~]$ mkdir app       存放课程所有软件的安装目录<br>        [hadoop@hadoop000 ~]$ mkdir data      存放课程中使用的数据<br>        [hadoop@hadoop000 ~]$ mkdir lib       存放课程中开发过的作业jar存放的目录<br>        [hadoop@hadoop000 ~]$ mkdir shell     存放课程中相关的脚本<br>        [hadoop@hadoop000 ~]$ mkdir maven_resp 存放课程中使用到的maven依赖包存放的目录<br>    学员问：root密码<br>        切换hadoop到root用户：[hadoop@hadoop000 ~]$ sudo -i<br>        切换root到hadoop用户：[root@hadoop000 ~]# su hadoop<br>        我OOTB环境中创建的hadoop用户是有sudo权限：sudo vi /etc/hosts<br>    Linux版本：<br>        以前的课程是centos6.4，本次课程升级成centos7</p><p>Hadoop环境搭建<br>    使用的Hadoop相关版本：CDH<br>    CDH相关软件包下载地址：<a href="http://archive.cloudera.com/cdh5/cdh/5/" target="_blank" rel="noopener">http://archive.cloudera.com/cdh5/cdh/5/</a><br>    Hadoop使用版本：hadoop-2.6.0-cdh5.15.1<br>    Hadoop下载：wget <a href="http://archive.cloudera.com/cdh5/cdh/5/hadoop-2.6.0-cdh5.15.1.tar.gz" target="_blank" rel="noopener">http://archive.cloudera.com/cdh5/cdh/5/hadoop-2.6.0-cdh5.15.1.tar.gz</a><br>    Hive使用版本：hive-1.1.0-cdh5.15.1</p><p>Hadoop/Hive/Spark相关框架的学习：<br>    使用单机版足够  <strong><strong>*<br>        如果使用集群学习会导致：从入门到放弃<br>    使用Linux/Mac学习<br>        一定不要使用Windows搭建Hadoop环境<br>        所以Linux基础是要会的  *</strong></strong></p><p>Hadoop安装前置要求<br>    Java  1.8+<br>    ssh</p><p>安装Java<br>    拷贝本地软件包到服务器：scp jdk-8u91-linux-x64.tar.gz <a href="mailto:hadoop@192.168.199.233" target="_blank" rel="noopener">hadoop@192.168.199.233</a>:<del>/software/<br>    解压jdk到</del>/app/：tar -zvxf jdk-8u91-linux-x64.tar.gz -C ~/app/<br>    把jdk配置系统环境变量中： ~/.bash_profile<br>        export JAVA_HOME=/home/hadoop/app/jdk1.8.0_91<br>        export PATH=$JAVA_HOME/bin:$PATH<br>    使得配置修改生效：source .bash_profile<br>    验证：java -version</p><p>安装ssh无密码登陆<br>    ls<br>    ls -a<br>    ls -la  并没有发现一个.ssh的文件夹</p><pre><code>ssh-keygen -t rsa  一路回车cd ~/.ssh[hadoop@hadoop000 .ssh]$ ll总用量 12-rw------- 1 hadoop hadoop 1679 10月 15 02:54 id_rsa  私钥-rw-r--r-- 1 hadoop hadoop  398 10月 15 02:54 id_rsa.pub 公钥-rw-r--r-- 1 hadoop hadoop  358 10月 15 02:54 known_hostscat id_rsa.pub &gt;&gt; authorized_keyschmod 600 authorized_keys</code></pre><p>Hadoop(HDFS)安装<br>    下载<br>    解压：~/app<br>    添加HADOOP_HOME/bin到系统环境变量<br>    修改Hadoop配置文件<br>        hadoop-env.sh<br>            export JAVA_HOME=/home/hadoop/app/jdk1.8.0_91</p><pre><code>    core-site.xml        &lt;property&gt;            &lt;name&gt;fs.defaultFS&lt;/name&gt;            &lt;value&gt;hdfs://hadoop000:8020&lt;/value&gt;        &lt;/property&gt;    hdfs-site.xml        &lt;property&gt;            &lt;name&gt;dfs.replication&lt;/name&gt;            &lt;value&gt;1&lt;/value&gt;        &lt;/property&gt;        &lt;property&gt;            &lt;name&gt;hadoop.tmp.dir&lt;/name&gt;            &lt;value&gt;/home/hadoop/app/tmp&lt;/value&gt;        &lt;/property&gt;    slaves        hadoop000启动HDFS：    第一次执行的时候一定要格式化文件系统，不要重复执行: hdfs namenode -format    启动集群：$HADOOP_HOME/sbin/start-dfs.sh    验证:        [hadoop@hadoop000 sbin]$ jps        60002 DataNode        60171 SecondaryNameNode        59870 NameNode        http://192.168.199.233:50070        如果发现jps ok，但是浏览器不OK？ 十有八九是防火墙问题        查看防火墙状态：sudo firewall-cmd --state        关闭防火墙: sudo systemctl stop firewalld.service        禁止防火墙开机启动：</code></pre><p>hadoop软件包常见目录说明<br>    bin：hadoop客户端名单<br>    etc/hadoop：hadoop相关的配置文件存放目录<br>    sbin：启动hadoop相关进程的脚本<br>    share：常用例子</p><p>注意：<br>    start/stop-dfs.sh与hadoop-daemons.sh的关系<br>    start-dfs.sh =<br>        hadoop-daemons.sh start namenode<br>        hadoop-daemons.sh start datanode<br>        hadoop-daemons.sh start secondarynamenode<br>    stop-dfs.sh =<br>        ….</p><p>HDFS命令行操作   *****<br>    shell-like<br>        mkdir  ls ….</p><p>hadoop fs [generic options]<br>    [-appendToFile <localsrc> … <dst>]<br>    [-cat [-ignoreCrc] <src> …]<br>    [-chgrp [-R] GROUP PATH…]<br>    [-chmod [-R] &lt;MODE[,MODE]… | OCTALMODE&gt; PATH…]<br>    [-chown [-R] [OWNER][:[GROUP]] PATH…]<br>    [-copyFromLocal [-f] [-p] [-l] <localsrc> … <dst>]<br>    [-copyToLocal [-p] [-ignoreCrc] [-crc] <src> … <localdst>]<br>    [-count [-q] [-h] [-v] [-x] <path></path> …]<br>    [-cp [-f] [-p | -p[topax]] <src> … <dst>]<br>    [-df [-h] [<path></path> …]]<br>    [-du [-s] [-h] [-x] <path></path> …]<br>    [-get [-p] [-ignoreCrc] [-crc] <src> … <localdst>]<br>    [-getmerge [-nl] <src> <localdst>]<br>    [-ls [-C] [-d] [-h] [-q] [-R] [-t] [-S] [-r] [-u] [<path></path> …]]<br>    [-mkdir [-p] <path></path> …]<br>    [-moveFromLocal <localsrc> … <dst>]<br>    [-moveToLocal <src> <localdst>]<br>    [-mv <src> … <dst>]<br>    [-put [-f] [-p] [-l] <localsrc> … <dst>]<br>    [-rm [-f] [-r|-R] [-skipTrash] <src> …]<br>    [-rmdir [–ignore-fail-on-non-empty] <dir> …]<br>    [-text [-ignoreCrc] <src> …]</src></dir></src></dst></localsrc></dst></src></localdst></src></dst></localsrc></localdst></src></localdst></src></dst></src></localdst></src></dst></localsrc></src></dst></localsrc></p><p>hadoop常用命令：<br>hadoop fs -ls /<br>hadoop fs -put<br>hadoop fs -copyFromLocal<br>hadoop fs -moveFromLocal<br>hadoop fs -cat<br>hadoop fs -text<br>hadoop fs -get<br>hadoop fs -mkdir<br>hadoop fs -mv  移动/改名<br>hadoop fs -getmerge<br>hadoop fs -rm<br>hadoop fs -rmdir<br>hadoop fs -rm -r</p><p>HDFS存储扩展：<br>    put: 1file ==&gt; 1…n block ==&gt; 存放在不同的节点上的<br>    get: 去nn上查找这个file对应的元数据信息<br>    了解底层的存储机制这才是我们真正要学习的东西，掌握API那是毛毛雨</p><p>使用HDFS API的方式来操作HDFS文件系统<br>    IDEA/Eclipse<br>    Java<br>    使用Maven来管理项目<br>        拷贝jar包<br>        我的所有课程都是使用maven来进行管理的</p><p>Caused by: org.apache.hadoop.ipc.RemoteException<br>(org.apache.hadoop.security.AccessControlException):<br>Permission denied: user=rocky, access=WRITE,<br>inode=”/“:hadoop:supergroup:drwxr-xr-x</p><p>HDFS操作：shell + Java API<br>综合性的HDFS实战：使用HDFS Java API才完成HDFS文件系统上的文件的词频统计<br>词频统计：wordcount<br>/path/1.txt<br>hello world hello</p><p>/path/2.txt<br>hello world hello</p><p>==&gt; (hello,4) (world,2)<br>将统计完的结果输出到HDFS上去。</p><p>假设：有的小伙伴了解过mr、spark等等，觉得这个操作很简单<br>本实战的要求：只允许使用HDFS API进行操作</p><p>目的：<br>    1）掌握HDFS API的操作<br>    2）通过这个案例，让你们对于后续要学习的mr有一个比较好的认识</p><p>硬编码 ： 非常忌讳的<br>==&gt; 可配置</p><p>可插拔的开发/管理方式  plugin</p><p>副本摆放策略<br>    1-本rack的一个节点上<br>    2-另外一个rack的节点上<br>    3-与2相同的rack的另外一个节点上</p><pre><code>1-本rack的一个节点上2-本rack的另外一个节点上3-不同rack的一个节点上</code></pre><p><img src="https://lixiangbetter.github.io/2020/10/21/hadoop%E5%9F%BA%E7%A1%80/HDFS%E8%AF%BB%E6%95%B0%E6%8D%AE%E6%B5%81%E7%A8%8B.png" alt="HDFS读数据流程"></p><p><img src="https://lixiangbetter.github.io/2020/10/21/hadoop%E5%9F%BA%E7%A1%80/HDFS%E5%86%99%E6%95%B0%E6%8D%AE%E6%B5%81%E7%A8%8B.png" alt="HDFS写数据流程"></p><p>HDFS的元数据管理<br>    元数据：HDFS的目录结构以及每个文件的BLOCK信息(id，副本系数、block存放在哪个DN上)<br>    存在什么地方：对应配置 ${hadoop.tmp.dir}/name/……<br>    元数据存放在文件中：</p><pre><code>/test1/test1/a.txt/test2/test2/1.txt/test2/2.txt</code></pre><p>![HDFS checkpoint](<a href="https://lixiangbetter.github.io/2020/10/21/hadoop基础/HDFS" target="_blank" rel="noopener">https://lixiangbetter.github.io/2020/10/21/hadoop基础/HDFS</a> checkpoint.png)    </p><h2 id="4-分布式计算框架MapReduce"><a href="#4-分布式计算框架MapReduce" class="headerlink" title="4.分布式计算框架MapReduce"></a>4.分布式计算框架MapReduce</h2><p>词频统计、流量统计</p><p><img src="https://lixiangbetter.github.io/2020/10/21/hadoop%E5%9F%BA%E7%A1%80/Combiner.png" alt="Combiner"></p><p><img src="https://lixiangbetter.github.io/2020/10/21/hadoop%E5%9F%BA%E7%A1%80/Partitioner.png" alt="Combiner"></p><h2 id="5-资源调度框架YARN"><a href="#5-资源调度框架YARN" class="headerlink" title="5.资源调度框架YARN"></a>5.资源调度框架YARN</h2><p>YARN产生背景<br>    MapReduce1.x ==&gt; MapReduce2.x<br>        master/slave : JobTracker/TaskTracker<br>        JobTracker：单点、压力大<br>        仅仅只能够支持mapreduce作业</p><pre><code>资源利用率    所有的计算框架运行在一个集群中，共享一个集群的资源，按需分配！</code></pre><p>master: resource management：ResourceManager (RM)<br>job scheduling/monitoring：per-application ApplicationMaster (AM)<br>slave: NodeManager (NM)</p><p>YARN架构<br>    Client、ResourceManager、NodeManager、ApplicationMaster<br>    master/slave: RM/NM</p><p>Client: 向RM提交任务、杀死任务等<br>ApplicationMaster：<br>    每个应用程序对应一个AM<br>    AM向RM申请资源用于在NM上启动对应的Task<br>    数据切分<br>    为每个task向RM申请资源（container）<br>    NodeManager通信<br>    任务的监控</p><p>NodeManager： 多个<br>    干活<br>    向RM发送心跳信息、任务的执行情况<br>    接收来自RM的请求来启动任务<br>    处理来自AM的命令</p><p>ResourceManager:集群中同一时刻对外提供服务的只有1个，负责资源相关<br>    处理来自客户端的请求：提交、杀死<br>    启动/监控AM<br>    监控NM<br>    资源相关</p><p>container：任务的运行抽象<br>    memory、cpu….<br>    task是运行在container里面的<br>    可以运行am、也可以运行map/reduce task</p><p>提交自己开发的MR作业到YARN上运行的步骤：<br>1）mvn clean package -DskipTests<br>    windows/Mac/Linux ==&gt; Maven<br>2）把编译出来的jar包(项目根目录/target/…jar)以及测试数据上传到服务器<br>    scp xxxx hadoop@hostname:directory<br>3) 把数据上传到HDFS<br>    hadoop fs -put xxx hdfspath<br>4) 执行作业<br>    hadoop jar xxx.jar 完整的类名(包名+类名) args…..<br>5) 到YARN UI(8088) 上去观察作业的运行情况<br>6）到输出目录去查看对应的输出结果</p><h2 id="6-电商项目实战Hadoop实现"><a href="#6-电商项目实战Hadoop实现" class="headerlink" title="6.电商项目实战Hadoop实现"></a>6.电商项目实战Hadoop实现</h2><p>用户行为日志：<br>    每一次访问的行为(访问、搜索)产生的日志<br>    历史行为数据 &lt;== 历史订单<br>    ==&gt; 推荐<br>    ==&gt; 订单的转换量/率</p><p>原始日志字段说明:<br>    第二个字段：url<br>    第十四字段：ip<br>    第十八字段：time</p><p>==&gt; 字段的解析<br>    ip =&gt; 地市：国家、省份、城市<br>    url =&gt; 页面ID</p><pre><code>referer</code></pre><h2 id="7-数据仓库Hive"><a href="#7-数据仓库Hive" class="headerlink" title="7.数据仓库Hive"></a>7.数据仓库Hive</h2><p>HDFS上的文件并没有schema的概念<br>    schema？</p><p>Hive底层执行引擎支持：MR/Tez/Spark</p><p>统一元数据管理：<br>    Hive数据是存放在HDFS<br>    元数据信息(记录数据的数据)是存放在MySQL中<br>    SQL on Hadoop： Hive、Spark SQL、impala….</p><p>Hive体系架构<br>    client：shell、thrift/jdbc(server/jdbc)、WebUI(HUE/Zeppelin)<br>    metastore：==&gt; MySQL<br>        database：name、location、owner….<br>        table：name、location、owner、column name/type ….</p><p>Hive部署<br>    1）下载<br>    2）解压到~/app<br>    3）添加HIVE_HOME到系统环境变量<br>    4）修改配置<br>        hive-env.sh<br>        hive-site.xml<br>    5) 拷贝MySQL驱动包到$HIVE_HOME/lib<br>    6) 前提是要准备安装一个MySQL数据库，yum install去安装一个MySQL数据库<br>    <a href="https://www.cnblogs.com/julyme/p/5969626.html" target="_blank" rel="noopener">https://www.cnblogs.com/julyme/p/5969626.html</a></p><pre><code>&lt;?xml version=&quot;1.0&quot;?&gt;&lt;?xml-stylesheet type=&quot;text/xsl&quot; href=&quot;configuration.xsl&quot;?&gt;&lt;configuration&gt;&lt;property&gt;  &lt;name&gt;javax.jdo.option.ConnectionURL&lt;/name&gt;  &lt;value&gt;jdbc:mysql://hadoop000:3306/hadoop_hive?createDatabaseIfNotExist=true&lt;/value&gt;&lt;/property&gt;&lt;property&gt;  &lt;name&gt;javax.jdo.option.ConnectionDriverName&lt;/name&gt;  &lt;value&gt;com.mysql.jdbc.Driver&lt;/value&gt;&lt;/property&gt;&lt;property&gt;  &lt;name&gt;javax.jdo.option.ConnectionUserName&lt;/name&gt;  &lt;value&gt;root&lt;/value&gt;&lt;/property&gt;&lt;property&gt;  &lt;name&gt;javax.jdo.option.ConnectionPassword&lt;/name&gt;  &lt;value&gt;root&lt;/value&gt;&lt;/property&gt;&lt;/configuration&gt;</code></pre><p>Hive部署架构图</p><p><img src="https://lixiangbetter.github.io/2020/10/21/hadoop%E5%9F%BA%E7%A1%80/Hive%E9%83%A8%E7%BD%B2%E6%9E%B6%E6%9E%84.png" alt></p><p>DDL：Hive Data Definition Language<br>    create、delete、alter…</p><p>Hive数据抽象/结构<br>    database     HDFS一个目录<br>        table    HDFS一个目录<br>            data  文件<br>            partition 分区表  HDFS一个目录<br>                data  文件<br>                bucket  分桶   HDFS一个文件</p><pre><code>CREATE (DATABASE|SCHEMA) [IF NOT EXISTS] database_name  [COMMENT database_comment]  [LOCATION hdfs_path]  [WITH DBPROPERTIES (property_name=property_value, ...)];</code></pre><p>CREATE DATABASE IF NOT EXISTS hive;</p><p>CREATE DATABASE IF NOT EXISTS hive2 LOCATION ‘/test/location’;</p><p>CREATE DATABASE IF NOT EXISTS hive3<br>WITH DBPROPERTIES(‘creator’=’pk’);</p><p>/user/hive/warehouse是Hive默认的存储在HDFS上的路径</p><p>CREATE TABLE emp(<br>empno int,<br>ename string,<br>job string,<br>mgr int,<br>hiredate string,<br>sal double,<br>comm double,<br>deptno int<br>) ROW FORMAT DELIMITED FIELDS TERMINATED BY ‘\t’;</p><p>LOAD DATA LOCAL INPATH ‘/home/hadoop/data/emp.txt’ OVERWRITE INTO TABLE emp;</p><pre><code>CREATE [TEMPORARY] [EXTERNAL] TABLE [IF NOT EXISTS] [db_name.]table_name    -- (Note: TEMPORARY available in Hive 0.14.0 and later)  [(col_name data_type [COMMENT col_comment], ... [constraint_specification])]  [COMMENT table_comment]  [PARTITIONED BY (col_name data_type [COMMENT col_comment], ...)]  [CLUSTERED BY (col_name, col_name, ...) [SORTED BY (col_name [ASC|DESC], ...)] INTO num_buckets BUCKETS]  [SKEWED BY (col_name, col_name, ...)                  -- (Note: Available in Hive 0.10.0 and later)]     ON ((col_value, col_value, ...), (col_value, col_value, ...), ...)     [STORED AS DIRECTORIES]  [   [ROW FORMAT row_format]    [STORED AS file_format]     | STORED BY &apos;storage.handler.class.name&apos; [WITH SERDEPROPERTIES (...)]  -- (Note: Available in Hive 0.6.0 and later)  ]  [LOCATION hdfs_path]  [TBLPROPERTIES (property_name=property_value, ...)]   -- (Note: Available in Hive 0.6.0 and later)  [AS select_statement];   -- (Note: Available in Hive 0.5.0 and later; not supported for external tables)LOAD DATA [LOCAL] INPATH &apos;filepath&apos; [OVERWRITE] INTO TABLE tablename [PARTITION (partcol1=val1, partcol2=val2 ...)]</code></pre><p>LOCAL：本地系统，如果没有local那么就是指的HDFS的路径<br>OVERWRITE：是否数据覆盖，如果没有那么就是数据追加</p><p>LOAD DATA LOCAL INPATH ‘/home/hadoop/data/emp.txt’ OVERWRITE INTO TABLE emp;</p><p>LOAD DATA INPATH ‘hdfs://hadoop000:8020/data/emp.txt’ INTO TABLE emp;</p><p>INSERT OVERWRITE LOCAL DIRECTORY ‘/tmp/hive/‘<br>ROW FORMAT DELIMITED FIELDS TERMINATED BY ‘\t’<br>select empno,ename,sal,deptno from emp;</p><p>聚合： max/min/sum/avg</p><p>分组函数： group by</p><pre><code>求每个部门的平均工资    出现在select中的字段，如果没有出现在聚合函数里，那么一定要实现在group by里    select deptno, avg(sal) from emp group by deptno;求每个部门、工作岗位的平均工资select deptno,job avg(sal) from emp group by deptno,job;求每个部门的平均工资大于2000的部门select deptno, avg(sal) avg_sal from emp group by deptno where avg_sal&gt;2000;对于分组函数过滤要使用havingselect deptno, avg(sal) avg_sal from emp group by deptno having avg_sal&gt;2000;    </code></pre><p>join ： 多表</p><p>emp<br>dept</p><p>CREATE TABLE dept(<br>deptno int,<br>dname string,<br>loc string<br>) ROW FORMAT DELIMITED FIELDS TERMINATED BY ‘\t’;</p><p>LOAD DATA LOCAL INPATH ‘/home/hadoop/data/dept.txt’ OVERWRITE INTO TABLE dept;</p><p>explain EXTENDED<br>select<br>e.empno,e.ename,e.sal,e.deptno,d.dname<br>from emp e join dept d<br>on e.deptno=d.deptno;</p><h2 id="8-电商项目实战Hive实现"><a href="#8-电商项目实战Hive实现" class="headerlink" title="8.电商项目实战Hive实现"></a>8.电商项目实战Hive实现</h2><p>技术架构图</p><p><img src="https://lixiangbetter.github.io/2020/10/21/hadoop%E5%9F%BA%E7%A1%80/%E6%8A%80%E6%9C%AF%E6%9E%B6%E6%9E%84.png" alt></p><p>Hive外部表</p><p>CREATE TABLE emp(<br>empno int,<br>ename string,<br>job string,<br>mgr int,<br>hiredate string,<br>sal double,<br>comm double,<br>deptno int<br>) ROW FORMAT DELIMITED FIELDS TERMINATED BY ‘\t’;</p><p>LOAD DATA LOCAL INPATH ‘/home/hadoop/data/emp.txt’ OVERWRITE INTO TABLE emp;</p><p>MANAGED_TABLE:内部表<br>删除表：HDFS上的数据被删除 &amp; Meta也被删除</p><p>CREATE EXTERNAL TABLE emp_external(<br>empno int,<br>ename string,<br>job string,<br>mgr int,<br>hiredate string,<br>sal double,<br>comm double,<br>deptno int<br>) ROW FORMAT DELIMITED FIELDS TERMINATED BY ‘\t’<br>location ‘/external/emp/‘;</p><p>LOAD DATA LOCAL INPATH ‘/home/hadoop/data/emp.txt’ OVERWRITE INTO TABLE emp_external;</p><p>EXTERNAL_TABLE<br>    HDFS上的数据不被删除 &amp; Meta被删除</p><p>分区表</p><p>create external table track_info(<br>ip string,<br>country string,<br>province string,<br>city string,<br>url string,<br>time string,<br>page string<br>) partitioned by (day string)<br>ROW FORMAT DELIMITED FIELDS TERMINATED BY ‘\t’<br>location ‘/project/trackinfo/‘;</p><p>crontab表达式进行调度<br>Azkaban调度：ETLApp==&gt;其他的统计分析<br>    PySpark及调度系统<br>        <a href="https://coding.imooc.com/class/chapter/249.html#Anchor" target="_blank" rel="noopener">https://coding.imooc.com/class/chapter/249.html#Anchor</a></p><p>LOAD DATA INPATH ‘hdfs://hadoop000:8020/project/input/etl’ OVERWRITE INTO TABLE track_info partition(day=’2013-07-21’);</p><p>select count(*) from track_info where day=’2013-07-21’  ;</p><p>select province,count(*) as cnt from track_info where day=’2013-07-21’ group by province ;</p><p>省份统计表<br>create table track_info_province_stat(<br>province string,<br>cnt bigint<br>) partitioned by (day string)<br>ROW FORMAT DELIMITED FIELDS TERMINATED BY ‘\t’;</p><p>insert overwrite table track_info_province_stat partition(day=’2013-07-21’)<br>select province,count(*) as cnt from track_info where day=’2013-07-21’ group by province ;</p><p>到现在为止，我们统计的数据已经在Hive表track_info_province_stat<br>而且这个表是一个分区表，后续统计报表的数据可以直接从这个表中查询<br>也可以将hive表的数据导出到RDBMS（sqoop）</p><p>1）ETL<br>2）把ETL输出的数据加载到track_info分区表里<br>3）各个维度统计结果的数据输出到各自维度的表里（track_info_province_stat）<br>4）将数据导出（optional）</p><p>如果一个框架不能落地到SQL层面，这个框架就不是一个非常适合的框架</p><h2 id="9-Hadoop分布式集群搭建"><a href="#9-Hadoop分布式集群搭建" class="headerlink" title="9.Hadoop分布式集群搭建"></a>9.Hadoop分布式集群搭建</h2><p>Hadoop集群规划<br>    HDFS: NN DN<br>    YARN: RM NM</p><p>hadoop000 192.168.199.234<br>    NN RM<br>    DN NM<br>hadoop001 192.168.199.235<br>    DN NM<br>hadoop002 192.168.199.236<br>    DN NM</p><p>(每台)<br>/etc/hostname: 修改hostname(hadoop000/hadoop001/hadoop002)<br>/etc/hosts： ip和hostname的映射关系<br>    192.168.199.234 hadoop000<br>    192.168.199.235 hadoop001<br>    192.168.199.236 hadoop002<br>    192.168.199.234 localhost</p><p>前置安装 ssh<br>(每台)ssh免密码登陆：ssh-keygen -t rsa<br>在hadoop000机器上进行caozuo<br>ssh-copy-id -i ~/.ssh/id_rsa.pub hadoop000<br>ssh-copy-id -i ~/.ssh/id_rsa.pub hadoop001<br>ssh-copy-id -i ~/.ssh/id_rsa.pub hadoop002</p><p>JDK安装<br>1）先在hadoop000机器上部署了jdk<br>2）将jdk bin配置到系统环境变量<br>3）将jdk拷贝到其他节点上去(从hadoop000机器出发)<br>scp -r jdk1.8.0_91 hadoop@hadoop001:<del>/app/<br>scp -r jdk1.8.0_91 hadoop@hadoop002:</del>/app/</p><p>scp <del>/.bash_profile hadoop@hadoop001:</del>/<br>scp <del>/.bash_profile hadoop@hadoop002:</del>/</p><p>Hadoop部署<br>1）hadoop-env.sh<br>    JAVA_HOME<br>2) core-site.xml<br><property><br>    <name>fs.default.name</name><br>    <value>hdfs://hadoop000:8020</value><br></property></p><p>3) hdfs-site.xml<br><property><br>  <name>dfs.namenode.name.dir</name><br>  <value>/home/hadoop/app/tmp/dfs/name</value><br></property></p><property>  <name>dfs.datanode.data.dir</name>  <value>/home/hadoop/app/tmp/dfs/data</value></property><p>4) yarn-site.xml<br><property><br>  <name>yarn.nodemanager.aux-services</name><br>  <value>mapreduce_shuffle</value><br> </property></p><property>    <name>yarn.resourcemanager.hostname</name>    <value>hadoop000</value></property>5) mapred-site.xml<property>    <name>mapreduce.framework.name</name>    <value>yarn</value></property><p>6) slaves</p><p>7) 分发hadoop到其他机器<br>scp -r hadoop-2.6.0-cdh5.15.1 hadoop@hadoop001:<del>/app/<br>scp -r hadoop-2.6.0-cdh5.15.1 hadoop@hadoop002:</del>/app/</p><p>scp <del>/.bash_profile hadoop@hadoop001:</del>/<br>scp <del>/.bash_profile hadoop@hadoop002:</del>/</p><p>8) NN格式化： hadoop namenode -format<br>9) 启动HDFS<br>10) 启动YARN</p>]]></content>
      
      
      <categories>
          
          <category> bigdata </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Hadoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>flink学习笔记</title>
      <link href="/2020/10/16/flink%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/"/>
      <url>/2020/10/16/flink%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<h1 id="Flink学习笔记"><a href="#Flink学习笔记" class="headerlink" title="Flink学习笔记"></a>Flink学习笔记</h1><h2 id="1-初识FLink"><a href="#1-初识FLink" class="headerlink" title="1.初识FLink"></a>1.初识FLink</h2><h3 id="Flink是什么"><a href="#Flink是什么" class="headerlink" title="Flink是什么"></a>Flink是什么</h3><p>Apache Flink是一个框架和分布式处理引擎，用于对<strong>无限制和有限制的</strong>数据流进行有状态的计算。Flink被设计为可在<strong>所有常见的集群环境中</strong>运行，<strong>以内存速度</strong>和<strong>任何规模</strong>执行计算。</p><p>Unbounded data: 有头无尾</p><p>Bounded data: 有头有尾</p><p>==&gt; 都可以使用flink来进行处理，对应的就是流处理和批处理</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Spark: Streaming 结构化流 批处理为主</span><br><span class="line">流处理是批处理的一个特例（mini batch)</span><br><span class="line">Flink: 流式为主，批处理是流式处理的一个特例</span><br><span class="line">Storm: 流式 tuple</span><br></pre></td></tr></table></figure><h2 id="2-快速上手开发第一个Flink应用程序"><a href="#2-快速上手开发第一个Flink应用程序" class="headerlink" title="2.快速上手开发第一个Flink应用程序"></a>2.快速上手开发第一个Flink应用程序</h2><p>环境准备<br>    JDK:<br>        下载地址：<a href="https://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html" target="_blank" rel="noopener">https://www.oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html</a><br>        Mac ：dmg<br>        Linux： tar.gz<br>        Windows： exe</p><pre><code>Maven    官网：maven.apache.org    下载地址：https://archive.apache.org/dist/maven/maven-3/3.3.9/binaries/apache-maven-3.3.9-bin.tar.gz    Linux/Mac/Windows：解压        tar -zxvf apache-maven-3.3.9-bin.tar.gz -C ~/app    conf/setting.xml    &lt;localRepository&gt;/Users/rocky/maven_repos&lt;/localRepository&gt;</code></pre><p>Flink开发批处理应用程序<br>    需求：词频统计(word count)<br>        一个文件，统计文件中每个单词出现的次数<br>        分隔符是\t<br>        统计结果我们直接打印在控制台（生产上肯定是Sink到目的地）<br>    实现：<br>        Flink + Java<br>            前置条件： Maven 3.0.4 (or higher) and Java 8.x</p><pre><code>    第一种创建项目的方式：        mvn archetype:generate                               \        -DarchetypeGroupId=org.apache.flink              \        -DarchetypeArtifactId=flink-quickstart-java      \        -DarchetypeVersion=1.7.0 \        -DarchetypeCatalog=local        out of the box：OOTB 开箱即用    开发流程/开发八股文编程        1）set up the batch execution environment        2）read        3）transform operations  开发的核心所在：开发业务逻辑        4）execute program    功能拆解        1）读取数据              hello    welcome        2）每一行的数据按照指定的分隔符拆分            hello            welcome        3）为每一个单词赋上次数为1            (hello,1)            (welcome,1)            4) 合并操作  groupBy    Flink + Scala    前置条件：Maven 3.0.4 (or higher) and Java 8.x     mvn archetype:generate                               \    -DarchetypeGroupId=org.apache.flink              \    -DarchetypeArtifactId=flink-quickstart-scala     \    -DarchetypeVersion=1.7.0 \    -DarchetypeCatalog=localFlink Java vs Scala    1) 算子  map  filter      2）简洁性</code></pre><h2 id="3-编程模型及核心概念"><a href="#3-编程模型及核心概念" class="headerlink" title="3.编程模型及核心概念"></a>3.编程模型及核心概念</h2><p>大数据处理的流程：<br>    MapReduce：input -&gt; map(reduce) -&gt; output<br>    Storm:  input -&gt; Spout/Bolt -&gt; output<br>    Spark: input -&gt; transformation/action –&gt; output<br>    Flink:  input -&gt;  transformation/sink –&gt; output</p><p>DataSet and DataStream<br>    immutable<br>    批处理：DataSet<br>    流处理：DataStream</p><p>Flink编程模型<br>    1）获取执行环境<br>    2）获取数据<br>    3）transformation<br>    4）sink<br>    5）触发执行</p><p>select a.<em>, b.</em> from a join b on a.id = b.id</p><p>&lt;a1,(a.*)&gt;</p><p>&lt;b1,(b.*)&gt;</p><h2 id="4-DataSet-API编程"><a href="#4-DataSet-API编程" class="headerlink" title="4.DataSet API编程"></a>4.DataSet API编程</h2><h2 id="5-DataStream-API编程"><a href="#5-DataStream-API编程" class="headerlink" title="5.DataStream API编程"></a>5.DataStream API编程</h2><h2 id="6-Flink-Table-API-amp-SQL编程"><a href="#6-Flink-Table-API-amp-SQL编程" class="headerlink" title="6.Flink Table API &amp; SQL编程"></a>6.Flink Table API &amp; SQL编程</h2><p>DataSet&amp;DataStream API<br>    1) 熟悉两套API：DataSet/DataStream   Java/Scala<br>        MapReduce ==&gt; Hive  SQL<br>        Spark ==&gt; Spark SQL<br>        Flink ==&gt; SQL<br>    2) Flink是支持批处理/流处理，如何做到API层面的统一    </p><p>==&gt; Table &amp; SQL API  关系型API</p><p>Everybody knows SQL</p><h2 id="7-Flink中的Time及Windows的使用"><a href="#7-Flink中的Time及Windows的使用" class="headerlink" title="7.Flink中的Time及Windows的使用"></a>7.Flink中的Time及Windows的使用</h2><p>对于Flink里面的三种时间<br>    事件时间  10:30<br>    摄取时间  11:00<br>    处理时间  11:30</p><p>思考：<br>对于流处理来说，你们觉得应该是以哪个时间作为基准时间来进行业务逻辑的处理呢？</p><p>幂等性</p><p>env.setStreamTimeCharacteristic(TimeCharacteristic.EventTime)<br>思考：默认的TimeCharacteristic是什么？</p><p>窗口分配器：定义如何将数据分配给窗口</p><p>A WindowAssigner is responsible for assigning each incoming element to one or more windows<br>每个传入的数据分配给一个或者多个窗口</p><p>tumbling windows 滚动窗口<br>    have a fixed size and do not overlap<br>sliding windows  滑动窗口<br>    overlapping   业务案例：每隔半小时，统计前一个小时的top n商品。<br>session windows  会话窗口<br>global windows   全局窗口</p><p>[start timestamp , end timestamp)</p><p>ReduceFunction: 两两进行处理</p><p>ProcessWindowFunction：整个窗口全部到了，再处理。场景：对窗口中的数据排序</p><p><a href="https://blog.csdn.net/lmalds/article/details/52704170" target="_blank" rel="noopener">https://blog.csdn.net/lmalds/article/details/52704170</a></p><h2 id="8-Flink-Connectors"><a href="#8-Flink-Connectors" class="headerlink" title="8.Flink Connectors"></a>8.Flink Connectors</h2><p>ZooKeeper<br>    <a href="https://archive.cloudera.com/cdh5/cdh/5/" target="_blank" rel="noopener">https://archive.cloudera.com/cdh5/cdh/5/</a></p><pre><code>ssh hadoop@192.168.199.2331) 从~/software下解压到~/app目录下2) 配置系统环境变量  ~/.bash_profile3) 配置文件  $ZK_HOME/conf.zoo.cfg  dataDir不要放在默认的/tmp下4) 启动ZK   $ZK_HOME/bin/zkServer.sh start5) 检查是否启动成功   jps  QuorumPeerMain</code></pre><p>Kafka<br>    wget <a href="http://mirrors.tuna.tsinghua.edu.cn/apache/kafka/1.1.1/kafka_2.11-1.1.1.tgz" target="_blank" rel="noopener">http://mirrors.tuna.tsinghua.edu.cn/apache/kafka/1.1.1/kafka_2.11-1.1.1.tgz</a></p><pre><code>ssh hadoop@192.168.199.2331) 从~/software下解压到~/app目录下2) 配置系统环境变量  ~/.bash_profile3) 配置文件 $KAFKA_HOME/config/server.properties    log.dirs 不要放在默认的/tmp下4) 启动Kafka$KAFKA_HOME/bin/kafka-server-start.sh -daemon /home/hadoop/app/kafka_2.11-1.1.1/config/server.properties 5) 检查是否启动成功 jps Kafka6) 创建topic</code></pre><p>./kafka-topics.sh –create –zookeeper hadoop000:2181 –replication-factor 1 –partitions 1 –topic pktest<br>    7) 查看所有的topic<br>./kafka-topics.sh –list –zookeeper hadoop000:2181<br>    8) 启动生产者<br>./kafka-console-producer.sh –broker-list hadoop000:9092 –topic pktest<br>    9) 启动消费者<br>./kafka-console-consumer.sh –bootstrap-server hadoop000:9092 –topic pktest    </p><p>作业：请使用Java语言开发FlinkKafkaConsumer/FlinkKafkaProducer实例</p><h2 id="9-Flink部署及作业提交"><a href="#9-Flink部署及作业提交" class="headerlink" title="9.Flink部署及作业提交"></a>9.Flink部署及作业提交</h2><p>Flink的单机部署方式<br>    开发/测试</p><p>前置条件：<br>    JDK8<br>    Maven3</p><p>ssh <a href="mailto:hadoop@192.168.199.233" target="_blank" rel="noopener">hadoop@192.168.199.233</a></p><p>通过下载Flink源码进行编译，不是使用直接下载二进制包<br>下载到:<br>    1）服务器：~/source  wget <a href="https://github.com/apache/flink/archive/release-1.7.0.tar.gz" target="_blank" rel="noopener">https://github.com/apache/flink/archive/release-1.7.0.tar.gz</a><br>    2) 本地：<a href="https://github.com/apache/flink/archive/release-1.7.0.tar.gz" target="_blank" rel="noopener">https://github.com/apache/flink/archive/release-1.7.0.tar.gz</a></p><p>mvn clean install -DskipTests -Pvendor-repos -Dfast -Dhadoop.version=2.6.0-cdh5.15.1</p><p>第一次编译是需要花费很长时间的，因为需要去中央仓库下载flink源码中所有的依赖包</p><p>Standalone的最简单的方式</p><p>./bin/flink run examples/streaming/SocketWindowWordCount.jar –port 9000</p><p>./bin/flink # 路径  $FLINK_HOME</p><p>Standalone-分布式<br>1） Java 1.8.x or higher<br>2） ssh  多个机器之间要互通     Hadoop详细讲解<br>    ping hadoop000<br>    ping hadoop001<br>    ping hadoop002</p><pre><code>JDKFlink  同一个目录  集群里面的机器 部署的目录都是一样每个机器需要添加ip和hostname的映射关系</code></pre><p>3) conf<br>    flink-conf.yaml<br>        jobmanager.rpc.address: 10.0.0.1  配置主节点的ip</p><pre><code>jobmanager   主节点taskmanager  从节点slaves    每一行配置一个ip/host</code></pre><p>4）常用配置<br>    jobmanager.rpc.address   master节点的地址<br>    jobmanager.heap.mb  jobmanager节点可用的内存<br>    taskmanager.heap.mb taskmanager节点可用的内存<br>    taskmanager.numberOfTaskSlots 每个机器可用的cpu个数<br>    parallelism.default   任务的并行度<br>    taskmanager.tmp.dirs  taskmanager的临时数据存储目录</p><p>扩展或者容错</p><p>ON YARN是企业级用的最多的方式  *****</p><p>-n taskmanager的数量<br>-jm jobmanager的内存<br>-tm taskmanager的内存</p><p>./bin/flink run ./examples/batch/WordCount.jar <br>-input hdfs://hadoop000:8020/LICENSE-2.0.txt <br>-output hdfs://hadoop000:8020/wordcount-result.txt</p><p>./bin/flink run -m yarn-cluster -yn 1 ./examples/batch/WordCount.jar</p><p>作业：<br>1） 快速开发一个Flink应用程序<br>    Scala&amp;Java<br>    批处理&amp;流处理</p><p>批处理Scala和批处理Java的Flink作业提交到YARN上去执行，任意YARN模式</p><p>2）可选 HA的配置<br><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.7/ops/jobmanager_high_availability.html" target="_blank" rel="noopener">https://ci.apache.org/projects/flink/flink-docs-release-1.7/ops/jobmanager_high_availability.html</a></p><h2 id="10-Flink监控及调优"><a href="#10-Flink监控及调优" class="headerlink" title="10.Flink监控及调优"></a>10.Flink监控及调优</h2><p>History Server<br>    Hadoop MapReduce<br>    Spark<br>    Flink</p><p>start/stop-xxx.sh<br>    看一下这些脚本的写法<br>    shell对于bigdata有用吗？ lower</p><p>配置：<br>historyserver.web.address: 0.0.0.0<br>historyserver.web.port: 8082<br>historyserver.archive.fs.refresh-interval: 10000</p><p>jobmanager.archive.fs.dir: hdfs://hadoop000:8020/completed-jobs-pk/<br>historyserver.archive.fs.dir: hdfs://hadoop000:8020/completed-jobs-pk/</p><p>启动：./historyserver.sh start</p><p>思考：有了HistoryServer之后为什么还需要提供REST API？</p><p>Ganglia</p><p>Flink中常用的优化策略<br>1）资源<br>2）并行度<br>    默认是1   适当的调整：好几种  ==&gt; 项目实战<br>3）数据倾斜<br>    100task  98-99跑完了  1-2很慢   ==&gt; 能跑完 、 跑不完<br>    group by： 二次聚合<br>        random_key  + random<br>        key  - random<br>    join on xxx=xxx<br>        repartition-repartition strategy  大大<br>        broadcast-forward strategy  大小</p><p><a href="https://dzone.com/articles/four-ways-to-optimize-your-flink-applications" target="_blank" rel="noopener">https://dzone.com/articles/four-ways-to-optimize-your-flink-applications</a></p><h2 id="11-基于Flink的互联网直播平台日志分析项目实战"><a href="#11-基于Flink的互联网直播平台日志分析项目实战" class="headerlink" title="11.基于Flink的互联网直播平台日志分析项目实战"></a>11.基于Flink的互联网直播平台日志分析项目实战</h2><p>项目背景<br>aliyun    CN    A    E    [17/Jul/2018:17:07:50 +0800]    2    223.104.18.110    -    112.<br>29.213.35:80    0    v2.go2yd.com    GET    <a href="http://v1.go2yd.com/user_upload/1531633977627104fdec" target="_blank" rel="noopener">http://v1.go2yd.com/user_upload/1531633977627104fdec</a><br>dc68fe7a2c4b96b2226fd3f4c.mp4_bd.mp4    HTTP/1.1    -    bytes 13869056-13885439/25136186    TCP_HIT/206    112.29.213.35    video/mp4    17168    16384    -:0    0    0    -    -    11451601    -    “JSP3/2.0.14”    “-“    “-“    “-“    http    -    2    v1.g<br>o2yd.com    0.002    25136186    16384    -    -    -    -    -    -    -    1531818470104-114516<br>01-112.29.213.66#2705261172    644514568</p><p>aliyun<br>CN<br>E<br>[17/Jul/2018:17:07:50 +0800]<br>223.104.18.110<br>v2.go2yd.com<br>17168</p><p>接入的数据类型就是日志<br>离线：Flume==&gt;HDFS<br>实时：Kafka==&gt;流处理引擎==&gt;ES==&gt;Kibana</p><p>项目架构图</p><p><img src="https://lixiangbetter.github.io/2020/10/16/flink%E5%AD%A6%E4%B9%A0%E7%AC%94%E8%AE%B0/1589529851438.jpg" alt></p><p>项目功能<br>1）统计一分钟内每个域名访问产生的流量<br>    Flink接收Kafka的进行处理<br>2）统计一分钟内每个用户产生的流量<br>    域名和用户是有对应关系的<br>    Flink接收Kafka的进行 + Flink读取域名和用户的配置数据  进行处理</p><p>数据：Mock  *****</p><p>Mock数据：务必要掌握的<br>    数据敏感<br>    多团队协作，你依赖了其他团队提供的服务或者接口</p><pre><code>通过Mock的方式往Kafka的broker里面发送数据Java/Scala Code：producerkafka控制台消费者：consumer</code></pre><p>需求：最近一分钟每个域名对应的流量</p><p>问题：<br>    可以到QQ群或者问答区进行交流，<br>    我在群里的，<br>    问答区的问题我会用最快的速度答疑</p><p>ES部署<br>    1） CentOS7.x<br>    2） 非root     hadoop</p><p>ELK</p><p>Kibana部署</p><p>curl -XPUT ‘<a href="http://hadoop000:9200/cdn&#39;" target="_blank" rel="noopener">http://hadoop000:9200/cdn&#39;</a></p><p>curl -H “Content-Type: application/json” -XPOST ‘<a href="http://hadoop000:9200/cdn/traffic/_mapping?pretty&#39;" target="_blank" rel="noopener">http://hadoop000:9200/cdn/traffic/_mapping?pretty&#39;</a> -d ‘{<br>“traffic”:{<br>    “properties”:{<br>        “domain”:{“type”:”text”},<br>        “traffics”:{“type”:”long”},<br>        “time”:{“type”:”date”,”format”: “yyyy-MM-dd HH:mm”}<br>        }<br>    }<br>}<br>‘</p><p>curl -XDELETE ‘hadoop000:9200/cdn’</p><p>curl -H “Content-Type: application/json” -XPOST ‘<a href="http://hadoop000:9200/cdn/traffic/_mapping?pretty&#39;" target="_blank" rel="noopener">http://hadoop000:9200/cdn/traffic/_mapping?pretty&#39;</a> -d ‘{<br>“traffic”:{<br>    “properties”:{<br>        “domain”:{“type”:”keyword”},<br>        “traffics”:{“type”:”long”},<br>        “time”:{“type”:”date”,”format”: “yyyy-MM-dd HH:mm”}<br>        }<br>    }<br>}<br>‘</p><p>作业：<br>1）代码我们都是本地IDEA中运行的，将代码打包，运行在YARN上<br>2）把代码中写死的信息(ip port)改成读配置的方式</p><p>需求：CDN业务<br>userid对应多个域名</p><p>userid: 8000000</p><p>domains:<br>    v1.go2yd.com<br>    v2.go2yd.com<br>    v3.go2yd.com<br>    v4.go2yd.com<br>    vmi.go2yd.com</p><p>userid: 8000001<br>    test.gifshow.com</p><p>用户id和域名的映射关系<br>    从日志里能拿到domain，还得从另外一个表(MySQL)里面去获取userid和domain的映射关系</p><p>CREATE TABLE user_domain_config(<br>id int unsigned auto_increment,<br>user_id varchar(40) not null,<br>domain varchar(40) not null,<br>primary key (id)<br>);</p><p>insert into user_domain_config(user_id,domain) values(‘8000000’,’v1.go2yd.com’);<br>insert into user_domain_config(user_id,domain) values(‘8000000’,’v2.go2yd.com’);<br>insert into user_domain_config(user_id,domain) values(‘8000000’,’v3.go2yd.com’);<br>insert into user_domain_config(user_id,domain) values(‘8000000’,’v4.go2yd.com’);<br>insert into user_domain_config(user_id,domain) values(‘8000000’,’vmi.go2yd.com’);</p><p>在做实时数据清洗的时候，不仅需要处理raw日志，还需要关联MySQL表里的数据</p><p>自定义一个Flink去读MySQL数据的数据源，然后把两个Stream关联起来</p><p>Flink进行数据的清洗<br>    读取Kafka的数据<br>    读取MySQL的数据<br>    connect</p><pre><code>业务逻辑的处理分析：水印 WindowFunction==&gt; ES 注意数据类型  &lt;= Kibana 图形化的统计结果展示</code></pre><p>Kibana：各个环节的监控  监控图形化</p><p>1 30<br>2 40<br>3 300<br>4 35</p><p>我们已经实现的 +  CDN业务文档的描述  ==&gt; 扩展</p><h2 id="12-Flink版本升级"><a href="#12-Flink版本升级" class="headerlink" title="12.Flink版本升级"></a>12.Flink版本升级</h2><p>1）代码层面 pom.xml flink.version</p><p>2）服务器运行环境的层面</p><p>​    standalone    每个服务器都得升级Flink版本</p><p>​    yarn    Flink仅仅是作为一个客户端进行作业的提交的，只需要在你的flink作业的提交机器上升级flink即可</p><p>3）Flink的部署包也要升级</p><p>​    获取到最新的Flink的源码，然后根据你的hadoop版本如何做好升级呢？</p><p><a href="https://ci.apache.org/projects/flink/flink-docs-release-1.11/ops/upgrading.html" target="_blank" rel="noopener">https://ci.apache.org/projects/flink/flink-docs-release-1.11/ops/upgrading.html</a></p>]]></content>
      
      
      <categories>
          
          <category> bigdata </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flink </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>架构设计&amp;分布式&amp;数据结构与算法笔记</title>
      <link href="/2020/07/19/%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1-%E5%88%86%E5%B8%83%E5%BC%8F-%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/"/>
      <url>/2020/07/19/%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1-%E5%88%86%E5%B8%83%E5%BC%8F-%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<h1 id="架构设计-分布式-数据结构与算法笔记"><a href="#架构设计-分布式-数据结构与算法笔记" class="headerlink" title="架构设计-分布式-数据结构与算法笔记"></a>架构设计-分布式-数据结构与算法笔记</h1><h2 id="架构设计"><a href="#架构设计" class="headerlink" title="架构设计"></a>架构设计</h2><h3 id="请列举出在JDK中几个常用的设计模式？"><a href="#请列举出在JDK中几个常用的设计模式？" class="headerlink" title="请列举出在JDK中几个常用的设计模式？"></a>请列举出在JDK中几个常用的设计模式？</h3><p>单例模式（Singleton pattern）用于Runtime，Calendar和其他的一些类中。工厂模式（Factory pattern）被用于各种不可变的类如 Boolean，像Boolean.valueOf，观察者模式（Observer pattern）被用于 Swing 和很多的事件监听中。装饰器设计模式（Decorator design pattern）被用于多个 Java IO 类中。</p><h3 id="什么是设计模式？你是否在你的代码里面使用过任何设计模式？"><a href="#什么是设计模式？你是否在你的代码里面使用过任何设计模式？" class="headerlink" title="什么是设计模式？你是否在你的代码里面使用过任何设计模式？"></a>什么是设计模式？你是否在你的代码里面使用过任何设计模式？</h3><p>设计模式是软件开发人员在软件开发过程中面临的一般问题的解决方案。这些解决方案是众多软件开发人员经过相当长的一段时间的试验和错误总结出来的。设计模式是代码可用性的延伸。</p><p>设计模式分类：创建型模式，结构型模式，行为型模式</p><h3 id="静态代理、JDK动态代理以及CGLIB动态代理"><a href="#静态代理、JDK动态代理以及CGLIB动态代理" class="headerlink" title="静态代理、JDK动态代理以及CGLIB动态代理"></a>静态代理、JDK动态代理以及CGLIB动态代理</h3><p>代理模式是java中最常用的设计模式之一，尤其是在spring框架中广泛应用。对于java的代理模式，一般可分为：静态代理、动态代理、以及CGLIB实现动态代理。</p><p>对于上述三种代理模式，分别进行说明。</p><h4 id="静态代理"><a href="#静态代理" class="headerlink" title="静态代理"></a>静态代理</h4><p>静态代理其实就是在程序运行之前，提前写好被代理方法的代理类，编译后运行。在程序运行之前，class已经存在。<br>下面我们实现一个静态代理demo:</p><p><img src="https://lixiangbetter.github.io/2020/07/19/%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1-%E5%88%86%E5%B8%83%E5%BC%8F-%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy8zMjM3NDMyLTVkNmViYWI3YTlmM2Q4MzIucG5n.jpeg" alt></p><p>定义一个接口Target</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.test.proxy;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">Target</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">execute</span><span class="params">()</span></span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>TargetImpl 实现接口Target</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.test.proxy;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TargetImpl</span> <span class="keyword">implements</span> <span class="title">Target</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">execute</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        System.out.println(<span class="string">"TargetImpl execute！"</span>);</span><br><span class="line">        <span class="keyword">return</span> <span class="string">"execute"</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.test.proxy;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Proxy</span> <span class="keyword">implements</span> <span class="title">Target</span></span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> Target target;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">Proxy</span><span class="params">(Target target)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.target = target;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">execute</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        System.out.println(<span class="string">"perProcess"</span>);</span><br><span class="line">        String result = <span class="keyword">this</span>.target.execute();</span><br><span class="line">        System.out.println(<span class="string">"postProcess"</span>);</span><br><span class="line">        <span class="keyword">return</span> result;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>测试类:</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.test.proxy;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ProxyTest</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">        Target target = <span class="keyword">new</span> TargetImpl();</span><br><span class="line">        Proxy p = <span class="keyword">new</span> Proxy(target);</span><br><span class="line">        String result =  p.execute();</span><br><span class="line">        System.out.println(result);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>运行结果:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">perProcess</span><br><span class="line">TargetImpl execute！</span><br><span class="line">postProcess</span><br><span class="line">execut</span><br></pre></td></tr></table></figure><p>静态代理需要针对被代理的方法提前写好代理类，如果被代理的方法非常多则需要编写很多代码，因此，对于上述缺点，通过动态代理的方式进行了弥补。</p><h4 id="动态代理"><a href="#动态代理" class="headerlink" title="动态代理"></a>动态代理</h4><p>动态代理主要是通过反射机制，在运行时动态生成所需代理的class.</p><p><img src="https://lixiangbetter.github.io/2020/07/19/%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1-%E5%88%86%E5%B8%83%E5%BC%8F-%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy8zMjM3NDMyLTYzMzU2NDcwZjY0MmQ2NjUucG5n.jpeg" alt></p><p>接口</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.test.dynamic;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">Target</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">execute</span><span class="params">()</span></span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>实现类</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.test.dynamic;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">TargetImpl</span> <span class="keyword">implements</span> <span class="title">Target</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">execute</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        System.out.println(<span class="string">"TargetImpl execute！"</span>);</span><br><span class="line">        <span class="keyword">return</span> <span class="string">"execute"</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>代理类</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.test.dynamic;</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.lang.reflect.InvocationHandler;</span><br><span class="line"><span class="keyword">import</span> java.lang.reflect.Method;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">DynamicProxyHandler</span> <span class="keyword">implements</span> <span class="title">InvocationHandler</span></span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> Target target;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">DynamicProxyHandler</span><span class="params">(Target target)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.target = target;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Object <span class="title">invoke</span><span class="params">(Object proxy, Method method, Object[] args)</span> <span class="keyword">throws</span> Throwable </span>&#123;</span><br><span class="line">        System.out.println(<span class="string">"========before=========="</span>);</span><br><span class="line">        Object result = method.invoke(target,args);</span><br><span class="line">        System.out.println(<span class="string">"========after==========="</span>);</span><br><span class="line">        <span class="keyword">return</span> result;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>测试类</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.test.dynamic;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.lang.reflect.Proxy;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">DynamicProxyTest</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        Target target = <span class="keyword">new</span> TargetImpl();</span><br><span class="line">        DynamicProxyHandler handler = <span class="keyword">new</span> DynamicProxyHandler(target);</span><br><span class="line">        Target proxySubject = (Target) Proxy.newProxyInstance(TargetImpl<span class="class">.<span class="keyword">class</span>.<span class="title">getClassLoader</span>(),<span class="title">TargetImpl</span>.<span class="title">class</span>.<span class="title">getInterfaces</span>(),<span class="title">handler</span>)</span>;</span><br><span class="line">        String result = proxySubject.execute();</span><br><span class="line">        System.out.println(result);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>运行结果：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">========before==========</span><br><span class="line">TargetImpl execute！</span><br><span class="line">========after===========</span><br><span class="line">execute</span><br></pre></td></tr></table></figure><p>无论是动态代理还是静态代理，都需要定义接口，然后才能实现代理功能。这同样存在局限性，因此，为了解决这个问题，出现了第三种代理方式：cglib代理。</p><h4 id="cglib代理"><a href="#cglib代理" class="headerlink" title="cglib代理"></a>cglib代理</h4><p>CGLib采用了非常底层的字节码技术，其原理是通过字节码技术为一个类创建子类，并在子类中采用方法拦截的技术拦截所有父类方法的调用，顺势织入横切逻辑。JDK动态代理与CGLib动态代理均是实现Spring AOP的基础。</p><p><img src="https://lixiangbetter.github.io/2020/07/19/%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1-%E5%88%86%E5%B8%83%E5%BC%8F-%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy8zMjM3NDMyLTYzOGY4MmM2ZDRkNTExNDUucG5n.jpeg" alt></p><p>目标类</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.test.cglib;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Target</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> String <span class="title">execute</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        String message = <span class="string">"-----------test------------"</span>;</span><br><span class="line">        System.out.println(message);</span><br><span class="line">        <span class="keyword">return</span> message;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>通用代理类</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.test.cglib;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> net.sf.cglib.proxy.MethodInterceptor;</span><br><span class="line"><span class="keyword">import</span> net.sf.cglib.proxy.MethodProxy;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> java.lang.reflect.Method;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">MyMethodInterceptor</span> <span class="keyword">implements</span> <span class="title">MethodInterceptor</span></span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Object <span class="title">intercept</span><span class="params">(Object obj, Method method, Object[] args, MethodProxy proxy)</span> <span class="keyword">throws</span> Throwable </span>&#123;</span><br><span class="line">        System.out.println(<span class="string">"&gt;&gt;&gt;&gt;MethodInterceptor start..."</span>);</span><br><span class="line">        Object result = proxy.invokeSuper(obj,args);</span><br><span class="line">        System.out.println(<span class="string">"&gt;&gt;&gt;&gt;MethodInterceptor ending..."</span>);</span><br><span class="line">        <span class="keyword">return</span> <span class="string">"result"</span>;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>测试类</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">package</span> com.test.cglib;</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> net.sf.cglib.proxy.Enhancer;</span><br><span class="line"></span><br><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">CglibTest</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        System.out.println(<span class="string">"***************"</span>);</span><br><span class="line">        Target target = <span class="keyword">new</span> Target();</span><br><span class="line">        CglibTest test = <span class="keyword">new</span> CglibTest();</span><br><span class="line">        Target proxyTarget = (Target) test.createProxy(Target<span class="class">.<span class="keyword">class</span>)</span>;</span><br><span class="line">        String res = proxyTarget.execute();</span><br><span class="line">        System.out.println(res);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Object <span class="title">createProxy</span><span class="params">(Class targetClass)</span> </span>&#123;</span><br><span class="line">        Enhancer enhancer = <span class="keyword">new</span> Enhancer();</span><br><span class="line">        enhancer.setSuperclass(targetClass);</span><br><span class="line">        enhancer.setCallback(<span class="keyword">new</span> MyMethodInterceptor());</span><br><span class="line">        <span class="keyword">return</span> enhancer.create();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>执行结果:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">***************</span><br><span class="line">&gt;&gt;&gt;&gt;MethodInterceptor start...</span><br><span class="line">-----------test------------</span><br><span class="line">&gt;&gt;&gt;&gt;MethodInterceptor ending...</span><br><span class="line">result</span><br></pre></td></tr></table></figure><p>代理对象的生成过程由Enhancer类实现，大概步骤如下：</p><ol><li>生成代理类Class的二进制字节码；</li><li>通过Class.forName加载二进制字节码，生成Class对象；</li><li>通过反射机制获取实例构造，并初始化代理类对象。</li></ol><h3 id="单例模式"><a href="#单例模式" class="headerlink" title="单例模式"></a>单例模式</h3><p>单例模式（Singleton Pattern）是 Java 中最简单的设计模式之一。这种类型的设计模式属于创建型模式，它提供了一种创建对象的最佳方式。</p><p><strong>意图</strong>：保证一个类仅有一个实例，并提供一个访问它的全局访问点。</p><p><strong>主要解决</strong>：一个全局使用的类频繁地创建与销毁。</p><p>懒汉式，线程安全</p><p><strong>代码实例</strong>：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Singleton2</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> Singleton2 instance;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="title">Singleton2</span><span class="params">()</span> </span>&#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">synchronized</span> Singleton2 <span class="title">getInstance</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (instance == <span class="keyword">null</span>) &#123;</span><br><span class="line">            instance = <span class="keyword">new</span> Singleton2();</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> instance;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>饿汉式，线程安全</p><p><strong>代码实例</strong>：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Singleton3</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> Singleton3 instance = <span class="keyword">new</span> Singleton3();</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="title">Singleton3</span><span class="params">()</span> </span>&#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> Singleton3 <span class="title">getInstance</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> instance;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>双检锁/双重校验锁 + volatile关键字</p><p><strong>代码实例</strong>：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Singleton7</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">static</span> <span class="keyword">volatile</span> Singleton7 instance = <span class="keyword">null</span>;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="title">Singleton7</span><span class="params">()</span> </span>&#123;&#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> Singleton7 <span class="title">getInstance</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (instance == <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="keyword">synchronized</span> (Singleton7<span class="class">.<span class="keyword">class</span>) </span>&#123;</span><br><span class="line">                <span class="keyword">if</span> (instance == <span class="keyword">null</span>) &#123;</span><br><span class="line">                    instance = <span class="keyword">new</span> Singleton7();</span><br><span class="line">                &#125;</span><br><span class="line">            &#125;</span><br><span class="line">        &#125;</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> instance;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h3 id="工厂模式"><a href="#工厂模式" class="headerlink" title="工厂模式"></a>工厂模式</h3><p>工厂模式（Factory Pattern）是 Java 中最常用的设计模式之一。这种类型的设计模式属于创建型模式，它提供了一种创建对象的最佳方式。</p><p>意图：定义一个创建对象的接口，让其子类自己决定实例化哪一个工厂类，工厂模式使其创建过程延迟到子类进行。</p><p>主要解决：主要解决接口选择的问题。</p><p>我们将创建一个 Shape 接口和实现 Shape 接口的实体类。下一步是定义工厂类 ShapeFactory。</p><p>FactoryPatternDemo，我们的演示类使用 ShapeFactory 来获取 Shape 对象。它将向 ShapeFactory 传递信息（CIRCLE / RECTANGLE / SQUARE），以便获取它所需对象的类型。</p><p><img src="https://lixiangbetter.github.io/2020/07/19/%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1-%E5%88%86%E5%B8%83%E5%BC%8F-%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/aHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL0pvdXJXb24vaW1hZ2UvbWFzdGVyLyVFOCVBRSVCRSVFOCVBRSVBMSVFNiVBOCVBMSVFNSVCQyU4Ri8lRTUlQjclQTUlRTUlOEUlODIlRTYlQTglQTElRTUlQkMlOEYuanBn.jpeg" alt></p><p>步骤 1</p><p>创建一个接口。</p><p><em>Shape.java</em></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">Shape</span> </span>&#123;</span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">draw</span><span class="params">()</span></span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>步骤 2</p><p>创建实现接口的实体类。</p><p><em>Rectangle.java</em></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Rectangle</span> <span class="keyword">implements</span> <span class="title">Shape</span> </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">draw</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        System.out.println(<span class="string">"Inside Rectangle::draw() method."</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><em>Square.java</em></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Square</span> <span class="keyword">implements</span> <span class="title">Shape</span> </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">draw</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        System.out.println(<span class="string">"Inside Square::draw() method."</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><em>Circle.java</em></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Circle</span> <span class="keyword">implements</span> <span class="title">Shape</span> </span>&#123;</span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">draw</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        System.out.println(<span class="string">"Inside Circle::draw() method."</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>步骤 3</p><p>创建一个工厂，生成基于给定信息的实体类的对象。</p><p><em>ShapeFactory.java</em></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ShapeFactory</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="comment">//使用 getShape 方法获取形状类型的对象</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> Shape <span class="title">getShape</span><span class="params">(String shapeType)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">if</span> (shapeType == <span class="keyword">null</span>) &#123;</span><br><span class="line">            <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">        &#125;</span><br><span class="line">        shapeType = shapeType.toLowerCase();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">switch</span> (shapeType) &#123;</span><br><span class="line">            <span class="keyword">case</span> <span class="string">"circle"</span>:</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> Circle();</span><br><span class="line">            <span class="keyword">case</span> <span class="string">"rectangle"</span>:</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> Rectangle();</span><br><span class="line">            <span class="keyword">case</span> <span class="string">"square"</span>:</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">new</span> Square();</span><br><span class="line">            <span class="keyword">default</span>:</span><br><span class="line">                <span class="keyword">return</span> <span class="keyword">null</span>;</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>步骤 4</p><p>使用该工厂，通过传递类型信息来获取实体类的对象。</p><p><em>FactoryPatternDemo.java</em></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">FactoryPatternDemo</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        ShapeFactory shapeFactory = <span class="keyword">new</span> ShapeFactory();</span><br><span class="line"></span><br><span class="line">        <span class="comment">//获取 Circle 的对象，并调用它的 draw 方法</span></span><br><span class="line">        Shape shape1 = shapeFactory.getShape(<span class="string">"CIRCLE"</span>);</span><br><span class="line">        <span class="comment">//调用 Circle 的 draw 方法</span></span><br><span class="line">        shape1.draw();</span><br><span class="line"></span><br><span class="line">        <span class="comment">//获取 Rectangle 的对象，并调用它的 draw 方法</span></span><br><span class="line">        Shape shape2 = shapeFactory.getShape(<span class="string">"RECTANGLE"</span>);</span><br><span class="line">        <span class="comment">//调用 Rectangle 的 draw 方法</span></span><br><span class="line">        shape2.draw();</span><br><span class="line"></span><br><span class="line">        <span class="comment">//获取 Square 的对象，并调用它的 draw 方法</span></span><br><span class="line">        Shape shape3 = shapeFactory.getShape(<span class="string">"SQUARE"</span>);</span><br><span class="line">        <span class="comment">//调用 Square 的 draw 方法</span></span><br><span class="line">        shape3.draw();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>步骤 5</p><p>验证输出。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">Inside Circle::draw() method.</span><br><span class="line">Inside Rectangle::draw() method.</span><br><span class="line">Inside Square::draw() method.</span><br></pre></td></tr></table></figure><h3 id="观察者模式"><a href="#观察者模式" class="headerlink" title="观察者模式"></a>观察者模式</h3><p>当对象间存在一对多关系时，则使用观察者模式（Observer Pattern）。比如，当一个对象被修改时，则会自动通知它的依赖对象。观察者模式属于行为型模式。</p><p>意图：定义对象间的一种一对多的依赖关系，当一个对象的状态发生改变时，所有依赖于它的对象都得到通知并被自动更新。</p><p>主要解决：一个对象状态改变给其他对象通知的问题，而且要考虑到易用和低耦合，保证高度的协作。</p><p>实现</p><p>观察者模式使用三个类 Subject、Observer 和 Client。Subject 对象带有绑定观察者到 Client 对象和从 Client 对象解绑观察者的方法。我们创建 Subject 类、Observer 抽象类和扩展了抽象类 Observer 的实体类。</p><p>ObserverPatternDemo，我们的演示类使用 Subject 和实体类对象来演示观察者模式。</p><p><img src="https://lixiangbetter.github.io/2020/07/19/%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1-%E5%88%86%E5%B8%83%E5%BC%8F-%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/aHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL0pvdXJXb24vaW1hZ2UvbWFzdGVyLyVFOCVBRSVCRSVFOCVBRSVBMSVFNiVBOCVBMSVFNSVCQyU4Ri8lRTglQTclODIlRTUlQUYlOUYlRTglODAlODUlRTYlQTglQTElRTUlQkMlOEYuanBn.jpeg" alt></p><p>步骤 1</p><p>创建 Subject 类。</p><p><em>Subject.java</em></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Subject</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> List&lt;Observer&gt; observers = <span class="keyword">new</span> ArrayList&lt;&gt;();</span><br><span class="line"></span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">int</span> state;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">int</span> <span class="title">getState</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">return</span> state;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">setState</span><span class="params">(<span class="keyword">int</span> state)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.state = state;</span><br><span class="line">        notifyAllObservers();</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">attach</span><span class="params">(Observer observer)</span> </span>&#123;</span><br><span class="line">        observers.add(observer);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">notifyAllObservers</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        <span class="keyword">for</span> (Observer observer : observers) &#123;</span><br><span class="line">            observer.update();</span><br><span class="line">        &#125;</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>步骤 2</p><p>创建 Observer 类。</p><p><em>Observer.java</em></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">Observer</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">protected</span> Subject subject;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="keyword">void</span> <span class="title">update</span><span class="params">()</span></span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>步骤 3</p><p>创建实体观察者类。</p><p><em>BinaryObserver.java</em></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">BinaryObserver</span> <span class="keyword">extends</span> <span class="title">Observer</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">BinaryObserver</span><span class="params">(Subject subject)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.subject = subject;</span><br><span class="line">        <span class="keyword">this</span>.subject.attach(<span class="keyword">this</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">update</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        System.out.println(<span class="string">"Binary String: "</span></span><br><span class="line">                + Integer.toBinaryString(subject.getState()));</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><em>OctalObserver.java</em></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">OctalObserver</span> <span class="keyword">extends</span> <span class="title">Observer</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">OctalObserver</span><span class="params">(Subject subject)</span></span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.subject = subject;</span><br><span class="line">        <span class="keyword">this</span>.subject.attach(<span class="keyword">this</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">update</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        System.out.println( <span class="string">"Octal String: "</span></span><br><span class="line">                + Integer.toOctalString( subject.getState() ) );</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><em>HexaObserver.java</em></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">HexaObserver</span> <span class="keyword">extends</span> <span class="title">Observer</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">HexaObserver</span><span class="params">(Subject subject)</span></span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.subject = subject;</span><br><span class="line">        <span class="keyword">this</span>.subject.attach(<span class="keyword">this</span>);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">update</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        System.out.println( <span class="string">"Hex String: "</span></span><br><span class="line">                + Integer.toHexString( subject.getState() ).toUpperCase() );</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>步骤 4</p><p>使用 <em>Subject</em> 和实体观察者对象。</p><p><em>ObserverPatternDemo.java</em></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">ObserverPatternDemo</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        Subject subject = <span class="keyword">new</span> Subject();</span><br><span class="line"></span><br><span class="line">        <span class="keyword">new</span> BinaryObserver(subject);</span><br><span class="line">        <span class="keyword">new</span> HexaObserver(subject);</span><br><span class="line">        <span class="keyword">new</span> OctalObserver(subject);</span><br><span class="line"></span><br><span class="line">        System.out.println(<span class="string">"First state change: 15"</span>);</span><br><span class="line">        subject.setState(<span class="number">15</span>);</span><br><span class="line">        System.out.println();</span><br><span class="line"></span><br><span class="line">        System.out.println(<span class="string">"Second state change: 10"</span>);</span><br><span class="line">        subject.setState(<span class="number">10</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>步骤 5</p><p>验证输出。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">First state change: 15</span><br><span class="line">Binary String: 1111</span><br><span class="line">Hex String: F</span><br><span class="line">Octal String: 17</span><br><span class="line"></span><br><span class="line">Second state change: 10</span><br><span class="line">Binary String: 1010</span><br><span class="line">Hex String: A</span><br><span class="line">Octal String: 12</span><br></pre></td></tr></table></figure><h3 id="装饰器模式"><a href="#装饰器模式" class="headerlink" title="装饰器模式"></a>装饰器模式</h3><p>装饰器模式（Decorator Pattern）允许向一个现有的对象添加新的功能，同时又不改变其结构。这种类型的设计模式属于结构型模式，它是作为现有的类的一个包装。</p><p>意图：动态地给一个对象添加一些额外的职责。就增加功能来说，装饰器模式相比生成子类更为灵活。</p><p>主要解决：一般的，我们为了扩展一个类经常使用继承方式实现，由于继承为类引入静态特征，并且随着扩展功能的增多，子类会很膨胀。</p><p>实现</p><p>我们将创建一个 Shape 接口和实现了 Shape 接口的实体类。然后我们创建一个实现了 Shape 接口的抽象装饰类 ShapeDecorator，并把 Shape 对象作为它的实例变量。</p><p>RedShapeDecorator 是实现了 ShapeDecorator 的实体类。</p><p>DecoratorPatternDemo，我们的演示类使用 RedShapeDecorator 来装饰 Shape 对象。</p><p><img src="https://lixiangbetter.github.io/2020/07/19/%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1-%E5%88%86%E5%B8%83%E5%BC%8F-%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/aHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL0pvdXJXb24vaW1hZ2UvbWFzdGVyLyVFOCVBRSVCRSVFOCVBRSVBMSVFNiVBOCVBMSVFNSVCQyU4Ri8lRTglQTMlODUlRTklQTUlQjAlRTUlOTklQTglRTYlQTglQTElRTUlQkMlOEYuanBn.jpeg" alt></p><p>步骤 1</p><p>创建一个接口。</p><p><em>Shape.java</em></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">interface</span> <span class="title">Shape</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">void</span> <span class="title">draw</span><span class="params">()</span></span>;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>步骤 2</p><p>创建实现接口的实体类。</p><p><em>Rectangle.java</em></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Rectangle</span> <span class="keyword">implements</span> <span class="title">Shape</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">draw</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        System.out.println(<span class="string">"Shape: Rectangle"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p><em>Circle.java</em></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">Circle</span> <span class="keyword">implements</span> <span class="title">Shape</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">draw</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        System.out.println(<span class="string">"Shape: Circle"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>步骤 3</p><p>创建实现了 <em>Shape</em> 接口的抽象装饰类。</p><p><em>ShapeDecorator.java</em></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="keyword">abstract</span> <span class="class"><span class="keyword">class</span> <span class="title">ShapeDecorator</span> <span class="keyword">implements</span> <span class="title">Shape</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="keyword">protected</span> Shape decoratorShape;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">ShapeDecorator</span><span class="params">(Shape decoratorShape)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">this</span>.decoratorShape = decoratorShape;</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">draw</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        decoratorShape.draw();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>步骤 4</p><p>创建扩展了 <em>ShapeDecorator</em> 类的实体装饰类。</p><p><em>RedShapeDecorator.java</em></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">RedShapeDecorator</span> <span class="keyword">extends</span> <span class="title">ShapeDecorator</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="title">RedShapeDecorator</span><span class="params">(Shape decoratorShape)</span> </span>&#123;</span><br><span class="line">        <span class="keyword">super</span>(decoratorShape);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="meta">@Override</span></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">void</span> <span class="title">draw</span><span class="params">()</span> </span>&#123;</span><br><span class="line">        decoratorShape.draw();</span><br><span class="line">        setRedBorder(decoratorShape);</span><br><span class="line">    &#125;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">private</span> <span class="keyword">void</span> <span class="title">setRedBorder</span><span class="params">(Shape decoratorShape)</span> </span>&#123;</span><br><span class="line">        System.out.println(<span class="string">"Border Color: Red"</span>);</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>步骤 5</p><p>使用 <em>RedShapeDecorator</em> 来装饰 <em>Shape</em> 对象。</p><p><em>DecoratorPatternDemo.java</em></p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">public</span> <span class="class"><span class="keyword">class</span> <span class="title">DecoratorPatternDemo</span> </span>&#123;</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">public</span> <span class="keyword">static</span> <span class="keyword">void</span> <span class="title">main</span><span class="params">(String[] args)</span> </span>&#123;</span><br><span class="line">        Shape circle = <span class="keyword">new</span> Circle();</span><br><span class="line">        Shape redCircle = <span class="keyword">new</span> RedShapeDecorator(<span class="keyword">new</span> Circle());</span><br><span class="line">        Shape redRectangle = <span class="keyword">new</span> RedShapeDecorator(<span class="keyword">new</span> Rectangle());</span><br><span class="line"></span><br><span class="line">        System.out.println(<span class="string">"Circle with normal border"</span>);</span><br><span class="line">        circle.draw();</span><br><span class="line"></span><br><span class="line">        System.out.println(<span class="string">"\nCircle of red border"</span>);</span><br><span class="line">        redCircle.draw();</span><br><span class="line"></span><br><span class="line">        System.out.println(<span class="string">"\nRectangle of red border"</span>);</span><br><span class="line">        redRectangle.draw();</span><br><span class="line">    &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>步骤 6</p><p>验证输出。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">Circle with normal border</span><br><span class="line">Shape: Circle</span><br><span class="line"></span><br><span class="line">Circle of red border</span><br><span class="line">Shape: Circle</span><br><span class="line">Border Color: Red</span><br><span class="line"></span><br><span class="line">Rectangle of red border</span><br><span class="line">Shape: Rectangle</span><br><span class="line">Border Color: Red</span><br></pre></td></tr></table></figure><h3 id="秒杀系统设计"><a href="#秒杀系统设计" class="headerlink" title="秒杀系统设计"></a>秒杀系统设计</h3><p><strong>什么是秒杀</strong></p><p>通俗一点讲就是网络商家为促销等目的组织的网上限时抢购活动</p><p><strong>业务特点</strong></p><ul><li>高并发：秒杀的特点就是这样<strong>时间极短</strong>、 <strong>瞬间用户量大</strong>。</li><li>库存量少：一般秒杀活动商品量很少，这就导致了只有极少量用户能成功购买到。</li><li>业务简单：流程比较简单，一般都是下订单、扣库存、支付订单</li><li>恶意请求，数据库压力大</li></ul><p>解决方案</p><p>前端：页面资源静态化，按钮控制，使用答题校验码可以防止秒杀器的干扰，让更多用户有机会抢到</p><p>nginx：校验恶意请求，转发请求，负载均衡；动静分离，不走tomcat获取静态资源；gzip压缩，减少静态文件传输的体积，节省带宽，提高渲染速度</p><p>业务层：集群，多台机器处理，提高并发能力</p><p>redis：集群保证高可用，持久化数据；分布式锁（悲观锁）；缓存热点数据（库存）</p><p>mq：削峰限流，MQ堆积订单，保护订单处理层的负载，Consumer根据自己的消费能力来取Task，实际上下游的压力就可控了。重点做好路由层和MQ的安全</p><p>数据库：读写分离，拆分事务提高并发度</p><p><strong>秒杀系统设计小结</strong></p><ul><li>秒杀系统就是一个“三高”系统，即高并发、高性能和高可用的分布式系统</li><li>秒杀设计原则：前台请求尽量少，后台数据尽量少，调用链路尽量短，尽量不要有单点</li><li>秒杀高并发方法：访问拦截、分流、动静分离</li><li>秒杀数据方法：减库存策略、热点、异步、限流降级</li><li>访问拦截主要思路：通过CDN和缓存技术，尽量把访问拦截在离用户更近的层，尽可能地过滤掉无效请求。</li><li>分流主要思路：通过分布式集群技术，多台机器处理，提高并发能力。</li></ul><h2 id="分布式"><a href="#分布式" class="headerlink" title="分布式"></a>分布式</h2><h3 id="分布式概述"><a href="#分布式概述" class="headerlink" title="分布式概述"></a>分布式概述</h3><h4 id="分布式-1"><a href="#分布式-1" class="headerlink" title="分布式"></a>分布式</h4><p>分布式（distributed）是为了解决单个物理服务器容量和性能瓶颈问题而采用的优化手段，将一个业务拆分成不同的子业务，分布在不同的机器上执行。服务之间通过远程调用协同工作，对外提供服务。</p><p>该领域需要解决的问题极多，在不同的技术层面上，又包括：分布式缓存、分布式数据库、分布式计算、分布式文件系统等，一些技术如MQ、Redis、zookeeper等都跟分布式有关。</p><p>从理念上讲，分布式的实现有两种形式：</p><p>水平扩展：当一台机器扛不住流量时，就通过添加机器的方式，将流量平分到所有服务器上，所有机器都可以提供 相同的服务；</p><p>垂直拆分：前端有多种查询需求时，一台机器扛不住，可以将不同的业务需求分发到不同的机器上，比如A机器处理余票查询的请求，B机器处理支付的请求。</p><h4 id="集群"><a href="#集群" class="headerlink" title="集群"></a>集群</h4><p>集群（cluster）是指在多台不同的服务器中部署相同应用或服务模块，构成一个集群，通过负载均衡设备对外提供服务。</p><p>两个特点</p><ul><li><p><strong>可扩展性</strong>：集群中的服务节点，可以动态的添加机器，从而增加集群的处理能力。</p></li><li><p><strong>高可用性</strong>：如果集群某个节点发生故障，这台节点上面运行的服务，可以被其他服务节点接管，从而增强集群的高可用性。</p></li></ul><p>两大能力</p><ul><li><p><strong>负载均衡</strong>：负载均衡能把任务比较均衡地分布到集群环境下的计算和网络资源。</p></li><li><p><strong>集群容错</strong>：当我们的系统中用到集群环境，因为各种原因在集群调用失败时，集群容错起到关键性的作用。</p></li></ul><h4 id="微服务"><a href="#微服务" class="headerlink" title="微服务"></a>微服务</h4><p>微服务就是很小的服务，小到一个服务只对应一个单一的功能，只做一件事。这个服务可以单独部署运行，服务之间通过远程调用协同工作，每个微服务都是由独立的小团队开发，测试，部署，上线，负责它的整个生命周期。</p><h4 id="多线程"><a href="#多线程" class="headerlink" title="多线程"></a>多线程</h4><p>多线程（multi-thread）：多线程是指程序中包含多个执行流，即在一个程序中可以同时运行多个不同的线程来执行不同的任务。多线程是为了提高CPU的利用率。</p><h4 id="高并发"><a href="#高并发" class="headerlink" title="高并发"></a>高并发</h4><p>高并发（High Concurrency）是一种系统运行过程中发生了一种“短时间内遇到大量请求”的情况，高并发对应的是访问请求，多线程是解决高并发的方法之一，高并发还可以通过分布式，集群，算法优化，数据库优化等方法解决。</p><h3 id="分布式系统设计理念"><a href="#分布式系统设计理念" class="headerlink" title="分布式系统设计理念"></a>分布式系统设计理念</h3><h4 id="分布式系统的目标与要素"><a href="#分布式系统的目标与要素" class="headerlink" title="分布式系统的目标与要素"></a>分布式系统的目标与要素</h4><p>分布式系统的目标是提升系统的整体性能和吞吐量另外还要尽量保证分布式系统的容错性（假如增加10台服务器才达到单机运行效果2倍左右的性能，那么这个分布式系统就根本没有存在的意义）。</p><p>即使采用了分布式系统，我们也要尽力运用并发编程、高性能网络框架等等手段提升单机上的程序性能。</p><h4 id="分布式系统设计两大思路：中心化和去中心化"><a href="#分布式系统设计两大思路：中心化和去中心化" class="headerlink" title="分布式系统设计两大思路：中心化和去中心化"></a>分布式系统设计两大思路：中心化和去中心化</h4><p><img src="https://lixiangbetter.github.io/2020/07/19/%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1-%E5%88%86%E5%B8%83%E5%BC%8F-%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/aHR0cHM6Ly91c2VyLWdvbGQtY2RuLnhpdHUuaW8vMjAxOC81LzI0LzE2MzkxNWQ1ZGZiM2VjZTk.jpeg" alt></p><p>中心化设计</p><ul><li><p>两个角色： 中心化的设计思想很简单，分布式集群中的节点机器按照角色分工，大体上分为两种角色： “领导” 和 “干活的”</p></li><li><p>角色职责： “领导”通常负责分发任务并监督“干活的”，发现谁太闲了，就想发设法地给其安排新任务，确保没有一个“干活的”能够偷懒，如果“领导”发现某个“干活的”因为劳累过度而病倒了，则是不会考虑先尝试“医治”他的，而是一脚踢出去，然后把他的任务分给其他人。其中微服务架构 Kubernetes 就恰好采用了这一设计思路。</p></li><li><p>中心化设计的问题</p><ol><li>中心化的设计存在的最大问题是“领导”的安危问题，如果“领导”出了问题，则群龙无首，整个集群就奔溃了。但我们难以同时安排两个“领导”以避免单点问题。</li><li>中心化设计还存在另外一个潜在的问题，既“领导”的能力问题：可以领导10个人高效工作并不意味着可以领导100个人高效工作，所以如果系统设计和实现得不好，问题就会卡在“领导”身上。</li></ol></li><li><p>领导安危问题的解决办法： 大多数中心化系统都采用了主备两个“领导”的设计方案，可以是热备或者冷备，也可以是自动切换或者手动切换，而且越来越多的新系统都开始具备自动选举切换“领导”的能力，以提升系统的可用性。</p></li></ul><p>去中心化设计</p><ul><li>众生地位平等： 在去中心化的设计里，通常没有“领导”和“干活的”这两种角色的区分，大家的角色都是一样的，地位是平等的，全球互联网就是一个典型的去中心化的分布式系统，联网的任意节点设备宕机，都只会影响很小范围的功能。</li><li>“去中心化”不是不要中心，而是由节点来自由选择中心。 （集群的成员会自发的举行“会议”选举新的“领导”主持工作。最典型的案例就是ZooKeeper及Go语言实现的Etcd）</li><li>去中心化设计的问题： 去中心化设计里最难解决的一个问题是 “脑裂”问题 ，这种情况的发生概率很低，但影响很大。脑裂指一个集群由于网络的故障，被分为至少两个彼此无法通信的单独集群，此时如果两个集群都各自工作，则可能会产生严重的数据冲突和错误。一般的设计思路是，当集群判断发生了脑裂问题时，规模较小的集群就“自杀”或者拒绝服务。</li></ul><h4 id="分布式与集群的区别是什么？"><a href="#分布式与集群的区别是什么？" class="headerlink" title="分布式与集群的区别是什么？"></a>分布式与集群的区别是什么？</h4><ul><li>分布式： 一个业务分拆多个子业务，部署在不同的服务器上</li><li>集群： 同一个业务，部署在多个服务器上。比如之前做电商网站搭的redis集群以及solr集群都是属于将redis服务器提供的缓存服务以及solr服务器提供的搜索服务部署在多个服务器上以提高系统性能、并发量解决海量存储问题。</li></ul><h3 id="CAP定理"><a href="#CAP定理" class="headerlink" title="CAP定理"></a>CAP定理</h3><p><img src="https://lixiangbetter.github.io/2020/07/19/%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1-%E5%88%86%E5%B8%83%E5%BC%8F-%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/aHR0cHM6Ly91c2VyLWdvbGQtY2RuLnhpdHUuaW8vMjAxOC81LzI0LzE2MzkxMmU5NzNlY2I5M2M.jpeg" alt></p><p>在理论计算机科学中，CAP定理（CAP theorem），又被称作布鲁尔定理（Brewer’s theorem），它指出对于一个分布式计算系统来说，不可能同时满足以下三点：</p><table><thead><tr><th>选项</th><th>描述</th></tr></thead><tbody><tr><td>Consistency（一致性）</td><td>指数据在多个副本之间能够保持一致的特性（严格的一致性）</td></tr><tr><td>Availability（可用性）</td><td>指系统提供的服务必须一直处于可用的状态，每次请求都能获取到非错的响应（不保证获取的数据为最新数据）</td></tr><tr><td>Partition tolerance（分区容错性）</td><td>分布式系统在遇到任何网络分区故障的时候，仍然能够对外提供满足一致性和可用性的服务，除非整个网络环境都发生了故障</td></tr></tbody></table><p><strong>Spring Cloud在CAP法则上主要满足的是A和P法则，Dubbo和Zookeeper在CAP法则主要满足的是C和P法则</strong></p><p>CAP仅适用于原子读写的NOSQL场景中，并不适合数据库系统。现在的分布式系统具有更多特性比如扩展性、可用性等等，在进行系统设计和开发时，我们不应该仅仅局限在CAP问题上。</p><p><strong>注意：不是所谓的3选2（不要被网上大多数文章误导了）</strong></p><p>现实生活中，大部分人解释这一定律时，常常简单的表述为：“一致性、可用性、分区容忍性三者你只能同时达到其中两个，不可能同时达到”。实际上这是一个非常具有误导性质的说法，而且在CAP理论诞生12年之后，CAP之父也在2012年重写了之前的论文。</p><p><strong>当发生网络分区的时候，如果我们要继续服务，那么强一致性和可用性只能2选1。也就是说当网络分区之后P是前提，决定了P之后才有C和A的选择。也就是说分区容错性（Partition tolerance）我们是必须要实现的。</strong></p><h2 id="CAP定理的证明"><a href="#CAP定理的证明" class="headerlink" title="CAP定理的证明"></a>CAP定理的证明</h2><p>关于CAP这三个特性我们就介绍完了，接下来我们试着证明一下<strong>为什么CAP不能同时满足</strong>。</p><p><img src="https://lixiangbetter.github.io/2020/07/19/%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1-%E5%88%86%E5%B8%83%E5%BC%8F-%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/aHR0cHM6Ly91c2VyLWdvbGQtY2RuLnhpdHUuaW8vMjAyMC8xLzExLzE2ZjkyMTEwOTczNjgzMDY.jpeg" alt></p><p>为了简化证明的过程，我们假设整个集群里只有两个N1和N2两个节点，如下图：</p><p>N1和N2当中各自有一个应用程序AB和数据库，当系统满足一致性的时候，我们认为N1和N2数据库中的数据保持一致。在满足可用性的时候，我们认为无论用户访问N1还是N2，都可以获得正确的结果，在满足分区容错性的时候，我们认为无论N1还是N2宕机或者是两者的通信中断，都不影响系统的运行。</p><p>我们假设一种极端情况，假设某个时刻N1和N2之间的网络通信突然中断了。如果系统满足分区容错性，那么显然可以支持这种异常。问题是在此前提下，一致性和可用性是否可以做到不受影响呢？</p><p>我们做个假象实验，如下图，突然某一时刻N1和N2之间的关联断开：</p><p><img src="https://lixiangbetter.github.io/2020/07/19/%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1-%E5%88%86%E5%B8%83%E5%BC%8F-%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/aHR0cHM6Ly91c2VyLWdvbGQtY2RuLnhpdHUuaW8vMjAyMC8xLzExLzE2ZjkyMTEzMTkxNmYxODY.jpeg" alt></p><p>有用户向N1发送了请求更改了数据，将数据库从V0更新成了V1。由于网络断开，所以N2数据库依然是V0，如果这个时候有一个请求发给了N2，但是N2并没有办法可以直接给出最新的结果V1，这个时候该怎么办呢？</p><p>这个时候无法两种方法，一种是将错就错，将错误的V0数据返回给用户。第二种是阻塞等待，等待网络通信恢复，N2中的数据更新之后再返回给用户。显然前者牺牲了一致性，后者牺牲了可用性。</p><p>这个例子虽然简单，但是说明的内容却很重要。在分布式系统当中，CAP三个特性我们是无法同时满足的，必然要舍弃一个。三者舍弃一个，显然排列组合一共有三种可能。</p><h3 id="BASE理论"><a href="#BASE理论" class="headerlink" title="BASE理论"></a>BASE理论</h3><p>BASE理论由eBay架构师Dan Pritchett提出，在2008年上被发表为论文，并且eBay给出了他们在实践中总结的基于BASE理论的一套新的分布式事务解决方案。</p><p>BASE 是 Basically Available（基本可用） 、Soft-state（软状态） 和 Eventually Consistent（最终一致性） 三个短语的缩写。BASE理论是对CAP中一致性和可用性权衡的结果，其来源于对大规模互联网系统分布式实践的总结，是基于CAP定理逐步演化而来的，它大大降低了我们对系统的要求。</p><h4 id="BASE理论的核心思想"><a href="#BASE理论的核心思想" class="headerlink" title="BASE理论的核心思想"></a>BASE理论的核心思想</h4><p>即使无法做到强一致性，但每个应用都可以根据自身业务特点，采用适当的方式来使系统达到最终一致性。也就是牺牲数据的一致性来满足系统的高可用性，系统中一部分数据不可用或者不一致时，仍需要保持系统整体“主要可用”。</p><p>针对数据库领域，BASE思想的主要实现是对业务数据进行拆分，让不同的数据分布在不同的机器上，以提升系统的可用性，当前主要有以下两种做法：</p><ul><li>按功能划分数据库</li><li>分片（如开源的Mycat、Amoeba等）。</li></ul><p>由于拆分后会涉及分布式事务问题，所以eBay在该BASE论文中提到了如何用最终一致性的思路来实现高性能的分布式事务。</p><h4 id="BASE理论三要素"><a href="#BASE理论三要素" class="headerlink" title="BASE理论三要素"></a>BASE理论三要素</h4><p><img src="https://lixiangbetter.github.io/2020/07/19/%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1-%E5%88%86%E5%B8%83%E5%BC%8F-%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/aHR0cHM6Ly91c2VyLWdvbGQtY2RuLnhpdHUuaW8vMjAxOC81LzI0LzE2MzkxNDgwNmQ5ZTE1YzY.jpeg" alt></p><h5 id="1-基本可用"><a href="#1-基本可用" class="headerlink" title="1. 基本可用"></a>1. 基本可用</h5><p>基本可用是指分布式系统在出现不可预知故障的时候，允许损失部分可用性。但是，这绝不等价于系统不可用。</p><p>比如：</p><ul><li>响应时间上的损失：正常情况下，一个在线搜索引擎需要在0.5秒之内返回给用户相应的查询结果，但由于出现故障，查询结果的响应时间增加了1~2秒</li><li>系统功能上的损失：正常情况下，在一个电子商务网站上进行购物的时候，消费者几乎能够顺利完成每一笔订单，但是在一些节日大促购物高峰的时候，由于消费者的购物行为激增，为了保护购物系统的稳定性，部分消费者可能会被引导到一个降级页面</li></ul><h5 id="2-软状态"><a href="#2-软状态" class="headerlink" title="2. 软状态"></a>2. 软状态</h5><p>软状态指允许系统中的数据存在中间状态，并认为该中间状态的存在不会影响系统的整体可用性，即允许系统在不同节点的数据副本之间进行数据同步的过程存在延时</p><h5 id="3-最终一致性"><a href="#3-最终一致性" class="headerlink" title="3. 最终一致性"></a>3. 最终一致性</h5><p>最终一致性强调的是系统中所有的数据副本，在经过一段时间的同步后，最终能够达到一个一致的状态。因此，最终一致性的本质是需要系统保证最终数据能够达到一致，而不需要实时保证系统数据的强一致性。</p><h2 id="数据结构与算法"><a href="#数据结构与算法" class="headerlink" title="数据结构与算法"></a>数据结构与算法</h2><h3 id="冒泡排序"><a href="#冒泡排序" class="headerlink" title="冒泡排序"></a>冒泡排序</h3><p><img src="https://lixiangbetter.github.io/2020/07/19/%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1-%E5%88%86%E5%B8%83%E5%BC%8F-%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/20190712143006969.gif" alt></p><h3 id="选择排序"><a href="#选择排序" class="headerlink" title="选择排序"></a>选择排序</h3><p><img src="https://lixiangbetter.github.io/2020/07/19/%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1-%E5%88%86%E5%B8%83%E5%BC%8F-%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/20190712143023558.gif" alt></p><h3 id="快速排序"><a href="#快速排序" class="headerlink" title="快速排序"></a>快速排序</h3><p><img src="https://lixiangbetter.github.io/2020/07/19/%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1-%E5%88%86%E5%B8%83%E5%BC%8F-%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/20190712143139347.gif" alt></p><h3 id="二分查找"><a href="#二分查找" class="headerlink" title="二分查找"></a>二分查找</h3><p><img src="https://lixiangbetter.github.io/2020/07/19/%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1-%E5%88%86%E5%B8%83%E5%BC%8F-%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/aHR0cHM6Ly91c2VyLWdvbGQtY2RuLnhpdHUuaW8vMjAxNy81LzI1L2E5ZWE3ZTVkNzQwM2I1NDEwYzJlZmQwOTQ0Njc1MWZk.jpeg" alt></p><h3 id="一致性Hash算法"><a href="#一致性Hash算法" class="headerlink" title="一致性Hash算法"></a>一致性Hash算法</h3><h4 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h4><p>一致性Hash是一种特殊的Hash算法，由于其均衡性、持久性的映射特点，被广泛的应用于负载均衡领域和分布式存储，如nginx和memcached都采用了一致性Hash来作为集群负载均衡的方案。</p><p>普通的Hash函数最大的作用是散列，或者说是将一系列在形式上具有相似性质的数据，打散成随机的、均匀分布的数据。不难发现，这样的Hash只要集群的数量N发生变化，之前的所有Hash映射就会全部失效。如果集群中的每个机器提供的服务没有差别，倒不会产生什么影响，但对于分布式缓存这样的系统而言，映射全部失效就意味着之前的缓存全部失效，后果将会是灾难性的。一致性Hash通过构建环状的Hash空间代替线性Hash空间的方法解决了这个问题。</p><p>良好的分布式cahce系统中的一致性hash算法应该满足以下几个方面：</p><ul><li><p>平衡性(Balance)<br>平衡性是指哈希的结果能够尽可能分布到所有的缓冲中去，这样可以使得所有的缓冲空间都得到利用。很多哈希算法都能够满足这一条件。</p></li><li><p>单调性(Monotonicity)<br>单调性是指如果已经有一些内容通过哈希分派到了相应的缓冲中，又有新的缓冲区加入到系统中，那么哈希的结果应能够保证原有已分配的内容可以被映射到新的缓冲区中去，而不会被映射到旧的缓冲集合中的其他缓冲区。</p></li><li><p>分散性(Spread)<br>在分布式环境中，终端有可能看不到所有的缓冲，而是只能看到其中的一部分。当终端希望通过哈希过程将内容映射到缓冲上时，由于不同终端所见的缓冲范围有可能不同，从而导致哈希的结果不一致，最终的结果是相同的内容被不同的终端映射到不同的缓冲区中。这种情况显然是应该避免的，因为它导致相同内容被存储到不同缓冲中去，降低了系统存储的效率。分散性的定义就是上述情况发生的严重程度。好的哈希算法应能够尽量避免不一致的情况发生，也就是尽量降低分散性。</p></li><li><p>负载(Load)<br>负载问题实际上是从另一个角度看待分散性问题。既然不同的终端可能将相同的内容映射到不同的缓冲区中，那么对于一个特定的缓冲区而言，也可能被不同的用户映射为不同的内容。与分散性一样，这种情况也是应当避免的，因此好的哈希算法应能够尽量降低缓冲的负荷。</p></li><li><p>平滑性(Smoothness)<br>平滑性是指缓存服务器的数目平滑改变和缓存对象的平滑改变是一致的。</p></li></ul><h4 id="一致性Hash算法原理"><a href="#一致性Hash算法原理" class="headerlink" title="一致性Hash算法原理"></a>一致性Hash算法原理</h4><p>简单来说，一致性哈希将整个哈希值空间组织成一个虚拟的圆环，如假设某哈希函数H的值空间为0-232-1（即哈希值是一个32位无符号整形），整个哈希空间环如下：整个空间按顺时针方向组织。0和232-1在零点中方向重合。</p><p><img src="https://lixiangbetter.github.io/2020/07/19/%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1-%E5%88%86%E5%B8%83%E5%BC%8F-%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/aHR0cHM6Ly91c2VyLWdvbGQtY2RuLnhpdHUuaW8vMjAxOC80LzI2LzE2MmZmZmYwMTlkNDhlZDU.jpeg" alt></p><p>下一步将各个服务器使用Hash进行一次哈希，具体可以选择服务器的ip或主机名作为关键字进行哈希，这样每台机器就能确定其在哈希环上的位置，这里假设将上文中四台服务器使用ip地址哈希后在环空间的位置如下：</p><p><img src="https://lixiangbetter.github.io/2020/07/19/%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1-%E5%88%86%E5%B8%83%E5%BC%8F-%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/aHR0cHM6Ly91c2VyLWdvbGQtY2RuLnhpdHUuaW8vMjAxOC80LzI2LzE2MmZmZmYwMTlmMWNlY2M.jpeg" alt></p><p>接下来使用如下算法定位数据访问到相应服务器：将数据key使用相同的函数Hash计算出哈希值，并确定此数据在环上的位置，从此位置沿环顺时针“行走”，第一台遇到的服务器就是其应该定位到的服务器。</p><p>例如我们有Object A、Object B、Object C、Object D四个数据对象，经过哈希计算后，在环空间上的位置如下：</p><p><img src="https://lixiangbetter.github.io/2020/07/19/%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1-%E5%88%86%E5%B8%83%E5%BC%8F-%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/aHR0cHM6Ly91c2VyLWdvbGQtY2RuLnhpdHUuaW8vMjAxOC80LzI2LzE2MmZmZmYwMWRkZTliMmI.jpeg" alt></p><p>根据一致性哈希算法，数据A会被定为到Node A上，B被定为到Node B上，C被定为到Node C上，D被定为到Node D上。</p><p>下面分析一致性哈希算法的容错性和可扩展性。现假设Node C不幸宕机，可以看到此时对象A、B、D不会受到影响，只有C对象被重定位到Node D。一般的，在一致性哈希算法中，如果一台服务器不可用，则受影响的数据仅仅是此服务器到其环空间中前一台服务器（即沿着逆时针方向行走遇到的第一台服务器）之间数据，其它不会受到影响。</p><p>下面考虑另外一种情况，如果在系统中增加一台服务器Node X，如下图所示：</p><p><img src="https://lixiangbetter.github.io/2020/07/19/%E6%9E%B6%E6%9E%84%E8%AE%BE%E8%AE%A1-%E5%88%86%E5%B8%83%E5%BC%8F-%E6%95%B0%E6%8D%AE%E7%BB%93%E6%9E%84%E4%B8%8E%E7%AE%97%E6%B3%95%E7%AC%94%E8%AE%B0/aHR0cHM6Ly91c2VyLWdvbGQtY2RuLnhpdHUuaW8vMjAxOC80LzI2LzE2MmZmZmYwMWRkZTliMmI.jpeg" alt></p><p>此时对象Object A、B、D不受影响，只有对象C需要重定位到新的Node X 。一般的，在一致性哈希算法中，如果增加一台服务器，则受影响的数据仅仅是新服务器到其环空间中前一台服务器（即沿着逆时针方向行走遇到的第一台服务器）之间数据，其它数据也不会受到影响。</p><p>综上所述，一致性哈希算法对于节点的增减都只需重定位环空间中的一小部分数据，具有较好的容错性和可扩展性。</p>]]></content>
      
      
      <categories>
          
          <category> bigdata </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 设计模式 分布式 数据结构 算法 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>ZooKeeper笔记</title>
      <link href="/2020/07/18/ZooKeeper%E7%AC%94%E8%AE%B0/"/>
      <url>/2020/07/18/ZooKeeper%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<h1 id="Zookeeper笔记"><a href="#Zookeeper笔记" class="headerlink" title="Zookeeper笔记"></a>Zookeeper笔记</h1><h2 id="1-ZooKeeper-是什么？"><a href="#1-ZooKeeper-是什么？" class="headerlink" title="1. ZooKeeper 是什么？"></a>1. ZooKeeper 是什么？</h2><p>ZooKeeper 是一个开源的分布式协调服务。它是一个为分布式应用提供一致性服务的软件，分布式应用程序可以基于 Zookeeper 实现诸如数据发布/订阅、负载均衡、命名服务、分布式协调/通知、集群管理、Master选举、分布式锁和分布式队列等功能。</p><p>ZooKeeper 的目标就是封装好复杂易出错的关键服务，将简单易用的接口和性能高效、功能稳定的系统提供给用户。</p><p>Zookeeper保证了如下分布式一致性特性：</p><p>（1）顺序一致性</p><p>（2）原子性</p><p>（3）单一视图</p><p>（4）可靠性</p><p>（5）实时性（最终一致性）</p><p>客户端的读请求可以被集群中的任意一台机器处理，如果读请求在节点上注册了监听器，这个监听器也是由所连接的 zookeeper 机器来处理。对于写请求，这些请求会同时发给其他 zookeeper 机器并且达成一致后，请求才会返回成功。因此，随着 zookeeper 的集群机器增多，读请求的吞吐会提高但是写请求的吞吐会下降。</p><p>有序性是 zookeeper 中非常重要的一个特性，所有的更新都是全局有序的，每个更新都有一个唯一的时间戳，这个时间戳称为 zxid（Zookeeper Transaction Id）。而读请求只会相对于更新有序，也就是读请求的返回结果中会带有这个zookeeper 最新的 zxid。</p><h2 id="2-ZooKeeper-提供了什么？"><a href="#2-ZooKeeper-提供了什么？" class="headerlink" title="2. ZooKeeper 提供了什么？"></a>2. ZooKeeper 提供了什么？</h2><ul><li>文件系统</li><li>通知机制</li></ul><h2 id="3-Zookeeper-文件系统"><a href="#3-Zookeeper-文件系统" class="headerlink" title="3. Zookeeper 文件系统"></a>3. Zookeeper 文件系统</h2><p>Zookeeper 提供一个多层级的节点命名空间（节点称为 znode）。与文件系统不同的是，这些节点都可以设置关联的数据，而文件系统中只有文件节点可以存放数据而目录节点不行。</p><p>Zookeeper 为了保证高吞吐和低延迟，在内存中维护了这个树状的目录结构，这种特性使得 Zookeeper 不能用于存放大量的数据，每个节点的存放数据上限为1M。</p><h2 id="4-Zookeeper-怎么保证主从节点的状态同步？"><a href="#4-Zookeeper-怎么保证主从节点的状态同步？" class="headerlink" title="4. Zookeeper 怎么保证主从节点的状态同步？"></a>4. Zookeeper 怎么保证主从节点的状态同步？</h2><p>Zookeeper 的核心是原子广播机制，这个机制保证了各个 server 之间的同步。实现这个机制的协议叫做 Zab 协议。Zab 协议有两种模式，它们分别是恢复模式和广播模式。</p><ol><li><p>恢复模式</p><p>当服务启动或者在领导者崩溃后，Zab就进入了恢复模式，当领导者被选举出来，且大多数 server 完成了和 leader 的状态同步以后，恢复模式就结束了。状态同步保证了 leader 和 server 具有相同的系统状态。</p></li><li><p>广播模式<br>一旦 leader 已经和多数的 follower 进行了状态同步后，它就可以开始广播消息了，即进入广播状态。这时候当一个 server 加入 ZooKeeper 服务中，它会在恢复模式下启动，发现 leader，并和 leader 进行状态同步。待到同步结束，它也参与消息广播。ZooKeeper 服务一直维持在 Broadcast 状态，直到 leader 崩溃了或者 leader 失去了大部分的 followers 支持。</p></li></ol><h2 id="5-四种类型的数据节点-Znode"><a href="#5-四种类型的数据节点-Znode" class="headerlink" title="5. 四种类型的数据节点 Znode"></a>5. 四种类型的数据节点 Znode</h2><p>（1）PERSISTENT-持久节点</p><p>除非手动删除，否则节点一直存在于 Zookeeper 上</p><p>（2）EPHEMERAL-临时节点</p><p>临时节点的生命周期与客户端会话绑定，一旦客户端会话失效（客户端与zookeeper 连接断开不一定会话失效），那么这个客户端创建的所有临时节点都会被移除。</p><p>（3）PERSISTENT_SEQUENTIAL-持久顺序节点</p><p>基本特性同持久节点，只是增加了顺序属性，节点名后边会追加一个由父节点维护的自增整型数字。</p><p>（4）EPHEMERAL_SEQUENTIAL-临时顺序节点</p><p>基本特性同临时节点，增加了顺序属性，节点名后边会追加一个由父节点维护的自增整型数字。</p><h2 id="16-zookeeper-是如何保证事务的顺序一致性的？"><a href="#16-zookeeper-是如何保证事务的顺序一致性的？" class="headerlink" title="16. zookeeper 是如何保证事务的顺序一致性的？"></a>16. zookeeper 是如何保证事务的顺序一致性的？</h2><p>zookeeper 采用了全局递增的事务 Id 来标识，所有的 proposal（提议）都在被提出的时候加上了 zxid，zxid 实际上是一个 64 位的数字，高 32 位是 epoch（ 时期; 纪元; 世; 新时代）用来标识 leader 周期，如果有新的 leader 产生出来，epoch会自增，低 32 位用来递增计数。当新产生 proposal 的时候，会依据数据库的两阶段过程，首先会向其他的 server 发出事务执行请求，如果超过半数的机器都能执行并且能够成功，那么就会开始执行。</p><h2 id="17-分布式集群中为什么会有-Master主节点？"><a href="#17-分布式集群中为什么会有-Master主节点？" class="headerlink" title="17. 分布式集群中为什么会有 Master主节点？"></a>17. 分布式集群中为什么会有 Master主节点？</h2><p>在分布式环境中，有些业务逻辑只需要集群中的某一台机器进行执行，其他的机器可以共享这个结果，这样可以大大减少重复计算，提高性能，于是就需要进行 leader 选举。</p><h2 id="18-zk-节点宕机如何处理？"><a href="#18-zk-节点宕机如何处理？" class="headerlink" title="18. zk 节点宕机如何处理？"></a>18. zk 节点宕机如何处理？</h2><p>Zookeeper 本身也是集群，推荐配置不少于 3 个服务器。Zookeeper 自身也要保证当一个节点宕机时，其他节点会继续提供服务。</p><p>如果是一个 Follower 宕机，还有 2 台服务器提供访问，因为 Zookeeper 上的数据是有多个副本的，数据并不会丢失；</p><p>如果是一个 Leader 宕机，Zookeeper 会选举出新的 Leader。</p><p>ZK 集群的机制是只要超过半数的节点正常，集群就能正常提供服务。只有在 ZK节点挂得太多，只剩一半或不到一半节点能工作，集群才失效。</p><p>所以</p><p>3 个节点的 cluster 可以挂掉 1 个节点(leader 可以得到 2 票&gt;1.5)</p><p>2 个节点的 cluster 就不能挂掉任何 1 个节点了(leader 可以得到 1 票&lt;=1)</p><h2 id="19-zookeeper-负载均衡和-nginx-负载均衡区别"><a href="#19-zookeeper-负载均衡和-nginx-负载均衡区别" class="headerlink" title="19. zookeeper 负载均衡和 nginx 负载均衡区别"></a>19. zookeeper 负载均衡和 nginx 负载均衡区别</h2><p>zk 的负载均衡是可以调控，nginx 只是能调权重，其他需要可控的都需要自己写插件；但是 nginx 的吞吐量比 zk 大很多，应该说按业务选择用哪种方式。</p><h2 id="20-Zookeeper-有哪几种几种部署模式？"><a href="#20-Zookeeper-有哪几种几种部署模式？" class="headerlink" title="20. Zookeeper 有哪几种几种部署模式？"></a>20. Zookeeper 有哪几种几种部署模式？</h2><p>Zookeeper 有三种部署模式：</p><ol><li>单机部署：一台集群上运行；</li><li>集群部署：多台集群运行；</li><li>伪集群部署：一台集群启动多个 Zookeeper 实例运行。</li></ol><h2 id="21-集群最少要几台机器，集群规则是怎样的？集群中有-3-台服务器，其中一个节点宕机，这个时候-Zookeeper-还可以使用吗？"><a href="#21-集群最少要几台机器，集群规则是怎样的？集群中有-3-台服务器，其中一个节点宕机，这个时候-Zookeeper-还可以使用吗？" class="headerlink" title="21. 集群最少要几台机器，集群规则是怎样的？集群中有 3 台服务器，其中一个节点宕机，这个时候 Zookeeper 还可以使用吗？"></a>21. 集群最少要几台机器，集群规则是怎样的？集群中有 3 台服务器，其中一个节点宕机，这个时候 Zookeeper 还可以使用吗？</h2><p>集群规则为 2N+1 台，N&gt;0，即 3 台。可以继续使用，单数服务器只要没超过一半的服务器宕机就可以继续使用。</p><ol start="22"><li>集群支持动态添加机器吗？<br>其实就是水平扩容了，Zookeeper 在这方面不太好。两种方式：</li></ol><p>全部重启：关闭所有 Zookeeper 服务，修改配置之后启动。不影响之前客户端的会话。</p><p>逐个重启：在过半存活即可用的原则下，一台机器重启不影响整个集群对外提供服务。这是比较常用的方式。</p><p>3.5 版本开始支持动态扩容。</p><h2 id="23-Zookeeper-对节点的-watch-监听通知是永久的吗？为什么不是永久的"><a href="#23-Zookeeper-对节点的-watch-监听通知是永久的吗？为什么不是永久的" class="headerlink" title="23. Zookeeper 对节点的 watch 监听通知是永久的吗？为什么不是永久的?"></a>23. Zookeeper 对节点的 watch 监听通知是永久的吗？为什么不是永久的?</h2><p>不是。官方声明：一个 Watch 事件是一个一次性的触发器，当被设置了 Watch的数据发生了改变的时候，则服务器将这个改变发送给设置了 Watch 的客户端，以便通知它们。</p><p>为什么不是永久的，举个例子，如果服务端变动频繁，而监听的客户端很多情况下，每次变动都要通知到所有的客户端，给网络和服务器造成很大压力。</p><p>一般是客户端执行 getData(“/节点 A”,true)，如果节点 A 发生了变更或删除，客户端会得到它的 watch 事件，但是在之后节点 A 又发生了变更，而客户端又没有设置 watch 事件，就不再给客户端发送。</p><p>在实际应用中，很多情况下，我们的客户端不需要知道服务端的每一次变动，我只要最新的数据即可。</p><h2 id="24-Zookeeper-的-java-客户端都有哪些？"><a href="#24-Zookeeper-的-java-客户端都有哪些？" class="headerlink" title="24. Zookeeper 的 java 客户端都有哪些？"></a>24. Zookeeper 的 java 客户端都有哪些？</h2><p>java 客户端：zk 自带的 zkclient 及 Apache 开源的 Curator。</p><h2 id="25-chubby-是什么，和-zookeeper-比你怎么看？"><a href="#25-chubby-是什么，和-zookeeper-比你怎么看？" class="headerlink" title="25. chubby 是什么，和 zookeeper 比你怎么看？"></a>25. chubby 是什么，和 zookeeper 比你怎么看？</h2><p>chubby 是 google 的，完全实现 paxos 算法，不开源。zookeeper 是 chubby的开源实现，使用 zab 协议，paxos 算法的变种。</p><h2 id="26-说几个-zookeeper-常用的命令。"><a href="#26-说几个-zookeeper-常用的命令。" class="headerlink" title="26. 说几个 zookeeper 常用的命令。"></a>26. 说几个 zookeeper 常用的命令。</h2><p>常用命令：ls get set create delete 等。</p><h2 id="27-ZAB-和-Paxos-算法的联系与区别？"><a href="#27-ZAB-和-Paxos-算法的联系与区别？" class="headerlink" title="27. ZAB 和 Paxos 算法的联系与区别？"></a>27. ZAB 和 Paxos 算法的联系与区别？</h2><p>相同点：</p><p>（1）两者都存在一个类似于 Leader 进程的角色，由其负责协调多个 Follower 进程的运行</p><p>（2）Leader 进程都会等待超过半数的 Follower 做出正确的反馈后，才会将一个提案进行提交</p><p>（3）ZAB 协议中，每个 Proposal 中都包含一个 epoch 值来代表当前的 Leader周期，Paxos 中名字为 Ballot</p><p>不同点：</p><p>ZAB 用来构建高可用的分布式数据主备系统（Zookeeper），Paxos 是用来构建分布式一致性状态机系统。</p><h2 id="28-Zookeeper-的典型应用场景"><a href="#28-Zookeeper-的典型应用场景" class="headerlink" title="28. Zookeeper 的典型应用场景"></a>28. Zookeeper 的典型应用场景</h2><p>Zookeeper 是一个典型的发布/订阅模式的分布式数据管理与协调框架，开发人员可以使用它来进行分布式数据的发布和订阅。</p><p>通过对 Zookeeper 中丰富的数据节点进行交叉使用，配合 Watcher 事件通知机制，可以非常方便的构建一系列分布式应用中年都会涉及的核心功能，如：</p><p>（1）数据发布/订阅</p><p>（2）负载均衡</p><p>（3）命名服务</p><p>（4）分布式协调/通知</p><p>（5）集群管理</p><p>（6）Master 选举</p><p>（7）分布式锁</p><p>（8）分布式队列</p><h2 id="29-Zookeeper-都有哪些功能？"><a href="#29-Zookeeper-都有哪些功能？" class="headerlink" title="29. Zookeeper 都有哪些功能？"></a>29. Zookeeper 都有哪些功能？</h2><ol><li>集群管理：监控节点存活状态、运行请求等；</li><li>主节点选举：主节点挂掉了之后可以从备用的节点开始新一轮选主，主节点选举说的就是这个选举的过程，使用 Zookeeper 可以协助完成这个过程；</li><li>分布式锁：Zookeeper 提供两种锁：独占锁、共享锁。独占锁即一次只能有一个线程使用资源，共享锁是读锁共享，读写互斥，即可以有多线线程同时读同一个资源，如果要使用写锁也只能有一个线程使用。Zookeeper 可以对分布式锁进行控制。</li><li>命名服务：在分布式系统中，通过使用命名服务，客户端应用能够根据指定名字来获取资源或服务的地址，提供者等信息。</li></ol><h2 id="30-说一下-Zookeeper-的通知机制？"><a href="#30-说一下-Zookeeper-的通知机制？" class="headerlink" title="30. 说一下 Zookeeper 的通知机制？"></a>30. 说一下 Zookeeper 的通知机制？</h2><p>client 端会对某个 znode 建立一个 watcher 事件，当该 znode 发生变化时，这些 client 会收到 zk 的通知，然后 client 可以根据 znode 变化来做出业务上的改变等。</p><h2 id="31-Zookeeper-和-Dubbo-的关系？"><a href="#31-Zookeeper-和-Dubbo-的关系？" class="headerlink" title="31. Zookeeper 和 Dubbo 的关系？"></a>31. Zookeeper 和 Dubbo 的关系？</h2><p>Zookeeper的作用：</p><p>zookeeper用来注册服务和进行负载均衡，哪一个服务由哪一个机器来提供必需让调用者知道，简单来说就是ip地址和服务名称的对应关系。当然也可以通过硬编码的方式把这种对应关系在调用方业务代码中实现，但是如果提供服务的机器挂掉调用者无法知晓，如果不更改代码会继续请求挂掉的机器提供服务。zookeeper通过心跳机制可以检测挂掉的机器并将挂掉机器的ip和服务对应关系从列表中删除。至于支持高并发，简单来说就是横向扩展，在不更改代码的情况通过添加机器来提高运算能力。通过添加新的机器向zookeeper注册服务，服务的提供者多了能服务的客户就多了。</p><p>dubbo：</p><p>是管理中间层的工具，在业务层到数据仓库间有非常多服务的接入和服务提供者需要调度，dubbo提供一个框架解决这个问题。<br>注意这里的dubbo只是一个框架，至于你架子上放什么是完全取决于你的，就像一个汽车骨架，你需要配你的轮子引擎。这个框架中要完成调度必须要有一个分布式的注册中心，储存所有服务的元数据，你可以用zk，也可以用别的，只是大家都用zk。</p><p>zookeeper和dubbo的关系：</p><p>Dubbo 的将注册中心进行抽象，它可以外接不同的存储媒介给注册中心提供服务，有 ZooKeeper，Memcached，Redis 等。</p><p>引入了 ZooKeeper 作为存储媒介，也就把 ZooKeeper 的特性引进来。首先是负载均衡，单注册中心的承载能力是有限的，在流量达到一定程度的时 候就需要分流，负载均衡就是为了分流而存在的，一个 ZooKeeper 群配合相应的 Web 应用就可以很容易达到负载均衡；资源同步，单单有负载均衡还不 够，节点之间的数据和资源需要同步，ZooKeeper 集群就天然具备有这样的功能；命名服务，将树状结构用于维护全局的服务地址列表，服务提供者在启动 的时候，向 ZooKeeper 上的指定节点 /dubbo/${serviceName}/providers 目录下写入自己的 URL 地址，这个操作就完成了服务的发布。 其他特性还有 Master 选举，分布式锁等。</p><p><img src="https://lixiangbetter.github.io/2020/07/18/ZooKeeper%E7%AC%94%E8%AE%B0/aHR0cHM6Ly91c2VyLWdvbGQtY2RuLnhpdHUuaW8vMjAxOS8xMC8zMS8xNmUyMTliYzc3MDA5OGRm.jpeg" alt></p>]]></content>
      
      
      <categories>
          
          <category> bigdata </category>
          
      </categories>
      
      
        <tags>
            
            <tag> zookeeper </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Tomcat笔记</title>
      <link href="/2020/07/17/Tomcat%E7%AC%94%E8%AE%B0/"/>
      <url>/2020/07/17/Tomcat%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<h1 id="Tomcat笔记"><a href="#Tomcat笔记" class="headerlink" title="Tomcat笔记"></a>Tomcat笔记</h1><h2 id="Tomcat是什么？"><a href="#Tomcat是什么？" class="headerlink" title="Tomcat是什么？"></a>Tomcat是什么？</h2><p>Tomcat 服务器Apache软件基金会项目中的一个核心项目，是一个免费的开放源代码的Web 应用服务器，属于轻量级应用服务器，在中小型系统和并发访问用户不是很多的场合下被普遍使用，是开发和调试JSP 程序的首选。</p><h2 id="Tomcat的缺省端口是多少，怎么修改"><a href="#Tomcat的缺省端口是多少，怎么修改" class="headerlink" title="Tomcat的缺省端口是多少，怎么修改"></a>Tomcat的缺省端口是多少，怎么修改</h2><ol><li>找到Tomcat目录下的conf文件夹</li><li>进入conf文件夹里面找到server.xml文件</li><li>打开server.xml文件</li><li>在server.xml文件里面找到下列信息</li><li>把Connector标签的8080端口改成你想要的端口</li></ol><figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="tag">&lt;<span class="name">Service</span> <span class="attr">name</span>=<span class="string">"Catalina"</span>&gt;</span></span><br><span class="line"><span class="tag">&lt;<span class="name">Connector</span> <span class="attr">port</span>=<span class="string">"8080"</span> <span class="attr">protocol</span>=<span class="string">"HTTP/1.1"</span> </span></span><br><span class="line"><span class="tag">               <span class="attr">connectionTimeout</span>=<span class="string">"20000"</span> </span></span><br><span class="line"><span class="tag">               <span class="attr">redirectPort</span>=<span class="string">"8443"</span> /&gt;</span></span><br></pre></td></tr></table></figure><h2 id="tomcat-有哪几种Connector-运行模式-优化-？"><a href="#tomcat-有哪几种Connector-运行模式-优化-？" class="headerlink" title="tomcat 有哪几种Connector 运行模式(优化)？"></a>tomcat 有哪几种Connector 运行模式(优化)？</h2><p>下面，我们先大致了解Tomcat Connector的三种运行模式。</p><ul><li><p><strong>BIO</strong>：同步并阻塞一个线程处理一个请求。缺点：并发量高时，线程数较多，浪费资源。Tomcat7或以下，在Linux系统中默认使用这种方式。<br> 配制项：protocol=”HTTP/1.1”</p></li><li><p><strong>NIO</strong>：同步非阻塞IO</p><p>利用Java的异步IO处理，可以通过少量的线程处理大量的请求，可以复用同一个线程处理多个connection(多路复用)。</p><p>Tomcat8在Linux系统中默认使用这种方式。</p><p>Tomcat7必须修改Connector配置来启动。</p><p>配制项：protocol=”org.apache.coyote.http11.Http11NioProtocol”</p><p>备注：我们常用的Jetty，Mina，ZooKeeper等都是基于java nio实现.</p></li><li><p><strong>APR</strong>：即Apache Portable Runtime，从操作系统层面解决io阻塞问题。<strong>AIO方式，</strong>异步非阻塞IO(Java NIO2又叫AIO) 主要与NIO的区别主要是操作系统的底层区别.可以做个比喻:比作快递，NIO就是网购后要自己到官网查下快递是否已经到了(可能是多次)，然后自己去取快递；AIO就是快递员送货上门了(不用关注快递进度)。</p><p>配制项：protocol=”org.apache.coyote.http11.Http11AprProtocol”</p><p>备注：需在本地服务器安装APR库。Tomcat7或Tomcat8在Win7或以上的系统中启动默认使用这种方式。Linux如果安装了apr和native，Tomcat直接启动就支持apr。</p></li></ul><h2 id="Tomcat有几种部署方式？"><a href="#Tomcat有几种部署方式？" class="headerlink" title="Tomcat有几种部署方式？"></a>Tomcat有几种部署方式？</h2><p>在Tomcat中部署Web应用的方式主要有如下几种：</p><ol><li><p>利用Tomcat的自动部署。</p><p>把web应用拷贝到webapps目录。Tomcat在启动时会加载目录下的应用，并将编译后的结果放入work目录下。</p></li><li><p>使用Manager App控制台部署。</p><p>在tomcat主页点击“Manager App” 进入应用管理控制台，可以指定一个web应用的路径或war文件。</p></li><li><p>修改conf/server.xml文件部署。</p><p>修改conf/server.xml文件，增加Context节点可以部署应用。</p></li><li><p>增加自定义的Web部署文件。</p><p>在conf/Catalina/localhost/ 路径下增加 xyz.xml文件，内容是Context节点，可以部署应用。</p></li></ol><h2 id="tomcat容器是如何创建servlet类实例？用到了什么原理？"><a href="#tomcat容器是如何创建servlet类实例？用到了什么原理？" class="headerlink" title="tomcat容器是如何创建servlet类实例？用到了什么原理？"></a>tomcat容器是如何创建servlet类实例？用到了什么原理？</h2><ol><li>当容器启动时，会读取在webapps目录下所有的web应用中的web.xml文件，然后对 xml文件进行解析，并读取servlet注册信息。然后，将每个应用中注册的servlet类都进行加载，并通过 反射的方式实例化。（有时候也是在第一次请求时实例化）</li><li>在servlet注册时加上1如果为正数，则在一开始就实例化，如果不写或为负数，则第一次请求实例化。</li></ol><h2 id="Tomcat工作模式"><a href="#Tomcat工作模式" class="headerlink" title="Tomcat工作模式"></a>Tomcat工作模式</h2><p>Tomcat作为servlet容器，有三种工作模式：</p><ol><li>独立的servlet容器，servlet容器是web服务器的一部分；</li><li>进程内的servlet容器，servlet容器是作为web服务器的插件和java容器的实现，web服务器插件在内部地址空间打开一个jvm使得java容器在内部得以运行。反应速度快但伸缩性不足；</li><li>进程外的servlet容器，servlet容器运行于web服务器之外的地址空间，并作为web服务器的插件和java容器实现的结合。反应时间不如进程内但伸缩性和稳定性比进程内优；</li></ol><p>进入Tomcat的请求可以根据Tomcat的工作模式分为如下两类：</p><ol><li>Tomcat作为应用程序服务器：请求来自于前端的web服务器，这可能是Apache, IIS, Nginx等；</li><li>Tomcat作为独立服务器：请求来自于web浏览器；</li></ol><p>面试时问到Tomcat相关问题的几率并不高，正式因为如此，很多人忽略了对Tomcat相关技能的掌握，下面这一篇文章整理了Tomcat相关的系统架构，介绍了Server、Service、Connector、Container之间的关系，各个模块的功能，可以说把这几个掌握住了，Tomcat相关的面试题你就不会有任何问题了！另外，在面试的时候你还要有意识无意识的往Tomcat这个地方引，就比如说常见的Spring MVC的执行流程，一个URL的完整调用链路，这些相关的题目你是可以往Tomcat处理请求的这个过程去说的！掌握了Tomcat这些技能，面试官一定会佩服你的！</p><p>学了本章之后你应该明白的是：</p><ul><li>Server、Service、Connector、Container四大组件之间的关系和联系，以及他们的主要功能点；</li><li>Tomcat执行的整体架构，请求是如何被一步步处理的；</li><li>Engine、Host、Context、Wrapper相关的概念关系；</li><li>Container是如何处理请求的；</li><li>Tomcat用到的相关设计模式；</li></ul><h2 id="Tomcat顶层架构"><a href="#Tomcat顶层架构" class="headerlink" title="Tomcat顶层架构"></a>Tomcat顶层架构</h2><p>俗话说，站在巨人的肩膀上看世界，一般学习的时候也是先总览一下整体，然后逐个部分个个击破，最后形成思路，了解具体细节，Tomcat的结构很复杂，但是 Tomcat 非常的模块化，找到了 Tomcat 最核心的模块，问题才可以游刃而解，了解了 Tomcat 的整体架构对以后深入了解 Tomcat 来说至关重要！</p><p>先上一张Tomcat的顶层结构图（图A），如下：<br><img src="https://lixiangbetter.github.io/2020/07/17/Tomcat%E7%AC%94%E8%AE%B0/20191021215330153.png" alt></p><p>Tomcat中最顶层的容器是Server，代表着整个服务器，从上图中可以看出，一个Server可以包含至少一个Service，即可以包含多个Service，用于具体提供服务。</p><p>Service主要包含两个部分：Connector和Container。从上图中可以看出 Tomcat 的心脏就是这两个组件，他们的作用如下：</p><ul><li>Connector用于处理连接相关的事情，并提供Socket与Request请求和Response响应相关的转化;</li><li>Container用于封装和管理Servlet，以及具体处理Request请求；</li></ul><p>一个Tomcat中只有一个Server，一个Server可以包含多个Service，一个Service只有一个Container，但是可以有多个Connectors，这是因为一个服务可以有多个连接，如同时提供Http和Https链接，也可以提供向相同协议不同端口的连接，示意图如下（Engine、Host、Context下面会说到）：</p><p><img src="https://lixiangbetter.github.io/2020/07/17/Tomcat%E7%AC%94%E8%AE%B0/20191021215344811.png" alt></p><p>多个 Connector 和一个Container 就形成了一个Service，有了 Service 就可以对外提供服务了，但是 Service 还要一个生存的环境，必须要有人能够给她生命、掌握其生死大权，那就非 Server 莫属了！所以整个 Tomcat 的生命周期由 Server 控制。</p><p>另外，上述的包含关系或者说是父子关系，都可以在tomcat的conf目录下的server.xml配置文件中看出，下图是删除了注释内容之后的一个完整的server.xml配置文件（Tomcat版本为8.0）</p><p><img src="https://lixiangbetter.github.io/2020/07/17/Tomcat%E7%AC%94%E8%AE%B0/20191021215355649.png" alt></p><p>上边的配置文件，还可以通过下边的一张结构图更清楚的理解：</p><p><img src="https://lixiangbetter.github.io/2020/07/17/Tomcat%E7%AC%94%E8%AE%B0/2019102121541531.png" alt></p><p>Server标签设置的端口号为8005，shutdown=”SHUTDOWN” ，表示在8005端口监听“SHUTDOWN”命令，如果接收到了就会关闭Tomcat。一个Server有一个Service，当然还可以进行配置，一个Service有多个Connector，Service左边的内容都属于Container的，Service下边是Connector。</p><h3 id="Tomcat顶层架构小结"><a href="#Tomcat顶层架构小结" class="headerlink" title="Tomcat顶层架构小结"></a>Tomcat顶层架构小结</h3><ol><li>Tomcat中只有一个Server，一个Server可以有多个Service，一个Service可以有多个Connector和一个Container；</li><li>Server掌管着整个Tomcat的生死大权；</li><li>Service 是对外提供服务的；</li><li>Connector用于接受请求并将请求封装成Request和Response来具体处理；</li><li>Container用于封装和管理Servlet，以及具体处理request请求；</li></ol><p>知道了整个Tomcat顶层的分层架构和各个组件之间的关系以及作用，对于绝大多数的开发人员来说Server和Service对我们来说确实很远，而我们开发中绝大部分进行配置的内容是属于Connector和Container的，所以接下来介绍一下Connector和Container。</p><h2 id="Connector和Container的微妙关系"><a href="#Connector和Container的微妙关系" class="headerlink" title="Connector和Container的微妙关系"></a>Connector和Container的微妙关系</h2><p>由上述内容我们大致可以知道一个请求发送到Tomcat之后，首先经过Service然后会交给我们的Connector，Connector用于接收请求并将接收的请求封装为Request和Response来具体处理，Request和Response封装完之后再交由Container进行处理，Container处理完请求之后再返回给Connector，最后在由Connector通过Socket将处理的结果返回给客户端，这样整个请求的就处理完了！</p><p>Connector最底层使用的是Socket来进行连接的，Request和Response是按照HTTP协议来封装的，所以Connector同时需要实现TCP/IP协议和HTTP协议！</p><p>Tomcat既然需要处理请求，那么肯定需要先接收到这个请求，接收请求这个东西我们首先就需要看一下Connector！</p><p><strong>Connector架构分析</strong></p><p>Connector用于接受请求并将请求封装成Request和Response，然后交给Container进行处理，Container处理完之后在交给Connector返回给客户端。</p><p>因此，我们可以把Connector分为四个方面进行理解：</p><ol><li>Connector如何接受请求的？</li><li>如何将请求封装成Request和Response的？</li><li>封装完之后的Request和Response如何交给Container进行处理的？</li><li>Container处理完之后如何交给Connector并返回给客户端的？</li></ol><p>首先看一下Connector的结构图（图B），如下所示：</p><p><img src="https://lixiangbetter.github.io/2020/07/17/Tomcat%E7%AC%94%E8%AE%B0/20191021215430677.png" alt></p><p>Connector就是使用ProtocolHandler来处理请求的，不同的ProtocolHandler代表不同的连接类型，比如：Http11Protocol使用的是普通Socket来连接的，Http11NioProtocol使用的是NioSocket来连接的。</p><p>其中ProtocolHandler由包含了三个部件：Endpoint、Processor、Adapter。</p><ol><li>Endpoint用来处理底层Socket的网络连接，Processor用于将Endpoint接收到的Socket封装成Request，Adapter用于将Request交给Container进行具体的处理。</li><li>Endpoint由于是处理底层的Socket网络连接，因此Endpoint是用来实现TCP/IP协议的，而Processor用来实现HTTP协议的，Adapter将请求适配到Servlet容器进行具体的处理。</li><li>Endpoint的抽象实现AbstractEndpoint里面定义的Acceptor和AsyncTimeout两个内部类和一个Handler接口。Acceptor用于监听请求，AsyncTimeout用于检查异步Request的超时，Handler用于处理接收到的Socket，在内部调用Processor进行处理。</li></ol><p>至此，我们应该很轻松的回答1，2，3的问题了，但是4还是不知道，那么我们就来看一下Container是如何进行处理的以及处理完之后是如何将处理完的结果返回给Connector的？</p><h2 id="Container架构分析"><a href="#Container架构分析" class="headerlink" title="Container架构分析"></a>Container架构分析</h2><p>Container用于封装和管理Servlet，以及具体处理Request请求，在Container内部包含了4个子容器，结构图如下（图C）：</p><p><img src="https://lixiangbetter.github.io/2020/07/17/Tomcat%E7%AC%94%E8%AE%B0/20191021215443306.png" alt></p><p>4个子容器的作用分别是：</p><ol><li>Engine：引擎，用来管理多个站点，一个Service最多只能有一个Engine；</li><li>Host：代表一个站点，也可以叫虚拟主机，通过配置Host就可以添加站点；</li><li>Context：代表一个应用程序，对应着平时开发的一套程序，或者一个WEB-INF目录以及下面的web.xml文件；</li><li>Wrapper：每一Wrapper封装着一个Servlet；</li></ol><p>下面找一个Tomcat的文件目录对照一下，如下图所示：</p><p><img src="https://lixiangbetter.github.io/2020/07/17/Tomcat%E7%AC%94%E8%AE%B0/20191021215455991.png" alt></p><p>Context和Host的区别是Context表示一个应用，我们的Tomcat中默认的配置下webapps下的每一个文件夹目录都是一个Context，其中ROOT目录中存放着主应用，其他目录存放着子应用，而整个webapps就是一个Host站点。</p><p>我们访问应用Context的时候，如果是ROOT下的则直接使用域名就可以访问，例如：<a href="http://www.baidu.com，如果是Host（webapps）下的其他应用，则可以使用www.baidu.com/docs进行访问，当然默认指定的根应用（ROOT）是可以进行设定的，只不过Host站点下默认的主应用是ROOT目录下的。" target="_blank" rel="noopener">www.baidu.com，如果是Host（webapps）下的其他应用，则可以使用www.baidu.com/docs进行访问，当然默认指定的根应用（ROOT）是可以进行设定的，只不过Host站点下默认的主应用是ROOT目录下的。</a></p><p>看到这里我们知道Container是什么，但是还是不知道Container是如何进行请求处理的以及处理完之后是如何将处理完的结果返回给Connector的？别急！下边就开始探讨一下Container是如何进行处理的！</p><h3 id="Container如何处理请求的"><a href="#Container如何处理请求的" class="headerlink" title="Container如何处理请求的"></a>Container如何处理请求的</h3><p>Container处理请求是使用Pipeline-Valve管道来处理的！（Valve是阀门之意）</p><p>Pipeline-Valve是责任链模式，责任链模式是指在一个请求处理的过程中有很多处理者依次对请求进行处理，每个处理者负责做自己相应的处理，处理完之后将处理后的结果返回，再让下一个处理者继续处理。</p><p><img src="https://lixiangbetter.github.io/2020/07/17/Tomcat%E7%AC%94%E8%AE%B0/20191021215507725.png" alt></p><p>但是！Pipeline-Valve使用的责任链模式和普通的责任链模式有些不同！区别主要有以下两点：</p><p>每个Pipeline都有特定的Valve，而且是在管道的最后一个执行，这个Valve叫做BaseValve，BaseValve是不可删除的；</p><p>在上层容器的管道的BaseValve中会调用下层容器的管道。</p><p>我们知道Container包含四个子容器，而这四个子容器对应的BaseValve分别在：StandardEngineValve、StandardHostValve、StandardContextValve、StandardWrapperValve。</p><p>Pipeline的处理流程图如下（图D）：</p><p><img src="https://lixiangbetter.github.io/2020/07/17/Tomcat%E7%AC%94%E8%AE%B0/20191021215519408.png" alt></p><ul><li>Connector在接收到请求后会首先调用最顶层容器的Pipeline来处理，这里的最顶层容器的Pipeline就是EnginePipeline（Engine的管道）；</li><li>在Engine的管道中依次会执行EngineValve1、EngineValve2等等，最后会执行StandardEngineValve，在StandardEngineValve中会调用Host管道，然后再依次执行Host的HostValve1、HostValve2等，最后在执行StandardHostValve，然后再依次调用Context的管道和Wrapper的管道，最后执行到StandardWrapperValve。</li><li>当执行到StandardWrapperValve的时候，会在StandardWrapperValve中创建FilterChain，并调用其doFilter方法来处理请求，这个FilterChain包含着我们配置的与请求相匹配的Filter和Servlet，其doFilter方法会依次调用所有的Filter的doFilter方法和Servlet的service方法，这样请求就得到了处理！</li><li>当所有的Pipeline-Valve都执行完之后，并且处理完了具体的请求，这个时候就可以将返回的结果交给Connector了，Connector在通过Socket的方式将结果返回给客户端。</li></ul><h2 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h2><p>至此，我们已经对Tomcat的整体架构有了大致的了解，从图A、B、C、D可以看出来每一个组件的基本要素和作用。我们在脑海里应该有一个大概的轮廓了！如果你面试的时候，让你简单的聊一下Tomcat，上面的内容你能脱口而出吗？当你能够脱口而出的时候，面试官一定会对你刮目相看的！</p>]]></content>
      
      
      <categories>
          
          <category> bigdata </category>
          
      </categories>
      
      
        <tags>
            
            <tag> tomcat </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>flume、sqoop笔记</title>
      <link href="/2020/07/12/flume%E3%80%81sqoop%E7%AC%94%E8%AE%B0/"/>
      <url>/2020/07/12/flume%E3%80%81sqoop%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<h1 id="flume、sqoop笔记"><a href="#flume、sqoop笔记" class="headerlink" title="flume、sqoop笔记"></a>flume、sqoop笔记</h1><h2 id="1-什么是flume"><a href="#1-什么是flume" class="headerlink" title="1.什么是flume"></a>1.什么是flume</h2><p>a.Flume是一个分布式、可靠、和高可用的海量日志采集、聚合和传输的系统。</p><p>b.Flume可以采集文件，socket数据包等各种形式源数据，又可以将采集到的数据输出到HDFS、hbase、hive、kafka等众多外部存储系统中</p><p>c.一般的采集需求，通过对flume的简单配置即可实现</p><p>d.Flume针对特殊场景也具备良好的自定义扩展能力，因此，flume可以适用于大部分的日常数据采集场景</p><h2 id="2-flume运行机制"><a href="#2-flume运行机制" class="headerlink" title="2.flume运行机制"></a>2.flume运行机制</h2><ol><li>Flume分布式系统中最核心的角色是agent，flume采集系统就是由一个个agent所连接起来形成</li><li>每一个agent相当于一个数据传递员，内部有三个组件：<ul><li>Source：采集源，用于跟数据源对接，以获取数据</li><li>Sink：下沉地，采集数据的传送目的，用于往下一级agent传递数据或者往最终存储系统传递数据</li><li>Channel：angent内部的数据传输通道，用于从source将数据传递到sink</li></ul></li></ol><p><img src="https://lixiangbetter.github.io/2020/07/12/flume%E3%80%81sqoop%E7%AC%94%E8%AE%B0/20200409122907293.png" alt></p><h2 id="3-Flume采集数据到Kafka中丢数据怎么办"><a href="#3-Flume采集数据到Kafka中丢数据怎么办" class="headerlink" title="3.Flume采集数据到Kafka中丢数据怎么办"></a>3.Flume采集数据到Kafka中丢数据怎么办</h2><p><strong>Source到Channel</strong>是事务性的，</p><p><strong>Channel到Sink</strong>也是事务性的，</p><p>这两个环节都不可能丢失数据。</p><p>唯一可能丢失数据的是Channel采用MemoryChannel.</p><h2 id="4-Flume怎么进行监控"><a href="#4-Flume怎么进行监控" class="headerlink" title="4.Flume怎么进行监控?"></a>4.Flume怎么进行监控?</h2><p>Ganglia</p><h2 id="5-Flume的三层架构，collector、agent、storage"><a href="#5-Flume的三层架构，collector、agent、storage" class="headerlink" title="5.Flume的三层架构，collector、agent、storage"></a>5.Flume的三层架构，collector、agent、storage</h2><p>Flume采用了三层架构，分别为agent，collector和storage，每一层均可以水平扩展。其中，所有agent和collector由master统一管理，这使得系统容易监控和维护，且master允许有多个（使用ZooKeeper进行管理和负载均衡），这就避免了单点故障问题。</p><p>1）Agent层：这一层包含了Flume的Agent组件，与需要传输数据的数据源连接在一起</p><p>2）Collector：这一层通过多个收集器收集Agent层的数据，然后将这些转发到下一层</p><p>3）storage：这一层接收collector层的数据并存储起来</p><p>分割线———————————————————————————————————</p><h2 id="1-Sqoop底层运行的任务是什么"><a href="#1-Sqoop底层运行的任务是什么" class="headerlink" title="1.Sqoop底层运行的任务是什么"></a>1.Sqoop底层运行的任务是什么</h2><p>只有Map阶段，没有Reduce阶段的任务。</p><h2 id="2-Sqoop的迁移数据的原理"><a href="#2-Sqoop的迁移数据的原理" class="headerlink" title="2.Sqoop的迁移数据的原理"></a>2.Sqoop的迁移数据的原理</h2><p>将导入或导出命令翻译成mapreduce程序来实现，在翻译出的mapreduce中主要是对inputformat和outputformat进行定制。</p><h2 id="3-Sqoop参数"><a href="#3-Sqoop参数" class="headerlink" title="3.Sqoop参数"></a>3.Sqoop参数</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">/opt/module/sqoop/bin/sqoop import \</span><br><span class="line"></span><br><span class="line">--connect \</span><br><span class="line"></span><br><span class="line">--username \</span><br><span class="line"></span><br><span class="line">--password \</span><br><span class="line"></span><br><span class="line">--target-dir \</span><br><span class="line"></span><br><span class="line">--delete-target-dir \</span><br><span class="line"></span><br><span class="line">--num-mappers \</span><br><span class="line"></span><br><span class="line">--fields-terminated-by   \</span><br><span class="line"></span><br><span class="line">--query   "$2" ' and $CONDITIONS;'</span><br></pre></td></tr></table></figure><h2 id="4-Sqoop导入导出Null存储一致性问题"><a href="#4-Sqoop导入导出Null存储一致性问题" class="headerlink" title="4.Sqoop导入导出Null存储一致性问题"></a>4.Sqoop导入导出Null存储一致性问题</h2><p>Hive中的Null在底层是以“\N”来存储，而MySQL中的Null在底层就是Null，为了保证数据两端的一致性。在导出数据时采用–input-null-string和–input-null-non-string两个参数。导入数据时采用–null-string和–null-non-string。</p><h2 id="5-Sqoop数据导出一致性问题"><a href="#5-Sqoop数据导出一致性问题" class="headerlink" title="5.Sqoop数据导出一致性问题"></a>5.Sqoop数据导出一致性问题</h2><p>1）场景1：如Sqoop在导出到Mysql时，使用4个Map任务，过程中有2个任务失败，那此时MySQL中存储了另外两个Map任务导入的数据，此时老板正好看到了这个报表数据。而开发工程师发现任务失败后，会调试问题并最终将全部数据正确的导入MySQL，那后面老板再次看报表数据，发现本次看到的数据与之前的不一致，这在生产环境是不允许的。</p><p>2）场景2：设置map数量为1个（不推荐，面试官想要的答案不只这个）</p><p>多个Map任务时，采用–staging-table方式，仍然可以解决数据一致性问题。</p>]]></content>
      
      
      <categories>
          
          <category> bigdata </category>
          
      </categories>
      
      
        <tags>
            
            <tag> flume sqoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>hive调优</title>
      <link href="/2020/07/12/hive%E8%B0%83%E4%BC%98/"/>
      <url>/2020/07/12/hive%E8%B0%83%E4%BC%98/</url>
      
        <content type="html"><![CDATA[<h1 id="hive调优"><a href="#hive调优" class="headerlink" title="hive调优"></a>hive调优</h1><p> HIVE调优是一个很大的课题，涉及到hive本身的调优，hive底层的mapreduce计算引擎的调优，sql的调优，数据倾斜调优，小文件问题的调优，数据压缩的调优等</p><p>以下提供一些主要的调优总结：</p><h2 id="1-数据的压缩与存储格式"><a href="#1-数据的压缩与存储格式" class="headerlink" title="1.数据的压缩与存储格式"></a>1.数据的压缩与存储格式</h2><p> hive底层的计算引擎是mapreduce，而mapreduce在运算时，免不了的就是要从hdfs中读取原始文件，然后在内部的map到reduce之间还要shuffle数据到各task所在的本地磁盘，最后的输出又避免不了要往HDFS中输出文件.</p><p>所以，在各个环节中，读写的数据量越小，读写的性能越高，对hive的整体执行效率肯定是有重要作用的，那么，如何降低这些环节上的文件io量以及提高文件io效率呢，一个最主要的办法就是“选择合适的文件格式”+选择合适的压缩编码</p><p>设置方式</p><ol><li>map阶段输出数据压缩 ，在这个阶段，优先选择一个低CPU开销的算法。</li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">set hive.exec.compress.intermediate=true</span><br><span class="line">set mapred.map.output.compression.codec= org.apache.hadoop.io.compress.SnappyCodec</span><br><span class="line">set mapred.map.output.compression.codec=com.hadoop.compression.lzo.LzoCodec;</span><br></pre></td></tr></table></figure><ol start="2"><li>对最终输出结果压缩</li></ol><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">set hive.exec.compress.output=true </span><br><span class="line">set mapred.output.compression.codec=org.apache.hadoop.io.compress.SnappyCodec</span><br><span class="line"><span class="meta">#</span><span class="bash"><span class="comment"># 当然，也可以在hive建表时指定表的文件格式和压缩编码</span></span></span><br></pre></td></tr></table></figure><p>结论，一般选择orcfile/parquet + snappy 方式</p><h2 id="2-合理利用分区、分桶"><a href="#2-合理利用分区、分桶" class="headerlink" title="2.合理利用分区、分桶"></a>2.合理利用分区、分桶</h2><p>​        分区是将表的数据在物理上分成不同的文件夹，以便于在查询时可以精准指定所要读取的分区目录，从来降低读取的数据量</p><p>​    分桶是将表数据按指定列的hash散列后分在了不同的文件中，将来查询时，hive可以根据分桶结构，快速定位到一行数据所在的分桶文件，从来提高读取效率.</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">-- 创建分桶表示例</span></span><br><span class="line"><span class="keyword">create</span> <span class="keyword">table</span> bucketed_user(<span class="keyword">id</span> <span class="built_in">int</span>,<span class="keyword">name</span> <span class="keyword">string</span>) clustered <span class="keyword">by</span> (<span class="keyword">id</span>)</span><br><span class="line"> sorted <span class="keyword">by</span>(<span class="keyword">name</span>) <span class="keyword">into</span> <span class="number">4</span> buckets <span class="keyword">row</span> <span class="keyword">format</span> <span class="keyword">delimited</span> <span class="keyword">fields</span> <span class="keyword">terminated</span> <span class="keyword">by</span> <span class="string">'\t'</span></span><br><span class="line"> <span class="keyword">stored</span> <span class="keyword">as</span> ORCFILE;</span><br></pre></td></tr></table></figure><h2 id="3-hive参数优化"><a href="#3-hive参数优化" class="headerlink" title="3.hive参数优化"></a>3.hive参数优化</h2><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">-- 让可以不走mapreduce任务的，就不走mapreduce任务</span><br><span class="line"><span class="meta">hive&gt;</span><span class="bash"> <span class="built_in">set</span> hive.fetch.task.conversion=more;</span></span><br><span class="line"> </span><br><span class="line">// 开启任务并行执行</span><br><span class="line"> set hive.exec.parallel=true;</span><br><span class="line">// 解释：当一个sql中有多个job时候，且这多个job之间没有依赖，则可以让顺序执行变为并行执行（一般为用到union all的时候）</span><br><span class="line"> </span><br><span class="line"> // 同一个sql允许并行任务的最大线程数 </span><br><span class="line">set hive.exec.parallel.thread.number=8;</span><br><span class="line"> </span><br><span class="line">// 设置jvm重用</span><br><span class="line">// JVM重用对hive的性能具有非常大的 影响，特别是对于很难避免小文件的场景或者task特别多的场景，这类场景大多数执行时间都很短。jvm的启动过程可能会造成相当大的开销，尤其是执行的job包含有成千上万个task任务的情况。</span><br><span class="line">set mapred.job.reuse.jvm.num.tasks=10; </span><br><span class="line"> </span><br><span class="line">// 合理设置reduce的数目</span><br><span class="line">// 方法1：调整每个reduce所接受的数据量大小</span><br><span class="line">set hive.exec.reducers.bytes.per.reducer=500000000; （500M）</span><br><span class="line">// 方法2：直接设置reduce数量</span><br><span class="line">set mapred.reduce.tasks = 20</span><br></pre></td></tr></table></figure><h2 id="4-sql优化"><a href="#4-sql优化" class="headerlink" title="4.sql优化"></a>4.sql优化</h2><p>（1）where条件优化<br>优化前（关系数据库不用考虑会自动优化）：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> m.cid,u.id <span class="keyword">from</span> <span class="keyword">order</span> m <span class="keyword">join</span> customer u <span class="keyword">on</span>( m.cid =u.id )<span class="keyword">where</span> m.dt=<span class="string">'20180808'</span>;</span><br></pre></td></tr></table></figure><p>优化后(where条件在map端执行而不是在reduce端执行）：</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> m.cid,u.id <span class="keyword">from</span> （<span class="keyword">select</span> * <span class="keyword">from</span> <span class="keyword">order</span> <span class="keyword">where</span> dt=<span class="string">'20180818'</span>） m <span class="keyword">join</span> customer u <span class="keyword">on</span>( m.cid =u.id);</span><br></pre></td></tr></table></figure><p>（2）union优化</p><p>尽量不要使用union （union 去掉重复的记录）而是使用 union all 然后在用group by 去重</p><p>（3）count distinct优化</p><p>不要使用count (distinct  cloumn) ,使用子查询</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">count</span>(<span class="number">1</span>) <span class="keyword">from</span> (<span class="keyword">select</span> <span class="keyword">id</span> <span class="keyword">from</span> tablename <span class="keyword">group</span> <span class="keyword">by</span> <span class="keyword">id</span>) tmp;</span><br></pre></td></tr></table></figure><p>（4） 用in 来代替join</p><p>如果需要根据一个表的字段来约束另为一个表，尽量用in来代替join . in 要比join 快</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">select</span> <span class="keyword">id</span>,<span class="keyword">name</span> <span class="keyword">from</span> tb1  a <span class="keyword">join</span> tb2 b <span class="keyword">on</span>(a.id = b.id);</span><br><span class="line"> </span><br><span class="line"><span class="keyword">select</span> <span class="keyword">id</span>,<span class="keyword">name</span> <span class="keyword">from</span> tb1 <span class="keyword">where</span> <span class="keyword">id</span> <span class="keyword">in</span>(<span class="keyword">select</span> <span class="keyword">id</span> <span class="keyword">from</span> tb2);</span><br></pre></td></tr></table></figure><p>（5）消灭子查询内的 group by 、 COUNT(DISTINCT)，MAX，MIN。 可以减少job的数量。</p><p>  (6) join 优化：</p><p>Common/shuffle/Reduce JOIN 连接发生的阶段，发生在reduce 阶段， 适用于大表 连接 大表(默认的方式)</p><p>Map join ： 连接发生在map阶段 ， 适用于小表 连接 大表<br>                       大表的数据从文件中读取<br>                       小表的数据存放在内存中（hive中已经自动进行了优化，自动判断小表，然后进行缓存）</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.auto.convert.join=<span class="literal">true</span>;</span><br></pre></td></tr></table></figure><p>SMB join<br>  Sort -Merge -Bucket Join 对大表连接大表的优化，用桶表的概念来进行优化。在一个桶内发送生笛卡尔积连接（需要是两个桶表进行join）</p><figure class="highlight sql"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">set</span> hive.auto.convert.sortmerge.join=<span class="literal">true</span>;  </span><br><span class="line"><span class="keyword">set</span> hive.optimize.bucketmapjoin = <span class="literal">true</span>;  </span><br><span class="line"><span class="keyword">set</span> hive.optimize.bucketmapjoin.sortedmerge = <span class="literal">true</span>;  </span><br><span class="line"><span class="keyword">set</span> hive.auto.convert.sortmerge.join.noconditionaltask=<span class="literal">true</span>;</span><br></pre></td></tr></table></figure><h2 id="5-数据倾斜"><a href="#5-数据倾斜" class="headerlink" title="5.数据倾斜"></a>5.数据倾斜</h2><p>表现：任务进度长时间维持在99%（或100%），查看任务监控页面，发现只有少量（1个或几个）reduce子任务未完成。因为其处理的数据量和其他reduce差异过大。</p><p>原因：某个reduce的数据输入量远远大于其他reduce数据的输入量</p><p>1)、key分布不均匀</p><p>2)、业务数据本身的特性</p><p>3)、建表时考虑不周</p><p>4)、某些SQL语句本身就有数据倾斜</p><table><thead><tr><th>关键词</th><th><strong>情形</strong></th><th><strong>后果</strong></th></tr></thead><tbody><tr><td>join</td><td>其中一个表较小，但是key集中</td><td>分发到某一个或几个Reduce上的数据远高于平均值</td></tr><tr><td>join</td><td>大表与大表，但是分桶的判断字段0值或空值过多</td><td>这些空值都由一个reduce处理，非常慢</td></tr><tr><td>group by</td><td>group by 维度过小，某值的数量过多</td><td>处理某值的reduce非常耗时</td></tr><tr><td>count distinct</td><td>某特殊值过多</td><td>处理此特殊值reduce耗时</td></tr></tbody></table><p>(1)参数调节</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">set hive.map.aggr=true  // map端聚合，降低传给reduce的数据量</span><br><span class="line">set hive.groupby.skewindata=true // 开启hive内置的数倾优化机制</span><br></pre></td></tr></table></figure><p>(2) 熟悉数据的分布，优化sql的逻辑，找出数据倾斜的原因。</p><p>比如，如果是在groupby中产生了数据倾斜，是否可以将group by的维度变得更细，如果没法变得更细，就可以在原分组key上添加随机数后分组聚合一次，然后对结果去掉随机数后再分组聚合</p><p>比如，在join时，有大量为null的join key，则可以将null转成一个随机字符串，也能让null key数据均匀分散到不同的reduce任务</p><h2 id="6-合并小文件"><a href="#6-合并小文件" class="headerlink" title="6.合并小文件"></a>6.合并小文件</h2><p>小文件的产生有三个地方，map输入，map输出，reduce输出，小文件过多也会影响hive的分析效率：</p><p>设置map输入的小文件合并</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">set mapred.max.split.size=256000000;  </span><br><span class="line">//一个节点上split的至少的大小(这个值决定了多个DataNode上的文件是否需要合并)</span><br><span class="line">set mapred.min.split.size.per.node=100000000;</span><br><span class="line">//一个交换机下split的至少的大小(这个值决定了多个交换机上的文件是否需要合并)  </span><br><span class="line">set mapred.min.split.size.per.rack=100000000;</span><br><span class="line">//执行Map前进行小文件合并</span><br><span class="line">set hive.input.format=org.apache.hadoop.hive.ql.io.CombineHiveInputFormat;</span><br></pre></td></tr></table></figure><p>设置map输出和reduce输出进行合并的相关参数：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">//设置map端输出进行合并，默认为true</span><br><span class="line">set hive.merge.mapfiles = true</span><br><span class="line">//设置reduce端输出进行合并，默认为false</span><br><span class="line">set hive.merge.mapredfiles = true</span><br><span class="line">//设置合并文件的大小</span><br><span class="line">set hive.merge.size.per.task = 256*1000*1000</span><br><span class="line">//当输出文件的平均大小小于该值时，启动一个独立的MapReduce任务进行文件merge。</span><br><span class="line">set hive.merge.smallfiles.avgsize=16000000</span><br></pre></td></tr></table></figure><h2 id="7-查看sql的执行计划"><a href="#7-查看sql的执行计划" class="headerlink" title="7.查看sql的执行计划"></a>7.查看sql的执行计划</h2><p>通过explain select …from ，来查看你的sql的执行计划，从来进行分析寻找，看是否有更优化的sql写法</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">explain sql</span><br></pre></td></tr></table></figure><p>学会查看sql的执行计划，优化业务逻辑 ，减少job的数据量。 对调优也非常重要</p>]]></content>
      
      
      <categories>
          
          <category> bigdata </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>数仓笔记</title>
      <link href="/2020/07/11/%E6%95%B0%E4%BB%93%E7%AC%94%E8%AE%B0/"/>
      <url>/2020/07/11/%E6%95%B0%E4%BB%93%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<h1 id="数仓笔记"><a href="#数仓笔记" class="headerlink" title="数仓笔记"></a>数仓笔记</h1><h2 id="1-维表和宽表的考查（主要考察维表的使用及维度退化手法）"><a href="#1-维表和宽表的考查（主要考察维表的使用及维度退化手法）" class="headerlink" title="1.维表和宽表的考查（主要考察维表的使用及维度退化手法）"></a>1.维表和宽表的考查（主要考察维表的使用及维度退化手法）</h2><p>维表数据一般根据ods层数据加工生成，在设计宽表的时候，可以适当的用一些维度退化手法，将维度退化到事实表中，减少事实表和维表的关联</p><h2 id="2-数仓表命名规范"><a href="#2-数仓表命名规范" class="headerlink" title="2.数仓表命名规范"></a>2.数仓表命名规范</h2><p>每个公司都会有点差别</p><p>ODS</p><p>ods.库名_表名_df/di/da/dz</p><p>CDM(dwd/dws)</p><p>dwd.主题_内容_df</p><h2 id="3-拉链表的使用场景"><a href="#3-拉链表的使用场景" class="headerlink" title="3.拉链表的使用场景"></a>3.拉链表的使用场景</h2><p>1.数据量比较大</p><p>2.表中的部分字段会被更新</p><p>3.需要查看某一个时间点或者时间段的历史快照信息</p><p>​    查看某一个订单在历史某一个时间点的状态<br>​    某一个用户在过去某一段时间，下单次数</p><p>4.更新的比例和频率不是很大<br>         如果表中信息变化不是很大，每天都保留一份全量，那么每次全量中会保存很多不变的信息，对存储是极大的浪费</p><h2 id="4-一亿条数据查的很慢-怎么查快一点"><a href="#4-一亿条数据查的很慢-怎么查快一点" class="headerlink" title="4.一亿条数据查的很慢,怎么查快一点"></a>4.一亿条数据查的很慢,怎么查快一点</h2><h2 id="5-有什么维表"><a href="#5-有什么维表" class="headerlink" title="5.有什么维表"></a>5.有什么维表</h2><p>时间维表，用户维表，医院维表等</p><h2 id="6-数据源都有哪些"><a href="#6-数据源都有哪些" class="headerlink" title="6.数据源都有哪些"></a>6.数据源都有哪些</h2><p>业务库数据源:mysql,oracle,mongo</p><p>日志数据：ng日志，埋点日志</p><p>爬虫数据</p><h2 id="7-你们最大的表是什么表-数据量多少"><a href="#7-你们最大的表是什么表-数据量多少" class="headerlink" title="7.你们最大的表是什么表,数据量多少"></a>7.你们最大的表是什么表,数据量多少</h2><p>ng日志表，三端(app,web,h5)中app端日志量最大，清洗入库后的数据一天大概xxxxW</p><h2 id="8-数仓架构体系"><a href="#8-数仓架构体系" class="headerlink" title="8.数仓架构体系"></a>8.数仓架构体系</h2><p><img src="https://lixiangbetter.github.io/2020/07/11/%E6%95%B0%E4%BB%93%E7%AC%94%E8%AE%B0/20200630165353204.png" alt></p><h2 id="9-数据平台是怎样的，用到了阿里的那一套吗？"><a href="#9-数据平台是怎样的，用到了阿里的那一套吗？" class="headerlink" title="9.数据平台是怎样的，用到了阿里的那一套吗？"></a>9.数据平台是怎样的，用到了阿里的那一套吗？</h2><p>没用到阿里那一套，数据平台为自研产品</p><h2 id="10-你了解的调度系统有那些？，你们公司用的是哪种调度系统"><a href="#10-你了解的调度系统有那些？，你们公司用的是哪种调度系统" class="headerlink" title="10.你了解的调度系统有那些？，你们公司用的是哪种调度系统"></a>10.你了解的调度系统有那些？，你们公司用的是哪种调度系统</h2><p>airflow，azkaban，ooize，我们公司使用的是airflow</p><h2 id="11-你们公司数仓底层是怎么抽数据的？"><a href="#11-你们公司数仓底层是怎么抽数据的？" class="headerlink" title="11.你们公司数仓底层是怎么抽数据的？"></a>11.你们公司数仓底层是怎么抽数据的？</h2><p>业务数据用的是datax</p><p>日志数据用的是logstash</p><h2 id="12-为什么datax抽数据要比sqoop-快？"><a href="#12-为什么datax抽数据要比sqoop-快？" class="headerlink" title="12.为什么datax抽数据要比sqoop 快？"></a>12.为什么datax抽数据要比sqoop 快？</h2><h2 id="13-埋点数据你们是怎样接入的"><a href="#13-埋点数据你们是怎样接入的" class="headerlink" title="13.埋点数据你们是怎样接入的"></a>13.埋点数据你们是怎样接入的</h2><p>logstash–&gt;kafka–&gt;logstash–&gt;hdfs</p><h2 id="14-如果你们业务库的表有更新，你们数仓怎么处理的？"><a href="#14-如果你们业务库的表有更新，你们数仓怎么处理的？" class="headerlink" title="14.如果你们业务库的表有更新，你们数仓怎么处理的？"></a>14.如果你们业务库的表有更新，你们数仓怎么处理的？</h2><p>根据表数据量及表特性，选择用全量表，增量表，追加表和拉链表处理</p><h2 id="15-能独立搭建数仓吗"><a href="#15-能独立搭建数仓吗" class="headerlink" title="15.能独立搭建数仓吗"></a>15.能独立搭建数仓吗</h2><h2 id="16-搭建过CDH-集群吗"><a href="#16-搭建过CDH-集群吗" class="headerlink" title="16.搭建过CDH 集群吗"></a>16.搭建过CDH 集群吗</h2><h2 id="17-说一下你们公司的大数据平台架构？你有参与吗？"><a href="#17-说一下你们公司的大数据平台架构？你有参与吗？" class="headerlink" title="17.说一下你们公司的大数据平台架构？你有参与吗？"></a>17.说一下你们公司的大数据平台架构？你有参与吗？</h2><h2 id="18-介绍一下你自己的项目和所用的技术"><a href="#18-介绍一下你自己的项目和所用的技术" class="headerlink" title="18.介绍一下你自己的项目和所用的技术"></a>18.介绍一下你自己的项目和所用的技术</h2><h2 id="19-对目前的流和批处理的认识？就是谈谈自己的感受"><a href="#19-对目前的流和批处理的认识？就是谈谈自己的感受" class="headerlink" title="19.对目前的流和批处理的认识？就是谈谈自己的感受"></a>19.对目前的流和批处理的认识？就是谈谈自己的感受</h2><h2 id="20-你了解那些OLAP-引擎，MPP-知道一些吗？clickHouse-了解一些吗？你自己做过测试性能吗？"><a href="#20-你了解那些OLAP-引擎，MPP-知道一些吗？clickHouse-了解一些吗？你自己做过测试性能吗？" class="headerlink" title="20.你了解那些OLAP 引擎，MPP 知道一些吗？clickHouse 了解一些吗？你自己做过测试性能吗？"></a>20.你了解那些OLAP 引擎，MPP 知道一些吗？clickHouse 了解一些吗？你自己做过测试性能吗？</h2><p>Kylin、druid、presto</p><p><strong>MPP架构</strong></p><ul><li>任务并行执行;</li><li>数据分布式存储(本地化);</li><li>分布式计算;</li><li>私有资源;</li><li>横向扩展;</li><li>Shared Nothing架构。</li></ul><p><strong>什么是ClickHouse</strong></p><p>ClickHouse是一个快速的开源OLAP数据库管理系统, 它是面向列的，并允许使用SQL查询实时生成分析报告。<br>也是一个新的开源列式数据库。</p><p><strong>ClickHouse 亮点</strong></p><ol><li>急速处理</li><li>硬件效率高</li><li>线性可扩展</li><li>容错</li><li>功能丰富</li><li>高度可靠</li></ol><h2 id="21-Kylin-有了解吗？介绍一下原理"><a href="#21-Kylin-有了解吗？介绍一下原理" class="headerlink" title="21.Kylin 有了解吗？介绍一下原理"></a>21.Kylin 有了解吗？介绍一下原理</h2><p>Kylin的出现就是为了解决大数据系统中<code>TB</code>级别数据的数据分析需求，它提供<code>Hadoop/Spark</code>之上的<code>SQL</code>查询接口及多维分析(<code>OLAP</code>)能力以支持超大规模数据，它能在亚秒内查询巨大的<code>Hive</code>表。其核心是预计算，计算结果存在<code>HBase</code>中.</p><p><strong>基本原理</strong></p><p>Kylin的核心思想是预计算。</p><p>理论基础是：以空间换时间。即多维分析可能用到的度量进行预计算，将计算好的结果保存成Cube并存储到HBase中，供查询时直接访问。</p><p>大致流程：将数据源(比如Hive)中的数据按照指定的维度和指标，由计算引擎Mapreduce离线计算出所有可能的查询结果(即Cube)存储到HBase中。HBase中每行记录的Rowkey由各维度的值拼接而成，度量会保存在column family中。为了减少存储代价，这里会对维度和度量进行编码。查询阶段，利用HBase列存储的特性就可以保证Kylin有良好的快速响应和高并发。</p><h2 id="22-datax-源码有改造过吗"><a href="#22-datax-源码有改造过吗" class="headerlink" title="22.datax 源码有改造过吗"></a>22.datax 源码有改造过吗</h2><p>没有</p><h2 id="23-你们数仓的APP-层是怎么对外提供服务的？"><a href="#23-你们数仓的APP-层是怎么对外提供服务的？" class="headerlink" title="23.你们数仓的APP 层是怎么对外提供服务的？"></a>23.你们数仓的APP 层是怎么对外提供服务的？</h2><p>1.直接存入mysql业务库，业务方直接读取</p><p>2.数据存入mysql，以接口的形式提供数据</p><p>3.数据存入kylin，需求方通过jdbc读取数据</p><h2 id="24-数据接入进来，你们是怎样规划的，有考虑数据的膨胀问题吗"><a href="#24-数据接入进来，你们是怎样规划的，有考虑数据的膨胀问题吗" class="headerlink" title="24.数据接入进来，你们是怎样规划的，有考虑数据的膨胀问题吗"></a>24.数据接入进来，你们是怎样规划的，有考虑数据的膨胀问题吗</h2><h2 id="25-简述拉链表，流水表以及快照表的含义和特点"><a href="#25-简述拉链表，流水表以及快照表的含义和特点" class="headerlink" title="25.简述拉链表，流水表以及快照表的含义和特点"></a>25.简述拉链表，流水表以及快照表的含义和特点</h2><p>拉链表：</p><p>（1）记录一个事物从开始，一直到当前状态的所有变化的信息；<br>（2）拉链表每次上报的都是历史记录的最终状态，是记录在当前时刻的历史总量；<br>（3）当前记录存的是当前时间之前的所有历史记录的最后变化量（总量）；<br>（4）封链时间可以是2999，3000，9999等等比较大的年份；拉链表到期数据要报0；</p><p>流水表：对于表的每一个修改都会记录，可以用于反映实际记录的变更<br>　区别于拉链表：　<br>  拉链表通常是对账户信息的历史变动进行处理保留的结果，流水表是每天的交易形成的历史；<br>  流水表用于统计业务相关情况，拉链表用于统计账户及客户的情况</p><p> 快照表：<br> 按天分区，每一天的数据都是截止到那一天mysql的全量数据</p><h2 id="26-全量表-df-增量表-di-追加表-da-，拉链表-dz-的区别及使用场景"><a href="#26-全量表-df-增量表-di-追加表-da-，拉链表-dz-的区别及使用场景" class="headerlink" title="26.全量表(df),增量表(di),追加表(da)，拉链表(dz)的区别及使用场景"></a>26.全量表(df),增量表(di),追加表(da)，拉链表(dz)的区别及使用场景</h2><p>全量表</p><ol><li>全量表，有无变化，都要报</li><li>每次上报的数据都是所有的数据（变化的 + 没有变化的）</li></ol><p>增量表：新增数据，增量数据是上次导出之后的新数据。</p><ul><li>记录每次增加的量，而不是总量；</li><li>增量表，只报变化量，无变化不用报</li><li>每天一个分区</li><li>业务库表中需要有主键及创建时间，修改时间</li></ul><h2 id="27-你们公司的数仓分层，每一层是怎么处理数据的"><a href="#27-你们公司的数仓分层，每一层是怎么处理数据的" class="headerlink" title="27.你们公司的数仓分层，每一层是怎么处理数据的"></a>27.你们公司的数仓分层，每一层是怎么处理数据的</h2><p>ODS层</p><ul><li>存放未经过处理的原始数据至数据仓库系统，结构上与源系统保持一致，是数据仓库的数据准备区</li></ul><p>数据公共层CDM：</p><ul><li>公共维度层（DIM）：基于维度建模理念思想，建立整个企业的一致性维度。降低数 据计算口径和算法不统一风险。</li><li>公共汇总粒度事实层（DWS）：以分析的主题对象作为建模驱动，基于上层的应用和产品的指标需求，构建公共粒度的汇总指标事实表，以宽表化手段物理化模型。</li><li>明细粒度事实层（DWD）：以业务过程作为建模驱动，基于每个具体的业务过程特点，构建最细粒度的明细层事实表。</li></ul><p>ADS层</p><p> 数据应用层ADS（Application Data Service）：存放数据产品个性化的统计指标数据。根据CDM与ODS层加工生成。 </p><h2 id="28-什么是事实表，什么是维表"><a href="#28-什么是事实表，什么是维表" class="headerlink" title="28.什么是事实表，什么是维表"></a>28.什么是事实表，什么是维表</h2><p>​    事实表（Fact Table）是指存储有事实记录的表，如系统日志、销售记录等；事实表的记录在不断地动态增长，所以它的体积通常远大于其他表。<br>​    维度表（Dimension Table）或维表，有时也称查找表（Lookup Table），是与事实表相对应的一种表；它保存了维度的属性值，可以跟事实表做关联；相当于将事实表上经常重复出现的属性抽取、规范出来用一张表进行管理。常见的维度表有：日期表（存储与日期对应的周、月、季度等的属性）、地点表（包含国家、省／州、城市等属性）等。</p><h2 id="29-星型模型和雪花模型"><a href="#29-星型模型和雪花模型" class="headerlink" title="29.星型模型和雪花模型"></a>29.星型模型和雪花模型</h2><ul><li>星形模型中有一张事实表，以及零个或多个维度表，事实表与维度表通过主键外键相关联，维度表之间没有关联，当所有维表都直接连接到“ 事实表”上时，整个图解就像星星一样，故将该模型称为星型模型。</li><li>当有一个或多个维表没有直接连接到事实表上，而是通过其他维表连接到事实表上时，其图解就像多个雪花连接在一起，故称雪花模型。</li></ul><h2 id="30-数据建模一般有哪几种方式，你们公司是用哪种方式进行数据建模的"><a href="#30-数据建模一般有哪几种方式，你们公司是用哪种方式进行数据建模的" class="headerlink" title="30.数据建模一般有哪几种方式，你们公司是用哪种方式进行数据建模的"></a>30.数据建模一般有哪几种方式，你们公司是用哪种方式进行数据建模的</h2><p>ER模型</p><p>维度模型</p><p>Data Vault模型</p><p>Anchor模型</p><p>我司用的是维度建模</p><h2 id="31-有没有实际工作中碰到的sql调优的例子，举例说明"><a href="#31-有没有实际工作中碰到的sql调优的例子，举例说明" class="headerlink" title="31.有没有实际工作中碰到的sql调优的例子，举例说明"></a>31.有没有实际工作中碰到的sql调优的例子，举例说明</h2><p>见博客hive调优</p><h2 id="32-你觉得数据仓库应该如何搭建，数据规范和标准如何落地"><a href="#32-你觉得数据仓库应该如何搭建，数据规范和标准如何落地" class="headerlink" title="32.你觉得数据仓库应该如何搭建，数据规范和标准如何落地"></a>32.你觉得数据仓库应该如何搭建，数据规范和标准如何落地</h2><h2 id="33-如何保证你们公司的数据质量"><a href="#33-如何保证你们公司的数据质量" class="headerlink" title="33.如何保证你们公司的数据质量"></a>33.如何保证你们公司的数据质量</h2><p><strong>如何提升数据质量</strong></p><p>1.事前定义数据的监控规则</p><ul><li>提炼规则：梳理对应指标、确定对象（多表、单表、字段）、通过影响程度确定资产等级、质量规则制定</li></ul><p>2.事中监控和控制数据生产过程</p><ul><li>质量监控和工作流无缝对接</li><li>支持定时调度</li><li>强弱规则控制ETL流程</li><li>对脏数据进行清洗</li></ul><p>3.事后分析和问题跟踪</p><ul><li>邮件短信报警并及时跟踪处理</li><li>稽核报告查询</li><li>数据质量报告的概览、历史趋势、异常查询、数据质量表覆盖率</li><li>异常评估、严重程度、影响范围、问题分类</li></ul><h2 id="34-对元数据的理解，元数据管理的意义及应用场景有哪些"><a href="#34-对元数据的理解，元数据管理的意义及应用场景有哪些" class="headerlink" title="34.对元数据的理解，元数据管理的意义及应用场景有哪些"></a>34.对元数据的理解，元数据管理的意义及应用场景有哪些</h2><ul><li>元数据主要记录数据仓库中模型的定义、各层级间的映射关系、监控数据仓库的数据状态及 ETL 的任务运行状态。在数据仓库系统中，元数据可以帮助数据仓库管理员和开发人员非常方便地找到他们所关心的数据，用于指导其进行数据管理和开发工作，可以极大的提升工作的效率。</li><li>元数据有重要的应用价值，是数据管理、数据内容、数据应用的基础，在数据管理方面为集团数据提供在计算、存储、成本、质量、安全、模型等治理领域上的数据支持。例如在计算上可以利用元数据查找超长运行节点，对这些节点进行专项治理，保障基线产出时间。在数据内容方面为集团数据进行数据域、数据主题、业务属性等的提取和分析提供数据素材。例如可以利用元数据构建知识图谱，给数据打标签，清楚地知道现在有哪些数据。在数据应用方面打通产品及应用链路，保障产品数据准确、及时产出。例如打通DP和应用数据，明确数据产等级，更有效地保障产品数据。</li><li>数据的真正价值在于数据驱动决策，通过数据指导运营。通过数据驱动的方法，我们能够判断趋势 ，从而展开有效行动，帮助自己发现问题，推动创新或解决方案的产生。这就是数据化运营。同样，对于元数据，可以用于指导数据相关人员进行日常工作，实现数据化“运营”。 比如对于数据使用者，可以通过元数据让其快速找到所需要的数据；对于ETL 工程师，可以通过元数据指导其进行模型设计、任务优化和任务下线等各种日常ETL 工作；对于运维工程师，可以通过元数据指导其进行整个集群的存储、计算和系统优化等运维工作。</li></ul><h2 id="35-如何判别模型的好坏，模型设计的原则有哪些"><a href="#35-如何判别模型的好坏，模型设计的原则有哪些" class="headerlink" title="35.如何判别模型的好坏，模型设计的原则有哪些"></a>35.如何判别模型的好坏，模型设计的原则有哪些</h2><p>基本原则：</p><ul><li><p>高内聚和低耦合</p><p>一个逻辑或者物理模型由哪些记录和字段组成，应该遵循最基本的软件设计方法论的高内聚和低耦合原则，主要从数据业务特性和访问特性两个角度来考虑：将业务相近或者相关，粒度相同的数据设计为一个逻辑或者物理模型，将高概率同时访问的数据放一起，将低概率同时访问的数据分开存储</p></li><li><p>核心模型与扩展模型分离<br>建立核心模型与扩展模型体系，核心模型包括的字段支持常用的核心业务，扩展模型包括的字段支持个性化或少量应用的需要，不能让扩展模型的字段过度侵入核心模型，以免破坏核心模型架构简洁性与可维护性</p></li><li><p>公共处理逻辑下沉及单一<br>越是底层共用的处理逻辑越应该放在数据调度依赖的底层进行封装与实现，不要让共用的处理逻辑暴露给应用层实现，不要让公共逻辑多处同时存在</p></li><li><p>成本与性能平衡<br>适当的数据冗余可换取查询和刷新性能，不宜过度数据冗余和数据复制</p></li><li><p>数据可回滚<br>处理逻辑不变，在不同时间多次运行数据结果确定不变</p></li><li><p>一致性<br>具有相同含义的字段在不同的表中的命名必须相同，必须使用规范定义中的名称</p></li><li><p>命名清晰,可理解<br>表命名需清晰，一致，表名需易于消费者理解和使用</p></li></ul><h2 id="36-对于数据中台的理解，和数据仓库，数据湖的区别"><a href="#36-对于数据中台的理解，和数据仓库，数据湖的区别" class="headerlink" title="36.对于数据中台的理解，和数据仓库，数据湖的区别"></a>36.对于数据中台的理解，和数据仓库，数据湖的区别</h2><p><strong>数据仓库(Data Warehouse)</strong>是一个面向主题的（Subject Oriented）、集成的（Integrated）、相对稳定的（Non-Volatile）、反映历史变化的（Time Variant）数据集合，用于支持管理决策和信息的全局共享。</p><p><strong>数据湖（Data Lake）是一个存储企业的各种各样原始数据的大型仓库，其中的数据可供存取、处理、分析及传输。</strong></p><p><strong>数据中台</strong>是一个承接技术，引领业务，构建规范定义的、全域可连接萃取的、智慧的数据处理平台，建设目标是为了高效满足前台数据分析和应用的需求。数据中台距离业务更近，能更快速的相应业务和应用开发的需求，可追溯，更精准。</p><p>数据湖、数据仓库更多地是面向不同对象的不同形态的数据资产，而数据中台更多强调的是服务于前台，实现逻辑、标签、算法、模型的复用沉淀。</p><p>数据中台像一个“数据工厂”，涵盖了数据湖、数据仓库等存储组件，随着数据中台的发展，未来很有可能数据湖和数据仓库的概念会被弱化。</p><h2 id="37-对于数据仓库的理解，数据仓库主要为解决什么问题"><a href="#37-对于数据仓库的理解，数据仓库主要为解决什么问题" class="headerlink" title="37.对于数据仓库的理解，数据仓库主要为解决什么问题"></a>37.对于数据仓库的理解，数据仓库主要为解决什么问题</h2><p><strong>（1）数据仓库能够为业务部门提供准确、及时的的报表。</strong></p><p><strong>（2）数据仓库可以赋予管理人员更强大的分析能力。</strong></p><p><strong>（3）数据仓库是进行数据挖掘、知识发现的基础。</strong></p><h2 id="38-数据仓库模型的理解，数据仓库分层设计的好处是什么"><a href="#38-数据仓库模型的理解，数据仓库分层设计的好处是什么" class="headerlink" title="38.数据仓库模型的理解，数据仓库分层设计的好处是什么"></a>38.数据仓库模型的理解，数据仓库分层设计的好处是什么</h2><ul><li>清晰数据结构：每一个数据分层都有它的作用域，这样我们在使用表的时候能更方便地定位和理解。</li><li>数据血缘追踪：简单来讲可以这样理解，我们最终给业务呈现的是一张能直接使用的张业务表，但是它的来源有很多，如果有一张来源表出问题了，我们希望能够快速准确地定位到问题，并清楚它的危害范围。</li><li>减少重复开发：规范数据分层，开发一些通用的中间层数据，能够减少极大的重复计算。</li><li>把复杂问题简单化：将一个复杂的任务分解成多个步骤来完成，每一层只处理单一的步骤，比较简单和容易理解。而且便于维护数据的准确性，当数据出现问题之后，可以不用修复所有的数据，只需要从有问题的步骤开始修复。</li></ul><h2 id="39-数仓主题划分的标准和依据"><a href="#39-数仓主题划分的标准和依据" class="headerlink" title="39.数仓主题划分的标准和依据"></a>39.数仓主题划分的标准和依据</h2><p><strong>关于主题：</strong></p><p>数据仓库中的数据是面向主题组织的，主题是在较高层次上将企业信息系统中的数据进行综合、归类和分析利用的一个抽象概念，每一个主题基本对应一个宏观的分析领域。如财务分析就是一个分析领域，因此这个数据仓库应用的主题就为“财务分析”。</p><p><strong>关于主题域：</strong></p><p>主题域通常是联系较为紧密的数据主题的集合。可以根据业务的关注点，将这些数据主题划分到不同的主题域(也说是对某个主题进行分析后确定的主题的边界。)</p><p><strong>关于主题域的划分：</strong></p><p>主题域的确定必须由最终用户和数据仓库的设计人员共同完成的， 而在划分主题域时，大家的切入点不同可能会造成一些争论、重构等的现象，考虑的点可能会是下方的某些方面：</p><ol><li>按照业务或业务过程划分：比如一个靠销售广告位置的门户网站主题域可能会有广告域，客户域等，而广告域可能就会有广告的库存，销售分析、内部投放分析等主题；</li><li>根据需求方划分：比如需求方为财务部，就可以设定对应的财务主题域，而财务主题域里面可能就会有员工工资分析，投资回报比分析等主题；</li><li>按照功能或应用划分：比如微信中的朋友圈数据域、群聊数据域等，而朋友圈数据域可能就会有用户动态信息主题、广告主题等；</li><li>按照部门划分：比如可能会有运营域、技术域等，运营域中可能会有工资支出分析、活动宣传效果分析等主题；</li></ol><h2 id="40-缓慢变化维如何处理，几种方式"><a href="#40-缓慢变化维如何处理，几种方式" class="headerlink" title="40.缓慢变化维如何处理，几种方式"></a>40.缓慢变化维如何处理，几种方式</h2><p>缓慢变化维：</p><p>数据仓库的重要特点之一是反映历史变化，所以如何处理维度的变化是维度设计的重要工作之一。缓慢变化维的提出是因为在现实世界中，维度的属性并不是静态的，它会随着时间的流逝发生缓慢的变化，与数据增长较为快速的事实表相比，维度变化相对缓慢。</p><p>在一些情况下，保留历史数据没有什么分析价值，而在另一些情况下,保留历史数据是非常重要的，在kimball理论中，有三种处理缓慢变化维的方式</p><p><strong>1.重写纬度值</strong></p><p>采用此种方式，不保留历史数据，始终取最新数据</p><p><strong>2.插入新的维度行</strong></p><p>插人新的维度行。采用此种方式，保留历史数据，</p><p>维度值变化前的事实和过去的维度值关联，维度值变化后的事实和当前的维度值关联</p><p><strong>3.添加维度列</strong></p><p>采用第二种处理方式不能将变化前后记录的事实归一为变化前的维度或者归一为变化后的维度。比如根据业务需求，需要将4月份的交易金额全部统计到类目2上，采用第二种处理方式无法实现。针对此问题，采用第三种处理方式，保留历史数据，可以使用任何一个属性列</p>]]></content>
      
      
      <categories>
          
          <category> bigdata </category>
          
      </categories>
      
      
        <tags>
            
            <tag> warehouse </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hbase笔记</title>
      <link href="/2020/07/11/Hbase%E7%AC%94%E8%AE%B0/"/>
      <url>/2020/07/11/Hbase%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<h1 id="Hbase笔记"><a href="#Hbase笔记" class="headerlink" title="Hbase笔记"></a>Hbase笔记</h1><h2 id="1-Hbase调优"><a href="#1-Hbase调优" class="headerlink" title="1.Hbase调优"></a>1.Hbase调优</h2><ul><li>高可用<br>在HBase中Hmaster负责监控RegionServer的生命周期，均衡RegionServer的负载，如果Hmaster挂掉了，那么整个HBase集群将陷入不健康的状态，并且此时的工作状态并不会维持太久。所以HBase支持对Hmaster的高可用配置。</li><li>预分区<br>每一个region维护着startRow与endRowKey，如果加入的数据符合某个region维护的rowKey范围，则该数据交给这个region维护。那么依照这个原则，我们可以将数据所要投放的分区提前大致的规划好，以提高HBase性能。</li><li>优化RowKey设计<br>一条数据的唯一标识就是rowkey，那么这条数据存储于哪个分区，取决于rowkey处于哪个一个预分区的区间内，设计rowkey的主要目的，就是让数据均匀的分布于所有的region中，在一定程度上防止数据倾斜</li><li>内存优化<br>HBase操作过程中需要大量的内存开销，毕竟Table是可以缓存在内存中的，一般会分配整个可用内存的70%给HBase的Java堆。但是不建议分配非常大的堆内存，因为GC过程持续太久会导致RegionServer处于长期不可用状态，一般16~48G内存就可以了，如果因为框架占用内存过高导致系统内存不足，框架一样会被系统服务拖死。</li></ul><h2 id="2-hbase的rowkey怎么创建好？列族怎么创建比较好？"><a href="#2-hbase的rowkey怎么创建好？列族怎么创建比较好？" class="headerlink" title="2.hbase的rowkey怎么创建好？列族怎么创建比较好？"></a>2.hbase的rowkey怎么创建好？列族怎么创建比较好？</h2><p>hbase存储时，数据按照Row key的字典序(byte order)排序存储。设计key时，要充分排序存储这个特性，将经常一起读取的行存储放到一起。(位置相关性)</p><p>一个列族在数据底层是一个文件，所以将经常一起查询的列放到一个列族中，列族尽量少，减少文件的寻址时间。</p><p>设计原则</p><p>1）rowkey 长度原则</p><p>2）rowkey 散列原则</p><p>3）rowkey 唯一原则</p><p>如何设计</p><p>1）生成随机数、hash、散列值</p><p>2）字符串反转</p><p>3)  字符串拼接</p><h2 id="3-hbase过滤器实现用途"><a href="#3-hbase过滤器实现用途" class="headerlink" title="3.hbase过滤器实现用途"></a>3.hbase过滤器实现用途</h2><p>增强hbase查询数据的功能</p><p>减少服务端返回给客户端的数据量</p><h2 id="4-HBase宕机如何处理"><a href="#4-HBase宕机如何处理" class="headerlink" title="4.HBase宕机如何处理"></a>4.HBase宕机如何处理</h2><p>答：宕机分为HMaster宕机和HRegisoner宕机，如果是HRegisoner宕机，HMaster会将其所管理的region重新分布到其他活动的RegionServer上，由于数据和日志都持久在HDFS中，该操作不会导致数据丢失。所以数据的一致性和安全性是有保障的。</p><p>如果是HMaster宕机，HMaster没有单点问题，HBase中可以启动多个HMaster，通过Zookeeper的Master Election机制保证总有一个Master运行。即ZooKeeper会保证总会有一个HMaster在对外提供服务。</p><h2 id="5-hive跟hbase的区别是？"><a href="#5-hive跟hbase的区别是？" class="headerlink" title="5.hive跟hbase的区别是？"></a>5.hive跟hbase的区别是？</h2><p>共同点：<br>1.hbase与hive都是架构在hadoop之上的。都是用hadoop作为底层存储</p><p>区别：<br>2.Hive是建立在Hadoop之上为了减少MapReduce jobs编写工作的批处理系统，HBase是为了支持弥补Hadoop对实时操作的缺陷的项目 。<br>3.想象你在操作RMDB数据库，如果是全表扫描，就用Hive+Hadoop,如果是索引访问，就用HBase+Hadoop 。<br>4.Hive query就是MapReduce jobs可以从5分钟到数小时不止，HBase是非常高效的，肯定比Hive高效的多。<br>5.Hive本身不存储和计算数据，它完全依赖于HDFS和MapReduce，Hive中的表纯逻辑。<br>6.hive借用hadoop的MapReduce来完成一些hive中的命令的执行<br>7.hbase是物理表，不是逻辑表，提供一个超大的内存hash表，搜索引擎通过它来存储索引，方便查询操作。<br>8.hbase是列存储。<br>9.hdfs作为底层存储，hdfs是存放文件的系统，而Hbase负责组织文件。<br>10.hive需要用到hdfs存储文件，需要用到MapReduce计算框架。</p><h2 id="6-hbase写流程"><a href="#6-hbase写流程" class="headerlink" title="6.hbase写流程"></a>6.hbase写流程</h2><p><img src="https://lixiangbetter.github.io/2020/07/11/Hbase%E7%AC%94%E8%AE%B0/20190622100115335.png" alt></p><ol><li>客户端要连接zookeeper, 从zk的/hbase节点找到hbase:meta表所在的regionserver（host:port）;</li><li>regionserver扫描hbase:meta中的每个region的起始行健，对比r000001这条数据在那个region的范围内；</li><li>从对应的 info:server key中存储了region是有哪个regionserver(host:port)在负责的；</li><li>客户端直接请求对应的regionserver；</li><li>regionserver接收到客户端发来的请求之后，就会将数据写入到region中</li></ol><h2 id="7-hbase读流程"><a href="#7-hbase读流程" class="headerlink" title="7.hbase读流程"></a>7.hbase读流程</h2><p><img src="https://lixiangbetter.github.io/2020/07/11/Hbase%E7%AC%94%E8%AE%B0/20200430133829262.png" alt></p><ol><li>首先Client连接zookeeper, 找到hbase:meta表所在的regionserver;</li><li>请求对应的regionserver，扫描hbase:meta表，根据namespace、表名和rowkey在meta表中找到r00001所在的region是由那个regionserver负责的；</li><li>找到这个region对应的regionserver</li><li>regionserver收到了请求之后，扫描对应的region返回数据到Client</li></ol><p>(先从MemStore找数据，如果没有，再到BlockCache里面读；BlockCache还没有，再到StoreFile上读(为了读取的效率)；</p><p>如果是从StoreFile里面读取的数据，不是直接返回给客户端，而是先写入BlockCache，再返回给客户端。)</p><h2 id="8-hbase数据flush过程"><a href="#8-hbase数据flush过程" class="headerlink" title="8.hbase数据flush过程"></a>8.hbase数据flush过程</h2><ol><li>当MemStore数据达到阈值（默认是128M，老版本是64M），将数据刷到硬盘，将内存中的数据删除，同时删除HLog中的历史数据；</li><li>并将数据存储到HDFS中；</li><li>在HLog中做标记点。</li></ol><h2 id="9-数据合并过程"><a href="#9-数据合并过程" class="headerlink" title="9.数据合并过程"></a>9.数据合并过程</h2><ol><li>当数据块达到4块，hmaster将数据块加载到本地，进行合并</li><li>当合并的数据超过256M，进行拆分，将拆分后的region分配给不同的hregionserver管理</li><li>当hregionser宕机后，将hregionserver上的hlog拆分，然后分配给不同的hregionserver加载，修改.META.</li><li>注意：hlog会同步到hdfs</li></ol><h2 id="10-Hmaster和HRgionserver职责"><a href="#10-Hmaster和HRgionserver职责" class="headerlink" title="10.Hmaster和HRgionserver职责"></a>10.Hmaster和HRgionserver职责</h2><p>Hmaster</p><p>1、管理用户对Table的增、删、改、查操作；</p><p>2、记录region在哪台Hregion server上</p><p>3、在Region Split后，负责新Region的分配；</p><p>4、新机器加入时，管理HRegion Server的负载均衡，调整Region分布</p><p>5、在HRegion Server宕机后，负责失效HRegion Server 上的Regions迁移。</p><p>HRegion server</p><ol><li>HRegion Server主要负责响应用户I/O请求，向HDFS文件系统中读写数据，是HBASE中最核心的模块。</li><li>HRegion Server管理了很多table的分区，也就是region。</li></ol><h2 id="11-HBase列族和region的关系？"><a href="#11-HBase列族和region的关系？" class="headerlink" title="11.HBase列族和region的关系？"></a>11.HBase列族和region的关系？</h2><p>HBase有多个RegionServer，每个RegionServer里有多个Region，一个Region中存放着若干行的行键以及所对应的数据，一个列族是一个文件夹，如果经常要搜索整个一条数据，列族越少越好，如果只有一部分的数据需要经常被搜索，那么将经常搜索的建立一个列族，其他不常搜索的建立列族检索较快。</p><h2 id="12-请简述Hbase的物理模型是什么"><a href="#12-请简述Hbase的物理模型是什么" class="headerlink" title="12.请简述Hbase的物理模型是什么"></a>12.请简述Hbase的物理模型是什么</h2><p>（1）Table在行的方向上分割为多个Region。<br>（2）Table中的所有行都按照row key的字典序排列，根据rowkey存储在不同的Region上。<br>（3）Region是按大小分割的，每个表开始只有一个region，随着数据增多，region不断增大，当增大到一个阈值的时候，region就会等分成两个新的region，之后会有越来越多的region。<br>（4）Region是HBase中分布式存储和负载均衡的最小单元。不同Region分布到不同RegionServer上。移动的时候是移动一个Region，进行不同RegionServer之间的负载均衡。<br>（5）Region虽然是分布式存储的最小单元，但并不是存储的最小单元，存储的最小单元是Cell。Region由一个或者多个Store组成，每个store保存一个columns family列簇。每个store又由一个memStore和0至多个StoreFile组成。memStore存储在内存中，StoreFile存储在HDFS上。memStore是内存中划分的一个区间，StoreFile是底层存储在HDFS上的文件。<br>（6）每个column family存储在HDFS上的一个单独文件中。Key和Version number在每个column family中均有一份。空值不会被保存。</p><h2 id="13-请问如果使用Hbase做即席查询，如何设计二级索引"><a href="#13-请问如果使用Hbase做即席查询，如何设计二级索引" class="headerlink" title="13.请问如果使用Hbase做即席查询，如何设计二级索引"></a>13.请问如果使用Hbase做即席查询，如何设计二级索引</h2><p> 1）索引与主数据存放在同一张表的不同Column Family 中</p><ul><li><p>索引与主数据划分到同一个Region 上，从索引抓取目标主数据减少RPC 次数，减少网络通信压力，把性能损失降低到最小</p></li><li><p>索引与主数据分配在不同的Column Family 中，实现了索引与主数据的物理分离</p><p>2）索引区的Column Family 不包含任何 Qualifier，是一个典型的“Dummy Column Family”</p></li><li><p>减少读写压力，快速定位</p><p>3）RowKey 格式：RegionStartKey-索引名-索引键-索引值</p></li><li><p>RegionStartKey：索引RowKey 的前缀固定为当前Region的StartKey，一方面，这个值处在Region 的RowKey 区间之内，它确保了索引必定跟随其主数据被划分到同一个Region 里；另一方面，这个值是RowKey 区间内的最小值，这保证了在同一Region 里所有索引会集中排在主数据之前，实现了索引与主数据的逻辑分离。</p></li><li><p>索引键：由目标记录各对应字段的值组合而成</p></li><li><p>索引值：记录对应的RowKey</p></li></ul><h2 id="14-如何避免读、写HBaes时访问热点问题？"><a href="#14-如何避免读、写HBaes时访问热点问题？" class="headerlink" title="14.如何避免读、写HBaes时访问热点问题？"></a>14.如何避免读、写HBaes时访问热点问题？</h2><p><strong>rowkey的散列或预分区</strong></p><p>预分区一开始就预建好了一部分region，这些region都维护着自己的start-end keys，我们将rowkey做一些处理，比如RowKey%i，写数据能均衡的命中这些预建的region，就能解决上面的那些缺点，大大提供性能。</p><p>而将rowkey散列化就是避免rowkey自增，这样也能解决上面所说的缺点。</p><ol><li>rowkey前面加随机数</li><li>哈希</li><li>反转</li><li>使用反转的时间戳作为rowkey的一部分</li></ol><h2 id="15-布隆过滤器在HBASE中的应用"><a href="#15-布隆过滤器在HBASE中的应用" class="headerlink" title="15.布隆过滤器在HBASE中的应用"></a>15.布隆过滤器在HBASE中的应用</h2><p>布隆过滤器的作用是，用户可以立即判断一个文件是否包含特定的行键，从而帮我们过滤掉一些不需要扫描的文件。</p><h2 id="16-Hbase是用来干嘛的-什么样的数据会放到hbase"><a href="#16-Hbase是用来干嘛的-什么样的数据会放到hbase" class="headerlink" title="16.Hbase是用来干嘛的?什么样的数据会放到hbase"></a>16.Hbase是用来干嘛的?什么样的数据会放到hbase</h2><p>当我们对于数据结构字段不够确定或杂乱无章很难按一个概念去进行抽取的数据适合用使用什么数据库？</p><p>最适合使用Hbase存储的数据是非常稀疏的数据（非结构化或者半结构化的数据）。Hbase之所以擅长存储这类数据，是因为Hbase是column-oriented列导向的存储机制，而我们熟知的RDBMS都是row- oriented行导向的存储机制（郁闷的是我看过N本关于关系数据库的介绍从来没有提到过row- oriented行导向存储这个概念）。在列导向的存储机制下对于Null值得存储是不占用任何空间的。比如，如果某个表 UserTable有10列，但在存储时只有一列有数据，那么其他空值的9列是不占用存储空间的（普通的数据库MySql是如何占用存储空间的呢？）。</p><p>Hbase适合存储非结构化的稀疏数据的另一原因是他对列集合 column families 处理机制。 </p><p>现在Hbase为未来的DBA也带来了这个激动人心的特性，你只需要告诉你的数据存储到Hbase的那个column families 就可以了，不需要指定它的具体类型：char,varchar,int,tinyint,text等等。</p>]]></content>
      
      
      <categories>
          
          <category> bigdata </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hbase </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Kafka笔记</title>
      <link href="/2020/07/10/Kafka%E7%AC%94%E8%AE%B0/"/>
      <url>/2020/07/10/Kafka%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<h1 id="Kafka笔记"><a href="#Kafka笔记" class="headerlink" title="Kafka笔记"></a>Kafka笔记</h1><h2 id="1-Kafka名词解释和工作方式"><a href="#1-Kafka名词解释和工作方式" class="headerlink" title="1.Kafka名词解释和工作方式"></a>1.Kafka名词解释和工作方式</h2><ol><li>Producer ：消息生产者，就是向kafka broker发消息的客户端。</li><li>Consumer ：消息消费者，向kafka broker取消息的客户端</li><li>Topic ：咋们可以理解为一个队列。</li><li>Consumer Group （CG）：这是kafka用来实现一个topic消息的广播（发给所有的consumer）和单播（发给任意一个consumer）的手段。一个topic可以有多个CG。topic的消息会复制（不是真的复制，是概念上的）到所有的CG，但每个partion只会把消息发给该CG中的一个consumer。如果需要实现广播，只要每个consumer有一个独立的CG就可以了。要实现单播只要所有的consumer在同一个CG。用CG还可以将consumer进行自由的分组而不需要多次发送消息到不同的topic。</li><li>Broker ：一台kafka服务器就是一个broker。一个集群由多个broker组成。一个broker可以容纳多个topic</li><li>Partition：为了实现扩展性，一个非常大的topic可以分布到多个broker（即服务器）上，一个topic可以分为多个partition，每个partition是一个有序的队列。partition中的每条消息都会被分配一个有序的id（offset）。kafka只保证按一个partition中的顺序将消息发给consumer，不保证一个topic的整体（多个partition间）的顺序。</li><li>Offset：kafka的存储文件都是按照offset.kafka来命名，用offset做名字的好处是方便查找。例如你想找位于2049的位置，只要找到2048.kafka的文件即可。当然the first offset就是00000000000.kafka</li></ol><h2 id="2-Consumer与topic关系"><a href="#2-Consumer与topic关系" class="headerlink" title="2.Consumer与topic关系"></a>2.Consumer与topic关系</h2><p>本质上kafka只支持Topic；</p><p>每个group中可以有多个consumer，每个consumer属于一个consumer group；</p><p>通常情况下，一个group中会包含多个consumer，这样不仅可以提高topic中消息的并发消费能力，而且还能提高”故障容错”性，如果group中的某个consumer失效那么其消费的partitions将会有其他consumer自动接管。</p><p>对于Topic中的一条特定的消息，只会被订阅此Topic的每个group中的其中一个consumer消费，此消息不会发送给一个group的多个consumer；</p><p>那么一个group中所有的consumer将会交错的消费整个Topic，每个group中consumer消息消费互相独立，我们可以认为一个group是一个”订阅”者。</p><p>在kafka中,一个partition中的消息只会被group中的一个consumer消费(同一时刻)；</p><p>一个Topic中的每个partions，只会被一个”订阅者”中的一个consumer消费，不过一个consumer可以同时消费多个partitions中的消息。</p><p>kafka的设计原理决定,对于一个topic，同一个group中不能有多于partitions个数的consumer同时消费，否则将意味着某些consumer将无法得到消息。</p><p><strong>kafka只能保证一个partition中的消息被某个consumer消费时是顺序的；事实上，从Topic角度来说,当有多个partitions时,消息仍不是全局有序的。</strong></p><h2 id="3-kafka中生产数据的时候，如何保证写入的容错性？"><a href="#3-kafka中生产数据的时候，如何保证写入的容错性？" class="headerlink" title="3.kafka中生产数据的时候，如何保证写入的容错性？"></a>3.kafka中生产数据的时候，如何保证写入的容错性？</h2><p>设置发送数据是否需要服务端的反馈,有三个值0,1,-1</p><p>0: producer不会等待broker发送ack</p><p>1: 当leader接收到消息之后发送ack</p><p>-1: 当所有的follower都同步消息成功后发送ack</p><p>request.required.acks=0</p><h2 id="4-如何保证kafka消费者消费数据是全局有序的"><a href="#4-如何保证kafka消费者消费数据是全局有序的" class="headerlink" title="4.如何保证kafka消费者消费数据是全局有序的"></a>4.如何保证kafka消费者消费数据是全局有序的</h2><p>伪命题</p><p>每个分区内，每条消息都有一个offset，故只能保证分区内有序。</p><p>如果要全局有序的，必须保证生产有序，存储有序，消费有序。</p><p>由于生产可以做集群，存储可以分片，消费可以设置为一个consumerGroup，要保证全局有序，就需要保证每个环节都有序。</p><p>只有一个可能，就是一个生产者，一个partition，一个消费者。这种场景和大数据应用场景相悖。</p><h2 id="5-有两个数据源，一个记录的是广告投放给用户的日志，一个记录用户访问日志，另外还有一个固定的用户基础表记录用户基本信息（比如学历，年龄等等）。现在要分析广告投放对与哪类用户更有效，请采用熟悉的技术描述解决思路。另外如果两个数据源都是实时数据源（比如来自kafka），他们数据在时间上相差5分钟，需要哪些调整来解决实时分析问题？"><a href="#5-有两个数据源，一个记录的是广告投放给用户的日志，一个记录用户访问日志，另外还有一个固定的用户基础表记录用户基本信息（比如学历，年龄等等）。现在要分析广告投放对与哪类用户更有效，请采用熟悉的技术描述解决思路。另外如果两个数据源都是实时数据源（比如来自kafka），他们数据在时间上相差5分钟，需要哪些调整来解决实时分析问题？" class="headerlink" title="5.有两个数据源，一个记录的是广告投放给用户的日志，一个记录用户访问日志，另外还有一个固定的用户基础表记录用户基本信息（比如学历，年龄等等）。现在要分析广告投放对与哪类用户更有效，请采用熟悉的技术描述解决思路。另外如果两个数据源都是实时数据源（比如来自kafka），他们数据在时间上相差5分钟，需要哪些调整来解决实时分析问题？"></a>5.有两个数据源，一个记录的是广告投放给用户的日志，一个记录用户访问日志，另外还有一个固定的用户基础表记录用户基本信息（比如学历，年龄等等）。现在要分析广告投放对与哪类用户更有效，请采用熟悉的技术描述解决思路。另外如果两个数据源都是实时数据源（比如来自kafka），他们数据在时间上相差5分钟，需要哪些调整来解决实时分析问题？</h2><h2 id="6-Kafka和Spark-Streaming如何集成"><a href="#6-Kafka和Spark-Streaming如何集成" class="headerlink" title="6.Kafka和Spark Streaming如何集成?"></a>6.Kafka和Spark Streaming如何集成?</h2><p>两种方式：</p><p>1.Direct</p><p>2.Receiver</p><h2 id="7-列举Kafka的优点，简述Kafka为什么可以做到每秒数十万甚至上百万消息的高效分发？"><a href="#7-列举Kafka的优点，简述Kafka为什么可以做到每秒数十万甚至上百万消息的高效分发？" class="headerlink" title="7.列举Kafka的优点，简述Kafka为什么可以做到每秒数十万甚至上百万消息的高效分发？"></a>7.列举Kafka的优点，简述Kafka为什么可以做到每秒数十万甚至上百万消息的高效分发？</h2><p>1、页缓存技术 + 磁盘顺序写</p><p>2、零拷贝技术</p><h2 id="8-为什么离线分析要用kafka？"><a href="#8-为什么离线分析要用kafka？" class="headerlink" title="8.为什么离线分析要用kafka？"></a>8.为什么离线分析要用kafka？</h2><p>Kafka的作用是解耦，如果直接从日志服务器上采集的话，实时离线都要采集，等于要采集两份数据，而使用了kafka的话，只需要从日志服务器上采集一份数据，然后在kafka中使用不同的两个组读取就行了</p><h2 id="9-Kafka怎么进行监控"><a href="#9-Kafka怎么进行监控" class="headerlink" title="9.Kafka怎么进行监控?"></a>9.Kafka怎么进行监控?</h2><p>Kafka Manager</p><h2 id="10-Kafka与传统的消息队列服务有什么不同"><a href="#10-Kafka与传统的消息队列服务有什么不同" class="headerlink" title="10.Kafka与传统的消息队列服务有什么不同"></a>10.Kafka与传统的消息队列服务有什么不同</h2><ul><li>较长时间持久化</li><li>高性能（7种的有点）</li></ul><h2 id="11-Kafka-api-low-level与high-level有什么区别，使用low-level需要处理哪些细节"><a href="#11-Kafka-api-low-level与high-level有什么区别，使用low-level需要处理哪些细节" class="headerlink" title="11.Kafka api  low-level与high-level有什么区别，使用low-level需要处理哪些细节"></a>11.Kafka api  low-level与high-level有什么区别，使用low-level需要处理哪些细节</h2><p><strong>High Level Consumer API</strong></p><ol><li>High Level Consumer API围绕着Consumer Group这个逻辑概念展开，它屏蔽了每个Topic的每个Partition的Offset管理（自动读取zookeeper中该Consumer group的last offset ）、Broker失败转移以及增减Partition、Consumer时的负载均衡(当Partition和Consumer增减时，Kafka自动进行负载均衡）</li><li>对于多个Partition，多个Consumer，如果consumer比partition多，是浪费，因为kafka的设计是在一个partition上是不允许并发的，所以consumer数不要大于partition数；</li><li>如果consumer比partition少，一个consumer会对应于多个partitions，这里主要合理分配consumer数和partition数，否则会导致partition里面的数据被取的不均匀。最好partiton数目是consumer数目的整数倍，所以partition数目很重要，比如取24，就很容易设定consumer数目</li><li>如果consumer从多个partition读到数据，不保证数据间的顺序性，kafka只保证在一个partition上数据是有序的，但多个partition，根据你读的顺序会有不同</li><li>增减consumer，broker，partition会导致rebalance，所以rebalance后consumer对应的partition会发生变化</li></ol><p><strong>Low Level Consumer API</strong></p><ol><li><p>Low Level Consumer API控制灵活性<br>Low Level Consumer API，作为底层的Consumer API，提供了消费Kafka Message更大的控制，如：<br>Read a message multiple times(重复读取）<br>Consume only a subset of the partitions in a topic in a process（跳读）<br>Manage transactions to make sure a message is processed once and only once（Exactly Once原语）</p></li><li><p>Low Level Consumer API的复杂性<br>软件没有银弹，Low Level Consumer API提供更大灵活控制是以复杂性为代价的：<br>Offset不再透明<br>Broker自动失败转移需要处理<br>增加Consumer、Partition、Broker需要自己做负载均衡</p></li></ol><h2 id="12-Kafka的ISR副本同步队列"><a href="#12-Kafka的ISR副本同步队列" class="headerlink" title="12.Kafka的ISR副本同步队列"></a>12.Kafka的ISR副本同步队列</h2><p>ISR（In-Sync Replicas），副本同步队列。ISR中包括Leader和Follower。如果Leader进程挂掉，会在ISR队列中选择一个服务作为新的Leader。有replica.lag.max.messages（延迟条数）和replica.lag.time.max.ms（延迟时间）两个参数决定一台服务是否可以加入ISR副本队列，在0.10版本移除了replica.lag.max.messages参数，防止服务频繁的进去队列。</p><p>任意一个维度超过阈值都会把Follower剔除出ISR，存入OSR（Outof-Sync Replicas）列表，新加入的Follower也会先存放在OSR中。</p><h2 id="13-Kafka消息数据积压，Kafka消费能力不足怎么处理？"><a href="#13-Kafka消息数据积压，Kafka消费能力不足怎么处理？" class="headerlink" title="13.Kafka消息数据积压，Kafka消费能力不足怎么处理？"></a>13.Kafka消息数据积压，Kafka消费能力不足怎么处理？</h2><ol><li>如果是Kafka消费能力不足，则可以考虑增加Topic的分区数，并且同时提升消费组的消费者数量，消费者数=分区数。（两者缺一不可）</li><li>如果是下游的数据处理不及时：提高每批次拉取的数量。批次拉取数据过少（拉取数据/处理时间&lt;生产速度），使处理的数据小于生产的数据，也会造成数据积压。</li></ol><h2 id="14-Kafka中的ISR、AR又代表什么？"><a href="#14-Kafka中的ISR、AR又代表什么？" class="headerlink" title="14.Kafka中的ISR、AR又代表什么？"></a>14.Kafka中的ISR、AR又代表什么？</h2><p> ISR：in-sync replica set (ISR)，与leader保持同步的follower集合</p><p> AR：分区的所有副本</p><h2 id="15-Kafka中的HW、LEO等分别代表什么？"><a href="#15-Kafka中的HW、LEO等分别代表什么？" class="headerlink" title="15.Kafka中的HW、LEO等分别代表什么？"></a>15.Kafka中的HW、LEO等分别代表什么？</h2><p>LEO：每个副本的最后那条消息的offset</p><p>HW：一个分区中所有副本最小的offset</p><h2 id="16-哪些情景会造成消息漏消费？"><a href="#16-哪些情景会造成消息漏消费？" class="headerlink" title="16.哪些情景会造成消息漏消费？"></a>16.哪些情景会造成消息漏消费？</h2><p>两种情况可能出现重复消费</p><ol><li><p>当ack=-1时，如果在follower同步完成后，broker发送ack之前，leader发生故障，导致没有返回ack给Producer，由于失败重试机制，又会给新选举出来的leader发送数据，造成数据重复。</p></li><li><p>（手动管理offset时，先消费后提交offset）消费者消费后没有commit offset(程序崩溃/强行kill/消费耗时/自动提交偏移情况下unscrible)</p></li></ol><p>三种情况可能出现漏消费</p><ol><li><p>（手动管理offset时，先提交offset后消费）先提交offset，后消费，有可能造成数据的漏消费</p><p>如果先提交offset，后消费，可能会出现数据漏消费问题。比如，要消费0,1,2,我先提交offset ，此时<strong>consumer_offsets的值为4，但等我提交完offset之后，还没有消费之前，消费者挂掉了，这时等消费者重新活过来后，读取的</strong>consumer_offsets值为4，就会从4开始消费，导致消息0,1,2出现漏消费问题。</p></li><li><p>当ack=0时，producer不等待broker的ack，这一操作提供了一个最低的延迟，broker一接收到还没有写入磁盘就已经返回，当broker故障时有可能丢失数据；</p></li><li><p>当ack=1时，producer等待broker的ack，partition的leader落盘成功后返回ack，如果在follower同步成功之前leader故障，而由于已经返回了ack，系统默认新选举的leader已经有了数据，从而不会进行失败重试，那么将会丢失数据</p></li></ol><h2 id="17-当你使用kafka-topics-sh创建了一个topic之后，Kafka背后会执行什么逻辑？"><a href="#17-当你使用kafka-topics-sh创建了一个topic之后，Kafka背后会执行什么逻辑？" class="headerlink" title="17.当你使用kafka-topics.sh创建了一个topic之后，Kafka背后会执行什么逻辑？"></a>17.当你使用kafka-topics.sh创建了一个topic之后，Kafka背后会执行什么逻辑？</h2><ol><li>会在zookeeper中的/brokers/topics节点下创建一个新的topic节点，如：/brokers/topics/first</li><li>触发Controller的监听程序</li><li>kafka Controller 负责topic的创建工作，并更新metadata cache</li></ol><h2 id="18-topic的分区数可不可以增加？如果可以怎么增加？如果不可以，那又是为什么？"><a href="#18-topic的分区数可不可以增加？如果可以怎么增加？如果不可以，那又是为什么？" class="headerlink" title="18.topic的分区数可不可以增加？如果可以怎么增加？如果不可以，那又是为什么？"></a>18.topic的分区数可不可以增加？如果可以怎么增加？如果不可以，那又是为什么？</h2><p>可以增加</p><p>bin/kafka-topics.sh –zookeeper localhost:2181/kafka –alter –topic topic-config –partitions 3</p><h2 id="19-topic的分区数可不可以减少？如果可以怎么减少？如果不可以，那又是为什么？"><a href="#19-topic的分区数可不可以减少？如果可以怎么减少？如果不可以，那又是为什么？" class="headerlink" title="19.topic的分区数可不可以减少？如果可以怎么减少？如果不可以，那又是为什么？"></a>19.topic的分区数可不可以减少？如果可以怎么减少？如果不可以，那又是为什么？</h2><p>不可以减少，被删除的分区数据难以处理。</p><h2 id="20-Kafka有内部的topic吗？如果有是什么？有什么所用？"><a href="#20-Kafka有内部的topic吗？如果有是什么？有什么所用？" class="headerlink" title="20.Kafka有内部的topic吗？如果有是什么？有什么所用？"></a>20.Kafka有内部的topic吗？如果有是什么？有什么所用？</h2><p>__consumer_offsets,保存消费者offset</p><h2 id="21-聊一聊Kafka-Controller的作用？"><a href="#21-聊一聊Kafka-Controller的作用？" class="headerlink" title="21.聊一聊Kafka Controller的作用？"></a>21.聊一聊Kafka Controller的作用？</h2><p>负责管理集群broker的上下线，所有topic的分区副本分配和leader选举等工作。</p><h2 id="22-失效副本是指什么？有那些应对措施？"><a href="#22-失效副本是指什么？有那些应对措施？" class="headerlink" title="22.失效副本是指什么？有那些应对措施？"></a>22.失效副本是指什么？有那些应对措施？</h2><p>不能及时与leader同步，暂时踢出ISR，等其追上leader之后再重新加入</p>]]></content>
      
      
      <categories>
          
          <category> bigdata </category>
          
      </categories>
      
      
        <tags>
            
            <tag> kafka </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark笔记</title>
      <link href="/2020/07/06/Spark%E7%AC%94%E8%AE%B0/"/>
      <url>/2020/07/06/Spark%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<h1 id="Spark笔记"><a href="#Spark笔记" class="headerlink" title="Spark笔记"></a>Spark笔记</h1><h2 id="1-rdd的属性"><a href="#1-rdd的属性" class="headerlink" title="1.rdd的属性"></a>1.rdd的属性</h2><ul><li>一组分片（Partition），即数据集的基本组成单位。对于RDD来说，每个分片都会被一个计算任务处理，并决定并行计算的粒度。用户可以在创建RDD时指定RDD的分片个数，如果没有指定，那么就会采用默认值。默认值就是程序所分配到的CPU Core的数目。</li><li>一个计算每个分区的函数。Spark中RDD的计算是以分片为单位的，每个RDD都会实现compute函数以达到这个目的。compute函数会对迭代器进行复合，不需要保存每次计算的结果。</li><li>RDD之间的依赖关系。RDD的每次转换都会生成一个新的RDD，所以RDD之间就会形成类似于流水线一样的前后依赖关系。在部分分区数据丢失时，Spark可以通过这个依赖关系重新计算丢失的分区数据，而不是对RDD的所有分区进行重新计算。</li><li>一个Partitioner，即RDD的分片函数。当前Spark中实现了两种类型的分片函数，一个是基于哈希的HashPartitioner，另外一个是基于范围的RangePartitioner。只有对于key-value的RDD，才会有Partitioner，非key-value的RDD的Parititioner的值是None。Partitioner函数不但决定了RDD本身的分片数量，也决定了parent RDD Shuffle输出时的分片数量。</li><li>一个列表，存储存取每个Partition的优先位置（preferred location）。对于一个HDFS文件来说，这个列表保存的就是每个Partition所在的块的位置。按照“移动数据不如移动计算”的理念，Spark在进行任务调度的时候，会尽可能地将计算任务分配到其所要处理数据块的存储位置。</li></ul><h2 id="2-算子分为哪几类-RDD支持哪几种类型的操作"><a href="#2-算子分为哪几类-RDD支持哪几种类型的操作" class="headerlink" title="2.算子分为哪几类(RDD支持哪几种类型的操作)"></a>2.算子分为哪几类(RDD支持哪几种类型的操作)</h2><p>转换（Transformation）  现有的RDD通过转换生成一个新的RDD。lazy模式，延迟执行。</p><p>转换函数包括：map，filter，flatMap，groupByKey，reduceByKey，aggregateByKey，union,join, coalesce 等等。</p><p>动作（Action）  在RDD上运行计算，并返回结果给驱动程序(Driver)或写入文件系统。</p><p>动作操作包括：reduce，collect，count，first，take，countByKey以及foreach等等。</p><p>collect  该方法把数据收集到driver端   Array数组类型</p><p>所有的transformation只有遇到action才能被执行。</p><p>当触发执行action之后，数据类型不再是rdd了，数据就会存储到指定文件系统中，或者直接打印结果或者收集起来。</p><h2 id="3-创建rdd的几种方式"><a href="#3-创建rdd的几种方式" class="headerlink" title="3.创建rdd的几种方式"></a>3.创建rdd的几种方式</h2><ol><li><p>集合并行化创建(有数据)</p><p>val arr = Array(1,2,3,4,5)</p><p>val rdd = sc.parallelize(arr)</p><p>val rdd =sc.makeRDD(arr)</p></li><li><p>读取外部文件系统，如hdfs，或者读取本地文件(最常用的方式)(没数据)</p><p>val rdd2 = sc.textFile(“hdfs://hdp-01:9000/words.txt”)</p><p>// 读取本地文件</p><p>val rdd2 = sc.textFile(“file:///root/words.txt”)</p></li><li><p>从父RDD转换成新的子RDD</p><p>调用Transformation类的方法，RDD</p></li></ol><h2 id="4-spark运行流程"><a href="#4-spark运行流程" class="headerlink" title="4.spark运行流程"></a>4.spark运行流程</h2><p><img src="https://lixiangbetter.github.io/2020/07/06/Spark%E7%AC%94%E8%AE%B0/20200409083030286.png" alt></p><p>Worker的功能： 定时和master通信；调度并管理自身的executor</p><p>executor： 由Worker启动的，程序最终在executor中运行，（程序运行的一个容器）</p><p>spark-submit命令执行时，会根据master地址去向 Master发送请求。</p><p>Master接收到Dirver端的任务请求之后，根据任务的请求资源进行调度，（打散的策略），尽可能的把任务资源平均分配，然后向Worker发送指令</p><p>Worker收到Master的指令之后，就根据相应的资源，启动executor（cores,memory）</p><p>executor会向dirver端建立请求，通知driver，任务已经可以运行了</p><p>driver运行任务的时候，会把任务发送到executor中去运行。</p><h2 id="5-Spark中coalesce与repartition的区别"><a href="#5-Spark中coalesce与repartition的区别" class="headerlink" title="5.Spark中coalesce与repartition的区别"></a>5.Spark中coalesce与repartition的区别</h2><p>1）关系：</p><p>两者都是用来改变 RDD 的 partition 数量的，repartition 底层调用的就是 coalesce 方法：coalesce(numPartitions, shuffle = true)</p><p>2）区别：</p><p>repartition 一定会发生 shuffle，coalesce 根据传入的参数来判断是否发生 shuffle</p><p>一般情况下增大 rdd 的 partition 数量使用 repartition，减少 partition 数量时使用coalesce</p><h2 id="6-sortBy-和-sortByKey的区别"><a href="#6-sortBy-和-sortByKey的区别" class="headerlink" title="6.sortBy 和 sortByKey的区别"></a>6.sortBy 和 sortByKey的区别</h2><p>sortBy既可以作用于RDD[K] ，还可以作用于RDD[(k,v)]</p><p>sortByKey  只能作用于 RDD[K,V] 类型上。</p><h2 id="7-map和mapPartitions的区别"><a href="#7-map和mapPartitions的区别" class="headerlink" title="7.map和mapPartitions的区别"></a>7.map和mapPartitions的区别</h2><p><img src="https://lixiangbetter.github.io/2020/07/06/Spark%E7%AC%94%E8%AE%B0/20200409083305358.png" alt></p><h2 id="8-数据存入Redis-优先使用map-mapPartitions-foreach-foreachPartions哪个"><a href="#8-数据存入Redis-优先使用map-mapPartitions-foreach-foreachPartions哪个" class="headerlink" title="8.数据存入Redis  优先使用map mapPartitions  foreach  foreachPartions哪个"></a>8.数据存入Redis  优先使用map mapPartitions  foreach  foreachPartions哪个</h2><p>使用 foreachPartition</p><ul><li><p>1,map mapPartition   是转换类的算子， 有返回值</p></li><li><p>2, 写mysql,redis 的连接</p><p>foreach  * 100万         100万次的连接</p><p>foreachPartions * 200 个分区     200次连接  一个分区中的数据，共用一个连接</p></li></ul><p><strong>foreachParititon 每次迭代一个分区，foreach每次迭代一个元素。</strong></p><p>该方法没有返回值，或者Unit</p><p>主要作用于，没有返回值类型的操作（打印结果，写入到mysql数据库中）</p><p>在写入到redis,mysql的时候，优先使用foreachPartititon</p><h2 id="9-reduceByKey和groupBykey的区别"><a href="#9-reduceByKey和groupBykey的区别" class="headerlink" title="9.reduceByKey和groupBykey的区别"></a>9.reduceByKey和groupBykey的区别</h2><p><img src="https://lixiangbetter.github.io/2020/07/06/Spark%E7%AC%94%E8%AE%B0/20200409083531763.png" alt></p><p>reduceByKey会传一个聚合函数， 相当于  groupByKey + mapValues</p><p>reduceByKey 会有一个分区内聚合，而groupByKey没有  最核心的区别  </p><p>结论： reduceByKey有分区内聚合，更高效，优先选择使用reduceByKey。</p><h2 id="10-cache和checkPoint的比较"><a href="#10-cache和checkPoint的比较" class="headerlink" title="10.cache和checkPoint的比较"></a>10.cache和checkPoint的比较</h2><p>都是做 RDD 持久化的</p><p>1.缓存，是在触发action之后，把数据写入到内存或者磁盘中。不会截断血缘关系</p><p>（设置缓存级别为memory_only： 内存不足，只会部分缓存或者没有缓存，缓存会丢失,memory_and_disk :内存不足，会使用磁盘）</p><p>2.checkpoint 也是在触发action之后，执行任务。单独再启动一个job，负责写入数据到hdfs中。（把rdd中的数据，以二进制文本的方式写入到hdfs中，有几个分区，就有几个二进制文件）</p><p>3.某一个RDD被checkpoint之后，他的父依赖关系会被删除，血缘关系被截断，该RDD转换成了CheckPointRDD，以后再对该rdd的所有操作，都是从hdfs中的checkpoint的具体目录来读取数据。 缓存之后，rdd的依赖关系还是存在的。</p><h2 id="11-spark-streaming流式统计单词数量代码"><a href="#11-spark-streaming流式统计单词数量代码" class="headerlink" title="11.spark streaming流式统计单词数量代码"></a>11.spark streaming流式统计单词数量代码</h2><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">WordCountAll</span> </span>&#123;</span><br><span class="line">  <span class="comment">// newValues当前批次的出现的单词次数， runningCount表示之前运行的单词出现的结果</span></span><br><span class="line"> <span class="comment">/* def updateFunction(newValues: Seq[Int], runningCount: Option[Int]): Option[Int] = &#123;</span></span><br><span class="line"><span class="comment">    val newCount =  newValues.sum + runningCount.getOrElse(0)// 将历史前几个批次的值和当前批次的值进行累加返回当前批次最终的结果</span></span><br><span class="line"><span class="comment">    Some(newCount)</span></span><br><span class="line"><span class="comment">  &#125;*/</span></span><br><span class="line">  <span class="comment">/**</span></span><br><span class="line"><span class="comment">    * String : 单词 hello</span></span><br><span class="line"><span class="comment">    * Seq[Int] ：单词在当前批次出现的次数</span></span><br><span class="line"><span class="comment">    * Option[Int] ： 历史结果</span></span><br><span class="line"><span class="comment">    */</span></span><br><span class="line">  <span class="keyword">val</span> updateFunc = (iter: <span class="type">Iterator</span>[(<span class="type">String</span>, <span class="type">Seq</span>[<span class="type">Int</span>], <span class="type">Option</span>[<span class="type">Int</span>])]) =&gt; &#123;</span><br><span class="line">    <span class="comment">//iter.flatMap(it=&gt;Some(it._2.sum + it._3.getOrElse(0)).map(x=&gt;(it._1,x)))</span></span><br><span class="line">    iter.flatMap&#123;<span class="keyword">case</span>(x,y,z)=&gt;<span class="type">Some</span>(y.sum + z.getOrElse(<span class="number">0</span>)).map(m=&gt;(x, m))&#125;</span><br><span class="line">  &#125;</span><br><span class="line">  <span class="comment">// 屏蔽日志</span></span><br><span class="line">  <span class="type">Logger</span>.getLogger(<span class="string">"org.apache"</span>).setLevel(<span class="type">Level</span>.<span class="type">ERROR</span>)</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]) &#123;</span><br><span class="line">    <span class="comment">// 必须要开启2个以上的线程，一个线程用来接收数据，另外一个线程用来计算</span></span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local[2]"</span>).setAppName(<span class="string">"NetworkWordCount"</span>)</span><br><span class="line">      <span class="comment">// 设置sparkjob计算时所采用的序列化方式</span></span><br><span class="line">      .set(<span class="string">"spark.serializer"</span>, <span class="string">"org.apache.spark.serializer.KryoSerializer"</span>)</span><br><span class="line">      .set(<span class="string">"spark.rdd.compress"</span>, <span class="string">"true"</span>) <span class="comment">// 节约大量的内存内容</span></span><br><span class="line">    <span class="comment">// 如果你的程序出现垃圾回收时间过程，可以设置一下java的垃圾回收参数</span></span><br><span class="line">    <span class="comment">// 同时也会创建sparkContext对象</span></span><br><span class="line">    <span class="comment">// 批次时间 &gt;= 批次处理的总时间 (批次数据量，集群的计算节点数量和配置)</span></span><br><span class="line">    <span class="keyword">val</span> ssc = <span class="keyword">new</span> <span class="type">StreamingContext</span>(conf, <span class="type">Seconds</span>(<span class="number">5</span>))</span><br><span class="line"> </span><br><span class="line">    <span class="comment">//做checkpoint 写入共享存储中</span></span><br><span class="line">    ssc.checkpoint(<span class="string">"c://aaa"</span>)</span><br><span class="line"> </span><br><span class="line">    <span class="comment">// 创建一个将要连接到 hostname:port 的 DStream，如 localhost:9999</span></span><br><span class="line">    <span class="keyword">val</span> lines: <span class="type">ReceiverInputDStream</span>[<span class="type">String</span>] = ssc.socketTextStream(<span class="string">"192.168.175.101"</span>, <span class="number">44444</span>)</span><br><span class="line">    <span class="comment">//updateStateByKey结果可以累加但是需要传入一个自定义的累加函数：updateFunc</span></span><br><span class="line">    <span class="keyword">val</span> results = lines.flatMap(_.split(<span class="string">" "</span>)).map((_,<span class="number">1</span>)).updateStateByKey(updateFunc, <span class="keyword">new</span> <span class="type">HashPartitioner</span>(ssc.sparkContext.defaultParallelism), <span class="literal">true</span>)</span><br><span class="line">    <span class="comment">//打印结果到控制台</span></span><br><span class="line">    results.print()</span><br><span class="line">    <span class="comment">//开始计算</span></span><br><span class="line">    ssc.start()</span><br><span class="line">    <span class="comment">//等待停止</span></span><br><span class="line">    ssc.awaitTermination()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">SparkWordCountApp</span> </span>&#123;</span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line">    <span class="keyword">val</span> sparkConf = <span class="keyword">new</span> <span class="type">SparkConf</span>().setMaster(<span class="string">"local"</span>).setAppName(<span class="string">"SparkWordCountApp"</span>)</span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(sparkConf)</span><br><span class="line"></span><br><span class="line">    <span class="keyword">val</span> rdd = sc.textFile(<span class="string">"file:////Users/lx/IdeaProjects/sparksql-train/data/input.txt"</span>)</span><br><span class="line">    <span class="comment">//    rdd.collect().foreach(println)</span></span><br><span class="line">    rdd.flatMap(_.split(<span class="string">","</span>))</span><br><span class="line">      .map(word =&gt; (word, <span class="number">1</span>))</span><br><span class="line">        .reduceByKey(_+_).map(x =&gt; (x._2,x._1)).sortByKey(<span class="literal">false</span>)</span><br><span class="line">      .map(x =&gt; (x._1,x._2))</span><br><span class="line">      .collect().foreach(println)</span><br><span class="line"></span><br><span class="line">    sc.stop()</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="12-简述map和flatMap的区别和应用场景"><a href="#12-简述map和flatMap的区别和应用场景" class="headerlink" title="12.简述map和flatMap的区别和应用场景"></a>12.简述map和flatMap的区别和应用场景</h2><p>map是对每一个元素进行操作，flatmap是对每一个元素操作后并压平</p><p>flatmap对每个元素进行操作后，放入一个对象返回。</p><h2 id="13-计算曝光数和点击数"><a href="#13-计算曝光数和点击数" class="headerlink" title="13.计算曝光数和点击数"></a>13.计算曝光数和点击数</h2><h2 id="14-分别列出几个常用的transformation和action算子"><a href="#14-分别列出几个常用的transformation和action算子" class="headerlink" title="14.分别列出几个常用的transformation和action算子"></a>14.分别列出几个常用的transformation和action算子</h2><ul><li>转换算子：map,flatmap,filter,reduceByKey,groupByKey,groupBy</li><li>行动算子：foreach，foreachpartition,collect,collectAsMap,take,top,first,count,countByKey</li></ul><h2 id="15-按照需求使用spark编写以下程序，要求使用scala语言"><a href="#15-按照需求使用spark编写以下程序，要求使用scala语言" class="headerlink" title="15.按照需求使用spark编写以下程序，要求使用scala语言"></a>15.按照需求使用spark编写以下程序，要求使用scala语言</h2><p>当前文件a.txt的格式，请统计每个单词出现的次数</p><p>A,b,c</p><p>B,b,f,e</p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">object</span> <span class="title">WordCount</span> </span>&#123;</span><br><span class="line"> </span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">main</span></span>(args: <span class="type">Array</span>[<span class="type">String</span>]): <span class="type">Unit</span> = &#123;</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">val</span> conf = <span class="keyword">new</span> <span class="type">SparkConf</span>()</span><br><span class="line">      .setAppName(<span class="keyword">this</span>.getClass.getSimpleName)</span><br><span class="line">      .setMaster(<span class="string">"local[*]"</span>)</span><br><span class="line">    <span class="keyword">val</span> sc = <span class="keyword">new</span> <span class="type">SparkContext</span>(conf)</span><br><span class="line"> </span><br><span class="line">    <span class="keyword">var</span> sData: <span class="type">RDD</span>[<span class="type">String</span>] = sc.textFile(<span class="string">"a.txt"</span>)</span><br><span class="line">    <span class="keyword">val</span> sortData: <span class="type">RDD</span>[(<span class="type">String</span>, <span class="type">Int</span>)] = sData.flatMap(_.split(<span class="string">","</span>)).map((_,<span class="number">1</span>)).reduceByKey(_+_)</span><br><span class="line">    sortData.foreach(print)</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><h2 id="16-spark应用程序的执行命令是什么？"><a href="#16-spark应用程序的执行命令是什么？" class="headerlink" title="16.spark应用程序的执行命令是什么？"></a>16.spark应用程序的执行命令是什么？</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line">/usr/local/spark-current2.3/bin/spark-submit \</span><br><span class="line"></span><br><span class="line">--class com.wedoctor.Application \</span><br><span class="line"></span><br><span class="line">--master yarn \</span><br><span class="line"></span><br><span class="line">--deploy-mode client \</span><br><span class="line"></span><br><span class="line">--driver-memory 1g \</span><br><span class="line"></span><br><span class="line">--executor-memory 2g \</span><br><span class="line"></span><br><span class="line">--queue root.wedw \</span><br><span class="line"></span><br><span class="line">--num-executors 200 \</span><br><span class="line"></span><br><span class="line">--jars /home/pgxl/liuzc/config-1.3.0.jar,/home/pgxl/liuzc/hadoop-lzo-0.4.20.jar,/home/pgxl/liuzc/elasticsearch-hadoop-hive-2.3.4.jar \</span><br><span class="line"></span><br><span class="line">/home/pgxl/liuzc/sen.jar</span><br></pre></td></tr></table></figure><h2 id="17-Spark应用执行有哪些模式，其中哪几种是集群模式"><a href="#17-Spark应用执行有哪些模式，其中哪几种是集群模式" class="headerlink" title="17.Spark应用执行有哪些模式，其中哪几种是集群模式"></a>17.Spark应用执行有哪些模式，其中哪几种是集群模式</h2><ul><li>本地local模式</li><li>standalone模式</li><li>spark on yarn模式</li><li>spark on mesos模式</li></ul><p>其中，standalone模式，spark on yarn模式，spark on mesos模式是集群模式</p><h2 id="18-请说明spark中广播变量的用途"><a href="#18-请说明spark中广播变量的用途" class="headerlink" title="18.请说明spark中广播变量的用途"></a>18.请说明spark中广播变量的用途</h2><p>使用广播变量，每个 Executor 的内存中，只驻留一份变量副本，而不是对 每个 task 都传输一次大变量，省了很多的网络传输， 对性能提升具有很大帮助， 而且会通过高效的广播算法来减少传输代价。</p><h2 id="19-以下代码会报错吗？如果会怎么解决-val-arr-new-ArrayList-String-arr-foreach-println"><a href="#19-以下代码会报错吗？如果会怎么解决-val-arr-new-ArrayList-String-arr-foreach-println" class="headerlink" title="19.以下代码会报错吗？如果会怎么解决 val arr = new ArrayList[String]; arr.foreach(println)"></a>19.以下代码会报错吗？如果会怎么解决 val arr = new ArrayList[String]; arr.foreach(println)</h2><p>val arr = new ArrayList[String]; 这里会报错，需要改成 val arr: Array[String] = new Array[String](10)</p><p>arr.foreach(println)打印不会报空指针</p><h2 id="20-写出你用过的spark中的算子，其中哪些会产生shuffle过程"><a href="#20-写出你用过的spark中的算子，其中哪些会产生shuffle过程" class="headerlink" title="20.写出你用过的spark中的算子，其中哪些会产生shuffle过程"></a>20.写出你用过的spark中的算子，其中哪些会产生shuffle过程</h2><p>reduceBykey：</p><p>groupByKey：</p><p>…ByKey:</p><h2 id="21-Spark中rdd与partition的区别"><a href="#21-Spark中rdd与partition的区别" class="headerlink" title="21.Spark中rdd与partition的区别"></a>21.Spark中rdd与partition的区别</h2><p>RDD是什么?弹性分布式数据集。<br>弹性:并不是指他可以动态扩展，而是血统容错机制。<br>分布式:顾名思义，RDD会在多个节点上存储，就和hdfs的分布式道理是一样的。hdfs文件被切分为多个block存储在各个节点上，而RDD是被切分为多个partition。不同的partition可能在不同的节点上。在spark读取hdfs的场景下，spark把hdfs的block读到内存就会抽象为spark的partition。至于后续遇到shuffle的操作，RDD的partition可以根据Hash再次进行划分(一般pairRDD是使用key做Hash再取余来划分partition）。</p><h2 id="22-请写出创建Dateset的几种方式"><a href="#22-请写出创建Dateset的几种方式" class="headerlink" title="22.请写出创建Dateset的几种方式"></a>22.请写出创建Dateset的几种方式</h2><ul><li><h5 id="由DataFrame-转化成为-Dataset"><a href="#由DataFrame-转化成为-Dataset" class="headerlink" title="由DataFrame 转化成为 Dataset"></a>由DataFrame 转化成为 Dataset</h5></li><li><h5 id="通过-SparkSession-createDataset-直接创建"><a href="#通过-SparkSession-createDataset-直接创建" class="headerlink" title="通过 SparkSession.createDataset() 直接创建"></a>通过 SparkSession.createDataset() 直接创建</h5></li><li><h5 id="通过toDS-方法意识转换"><a href="#通过toDS-方法意识转换" class="headerlink" title="通过toDS 方法意识转换"></a>通过toDS 方法意识转换</h5></li></ul><h2 id="23-描述一下RDD，DataFrame，DataSet的区别？"><a href="#23-描述一下RDD，DataFrame，DataSet的区别？" class="headerlink" title="23.描述一下RDD，DataFrame，DataSet的区别？"></a>23.描述一下RDD，DataFrame，DataSet的区别？</h2><p>1）RDD</p><p>优点:</p><p>编译时类型安全</p><p>编译时就能检查出类型错误</p><p>面向对象的编程风格</p><p>直接通过类名点的方式来操作数据</p><p>缺点:</p><p>序列化和反序列化的性能开销</p><p>无论是集群间的通信, 还是 IO 操作都需要对对象的结构和数据进行序列化和反序列化。</p><p>GC 的性能开销，频繁的创建和销毁对象, 势必会增加 GC</p><p>2）DataFrame</p><p>DataFrame 引入了 schema 和 off-heap</p><p>schema : RDD 每一行的数据, 结构都是一样的，这个结构就存储在 schema 中。 Spark 通过 schema 就能够读懂数据, 因此在通信和 IO 时就只需要序列化和反序列化数据, 而结构的部分就可以省略了。</p><p>3）DataSet</p><p>DataSet 结合了 RDD 和 DataFrame 的优点，并带来的一个新的概念 Encoder。</p><p>当序列化数据时，Encoder 产生字节码与 off-heap 进行交互，能够达到按需访问数据的效果，而不用反序列化整个对象。Spark 还没有提供自定义 Encoder 的 API，但是未来会加入。<br>三者之间的转换：</p><p><img src="https://lixiangbetter.github.io/2020/07/06/Spark%E7%AC%94%E8%AE%B0/20200409085324709.png" alt></p><h2 id="24-描述一下Spark中stage是如何划分的？描述一下shuffle的概念"><a href="#24-描述一下Spark中stage是如何划分的？描述一下shuffle的概念" class="headerlink" title="24.描述一下Spark中stage是如何划分的？描述一下shuffle的概念"></a>24.描述一下Spark中stage是如何划分的？描述一下shuffle的概念</h2><ul><li><strong>Stage概念</strong></li></ul><p>Spark任务会根据<strong>RDD之间的依赖关系，形成一个DAG有向无环图</strong>，DAG会提交给DAGScheduler，DAGScheduler会把DAG划分相互依赖的多个stage，划分stage的依据就是RDD之间的宽窄依赖。<strong>遇到宽依赖就划分stage</strong>,每个stage包含一个或多个task任务。然后将这些task以taskSet的形式提交给<strong>TaskScheduler运行</strong>。   <strong>stage是由一组并行的task组成。</strong></p><ul><li><p><strong>stage切割规则</strong></p><p>切割规则：<strong>从后往前</strong>，<strong>遇到宽依赖就切割stage。</strong></p></li></ul><p><img src="https://lixiangbetter.github.io/2020/07/06/Spark%E7%AC%94%E8%AE%B0/1250469-20180205010816029-1089510889.png" alt="img"></p><ul><li><p><strong>stage计算模式</strong></p><p>pipeline管道计算模式,pipeline只是一种计算思想，模式。</p></li></ul><p>备注：图中几个理解点：</p><p>  1、Spark的pipeLine的计算模式，相当于执行了一个高阶函数f3(f2(f1(textFile))) !+!+!=3 也就是来一条数据然后计算一条数据，把所有的逻辑走完，然后落地，准确的说一个task处理遗传分区的数据 因为跨过了不同的逻辑的分区。而MapReduce是 1+1=2,2+1=3的模式，也就是计算完落地，然后在计算，然后再落地到磁盘或内存，最后数据是落在计算节点上，按reduce的hash分区落地。所以这也是比Mapreduce快的原因，完全基于内存计算。</p><p>  2、管道中的数据何时落地：<strong>shuffle write的时候，对RDD进行持久化的时候。</strong></p><ol start="3"><li><p><strong>Stage的task并行度是由stage的最后一个RDD的分区数来决定的 。一般来说，一个partiotion对应一个task,但最后reduce的时候可以手动改变reduce的个数，也就是分区数，即改变了并行度。例如reduceByKey(XXX,3),GroupByKey(4)，union由的分区数由前面的相加。</strong></p></li><li><p>、<strong>如何提高stage的并行度</strong>：reduceBykey(xxx,numpartiotion),join(xxx,numpartiotion)</p></li></ol><h2 id="shuffle-和-stage"><a href="#shuffle-和-stage" class="headerlink" title="shuffle 和 stage"></a>shuffle 和 stage</h2><p>Shuffle:当Map的输出结果要被Reduce使用时，输出结果需要按key哈希，并且分发到每一个Reducer上去，这个过程就是shuffle。</p><p>spark中的shuffle:宽依赖指子 RDD 的各个分片会依赖于父RDD 的多个分片,所以会造成父 RDD 的各个分片在集群中重新分片</p><h2 id="25-Spark-在yarn上运行需要做哪些关键的配置工作？如何kill-个Spark在yarn运行中Application"><a href="#25-Spark-在yarn上运行需要做哪些关键的配置工作？如何kill-个Spark在yarn运行中Application" class="headerlink" title="25.Spark 在yarn上运行需要做哪些关键的配置工作？如何kill -个Spark在yarn运行中Application"></a>25.Spark 在yarn上运行需要做哪些关键的配置工作？如何kill -个Spark在yarn运行中Application</h2><p><strong>修改 spark-env.sh文件，配置hadoop的配置文件，或者yarn的配置文件即可（两者选择其中一种即可）</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">yarn application -kill &lt;applicationId&gt;</span><br></pre></td></tr></table></figure><h2 id="26-通常来说，Spark与MapReduce相比，Spark运行效率更高。请说明效率更高来源于Spark内置的哪些机制？并请列举常见spark的运行模式？"><a href="#26-通常来说，Spark与MapReduce相比，Spark运行效率更高。请说明效率更高来源于Spark内置的哪些机制？并请列举常见spark的运行模式？" class="headerlink" title="26.通常来说，Spark与MapReduce相比，Spark运行效率更高。请说明效率更高来源于Spark内置的哪些机制？并请列举常见spark的运行模式？"></a>26.通常来说，Spark与MapReduce相比，Spark运行效率更高。请说明效率更高来源于Spark内置的哪些机制？并请列举常见spark的运行模式？</h2><ol><li><p>spark中具有DAG有向无环图，DAG有向无环图在此过程中减少了shuffle以及落地磁盘的次数</p><p>Spark 计算比 MapReduce 快的根本原因在于 DAG 计算模型。一般而言，DAG 相比MapReduce 在大多数情况下可以减少 shuffle 次数。Spark 的 DAGScheduler 相当于一个改进版的 MapReduce，如果计算不涉及与其他节点进行数据交换，Spark 可以在内存中一次性完成这些操作，也就是中间结果无须落盘，减少了磁盘 IO 的操作。<br>spark把运算的中间数据(shuffle阶段产生的数据)存放在内存，迭代计算效率更高，mapreduce的中间结果需要落地，保存到磁盘</p></li><li><p>Spark容错性高，它通过弹性分布式数据集RDD来实现高效容错，RDD是一组分布式的存储在 节点内存中的只读性的数据集，这些集合石弹性的，某一部分丢失或者出错，可以通过整个数据集的计算流程的血缘关系来实现重建，mapreduce的容错只能重新计算</p></li><li><p>spark是粗粒度资源申请，也就是当提交spark application的时候，application会将所有的资源申请完毕，如果申请不到资源就等待，如果申请到资源才执行application，task在执行的时候就不需要自己去申请资源，task执行快，当最后一个task执行完之后task才会被释放。</p><p>优点是执行速度快，缺点是不能使集群得到充分的利用</p><p>MapReduce是细粒度资源申请，当提交application的时候，task执行时，自己申请资源，自己释放资源，task执行完毕之后，资源立即会被释放，task执行的慢，application执行的相对比较慢。</p><p>优点是集群资源得到充分利用，缺点是application执行的相对比较慢。</p></li></ol><h2 id="27-RDD中的数据在哪？"><a href="#27-RDD中的数据在哪？" class="headerlink" title="27.RDD中的数据在哪？"></a>27.RDD中的数据在哪？</h2><p>RDD中的数据在数据源，RDD只是一个抽象的数据集，我们通过对RDD的操作就相当于对数据进行操作。</p><h2 id="28-如果对RDD进行cache操作后，数据在哪里？"><a href="#28-如果对RDD进行cache操作后，数据在哪里？" class="headerlink" title="28.如果对RDD进行cache操作后，数据在哪里？"></a>28.如果对RDD进行cache操作后，数据在哪里？</h2><p>数据在第一执行cache算子时会被加载到各个Executor进程的内存中，第二次就会直接从内存中读取而不会区磁盘。</p><h2 id="29-Spark中Partition的数量由什么决定"><a href="#29-Spark中Partition的数量由什么决定" class="headerlink" title="29.Spark中Partition的数量由什么决定"></a>29.Spark中Partition的数量由什么决定</h2><p>和Mr一样，但是Spark默认最少有两个分区。</p><p>如果是读取hdfs的文件，一般来说，partition的数量等于文件的数量。</p><p>如果单个文件的大小大于hdfs的分块大小，partition的数量就等于 “文件大小/分块大小”。</p><p>同时，也可以使用rdd的repartition方法重新划分partition。</p><p>另外，在使用聚合函数比如 reducebykey, groupbykey，可以通过指定partitioner来指定partition的数量。</p><h2 id="30-Scala里面的函数和方法有什么区别"><a href="#30-Scala里面的函数和方法有什么区别" class="headerlink" title="30.Scala里面的函数和方法有什么区别"></a>30.Scala里面的函数和方法有什么区别</h2><p>Scala 有函数和方法，二者在语义上的区别很小。Scala 方法是类的一部分，而函数是一个对象可以赋值给一个变量。换句话来说在类中定义的函数即是方法。</p><h2 id="31-SparkStreaming怎么进行监控"><a href="#31-SparkStreaming怎么进行监控" class="headerlink" title="31.SparkStreaming怎么进行监控?"></a>31.SparkStreaming怎么进行监控?</h2><ol><li><p>Spark Streaming也提供了Jobs、Stages、Storage、Enviorment、Executors以及Streaming的监控</p></li><li><p>Spark Streaming能够提供如此优雅的数据监控，是因在对监听器设计模式的使用。如若Spark UI无法满足你所需的监控需要，用户可以定制个性化监控信息。Spark Streaming提供了StreamingListener特质，通过继承此方法，就可以定制所需的监控</p></li></ol><h2 id="32-Spark判断Shuffle的依据"><a href="#32-Spark判断Shuffle的依据" class="headerlink" title="32.Spark判断Shuffle的依据?"></a>32.Spark判断Shuffle的依据?</h2><p>父RDD的一个分区中的数据有可能被分配到子RDD的多个分区中</p><h2 id="33-Scala有没有多继承？可以实现多继承么？"><a href="#33-Scala有没有多继承？可以实现多继承么？" class="headerlink" title="33.Scala有没有多继承？可以实现多继承么？"></a>33.Scala有没有多继承？可以实现多继承么？</h2><p>trait实现多继承</p><p>由上可见，super.log通常调用trait从最后一个开始，从右往左调用。但是如果右边的trait是左边trait的超类，那么次序会调换，先调用子再调用父。</p><h2 id="34-Sparkstreaming和flink做实时处理的区别"><a href="#34-Sparkstreaming和flink做实时处理的区别" class="headerlink" title="34.Sparkstreaming和flink做实时处理的区别"></a>34.Sparkstreaming和flink做实时处理的区别</h2><ul><li>Flink的计算模型抽象是有状态的流，即源源不断没有边界的数据，并且数据的状态可以改变，对于批处理则认为是有边界的流进行处理</li><li>Spark的计算模型抽象是批，所有数据的表示本质上都是RDD抽象，对于流处理的支持，则是基于时间将流划分为多个批次，依次进行处理</li></ul><h2 id="35-Sparkcontext的作用"><a href="#35-Sparkcontext的作用" class="headerlink" title="35.Sparkcontext的作用"></a>35.Sparkcontext的作用</h2><p>官方解释：SparkContext是spark功能的主要入口。其代表与spark集群的连接，能够用来在集群上创建RDD、累加器、广播变量。每个JVM里只能存在一个处于激活状态的SparkContext，在创建新的SparkContext之前必须调用stop()来关闭之前的SparkContext。<br>下面我们看下SparkContext究竟有什么作用：<br>首先，每一个Spark应用都是一个SparkContext实例，可以理解为一个SparkContext就是一个spark application的生命周期，一旦SparkContext创建之后，就可以用这个SparkContext来创建RDD、累加器、广播变量，并且可以通过SparkContext访问Spark的服务，运行任务。spark context设置内部服务，并建立与spark执行环境的连接。</p><p>SparkContext在spark应用中起到了master的作用，掌控了所有Spark的生命活动，统筹全局，除了具体的任务在executor中执行，其他的任务调度、提交、监控、RDD管理等关键活动均由SparkContext主体来完成。</p><h2 id="36-Sparkstreaming读取kafka数据为什么选择直连方式"><a href="#36-Sparkstreaming读取kafka数据为什么选择直连方式" class="headerlink" title="36.Sparkstreaming读取kafka数据为什么选择直连方式"></a>36.Sparkstreaming读取kafka数据为什么选择直连方式</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Direct方式则采用的是低层次的API，直接连接kafka服务器上读取数据。需要我们自己去手动维护偏移量，代码量稍微大些。不过这种方式的优点有：</span><br><span class="line">1.当我们读取Topic下的数据时，它会自动对应Topic下的Partition生成相对应数量的RDD Partition，提高了计算时的并行度，提高了效率。</span><br><span class="line">2.它不需要通过WAL来维持数据的完整性。采取Direct直连方式时，当数据发生丢失，只要kafka上的数据进行了复制，就可以根据副本来进行数据重新拉取。</span><br><span class="line">3.它保证了数据只消费一次。因为我们将偏移量保存在一个地方，当我们读取数据时，从这里拿到数据的起始偏移量和读取偏移量确定读取范围，通过这些我们可以读取数据，当读取完成后会更新偏移量，这就保证了数据只消费一次。</span><br></pre></td></tr></table></figure><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">Receive是使用的高级API，需要消费者连接Zookeeper来读取数据。是由Zookeeper来维护偏移量，不用我们来手动维护，这样的话就比较简单一些，减少了代码量。但是天下没有免费的午餐，它也有很多缺点：</span><br><span class="line">1.导致丢失数据。它是由Executor内的Receive来拉取数据并存放在内存中，再由Driver端提交的job来处理数据。这样的话，如果底层节点出现错误，就会发生数据丢失。</span><br><span class="line">2.浪费资源。可以采取WALs方式将数据同步到高可用数据存储平台上（HDFS，S3），那么如果再发生错误，就可以从中再次读取数据。但是这样会导致同样的数据存储了两份，浪费了资源。</span><br><span class="line">3.可能会导致重复读取数据。对于公司来说，一些数据宁可丢失了一小小部分也不能重复读取，但是这种由Zookeeper来记录偏移量的方式，可能会因为Spark和Zookeeper不同步，导致一份数据读取了两次。</span><br><span class="line">4.效率低。因为是分批次执行的，它是接收数据，直到达到了设定的时间间隔，才可是进行计算。而且我们在KafkaUtils.createStream()中设定的partition数量，只会增加receive的数量，不能提高并行计算的效率，但我们可以设定不同的Group和Topic创建DStream，然后再用Union合并DStream，提高并行效率。</span><br></pre></td></tr></table></figure><h2 id="37-离线分析什么时候用sparkcore和sparksql"><a href="#37-离线分析什么时候用sparkcore和sparksql" class="headerlink" title="37.离线分析什么时候用sparkcore和sparksql"></a>37.离线分析什么时候用sparkcore和sparksql</h2><p>经过分析，得出结论，Core适合读取结构复杂，多重map嵌套的数据。比如Avro这种数据复杂的文件类型。</p><p>SparkSQL适合读取结构简单的数据，比如parquet。</p><h2 id="38-Sparkstreaming实时的数据不丢失的问题"><a href="#38-Sparkstreaming实时的数据不丢失的问题" class="headerlink" title="38.Sparkstreaming实时的数据不丢失的问题"></a>38.Sparkstreaming实时的数据不丢失的问题</h2><p>在这种模式下，我们可以使用checkpoint + WAL + ReliableReceiver的方式保证不丢失数据，就是说在driver端打开chechpoint，用于定期的保存driver端的状态信息到HDFS上，保证driver端的状态信息不会丢失；在接收数据Receiver所在的Executor上打开WAL，使得接收到的数据保存在HDFS中，保证接收到的数据不会丢失；因为我们使用的是ReliableReceiver，所以在Receiver挂掉的期间，是不会接收数据，当这个Receiver重启的时候，会从上次消费的地方开始消费。</p><h2 id="39-简述宽依赖和窄依赖概念，groupByKey-reduceByKey-map-filter-union五种操作哪些会导致宽依赖，哪些会导致窄依赖"><a href="#39-简述宽依赖和窄依赖概念，groupByKey-reduceByKey-map-filter-union五种操作哪些会导致宽依赖，哪些会导致窄依赖" class="headerlink" title="39.简述宽依赖和窄依赖概念，groupByKey,reduceByKey,map,filter,union五种操作哪些会导致宽依赖，哪些会导致窄依赖?"></a>39.简述宽依赖和窄依赖概念，groupByKey,reduceByKey,map,filter,union五种操作哪些会导致宽依赖，哪些会导致窄依赖?</h2><p><strong>宽依赖：</strong>父RDD的分区被子RDD的多个分区使用  例如 groupByKey、reduceByKey、sortByKey等操作会产生宽依赖，会产生shuffle</p><p><strong>窄依赖：</strong>父RDD的每个分区都只被子RDD的一个分区使用 例如map、filter、union等操作会产生窄依赖</p><h2 id="40-数据倾斜可能会导致哪些问题，如何监控和排查，在设计之初，要考虑哪些来避免"><a href="#40-数据倾斜可能会导致哪些问题，如何监控和排查，在设计之初，要考虑哪些来避免" class="headerlink" title="40.数据倾斜可能会导致哪些问题，如何监控和排查，在设计之初，要考虑哪些来避免?"></a>40.数据倾斜可能会导致哪些问题，如何监控和排查，在设计之初，要考虑哪些来避免?</h2><p>危害一：任务长时间挂起，资源利用率下降</p><p>计算作业通常是分阶段进行的，阶段与阶段之间通常存在数据上的依赖关系，也就是说后一阶段需要等前一阶段执行完才能开始。</p><p>举个例子，Stage1在Stage0之后执行，假如Stage1依赖Stage0产生的数据结果，那么Stage1必须等待Stage0执行完成后才能开始，如果这时Stage0因为数据倾斜问题，导致任务执行时长过长，或者直接挂起，那么Stage1将一直处于等待状态，整个作业也就一直挂起。这个时候，资源被这个作业占据，但是却只有极少数task在执行，造成计算资源的严重浪费，利用率下降。</p><p>危害二：由引发内存溢出，导致任务失败</p><p>数据发生倾斜时，可能导致大量数据集中在少数几个节点上，在计算执行中由于要处理的数据超出了单个节点的能力范围，最终导致内存被撑爆，报OOM异常，直接导致任务失败。</p><p>危害三：作业执行时间超出预期，导致后续依赖数据结果的作业出错</p><p>有时候作业与作业之间，并没有构建强依赖关系，而是通过执行时间的前后时间差来调度，当前置作业未在预期时间范围内完成执行，那么当后续作业启动时便无法读取到其所需要的最新数据，从而导致连续出错。</p><p>可以看出，数据倾斜问题，就像是一个隐藏的杀手，潜伏在数据处理与分析的过程中，只要一出手，非死即伤。那么它又是如何产生的呢？想要解决它，我们就要先了解它。</p><p><strong>为什么会产生数据倾斜？</strong></p><p>3.1：读入数据的时候就是倾斜的</p><p>读入数据是计算任务的开始，但是往往这个阶段就可能已经开始出现问题了。</p><p>对于一些本身就可能倾斜的数据源，在读入阶段就可能出现个别partition执行时长过长或直接失败，如读取id分布跨度较大的mysql数据、partition分配不均的kafka数据或不可分割的压缩文件。</p><p>这些场景下，数据在读取阶段或者读取后的第一个计算阶段，就会容易执行过慢或报错。</p><p>3.2：shuffle产生倾斜</p><p>在shuffle阶段造成倾斜，在实际的工作中更加常见，比如特定key值数量过多，导致join发生时，大量数据涌向一个节点，导致数据严重倾斜，个别节点的读写压力是其他节点的好几倍，容易引发OOM错误。</p><p>3.3：过滤导致倾斜</p><p>有些场景下，数据原本是均衡的，但是由于进行了一系列的数据剔除操作，可能在过滤掉大量数据后，造成数据的倾斜。</p><p>例如，大部分节点都被过滤掉了很多数据，只剩下少量数据，但是个别节点的数据被过滤掉的很少，保留着大部分的数据。这种情况下，一般不会OOM，但是倾斜的数据可能会随着计算逐渐累积，最终引发问题。</p><p><strong>怎么预防或解决数据倾斜问题？</strong></p><p>4.1.尽量保证数据源是均衡的</p><p>程序读入的数据源通常是上个阶段其他作业产生的，那么我们在上个阶段作业生成数据时，就要注意这个问题，尽量不要给下游作业埋坑。</p><p>如果所有作业都注意到并谨慎处理了这个问题，那么出现读入时倾斜的可能性会大大降低。</p><p>这个有个小建议，在程序输出写文件时，尽量不要用coalesce，而是用repartition，这样写出的数据，各文件大小往往是均衡的。</p><p>4.2.对大数据集做过滤，结束后做repartition</p><p>对比较大的数据集做完过滤后，如果过滤掉了绝大部分数据，在进行下一步操作前，最好可以做一次repartition，让数据重回均匀分布的状态，否则失衡的数据集，在进行后续计算时，可能会逐渐累积倾斜的状态，容易产生错误。</p><p>4.3.对小表进行广播</p><p>如果两个数据量差异较大的表做join时，发生数据倾斜的常见解决方法，是将小表广播到每个节点去，这样就可以实现map端join，从而省掉shuffle，避免了大量数据在个别节点上的汇聚，执行效率也大大提升。</p><p>4.4.编码时要注意，不要人为造成倾斜</p><p>在写代码时，也要多加注意不要使用容易出问题的算子，如上文提到的coalesce。</p><p>另外，也要注意不要人为造成倾斜，如作者一次在帮别人排查倾斜问题时发现，他在代码中使用开窗函数，其中写到over (partition by 1)，这样就把所有数据分配到一个分区内，人为造成了倾斜。</p><p>4.5.join前优化</p><p>个别场景下，两个表join，某些特殊key值可能很多，很容易产生数据倾斜，这时可以根据实际计算进行join前优化。</p><p>如计算是先join后根据key聚合，那可以改为先根据key聚合然后再join。又如，需求是join后做distinct操作，在不影响结果的前提下，可以改为先distinct，然后再join。这些措施都是可以有效避免重复key过多导致join时倾斜。</p><h2 id="41-有一千万条短信，有重复，以文本文件的形式保存，一行一条数据，请用五分钟时间，找出重复出现最多的前10条"><a href="#41-有一千万条短信，有重复，以文本文件的形式保存，一行一条数据，请用五分钟时间，找出重复出现最多的前10条" class="headerlink" title="41.有一千万条短信，有重复，以文本文件的形式保存，一行一条数据，请用五分钟时间，找出重复出现最多的前10条"></a>41.有一千万条短信，有重复，以文本文件的形式保存，一行一条数据，请用五分钟时间，找出重复出现最多的前10条</h2><p>采用内存映射办法。</p><p>首先，1千万条短信按现在的短信长度将不会超过1GB空间，使用内存映射文件比较合适，可以一次映射 （如果有更大的数据量，可以采用分段映射），由于不需要频繁使用文件I/O和频繁分配小内存，这将大大提高了数据的加载速度。</p><p>其次，对每条短信的第i（i从0到70）个字母按ASCII码进行分组，也就是创建树。i是树的深度，也是短信第i个字母。</p><p>这个问题主要是解决两方面的问题：</p><ol><li><p>内容的加载。</p></li><li><p>短信内容的比较。</p></li></ol><p>采用内存映射技术可以解决内容加载的性能问题（不仅是不需要调用文件I/O函数，而且也不需要每读出一条短信都要分配一小块内存），而使用树技术可以有效地减少比较次数。</p><h2 id="42-现有一文件，格式如下，请用spark统计每个单词出现的次数"><a href="#42-现有一文件，格式如下，请用spark统计每个单词出现的次数" class="headerlink" title="42.现有一文件，格式如下，请用spark统计每个单词出现的次数"></a>42.现有一文件，格式如下，请用spark统计每个单词出现的次数</h2><p>见15</p><h2 id="43-共享变量和累加器"><a href="#43-共享变量和累加器" class="headerlink" title="43.共享变量和累加器"></a>43.共享变量和累加器</h2><p>累加器（accumulator）是 Spark 中提供的一种分布式的变量机制，其原理类似于mapreduce，即分布式的改变，然后聚合这些改变。累加器的一个常见用途是在调试时对作业执行过程中的事件进行计数。而广播变量用来高效分发较大的对象。</p><p>共享变量出现的原因：</p><p>通常在向 Spark 传递函数时，比如使用 map() 函数或者用 filter() 传条件时，可以使用驱动器程序中定义的变量，但是集群中运行的每个任务都会得到这些变量的一份新的副本，更新这些副本的值也不会影响驱动器中的对应变量。</p><p>Spark 的两个共享变量，累加器与广播变量，分别为结果聚合与广播这两种常见的通信模式突破了这一限制。</p><h2 id="44-当Spark涉及到数据库的操作时，如何减少Spark运行中的数据库连接数？"><a href="#44-当Spark涉及到数据库的操作时，如何减少Spark运行中的数据库连接数？" class="headerlink" title="44.当Spark涉及到数据库的操作时，如何减少Spark运行中的数据库连接数？"></a>44.当Spark涉及到数据库的操作时，如何减少Spark运行中的数据库连接数？</h2><p>使用 foreachPartition 代替 foreach，在 foreachPartition 内获取数据库的连接。</p><h2 id="45-特别大的数据，怎么发送到excutor中？"><a href="#45-特别大的数据，怎么发送到excutor中？" class="headerlink" title="45.特别大的数据，怎么发送到excutor中？"></a>45.特别大的数据，怎么发送到excutor中？</h2><p>当在excutor端使用了Driver变量，不使用广播变量，在每个excutor中有多少的task就有多少个Driver端变量副本<br>导致的问题：占用了网络IO,速度慢<br>如果使用广播变量在每一个excutor端只有一份Driver端的变量副本</p><h2 id="46-spark调优都做过哪些方面？"><a href="#46-spark调优都做过哪些方面？" class="headerlink" title="46.spark调优都做过哪些方面？"></a>46.spark调优都做过哪些方面？</h2><p>​    <img src="https://lixiangbetter.github.io/2020/07/06/Spark%E7%AC%94%E8%AE%B0/20160617154014702.png" alt></p><p>​    了解完了Spark作业运行的基本原理之后，对资源相关的参数就容易理解了。所谓的Spark资源参数调优，其实主要就是对Spark运行过程中各个使用资源的地方，通过调节各种参数，来优化资源使用的效率，从而提升Spark作业的执行性能。以下参数就是Spark中主要的资源参数，每个参数都对应着作业运行原理中的某个部分，我们同时也给出了一个调优的参考值。</p><h2 id="num-executors"><a href="#num-executors" class="headerlink" title="num-executors"></a>num-executors</h2><p>　　参数说明：该参数用于设置Spark作业总共要用多少个Executor进程来执行。Driver在向YARN集群管理器申请资源时，YARN集群管理器会尽可能按照你的设置来在集群的各个工作节点上，启动相应数量的Executor进程。这个参数非常之重要，如果不设置的话，默认只会给你启动少量的Executor进程，此时你的Spark作业的运行速度是非常慢的。</p><p>　　参数调优建议：<strong>每个Spark作业的运行一般设置50~100个左右的Executor进程比较合适</strong>，设置太少或太多的Executor进程都不好。设置的太少，无法充分利用集群资源；设置的太多的话，大部分队列可能无法给予充分的资源。</p><h2 id="executor-memory"><a href="#executor-memory" class="headerlink" title="executor-memory"></a>executor-memory</h2><p>　　参数说明：该参数用于设置每个Executor进程的内存。Executor内存的大小，很多时候直接决定了Spark作业的性能，而且跟常见的JVM OOM异常，也有直接的关联。</p><p>　　参数调优建议：每个Executor进程的内存设置4G<del>8G较为合适。但是这只是一个参考值，具体的设置还是得根据不同部门的资源队列来定。可以看看自己团队的资源队列的最大内存限制是多少，num-executors乘以executor-memory，就代表了你的Spark作业申请到的总内存量（也就是所有Executor进程的内存总和），这个量是不能超过队列的最大内存量的。此外，如果你是跟团队里其他人共享这个资源队列，那么申请的总内存量最好不要超过资源队列最大总内存的1/3</del>1/2，避免你自己的Spark作业占用了队列所有的资源，导致别的同学的作业无法运行。</p><h2 id="executor-cores"><a href="#executor-cores" class="headerlink" title="executor-cores"></a>executor-cores</h2><p>　　参数说明：该参数用于设置每个Executor进程的CPU core数量。这个参数决定了每个Executor进程并行执行task线程的能力。因为每个CPU core同一时间只能执行一个task线程，因此每个Executor进程的CPU core数量越多，越能够快速地执行完分配给自己的所有task线程。</p><p>　　参数调优建议：Executor的CPU core数量设置为2<del>4个较为合适。同样得根据不同部门的资源队列来定，可以看看自己的资源队列的最大CPU core限制是多少，再依据设置的Executor数量，来决定每个Executor进程可以分配到几个CPU core。同样建议，如果是跟他人共享这个队列，那么num-executors * executor-cores不要超过队列总CPU core的1/3</del>1/2左右比较合适，也是避免影响其他同学的作业运行。</p><h2 id="driver-memory"><a href="#driver-memory" class="headerlink" title="driver-memory"></a>driver-memory</h2><p>　　参数说明：该参数用于设置Driver进程的内存。</p><p>　　参数调优建议：<strong>Driver的内存通常来说不设置，或者设置1G左右应该就够了。</strong>唯一需要注意的一点是，如果需要使用collect算子将RDD的数据全部拉取到Driver上进行处理，那么必须确保Driver的内存足够大，否则会出现OOM内存溢出的问题。</p><h2 id="spark-default-parallelism"><a href="#spark-default-parallelism" class="headerlink" title="spark.default.parallelism"></a>spark.default.parallelism</h2><p>　　参数说明：该参数用于设置每个stage的默认task数量。这个参数极为重要，如果不设置可能会直接影响你的Spark作业性能。</p><p>　　参数调优建议：<strong>Spark作业的默认task数量为500~1000个较为合适。很多同学常犯的一个错误就是不去设置这个参数</strong>，那么此时就会导致Spark自己根据底层HDFS的block数量来设置task的数量，默认是一个HDFS block对应一个task。通常来说，Spark默认设置的数量是偏少的（比如就几十个task），如果task数量偏少的话，就会导致你前面设置好的Executor的参数都前功尽弃。试想一下，无论你的Executor进程有多少个，内存和CPU有多大，但是task只有1个或者10个，那么90%的Executor进程可能根本就没有task执行，也就是白白浪费了资源！因此Spark官网建议的设置原则是，设置该参数为num-executors * executor-cores的2~3倍较为合适，比如Executor的总CPU core数量为300个，那么设置1000个task是可以的，此时可以充分地利用Spark集群的资源。</p><h2 id="spark-storage-memoryFraction"><a href="#spark-storage-memoryFraction" class="headerlink" title="spark.storage.memoryFraction"></a>spark.storage.memoryFraction</h2><p>　　参数说明：该参数用于设置RDD持久化数据在Executor内存中能占的比例，默认是0.6。也就是说，默认Executor 60%的内存，可以用来保存持久化的RDD数据。根据你选择的不同的持久化策略，如果内存不够时，可能数据就不会持久化，或者数据会写入磁盘。</p><p>　　参数调优建议：如果Spark作业中，有较多的RDD持久化操作，该参数的值可以适当提高一些，保证持久化的数据能够容纳在内存中。避免内存不够缓存所有的数据，导致数据只能写入磁盘中，降低了性能。但是如果Spark作业中的shuffle类操作比较多，而持久化操作比较少，那么这个参数的值适当降低一些比较合适。此外，如果发现作业由于频繁的gc导致运行缓慢（通过spark web ui可以观察到作业的gc耗时），意味着task执行用户代码的内存不够用，那么同样建议调低这个参数的值。</p><h2 id="spark-shuffle-memoryFraction"><a href="#spark-shuffle-memoryFraction" class="headerlink" title="spark.shuffle.memoryFraction"></a>spark.shuffle.memoryFraction</h2><p>　　参数说明：该参数用于设置shuffle过程中一个task拉取到上个stage的task的输出后，进行聚合操作时能够使用的Executor内存的比例，默认是0.2。也就是说，Executor默认只有20%的内存用来进行该操作。shuffle操作在进行聚合时，如果发现使用的内存超出了这个20%的限制，那么多余的数据就会溢写到磁盘文件中去，此时就会极大地降低性能。</p><p>　　参数调优建议：如果Spark作业中的RDD持久化操作较少，shuffle操作较多时，建议降低持久化操作的内存占比，提高shuffle操作的内存占比比例，避免shuffle过程中数据过多时内存不够用，必须溢写到磁盘上，降低了性能。此外，如果发现作业由于频繁的gc导致运行缓慢，意味着task执行用户代码的内存不够用，那么同样建议调低这个参数的值。<br>资源参数的调优，没有一个固定的值，需要同学们根据自己的实际情况（包括Spark作业中的shuffle操作数量、RDD持久化操作数量以及spark web ui中显示的作业gc情况），同时参考本篇文章中给出的原理以及调优建议，合理地设置上述参数。</p><h2 id="47-spark任务为什么会被yarn-kill掉？"><a href="#47-spark任务为什么会被yarn-kill掉？" class="headerlink" title="47.spark任务为什么会被yarn kill掉？"></a>47.spark任务为什么会被yarn kill掉？</h2><p>因为spark任务是向yarn申请资源的。</p><h2 id="48-Spark-on-Yarn作业执行流程？yarn-client和yarn-cluster有什么区别？"><a href="#48-Spark-on-Yarn作业执行流程？yarn-client和yarn-cluster有什么区别？" class="headerlink" title="48.Spark on Yarn作业执行流程？yarn-client和yarn-cluster有什么区别？"></a>48.Spark on Yarn作业执行流程？yarn-client和yarn-cluster有什么区别？</h2><p><strong>Spark on Yarn作业执行流程？</strong></p><p>1.Spark Yarn Client 向 Yarn 中提交应用程序。<br>2.ResourceManager 收到请求后，在集群中选择一个 NodeManager，并为该应用程序分配一个 Container，在这个 Container 中启动应用程序的 ApplicationMaster， ApplicationMaster 进行 SparkContext 等的初始化。<br>3.ApplicationMaster 向 ResourceManager 注册，这样用户可以直接通过 ResourceManager 查看应用程序的运行状态，然后它将采用轮询的方式通过RPC协议为各个任务申请资源，并监控它们的运行状态直到运行结束。<br>4.ApplicationMaster 申请到资源（也就是Container）后，便与对应的 NodeManager 通信，并在获得的 Container 中启动 CoarseGrainedExecutorBackend，启动后会向 ApplicationMaster 中的 SparkContext 注册并申请 Task。<br>5.ApplicationMaster 中的 SparkContext 分配 Task 给 CoarseGrainedExecutorBackend 执行，CoarseGrainedExecutorBackend 运行 Task 并向ApplicationMaster 汇报运行的状态和进度，以让 ApplicationMaster 随时掌握各个任务的运行状态，从而可以在任务失败时重新启动任务。<br>6.应用程序运行完成后，ApplicationMaster 向 ResourceManager申请注销并关闭自己。</p><p><strong>yarn-client和yarn-cluster有什么区别？</strong></p><p>1.理解YARN-Client和YARN-Cluster深层次的区别之前先清楚一个概念：Application Master。在YARN中，每个Application实例都有一个ApplicationMaster进程，它是Application启动的第一个容器。它负责和ResourceManager打交道并请求资源，获取资源之后告诉NodeManager为其启动Container。从深层次的含义讲YARN-Cluster和YARN-Client模式的区别其实就是ApplicationMaster进程的区别</p><ol start="2"><li>YARN-Cluster模式下，Driver运行在AM(Application Master)中，它负责向YARN申请资源，并监督作业的运行状况。当用户提交了作业之后，就可以关掉Client，作业会继续在YARN上运行，因而YARN-Cluster模式不适合运行交互类型的作业 3. YARN-Client模式下，Application Master仅仅向YARN请求Executor，Client会和请求的Container通信来调度他们工作，也就是说Client不能离开。</li></ol><h2 id="49-Flatmap底层编码实现？"><a href="#49-Flatmap底层编码实现？" class="headerlink" title="49.Flatmap底层编码实现？"></a>49.Flatmap底层编码实现？</h2><p><strong>Spark flatMap 源码：</strong></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/**</span></span><br><span class="line"><span class="comment">   *  Return a new RDD by first applying a function to all elements of this</span></span><br><span class="line"><span class="comment">   *  RDD, and then flattening the results.</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">flatMap</span></span>[<span class="type">U</span>: <span class="type">ClassTag</span>](f: <span class="type">T</span> =&gt; <span class="type">TraversableOnce</span>[<span class="type">U</span>]): <span class="type">RDD</span>[<span class="type">U</span>] = withScope &#123;</span><br><span class="line">    <span class="keyword">val</span> cleanF = sc.clean(f)</span><br><span class="line">    <span class="keyword">new</span> <span class="type">MapPartitionsRDD</span>[<span class="type">U</span>, <span class="type">T</span>](<span class="keyword">this</span>, (context, pid, iter) =&gt; iter.flatMap(cleanF))</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p><strong>Scala flatMap 源码：</strong></p><figure class="highlight scala"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">/** Creates a new iterator by applying a function to all values produced by this iterator</span></span><br><span class="line"><span class="comment">   *  and concatenating the results.</span></span><br><span class="line"><span class="comment">   *</span></span><br><span class="line"><span class="comment">   *  @param f the function to apply on each element.</span></span><br><span class="line"><span class="comment">   *  @return  the iterator resulting from applying the given iterator-valued function</span></span><br><span class="line"><span class="comment">   *           `f` to each value produced by this iterator and concatenating the results.</span></span><br><span class="line"><span class="comment">   *  @note    Reuse: $consumesAndProducesIterator</span></span><br><span class="line"><span class="comment">   */</span></span><br><span class="line">  <span class="function"><span class="keyword">def</span> <span class="title">flatMap</span></span>[<span class="type">B</span>](f: <span class="type">A</span> =&gt; <span class="type">GenTraversableOnce</span>[<span class="type">B</span>]): <span class="type">Iterator</span>[<span class="type">B</span>] = <span class="keyword">new</span> <span class="type">AbstractIterator</span>[<span class="type">B</span>] &#123;</span><br><span class="line">    <span class="keyword">private</span> <span class="keyword">var</span> cur: <span class="type">Iterator</span>[<span class="type">B</span>] = empty</span><br><span class="line">    <span class="keyword">private</span> <span class="function"><span class="keyword">def</span> <span class="title">nextCur</span></span>() &#123; cur = f(self.next()).toIterator &#125;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">hasNext</span></span>: <span class="type">Boolean</span> = &#123;</span><br><span class="line">      <span class="comment">// Equivalent to cur.hasNext || self.hasNext &amp;&amp; &#123; nextCur(); hasNext &#125;</span></span><br><span class="line">      <span class="comment">// but slightly shorter bytecode (better JVM inlining!)</span></span><br><span class="line">      <span class="keyword">while</span> (!cur.hasNext) &#123;</span><br><span class="line">        <span class="keyword">if</span> (!self.hasNext) <span class="keyword">return</span> <span class="literal">false</span></span><br><span class="line">        nextCur()</span><br><span class="line">      &#125;</span><br><span class="line">      <span class="literal">true</span></span><br><span class="line">    &#125;</span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">next</span></span>(): <span class="type">B</span> =&lt;span style=<span class="string">"color:#ffffff"</span>&gt; &lt;span style=<span class="string">"background-color:rgb(255,0,0)"</span>&gt;(<span class="keyword">if</span> (hasNext) cur <span class="keyword">else</span> empty).next()&lt;/span&gt;&lt;/span&gt;</span><br><span class="line">  &#125;</span><br></pre></td></tr></table></figure><p>flatMap其实就是将RDD里的每一个元素执行自定义函数f，这时这个元素的结果转换成iterator，最后将这些再拼接成一个新的RDD，也可以理解成原本的每个元素由横向执行函数f后再变为纵向。画红部分一直在回调，当RDD内没有元素为止。</p>]]></content>
      
      
      <categories>
          
          <category> bigdata </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>hive笔记</title>
      <link href="/2020/07/05/hive%E7%AC%94%E8%AE%B0/"/>
      <url>/2020/07/05/hive%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<h1 id="hive笔记"><a href="#hive笔记" class="headerlink" title="hive笔记"></a>hive笔记</h1><h2 id="1-大表join小表产生的问题，怎么解决？"><a href="#1-大表join小表产生的问题，怎么解决？" class="headerlink" title="1.大表join小表产生的问题，怎么解决？"></a>1.大表join小表产生的问题，怎么解决？</h2><p>mapjoin优化是在Map阶段进行join，而不是通常那样在Reduce阶段按照join列进行分发后在每个Reduce节点上进行join，不需要分发也就没有倾斜的问题，相反，Hive会将小表全量复制到每个Map任务节点（对于本例是dim_seller表，当然仅全量复制b表sql指定的列），然后每个Map任务节点执行lookup小表即可。</p><h2 id="2-udf-udaf-udtf区别"><a href="#2-udf-udaf-udtf区别" class="headerlink" title="2.udf udaf udtf区别"></a>2.udf udaf udtf区别</h2><p>UDF操作作用于单个数据行，并且产生一个数据行作为输出。大多数函数都属于这一类（比如数学函数和字符串函数）。</p><p>UDAF 接受多个输入数据行，并产生一个输出数据行。像COUNT和MAX这样的函数就是聚集函数。</p><p>UDTF 操作作用于单个数据行，并且产生多个数据行——-一个表作为输出。lateral view explore()</p><p>简单来说：</p><p>UDF: 返回对应值，一对一</p><p>UDAF：返回聚类值，多对一</p><p>UDTF：返回拆分值，一对多</p><h2 id="3-hive有哪些保存元数据的方式，各有什么特点。"><a href="#3-hive有哪些保存元数据的方式，各有什么特点。" class="headerlink" title="3.hive有哪些保存元数据的方式，各有什么特点。"></a>3.hive有哪些保存元数据的方式，各有什么特点。</h2><ul><li>内存数据库derby，安装小，但是数据存在内存，不稳定</li><li>mysql数据库，数据存储模式可以自己设置，持久化好，查看方便。</li></ul><h2 id="4-hive内部表和外部表的区别"><a href="#4-hive内部表和外部表的区别" class="headerlink" title="4.hive内部表和外部表的区别"></a>4.hive内部表和外部表的区别</h2><p>内部表：加载数据到hive所在的hdfs目录，删除时，元数据和数据文件都删除</p><p>外部表：不加载数据到hive所在的hdfs目录，删除时，只删除表结构。</p><p>这样外部表相对来说更加安全些，数据组织也更加灵活，方便共享源数据。 </p><h2 id="5-生产环境中为什么建议使用外部表？"><a href="#5-生产环境中为什么建议使用外部表？" class="headerlink" title="5.生产环境中为什么建议使用外部表？"></a>5.生产环境中为什么建议使用外部表？</h2><ol><li>因为外部表不会加载数据到hive，减少数据传输、数据还能共享。</li><li>hive不会修改数据，所以无需担心数据的损坏</li><li>删除表时，只删除表结构、不删除数据。</li></ol><h2 id="6-insert-into-和-override-write区别？"><a href="#6-insert-into-和-override-write区别？" class="headerlink" title="6.insert into 和 override write区别？"></a>6.insert into 和 override write区别？</h2><p>insert into：将数据写到表中</p><p>override write：覆盖之前的内容。</p><h2 id="7-hive的判断函数有哪些"><a href="#7-hive的判断函数有哪些" class="headerlink" title="7.hive的判断函数有哪些"></a>7.hive的判断函数有哪些</h2><p>hive 的条件判断（if、coalesce、case）</p><h2 id="8-简单描述一下HIVE的功能？用hive创建表有几种方式？hive表有几种？"><a href="#8-简单描述一下HIVE的功能？用hive创建表有几种方式？hive表有几种？" class="headerlink" title="8.简单描述一下HIVE的功能？用hive创建表有几种方式？hive表有几种？"></a>8.简单描述一下HIVE的功能？用hive创建表有几种方式？hive表有几种？</h2><p>hive主要是做离线分析的</p><p>hive建表有三种方式</p><ul><li>直接建表法</li><li>查询建表法(通过AS 查询语句完成建表：将子查询的结果存在新表里，有数据<strong>，</strong>一般用于中间表)</li><li>like建表法(会创建结构完全相同的表，但是没有数据)</li></ul><p>hive表有2种：内部表和外部表</p><h2 id="9-线上业务每天产生的业务日志（压缩后-gt-3G），每天需要加载到hive的log表中，将每天产生的业务日志在压缩之后load到hive的log表时，最好使用的压缩算法是哪个-并说明其原因"><a href="#9-线上业务每天产生的业务日志（压缩后-gt-3G），每天需要加载到hive的log表中，将每天产生的业务日志在压缩之后load到hive的log表时，最好使用的压缩算法是哪个-并说明其原因" class="headerlink" title="9.线上业务每天产生的业务日志（压缩后&gt;=3G），每天需要加载到hive的log表中，将每天产生的业务日志在压缩之后load到hive的log表时，最好使用的压缩算法是哪个,并说明其原因"></a>9.线上业务每天产生的业务日志（压缩后&gt;=3G），每天需要加载到hive的log表中，将每天产生的业务日志在压缩之后load到hive的log表时，最好使用的压缩算法是哪个,并说明其原因</h2><ul><li>lzo压缩</li></ul><p>优点：压缩/解压速度也比较快，合理的压缩率；支持split，是hadoop中最流行的压缩格式；支持hadoop native库；可以在linux系统下安装lzop命令，使用方便。</p><p>缺点：压缩率比gzip要低一些；hadoop本身不支持，需要安装；在应用中对lzo格式的文件需要做一些特殊处理（为了支持split需要建索引，还需要指定inputformat为lzo格式）。</p><p>应用场景：一个很大的文本文件，压缩之后还大于200M以上的可以考虑，而且单个文件越大，lzo优点越明显。</p><ul><li>snappy压缩</li></ul><p>优点：高速压缩速度和合理的压缩率；支持hadoop native库。</p><p>缺点：不支持split；压缩率比gzip要低；hadoop本身不支持，需要安装；linux系统下没有对应的命令。</p><p>应用场景：当mapreduce作业的map输出的数据比较大的时候，作为map到reduce的中间数据的压缩格式；或者作为一个mapreduce作业的输出和另外一个mapreduce作业的输入。</p><h2 id="10-若在hive中建立分区仍不能优化查询效率，建表时如何优化"><a href="#10-若在hive中建立分区仍不能优化查询效率，建表时如何优化" class="headerlink" title="10.若在hive中建立分区仍不能优化查询效率，建表时如何优化"></a>10.若在hive中建立分区仍不能优化查询效率，建表时如何优化</h2><p>hive分桶</p><p>对于每一个表（table）或者分区，Hive可以进一步组织成桶，也就是说桶是更为细粒度的数据范围划分。<br>Hive是针对某一列进行分桶。<br>Hive采用对列值哈希，然后除以桶的个数求余的方式决定该条记录存放在哪个桶当中。</p><h2 id="11-union-all和union的区别"><a href="#11-union-all和union的区别" class="headerlink" title="11.union all和union的区别"></a>11.union all和union的区别</h2><p>union 去重</p><p>union all 不去重</p><h2 id="12-如何解决hive数据倾斜的问题"><a href="#12-如何解决hive数据倾斜的问题" class="headerlink" title="12.如何解决hive数据倾斜的问题"></a>12.如何解决hive数据倾斜的问题</h2><p>1）group by</p><p>注：group by 优于 distinct group</p><p>情形：group by 维度过小，某值的数量过多</p><p>后果：处理某值的 reduce 非常耗时</p><p>解决方式：采用 sum() group by 的方式来替换 count(distinct)完成计算。</p><p>2）count(distinct)</p><p>count(distinct xx)</p><p>情形：某特殊值过多</p><p>后果：处理此特殊值的 reduce 耗时；只有一个 reduce 任务</p><p>解决方式：count distinct 时，将值为空的情况单独处理，比如可以直接过滤空值的行，</p><p>在最后结果中加 1。如果还有其他计算，需要进行 group by，可以先将值为空的记录单独处</p><p>理，再和其他计算结果进行 union。</p><p>3）mapjoin</p><p>​    小表不小不大，怎么用 map join 解决倾斜问题</p><p>​    使用 map join 解决小表(记录数少)关联大表的数据倾斜问题，这个方法使用的频率非常高，但如果小表很大，大到map join会出现bug或异常，这时就需要特别的处理。</p><p>4）不同数据类型关联产生数据倾斜</p><p>情形：比如用户表中 user_id 字段为 int，log 表中 user_id 字段既有 string 类型也有 int 类型。当按照 user_id 进行两个表的 Join 操作时。</p><p>后果：处理此特殊值的 reduce 耗时；只有一个 reduce 任务</p><p>默认的 Hash 操作会按 int 型的 id 来进行分配，这样会导致所有 string 类型 id 的记录都分配到一个 Reducer 中。</p><p>解决方式：把数字类型转换成字符串类型</p><p>select * from users a</p><p>left outer join logs b</p><p>on a.usr_id = cast(b.user_id as string)</p><p>5）开启数据倾斜时负载均衡</p><p>set hive.groupby.skewindata=true;</p><p>思想：就是先随机分发并处理，再按照 key group by 来分发处理。</p><p>操作：当选项设定为 true，生成的查询计划会有两个 MRJob。</p><p>第一个 MRJob 中，Map 的输出结果集合会随机分布到 Reduce 中，每个 Reduce 做部分聚合操作，并输出结果，这样处理的结果是相同的 GroupBy Key 有可能被分发到不同的Reduce 中，从而达到负载均衡的目的；</p><p>第二个 MRJob 再根据预处理的数据结果按照 GroupBy Key 分布到 Reduce 中（这个过程可以保证相同的原始 GroupBy Key 被分布到同一个 Reduce 中），最后完成最终的聚合操作。</p><p>点评：它使计算变成了两个 mapreduce，先在第一个中在 shuffle 过程 partition 时随机给 key 打标记，使每个 key 随机均匀分布到各个 reduce 上计算，但是这样只能完成部分计算，因为相同 key 没有分配到相同 reduce 上。</p><p>所以需要第二次的 mapreduce,这次就回归正常 shuffle,但是数据分布不均匀的问题在第一次 mapreduce 已经有了很大的改善，因此基本解决数据倾斜。因为大量计算已经在第一次mr 中随机分布到各个节点完成。</p><p>6）控制空值分布</p><p>将为空的 key 转变为字符串加随机数或纯随机数，将因空值而造成倾斜的数据分不到多个 Reducer。</p><p>注：对于异常值如果不需要的话，最好是提前在 where 条件里过滤掉，这样可以使计算量大大减少</p><h2 id="13-hive性能优化常用的方法"><a href="#13-hive性能优化常用的方法" class="headerlink" title="13.hive性能优化常用的方法"></a>13.hive性能优化常用的方法</h2><p>1）MapJoin</p><p>如果不指定 MapJoin 或者不符合 MapJoin 的条件，那么 Hive 解析器会将 Join 操作转换成 Common Join，即：在 Reduce 阶段完成 join。容易发生数据倾斜。可以用 MapJoin 把小表全部加载到内存在 map 端进行 join，避免 reducer 处理。</p><p>2）行列过滤</p><p>列处理：在 SELECT 中，只拿需要的列，如果有，尽量使用分区过滤，少用 SELECT *。</p><p>行处理：在分区剪裁中，当使用外关联时，如果将副表的过滤条件写在 Where 后面，那么就会先全表关联，之后再过滤。（先子查询，再关联）</p><p>3）列式存储</p><p>4）采用分区技术</p><p>5）合理设置 Map 数</p><p>（1）通常情况下，作业会通过 input 的目录产生一个或者多个 map 任务。</p><p>主要的决定因素有：input 的文件总个数，input 的文件大小，集群设置的文件块大小。</p><p>（2）是不是 map 数越多越好？</p><p>答案是否定的。如果一个任务有很多小文件（远远小于块大小 128m），则每个小文件</p><p>也会被当做一个块，用一个 map 任务来完成，而一个 map 任务启动和初始化的时间远远大</p><p>于逻辑处理的时间，就会造成很大的资源浪费。而且，同时可执行的 map 数是受限的。</p><p>（3）是不是保证每个 map 处理接近 128m 的文件块，就高枕无忧了？</p><p>答案也是不一定。比如有一个 127m 的文件，正常会用一个 map 去完成，但这个文件只</p><p>有一个或者两个小字段，却有几千万的记录，如果 map 处理的逻辑比较复杂，用一个 map</p><p>任务去做，肯定也比较耗时。</p><p>针对上面的问题 2 和 3，我们需要采取两种方式来解决：即减少 map 数和增加 map 数；</p><p>6）小文件进行合并</p><p>在 Map 执行前合并小文件，减少 Map 数：CombineHiveInputFormat 具有对小文件进行</p><p>合并的功能（系统默认的格式）。HiveInputFormat 没有对小文件合并功能。</p><p>7）合理设置 Reduce 数</p><p>Reduce 个数并不是越多越好</p><p>（1）过多的启动和初始化 Reduce 也会消耗时间和资源；</p><p>（2）另外，有多少个 Reduce，就会有多少个输出文件，如果生成了很多个小文件，那</p><p>么如果这些小文件作为下一个任务的输入，则也会出现小文件过多的问题；</p><p>在设置 Reduce 个数的时候也需要考虑这两个原则：处理大数据量利用合适的 Reduce</p><p>数；使单个 Reduce 任务处理数据量大小要合适；</p><p>8）常用参数</p><p>// 输出合并小文件</p><p>SET hive.merge.mapfiles = true; – 默认 true，在 map-only 任务结束时合并</p><p>小文件</p><p>SET hive.merge.mapredfiles = true; – 默认 false，在 map-reduce 任务结</p><p>束时合并小文件</p><p>SET hive.merge.size.per.task = 268435456; – 默认 256M</p><p>SET hive.merge.smallfiles.avgsize = 16777216; – 当输出文件的平均大小</p><p>小于 16m 该值时，启动一个独立的 map-reduce 任务进行文件 merge</p><p>9）开启 map 端 combiner（不影响最终业务逻辑）</p><p>set hive.map.aggr=true；</p><p>10）压缩（选择快的）</p><p>设置 map 端输出、中间结果压缩。（不完全是解决数据倾斜的问题，但是减少了 IO 读</p><p>写和网络传输，能提高很多效率）</p><p>11）开启 JVM 重用</p><h2 id="14-简述delete，drop，truncate的区别"><a href="#14-简述delete，drop，truncate的区别" class="headerlink" title="14.简述delete，drop，truncate的区别"></a>14.简述delete，drop，truncate的区别</h2><p>delete 删除数据</p><p>drop 删除表</p><p>truncate 清空数据</p><h2 id="15-四个by的区别"><a href="#15-四个by的区别" class="headerlink" title="15.四个by的区别"></a>15.四个by的区别</h2><ol><li><p>Sort By：分区内有序；sort by在每个reducer端都会做排序，为每个reduce产生一个排序文件。也就是说sort by能保证局部有序（每个reducer出来的数据是有序的，但是不能保证所有的数据是有序的，除非只有一个reducer）</p></li><li><p>Order By：全局排序，只有一个 Reducer；</p></li><li><p>Distrbute By：类似 MR 中 Partition，进行分区，结合 sort by 使用。DISTRIBUTE BY 是控制在map端如何拆分数据给reduce端的。hive会根据distribute by后面列，对应reduce的个数进行分发，默认是采用hash算法。</p><p>相同的将会分配到一个reducer.</p></li><li><p>Cluster By：当 Distribute by 和 Sorts by 字段相同时，可以使用 Cluster by 方式。Cluster by 除了具有Distribute by 的功能外还兼具 Sort by 的功能。但是排序只能是升序排序，不能 指定排序规则为 ASC 或者 DESC。</p></li></ol><h2 id="16-Hive里边字段的分隔符用的什么？为什么用-t？有遇到过字段里边有-t-的情况吗，怎么处理的？为什么不用Hive默认的分隔符，默认的分隔符是什么？"><a href="#16-Hive里边字段的分隔符用的什么？为什么用-t？有遇到过字段里边有-t-的情况吗，怎么处理的？为什么不用Hive默认的分隔符，默认的分隔符是什么？" class="headerlink" title="16.Hive里边字段的分隔符用的什么？为什么用\t？有遇到过字段里边有\t 的情况吗，怎么处理的？为什么不用Hive默认的分隔符，默认的分隔符是什么？"></a>16.Hive里边字段的分隔符用的什么？为什么用\t？有遇到过字段里边有\t 的情况吗，怎么处理的？为什么不用Hive默认的分隔符，默认的分隔符是什么？</h2><p>hive 默认的字段分隔符为 ascii 码的控制符\001（^A）,建表的时候用 fields terminated by ‘\001’</p><p>遇到过字段里边有\t 的情况，自定义 InputFormat，替换为其他分隔符再做后续处理</p><h2 id="17-分区分桶的区别，为什么要分区"><a href="#17-分区分桶的区别，为什么要分区" class="headerlink" title="17.分区分桶的区别，为什么要分区"></a>17.分区分桶的区别，为什么要分区</h2><p>分区表： 原来的一个大表存储的时候分成不同的数据目录进行存储。如果说是单分区表，那么在表的目录下就只有一级子目录，如果说是多分区表，那么在表的目录下有多少分区就有多少级子目录。不管是单分区表，还是多分区表，在表的目录下，和非最终分区目录下是不能直接存储数据文件的 </p><p>分桶表： 原理和hashpartitioner 一样，将hive中的一张表的数据进行归纳分类的时候，归纳分类规则就是hashpartitioner。（需要指定分桶字段，指定分成多少桶）</p><p>分区表和分桶的区别除了存储的格式不同外，最主要的是作用：</p><ul><li>分区表：细化数据管理，缩小mapreduce程序 需要扫描的数据量。</li><li>分桶表：提高join查询的效率，在一份数据会被经常用来做连接查询的时候建立分桶，分桶字段就是连接字段；提高采样的效率。</li></ul><h2 id="18-mapjoin的原理"><a href="#18-mapjoin的原理" class="headerlink" title="18.mapjoin的原理"></a>18.mapjoin的原理</h2><p>MapJoin通常用于一个很小的表和一个大表进行join的场景，具体小表有多小，由参数hive.mapjoin.smalltable.filesize来决定，该参数表示小表的总大小，默认值为25000000字节，即25M。</p><p>Hive0.7之前，需要使用hint提示 /*+ mapjoin(table) */才会执行MapJoin,否则执行Common Join，但在0.7版本之后，默认自动会转换Map Join，由参数hive.auto.convert.join来控制，默认为true.<br>假设a表为一张大表，b为小表，并且hive.auto.convert.join=true,那么Hive在执行时候会自动转化为MapJoin。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">MapJoin简单说就是在Map阶段将小表读入内存，顺序扫描大表完成Join。减少昂贵的shuffle操作及reduce操作</span><br><span class="line">MapJoin分为两个阶段：</span><br><span class="line">- 通过MapReduce Local Task，将小表读入内存，生成HashTableFiles上传至Distributed Cache中，这里会HashTableFiles进行压缩。</span><br><span class="line">- MapReduce Job在Map阶段，每个Mapper从Distributed Cache读取HashTableFiles到内存中，顺序扫描大表，在Map阶段直接进行Join，将数据传递给下一个MapReduce任务。</span><br></pre></td></tr></table></figure><h2 id="19-在hive的row-number中distribute-by-和-partition-by的区别"><a href="#19-在hive的row-number中distribute-by-和-partition-by的区别" class="headerlink" title="19.在hive的row_number中distribute by 和 partition by的区别"></a>19.在hive的row_number中distribute by 和 partition by的区别</h2><p>distribute by + sort by = cluster by </p><p>partition by + order by : partition by 分组列 order by 排序列</p><p>Partition by<br>通常查询时会对整个数据库查询，而这带来了大量的开销，因此引入了partition的概念，在建表的时候通过设置partition的字段, 会根据该字段对数据分区存放，更具体的说是存放在不同的文件夹,这样通过指定设置Partition的字段条件查询时可以减少大量的开销</p><h2 id="20-hive开发中遇到什么问题"><a href="#20-hive开发中遇到什么问题" class="headerlink" title="20.hive开发中遇到什么问题?"></a>20.hive开发中遇到什么问题?</h2><p>数据倾斜、调优</p><h2 id="21-什么时候使用内部表-什么时候使用外部表"><a href="#21-什么时候使用内部表-什么时候使用外部表" class="headerlink" title="21.什么时候使用内部表,什么时候使用外部表"></a>21.什么时候使用内部表,什么时候使用外部表</h2><p>每天收集到的ng日志和埋点日志数据,需要做大量的统计数据分析,所以可以使用外部表进行存储，方便数据的共享，并且在对表做操作的时候不会误删原始数据。</p><p>在做统计分析时候用到的中间表，结果表可以使用内部表，因为这些数据不需要共享，使用内部表更为合适。并且很多时候分区表我们只需要保留最近3天的数据，用外部表的时候删除分区时无法删除数据。</p><h2 id="22-hive都有哪些函数，你平常工作中用到哪些"><a href="#22-hive都有哪些函数，你平常工作中用到哪些" class="headerlink" title="22.hive都有哪些函数，你平常工作中用到哪些"></a>22.hive都有哪些函数，你平常工作中用到哪些</h2><ul><li><p>数学函数<br>round(DOUBLE a)</p><p>floor(DOUBLE a)</p><p>ceil(DOUBLE a)</p><p>rand()</p></li><li><p>集合函数<br>size(Map&lt;K.V&gt;)</p><p>map_keys(Map&lt;K.V&gt;)</p><p>map_values(Map&lt;K.V&gt;)</p><p>array_contains(Array<t>, value)</t></p><p>sort_array(Array<t>)</t></p></li><li><p>类型转换函数<br>cast(expr as <type>)</type></p></li><li><p>日期函数<br>date_format函数（根据格式整理日期）<br>date_add、date_sub函数（加减日期）<br>next_day函数<br>last_day函数（求当月最后一天日期）<br>collect_set函数<br>get_json_object解析json函数</p><p>from_unixtime(bigint unixtime, string format)<br>to_date(string timestamp)<br>year(string date)<br>month(string date)<br>hour(string date)<br>weekofyear(string date)<br>datediff(string enddate, string startdate)<br>add_months(string start_date, int num_months)<br>date_format(date/timestamp/string ts, string fmt)</p></li><li><p>条件函数<br>if(boolean testCondition, T valueTrue, T valueFalseOrNull)</p><p>nvl(T value, T default_value)</p><p>COALESCE(T v1, T v2, …)</p><p>CASE a WHEN b THEN c [WHEN d THEN e]* [ELSE f] END</p><p>isnull( a )</p><p>isnotnull ( a )</p></li><li><p>字符函数<br>concat(string|binary A, string|binary B…)</p><p>concat_ws(string SEP, string A, string B…)</p><p>get_json_object(string json_string, string path)</p><p>length(string A)</p><p>lower(string A) lcase(string A)</p><p>parse_url(string urlString, string partToExtract [, string keyToExtract])</p><p>regexp_replace(string INITIAL_STRING, string PATTERN, string REPLACEMENT)</p><p>reverse(string A)</p><p>split(string str, string pat)</p><p>substr(string|binary A, int start) substring(string|binary A, int start)</p></li><li><p>聚合函数<br>count  sum min max avg</p></li><li><p>表生成函数<br>explode(array<type> a)</type></p><p>explode(ARRAY)</p><p>json_tuple(jsonStr, k1, k2, …)</p><p>parse_url_tuple(url, p1, p2, …)</p></li></ul>]]></content>
      
      
      <categories>
          
          <category> bigdata </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hive </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hadoop笔记</title>
      <link href="/2020/07/02/Hadoop%E7%AC%94%E8%AE%B0/"/>
      <url>/2020/07/02/Hadoop%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<h1 id="Hadoop笔记"><a href="#Hadoop笔记" class="headerlink" title="Hadoop笔记"></a>Hadoop笔记</h1><h1 id="一-Hadoop"><a href="#一-Hadoop" class="headerlink" title="一.Hadoop"></a>一.Hadoop</h1><h2 id="1-hdfs写流程"><a href="#1-hdfs写流程" class="headerlink" title="1.hdfs写流程"></a>1.hdfs写流程</h2><p><img src="https://lixiangbetter.github.io/2020/07/02/Hadoop%E7%AC%94%E8%AE%B0/20200408194104141.png" alt></p><ol><li>客户端跟namenode通信请求上传文件，namenode检查目标文件是否已存在，父目录是否存在</li><li>namenode返回是否可以上传</li><li>client请求第一个 block该传输到哪些datanode服务器上</li><li>namenode返回3个datanode服务器ABC</li><li>client请求3台dn中的一台A上传数据（本质上是一个RPC调用，建立pipeline），A收到请求会继续调用B，然后B调用C，将真个pipeline建立完成，逐级返回客户端</li><li>client开始往A上传第一个block（先从磁盘读取数据放到一个本地内存缓存），以packet为单位，A收到一个packet就会传给B，B传给C；A每传一个packet会放入一个应答队列等待应答</li><li>当一个block传输完成之后，client再次请求namenode上传第二个block的服务器。</li></ol><h2 id="2-hdfs读流程"><a href="#2-hdfs读流程" class="headerlink" title="2.hdfs读流程"></a>2.hdfs读流程</h2><p><img src="https://lixiangbetter.github.io/2020/07/02/Hadoop%E7%AC%94%E8%AE%B0/20200408194104141.png" alt></p><ol><li>client跟namenode通信查询元数据，找到文件块所在的datanode服务器</li><li>挑选一台datanode（就近原则，然后随机）服务器，请求建立socket流</li><li>datanode开始发送数据（从磁盘里面读取数据放入流，以packet为单位来做校验）</li><li>客户端以packet为单位接收，现在本地缓存，然后写入目标文件</li></ol><h2 id="3-hdfs的体系结构"><a href="#3-hdfs的体系结构" class="headerlink" title="3.hdfs的体系结构"></a>3.hdfs的体系结构</h2><p>hdfs有namenode、secondraynamenode、datanode组成。为n+1模式</p><ol><li>NameNode负责管理和记录整个文件系统的元数据</li><li>DataNode 负责管理用户的文件数据块，文件会按照固定的大小（blocksize）切成若干块后分布式存储在若干台datanode上，每一个文件块可以有多个副本，并存放在不同的datanode上，Datanode会定期向Namenode汇报自身所保存的文件block信息，而namenode则会负责保持文件的副本数量</li><li>HDFS的内部工作机制对客户端保持透明，客户端请求访问HDFS都是通过向namenode申请来进行</li><li>secondraynamenode负责合并日志</li></ol><h2 id="4-一个datanode-宕机-怎么一个流程恢复"><a href="#4-一个datanode-宕机-怎么一个流程恢复" class="headerlink" title="4.一个datanode 宕机,怎么一个流程恢复"></a>4.一个datanode 宕机,怎么一个流程恢复</h2><p>Datanode宕机了后，如果是短暂的宕机，可以事先写好脚本监控，将它启动起来。如果是长时间宕机了，那么datanode上的数据应该已经被备份到其他机器了，那这台datanode就是一台新的datanode了，删除他的所有数据文件和状态文件，重新启动。</p><h2 id="5-hadoop-的-namenode-宕机-怎么解决"><a href="#5-hadoop-的-namenode-宕机-怎么解决" class="headerlink" title="5.hadoop 的 namenode 宕机,怎么解决"></a>5.hadoop 的 namenode 宕机,怎么解决</h2><p>先分析宕机后的损失，宕机后直接导致client无法访问，内存中的元数据丢失，但是硬盘中的元数据应该还存在，如果只是节点挂了，重启即可，如果是机器挂了，重启机器后看节点是否能重启，不能重启就要找到原因修复了。但是最终的解决方案应该是在设计集群的初期就考虑到这个问题，做namenode的HA。</p><h2 id="6-namenode对元数据的管理"><a href="#6-namenode对元数据的管理" class="headerlink" title="6.namenode对元数据的管理"></a>6.namenode对元数据的管理</h2><p>namenode对数据的管理采用了三种存储形式：</p><ul><li>内存元数据(NameSystem)</li><li>磁盘元数据镜像文件(fsimage镜像)</li><li>数据操作日志文件（可通过日志运算出元数据）(edit日志文件)</li></ul><h2 id="7-元数据的checkpoint"><a href="#7-元数据的checkpoint" class="headerlink" title="7.元数据的checkpoint"></a>7.元数据的checkpoint</h2><p>每隔一段时间，会由secondary namenode将namenode上积累的所有edits和一个最新的fsimage下载到本地，并加载到内存进行merge（这个过程称为checkpoint）</p><p><img src="https://lixiangbetter.github.io/2020/07/02/Hadoop%E7%AC%94%E8%AE%B0/20200408195459150.png" alt></p><p>namenode和secondary namenode的工作目录存储结构完全相同，所以，当namenode故障退出需要重新恢复时，可以从secondary namenode的工作目录中将fsimage拷贝到namenode的工作目录，以恢复namenode的元数据</p><h2 id="8-yarn资源调度流程"><a href="#8-yarn资源调度流程" class="headerlink" title="8.yarn资源调度流程"></a>8.yarn资源调度流程</h2><p><img src="https://lixiangbetter.github.io/2020/07/02/Hadoop%E7%AC%94%E8%AE%B0/20200408195710128.png" alt></p><ol><li>用户向YARN 中提交应用程序， 其中包括ApplicationMaster 程序、启动ApplicationMaster 的命令、用户程序等。</li><li>ResourceManager 为该应用程序分配第一个Container， 并与对应的NodeManager 通信，要求它在这个Container 中启动应用程序的ApplicationMaster。</li><li>ApplicationMaster 首先向ResourceManager 注册， 这样用户可以直接通过ResourceManager 查看应用程序的运行状态，然后它将为各个任务申请资源，并监控它的运行状态，直到运行结束，即重复步骤4~7。</li><li>ApplicationMaster 采用轮询的方式通过RPC 协议向ResourceManager 申请和领取资源。</li><li>一旦ApplicationMaster 申请到资源后，便与对应的NodeManager 通信，要求它启动任务。</li><li>NodeManager 为任务设置好运行环境（包括环境变量、JAR 包、二进制程序等）后，将任务启动命令写到一个脚本中，并通过运行该脚本启动任务。</li><li>各个任务通过某个RPC 协议向ApplicationMaster 汇报自己的状态和进度，以让ApplicationMaster 随时掌握各个任务的运行状态，从而可以在任务失败时重新启动任务。在应用程序运行过程中，用户可随时通过RPC 向ApplicationMaster 查询应用程序的当前运行状态。</li><li>应用程序运行完成后，ApplicationMaster 向ResourceManager 注销并关闭自己。</li></ol><h2 id="9-hadoop中combiner和partition的作用"><a href="#9-hadoop中combiner和partition的作用" class="headerlink" title="9.hadoop中combiner和partition的作用"></a>9.hadoop中combiner和partition的作用</h2><ul><li>combiner是发生在map的最后一个阶段，父类就是Reducer，意义就是对每一个maptask的输出进行局部汇总，以减小网络传输量，缓解网络传输瓶颈，提高reducer的执行效率。</li><li>partition的主要作用将map阶段产生的所有kv对分配给不同的reducer task处理，可以将reduce阶段的处理负载进行分摊</li></ul><h2 id="10-用mapreduce怎么处理数据倾斜问题？"><a href="#10-用mapreduce怎么处理数据倾斜问题？" class="headerlink" title="10.用mapreduce怎么处理数据倾斜问题？"></a>10.用mapreduce怎么处理数据倾斜问题？</h2><p>数据倾斜：map /reduce程序执行时，reduce节点大部分执行完毕，但是有一个或者几个reduce节点运行很慢，导致整个程序的处理时间很长，这是因为某一个key的条数比其他key多很多（有时是百倍或者千倍之多），这条key所在的reduce节点所处理的数据量比其他节点就大很多，从而导致某几个节点迟迟运行不完，此称之为数据倾斜。</p><p>（1）局部聚合加全局聚合。</p><p>第一次在 map 阶段对那些导致了数据倾斜的 key 加上 1 到 n 的随机前缀，这样本来相</p><p>同的 key 也会被分到多个 Reducer 中进行局部聚合，数量就会大大降低。</p><p>第二次 mapreduce，去掉 key 的随机前缀，进行全局聚合。</p><p>思想：二次 mr，第一次将 key 随机散列到不同 reducer 进行处理达到负载均衡目的。第</p><p>二次再根据去掉 key 的随机前缀，按原 key 进行 reduce 处理。</p><p>这个方法进行两次 mapreduce，性能稍差。</p><p>（2）增加 Reducer，提升并行度</p><p>JobConf.setNumReduceTasks(int)</p><p>（3）实现自定义分区</p><p>根据数据分布情况，自定义散列函数，将 key 均匀分配到不同 Reducer</p><h2 id="11-shuffle-阶段-你怎么理解的"><a href="#11-shuffle-阶段-你怎么理解的" class="headerlink" title="11.shuffle 阶段,你怎么理解的"></a>11.shuffle 阶段,你怎么理解的</h2><p><img src="https://lixiangbetter.github.io/2020/07/02/Hadoop%E7%AC%94%E8%AE%B0/20200408221056627.png" alt>shuffle: 洗牌、发牌——（核心机制：缓存，数据分区，排序，Merge进行局部value的合并）；</p><p>具体来说：就是将maptask输出的处理结果数据，分发给reducetask，并在分发的过程中，对数据按key进行了分区和排序；</p><p>1）Map 方法之后 Reduce 方法之前这段处理过程叫 Shuffle</p><p>2）Map 方法之后，数据首先进入到分区方法，把数据标记好分区，然后把数据发送到 环形缓冲区；环形缓冲区默认大小 100m，环形缓冲区达到 80%时，进行溢写；溢写前对数 据进行排序，排序按照对 key 的索引进行字典顺序排序，排序的手段快排；溢写产生大量溢 写文件，需要对溢写文件进行归并排序；对溢写的文件也可以进行 Combiner 操作，前提是汇总操作，求平均值不行。最后将文件按照分区存储到磁盘，等待 Reduce 端拉取。</p><p>3）每个 Reduce 拉取 Map 端对应分区的数据。拉取数据后先存储到内存中，内存不够 了，再存储到磁盘。拉取完所有数据后，采用归并排序将内存和磁盘中的数据都进行排序。</p><p>在进入 Reduce 方法前，可以对数据进行分组操作。</p><h2 id="13-MapReduce优化经验"><a href="#13-MapReduce优化经验" class="headerlink" title="13.MapReduce优化经验"></a>13.MapReduce优化经验</h2><ol><li>设置合理的map和reduce的个数。合理设置blocksize</li><li>避免出现数据倾斜</li><li>combine函数</li><li>对数据进行压缩</li><li>小文件处理优化：事先合并成大文件，combineTextInputformat，在hdfs上用mapreduce将小文件合并SequenceFile大文件（key:文件名，value：文件内容）</li><li>参数优化</li></ol><h2 id="14-分别举例什么情况要使用-combiner，什么情况不使用？"><a href="#14-分别举例什么情况要使用-combiner，什么情况不使用？" class="headerlink" title="14.分别举例什么情况要使用 combiner，什么情况不使用？"></a>14.分别举例什么情况要使用 combiner，什么情况不使用？</h2><p>求平均数的时候就不需要用combiner，因为不会减少reduce执行数量。在其他的时候，可以依据情况，使用combiner，来减少map的输出数量，减少拷贝到reduce的文件，从而减轻reduce的压力，节省网络开销，提升执行效率</p><h2 id="15-MR运行流程解析"><a href="#15-MR运行流程解析" class="headerlink" title="15.MR运行流程解析"></a>15.MR运行流程解析</h2><ol><li>一个mr程序启动的时候，最先启动的是MRAppMaster，MRAppMaster启动后根据本次job的描述信息，计算出需要的maptask实例数量，然后向集群申请机器启动相应数量的maptask进程</li><li>maptask进程启动之后，根据给定的数据切片范围进行数据处理，主体流程为：<ol><li>利用客户指定的inputformat来获取RecordReader读取数据，形成输入KV对</li><li>将输入KV对传递给客户定义的map()方法，做逻辑运算，并将map()方法输出的KV对收集到缓存</li><li>将缓存中的KV对按照K分区排序后不断溢写到磁盘文件</li></ol></li><li>MRAppMaster监控到所有maptask进程任务完成之后，会根据客户指定的参数启动相应数量的reducetask进程，并告知reducetask进程要处理的数据范围（数据分区）</li><li>Reducetask进程启动之后，根据MRAppMaster告知的待处理数据所在位置，从若干台maptask运行所在机器上获取到若干个maptask输出结果文件，并在本地进行重新归并排序，然后按照相同key的KV为一个组，调用客户定义的reduce()方法进行逻辑运算，并收集运算输出的结果KV，然后调用客户指定的outputformat将结果数据输出到外部存储</li></ol><h2 id="16-简单描述一下HDFS的系统架构，怎么保证数据安全"><a href="#16-简单描述一下HDFS的系统架构，怎么保证数据安全" class="headerlink" title="16.简单描述一下HDFS的系统架构，怎么保证数据安全"></a>16.简单描述一下HDFS的系统架构，怎么保证数据安全</h2><p><img src="https://lixiangbetter.github.io/2020/07/02/Hadoop%E7%AC%94%E8%AE%B0/aHR0cHM6Ly9pbWcyMDE4LmNuYmxvZ3MuY29tL2Jsb2cvMTU3MzQ4Mi8yMDE5MDMvMTU3MzQ4Mi0yMDE5MDMwMTEzNTkzNzE1OS03ODYwNDI2MTgucG5n.jpeg" alt></p><p>HDFS数据安全性如何保证？</p><ol><li>存储在HDFS系统上的文件，会分割成128M大小的block存储在不同的节点上，block的副本数默认3份，也可配置成更多份；</li><li>第一个副本一般放置在与client（客户端）所在的同一节点上（若客户端无datanode，则随机放），第二个副本放置到与第一个副本同一机架的不同节点，第三个副本放到不同机架的datanode节点，当取用时遵循就近原则；</li><li>datanode以block为单位，每3s报告心跳状态，做10min内不报告心跳状态则namenode认为block已死掉，namonode会把其上面的数据备份到其他一个datanode节点上，保证数据的副本数量；</li><li>datanode会默认每小时把自己节点上的所有块状态信息报告给namenode；</li><li>采用safemode模式：datanode会周期性的报告block信息。Namenode会计算block的损坏率，当阀值&lt;0.999f时系统会进入安全模式，HDFS只读不写。 HDFS元数据采用secondaryname备份或者HA备份</li></ol><h2 id="17-在通过客户端向hdfs中写数据的时候，如果某一台机器宕机了，会怎么处理"><a href="#17-在通过客户端向hdfs中写数据的时候，如果某一台机器宕机了，会怎么处理" class="headerlink" title="17.在通过客户端向hdfs中写数据的时候，如果某一台机器宕机了，会怎么处理"></a>17.在通过客户端向hdfs中写数据的时候，如果某一台机器宕机了，会怎么处理</h2><p>在写入的时候不会重新分配datanode。 如果写入时，一个datanode挂掉，会将已经写入的数据放置到queue的顶部，并将挂掉的datanode移出pipline，将数据写入到剩余的datanode，在写入结束后， namenode会收集datanode的信息，发现此文件的replication没有达到配置的要求（default=3）,然后寻找一个datanode保存副本。</p><h2 id="18-Hadoop优化有哪些方面"><a href="#18-Hadoop优化有哪些方面" class="headerlink" title="18.Hadoop优化有哪些方面"></a>18.Hadoop优化有哪些方面</h2><p>0）HDFS 小文件影响</p><p>（1）影响 NameNode 的寿命，因为文件元数据存储在 NameNode 的内存中</p><p>（2）影响计算引擎的任务数量，比如每个小的文件都会生成一个 Map 任务</p><p>1）数据输入小文件处理：</p><p>（1）合并小文件：对小文件进行归档（Har）、自定义 Inputformat 将小文件存储成SequenceFile 文件。</p><p>（2）采用 ConbinFileInputFormat 来作为输入，解决输入端大量小文件场景。</p><p>（3）对于大量小文件 Job，可以开启 JVM 重用。</p><p>2）Map 阶段</p><p>（1）增大环形缓冲区大小。由 100m 扩大到 200m</p><p>（2）增大环形缓冲区溢写的比例。由 80%扩大到 90%</p><p>（3）减少对溢写文件的 merge 次数。（10 个文件，一次 20 个 merge）</p><p>（4）不影响实际业务的前提下，采用 Combiner 提前合并，减少 I/O。</p><p>3）Reduce 阶段</p><p>（1）合理设置 Map 和 Reduce 数：两个都不能设置太少，也不能设置太多。太少，会导致 Task 等待，延长处理时间；太多，会导致 Map、Reduce 任务间竞争资源，造成处理超时等错误。</p><p>（2）设置 Map、Reduce 共存：调整 slowstart.completedmaps 参数，使 Map 运行到一定程度后，Reduce 也开始运行，减少 Reduce 的等待时间。</p><p>（3）规避使用 Reduce，因为 Reduce 在用于连接数据集的时候将会产生大量的网络消耗。</p><p>（4）增加每个 Reduce 去 Map 中拿数据的并行数</p><p>（5）集群性能可以的前提下，增大 Reduce 端存储数据内存的大小。</p><p>4）IO 传输</p><p>（1）采用数据压缩的方式，减少网络 IO 的的时间。安装 Snappy 和 LZOP 压缩编码器。</p><p>（2）使用 SequenceFile 二进制文件</p><p>5）整体</p><p>（1）MapTask 默认内存大小为 1G，可以增加 MapTask 内存大小为 4-5g</p><p>（2）ReduceTask 默认内存大小为 1G，可以增加 ReduceTask 内存大小为 4-5g</p><p>（3）可以增加 MapTask 的 cpu 核数，增加 ReduceTask 的 CPU 核数</p><p>（4）增加每个 Container 的 CPU 核数和内存大小</p><p>（5）调整每个 Map Task 和 Reduce Task 最大重试次数</p><h2 id="19-大量数据求topN-写出mapreduce的实现思路）"><a href="#19-大量数据求topN-写出mapreduce的实现思路）" class="headerlink" title="19.大量数据求topN(写出mapreduce的实现思路）"></a>19.大量数据求topN(写出mapreduce的实现思路）</h2><p><a href="https://www.eyesmoons.com/article/31" target="_blank" rel="noopener">https://www.eyesmoons.com/article/31</a></p><p>​     在最初接触mapreduce时，top n问题的解决办法是将mapreduce输出（排序后）放入一个集合中，取前n个，但这种写法过于简单，内存能够加载的集合的大小是有上限的，一旦数据量大，很容易出现内存溢出。<br>​    如果要取top 5，则应该定义一个长度为6的数组，map所要做的事情就是将每条日志的那个需要排序的字段放入数组第一个元素中，调用Arrays.sort(Array[])方法可以将数组按照正序，从数字角度说是从小到大排序，比如第一条记录是9000，那么排序结果是[0,0,0,0,0,9000]，第二条日志记录是8000，排序结果是[0,0,0,0,8000,9000]，第三条日志记录是8500，排序结果是[0,0,0,8000,8500,9000]，以此类推，每次放进去一个数字如果大于数组里面最小的元素，相当于将最小的覆盖掉了，也就是说数组中元素永远是拿到日志中最大的那些个记录<br>​    ok，map将数组原封不动按照顺序输出，reduce接收到从每个map拿到的五个排好序的元素，在进行跟map一样的排序，排序后数组里面就是按照从小到大排好序的元素，将这些元素倒序输出就是最终我们要的结果了<br>​    与之前的方式做个比较，之前的map做的事情很少，在reduce中排序后拿前5条，reduce的压力是很大的，要把所有的数据都处理一遍，而一般设置reduce的个数较少，一旦数据较多，reduce就会承受不了，悲剧了。而现在的方式巧妙的将reduce的压力转移到了map，而map是集群效应的，很多台服务器来做这件事情，减少了一台机器上的负担，每个map其实只是输出了5个元素而已，如果有5个map，其实reduce才对5*5个数据进行了操作，也就不会出现内存溢出等问题了.</p><p>​    同样的思想，这个数据结构可以是，treemap,因为treemap的底层原理是红黑树，是有序的。</p><h2 id="20-列出正常工作的hadoop集群中hadoop都分别启动哪些进程以及他们的作用"><a href="#20-列出正常工作的hadoop集群中hadoop都分别启动哪些进程以及他们的作用" class="headerlink" title="20.列出正常工作的hadoop集群中hadoop都分别启动哪些进程以及他们的作用"></a>20.列出正常工作的hadoop集群中hadoop都分别启动哪些进程以及他们的作用</h2><p>1.NameNode它是hadoop中的主服务器，管理文件系统名称空间和对集群中存储的文件的访问，保存有metadate。</p><p>2.SecondaryNameNode它不是namenode的冗余守护进程，而是提供周期检查点和清理任务。帮助NN合并editslog，减少NN启动时间。</p><p>3.DataNode它负责管理连接到节点的存储（一个集群中可以有多个节点）。每个存储数据的节点运行一个datanode守护进程。</p><p>4.ResourceManager（JobTracker）JobTracker负责调度DataNode上的工作。每个DataNode有一个TaskTracker，它们执行实际工作。</p><p>5.NodeManager（TaskTracker）执行任务</p><p>6.DFSZKFailoverController高可用时它负责监控NN的状态，并及时的把状态信息写入ZK。它通过一个独立线程周期性的调用NN上的一个特定接口来获取NN的健康状态。FC也有选择谁作为Active NN的权利，因为最多只有两个节点，目前选择策略还比较简单（先到先得，轮换）。</p><p>7.JournalNode 高可用情况下存放namenode的editlog文件.</p><h2 id="21-Hadoop总job和Tasks之间的区别是什么？"><a href="#21-Hadoop总job和Tasks之间的区别是什么？" class="headerlink" title="21.Hadoop总job和Tasks之间的区别是什么？"></a>21.Hadoop总job和Tasks之间的区别是什么？</h2><p>Job是我们对一个完整的mapreduce程序的抽象封装</p><p>Task是job运行时，每一个处理阶段的具体实例，如map task，reduce task，maptask和reduce task都会有多个并发运行的实例</p><h2 id="22-Hadoop高可用HA模式"><a href="#22-Hadoop高可用HA模式" class="headerlink" title="22.Hadoop高可用HA模式"></a>22.Hadoop高可用HA模式</h2><p>HDFS高可用原理：</p><p>Hadoop HA（High Available）通过同时配置两个处于Active/Passive模式的Namenode来解决上述问题，状态分别是Active和Standby. Standby Namenode作为热备份，从而允许在机器发生故障时能够快速进行故障转移，同时在日常维护的时候使用优雅的方式进行Namenode切换。Namenode只能配置一主一备，不能多于两个Namenode。</p><p>主Namenode处理所有的操作请求（读写），而Standby只是作为slave，维护尽可能同步的状态，使得故障时能够快速切换到Standby。为了使Standby Namenode与Active Namenode数据保持同步，两个Namenode都与一组Journal Node进行通信。当主Namenode进行任务的namespace操作时，都会确保持久会修改日志到Journal Node节点中。Standby Namenode持续监控这些edit，当监测到变化时，将这些修改同步到自己的namespace。</p><p>当进行故障转移时，Standby在成为Active Namenode之前，会确保自己已经读取了Journal Node中的所有edit日志，从而保持数据状态与故障发生前一致。</p><p>为了确保故障转移能够快速完成，Standby Namenode需要维护最新的Block位置信息，即每个Block副本存放在集群中的哪些节点上。为了达到这一点，Datanode同时配置主备两个Namenode，并同时发送Block报告和心跳到两台Namenode。</p><p>确保任何时刻只有一个Namenode处于Active状态非常重要，否则可能出现数据丢失或者数据损坏。当两台Namenode都认为自己的Active Namenode时，会同时尝试写入数据（不会再去检测和同步数据）。为了防止这种脑裂现象，Journal Nodes只允许一个Namenode写入数据，内部通过维护epoch数来控制，从而安全地进行故障转移。</p><h2 id="23-简要描述安装配置一个hadoop集群的步骤"><a href="#23-简要描述安装配置一个hadoop集群的步骤" class="headerlink" title="23.简要描述安装配置一个hadoop集群的步骤"></a>23.简要描述安装配置一个hadoop集群的步骤</h2><ol><li>使用root账户登录。</li><li>修改IP。</li><li>修改Host主机名。</li><li>配置SSH免密码登录。</li><li>关闭防火墙。</li><li>安装JDK。</li><li>上传解压Hadoop安装包。</li><li>配置Hadoop的核心配置文件hadoop-evn.sh，core-site.xml，mapred-site.xml，hdfs-site.xml，yarn-site.xml</li><li>配置hadoop环境变量</li><li>格式化hdfs # bin/hadoop  namenode  -format</li><li>启动节点start-all.sh</li></ol><h2 id="24-fsimage和edit的区别"><a href="#24-fsimage和edit的区别" class="headerlink" title="24.fsimage和edit的区别"></a>24.fsimage和edit的区别</h2><p>fsimage：filesystem image 的简写，文件镜像。</p><p>客户端修改文件时候，先更新内存中的metadata信息,只有当对文件操作成功的时候，才会写到editlog。</p><p>fsimage是文件meta信息的持久化的检查点。secondary namenode会定期的将fsimage和editlog合并dump成新的fsimage</p><h2 id="25-yarn的三大调度策略"><a href="#25-yarn的三大调度策略" class="headerlink" title="25.yarn的三大调度策略"></a>25.yarn的三大调度策略</h2><p>FIFO Scheduler把应用按提交的顺序排成一个队列，这是一个先进先出队列，在进行资源分配的时候，先给队列中最头上的应用进行分配资源，待最头上的应用需求满足后再给下一个分配，以此类推。</p><p>Capacity（容量）调度器，有一个专门的队列用来运行小任务，但是为小任务专门设置一个队列会预先占用一定的集群资源，这就导致大任务的执行时间会落后于使用FIFO调度器时的时间。</p><p>在Fair（公平）调度器中，我们不需要预先占用一定的系统资源，Fair调度器会为所有运行的job动态的调整系统资源。当第一个大job提交时，只有这一个job在运行，此时它获得了所有集群资源；当第二个小任务提交后，Fair调度器会分配一半资源给这个小任务，让这两个任务公平的共享集群资源。</p><p>  需要注意的是，在下图Fair调度器中，从第二个任务提交到获得资源会有一定的延迟，因为它需要等待第一个任务释放占用的Container。小任务执行完成之后也会释放自己占用的资源，大任务又获得了全部的系统资源。最终的效果就是Fair调度器即得到了高的资源利用率又能保证小任务及时完成。</p><h2 id="26-hadoop的shell命令用的多吗-说出一些常用的"><a href="#26-hadoop的shell命令用的多吗-说出一些常用的" class="headerlink" title="26.hadoop的shell命令用的多吗?,说出一些常用的"></a>26.hadoop的shell命令用的多吗?,说出一些常用的</h2><p>hadoop  fs  -ls   /</p><p>hadoop fs  -mkdir  /aaa</p><p>hadoop fs -put /root/2.txt /haha/</p><p>hadoop fs -cat /haha/2.txt</p><p>hadoop fs -get /haha/2.txt </p><p>hadoop fs -rm -r -f /haha</p>]]></content>
      
      
      <categories>
          
          <category> bigdata </category>
          
      </categories>
      
      
        <tags>
            
            <tag> hadoop </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MySQL数据库笔记</title>
      <link href="/2020/07/01/MySQL%E6%95%B0%E6%8D%AE%E5%BA%93%E7%AC%94%E8%AE%B0/"/>
      <url>/2020/07/01/MySQL%E6%95%B0%E6%8D%AE%E5%BA%93%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<h1 id="MySQL数据库笔记"><a href="#MySQL数据库笔记" class="headerlink" title="MySQL数据库笔记"></a>MySQL数据库笔记</h1><h2 id="数据库基础知识"><a href="#数据库基础知识" class="headerlink" title="数据库基础知识"></a>数据库基础知识</h2><h3 id="为什么要使用数据库"><a href="#为什么要使用数据库" class="headerlink" title="为什么要使用数据库"></a>为什么要使用数据库</h3><p><strong>数据保存在内存</strong></p><p>优点： 存取速度快</p><p>缺点： 数据不能永久保存</p><p><strong>数据保存在文件</strong></p><p>优点： 数据永久保存</p><p>缺点：1）速度比内存操作慢，频繁的IO操作。2）查询数据不方便</p><p><strong>数据保存在数据库</strong></p><p>1）数据永久保存</p><p>2）使用SQL语句，查询方便效率高。</p><p>3）管理数据方便</p><h3 id="什么是SQL？"><a href="#什么是SQL？" class="headerlink" title="什么是SQL？"></a>什么是SQL？</h3><p>结构化查询语言(Structured Query Language)简称SQL，是一种数据库查询语言。</p><p>作用：用于存取数据、查询、更新和管理关系数据库系统。</p><h3 id="什么是MySQL"><a href="#什么是MySQL" class="headerlink" title="什么是MySQL?"></a>什么是MySQL?</h3><p>MySQL是一个关系型数据库管理系统，由瑞典MySQL AB 公司开发，属于 Oracle 旗下产品。MySQL 是最流行的关系型数据库管理系统之一，在 WEB 应用方面，MySQL是最好的 RDBMS (Relational Database Management System，关系数据库管理系统) 应用软件之一。在Java企业级开发中非常常用，因为 MySQL 是开源免费的，并且方便扩展。</p><h3 id="数据库三大范式是什么"><a href="#数据库三大范式是什么" class="headerlink" title="数据库三大范式是什么"></a>数据库三大范式是什么</h3><p>第一范式：每个列都不可以再拆分。</p><p>第二范式：在第一范式的基础上，非主键列完全依赖于主键，而不能是依赖于主键的一部分。</p><p>第三范式：在第二范式的基础上，非主键列只依赖于主键，不依赖于其他非主键。</p><p>在设计数据库结构的时候，要尽量遵守三范式，如果不遵守，必须有足够的理由。比如性能。事实上我们经常会为了性能而妥协数据库的设计。</p><h3 id="mysql有关权限的表都有哪几个"><a href="#mysql有关权限的表都有哪几个" class="headerlink" title="mysql有关权限的表都有哪几个"></a>mysql有关权限的表都有哪几个</h3><p>MySQL服务器通过权限表来控制用户对数据库的访问，权限表存放在mysql数据库里，由mysql_install_db脚本初始化。这些权限表分别user，db，table_priv，columns_priv和host。下面分别介绍一下这些表的结构和内容：</p><ul><li>user权限表：记录允许连接到服务器的用户帐号信息，里面的权限是全局级的。</li><li>db权限表：记录各个帐号在各个数据库上的操作权限。</li><li>table_priv权限表：记录数据表级的操作权限。</li><li>columns_priv权限表：记录数据列级的操作权限。</li><li>host权限表：配合db权限表对给定主机上数据库级操作权限作更细致的控制。这个权限表不受GRANT和REVOKE语句的影响。</li></ul><h3 id="MySQL的binlog有有几种录入格式？分别有什么区别？"><a href="#MySQL的binlog有有几种录入格式？分别有什么区别？" class="headerlink" title="MySQL的binlog有有几种录入格式？分别有什么区别？"></a>MySQL的binlog有有几种录入格式？分别有什么区别？</h3><p>有三种格式，statement，row和mixed。</p><ul><li>statement模式下，每一条会修改数据的sql都会记录在binlog中。不需要记录每一行的变化，减少了binlog日志量，节约了IO，提高性能。由于sql的执行是有上下文的，因此在保存的时候需要保存相关的信息，同时还有一些使用了函数之类的语句无法被记录复制。</li><li>row级别下，不记录sql语句上下文相关信息，仅保存哪条记录被修改。记录单元为每一行的改动，基本是可以全部记下来但是由于很多操作，会导致大量行的改动(比如alter table)，因此这种模式的文件保存的信息太多，日志量太大。</li><li>mixed，一种折中的方案，普通操作使用statement记录，当无法使用statement的时候使用row。</li></ul><p>此外，新版的MySQL中对row级别也做了一些优化，当表结构发生变化的时候，会记录语句而不是逐行记录。</p><h2 id="数据类型"><a href="#数据类型" class="headerlink" title="数据类型"></a>数据类型</h2><h3 id="mysql有哪些数据类型"><a href="#mysql有哪些数据类型" class="headerlink" title="mysql有哪些数据类型"></a>mysql有哪些数据类型</h3><p>1、整数类型，包括TINYINT、SMALLINT、MEDIUMINT、INT、BIGINT，分别表示1字节、2字节、3字节、4字节、8字节整数。任何整数类型都可以加上UNSIGNED属性，表示数据是无符号的，即非负整数。<br>长度：整数类型可以被指定长度，例如：INT(11)表示长度为11的INT类型。长度在大多数场景是没有意义的，它不会限制值的合法范围，只会影响显示字符的个数，而且需要和UNSIGNED ZEROFILL属性配合使用才有意义。<br>例子，假定类型设定为INT(5)，属性为UNSIGNED ZEROFILL，如果用户插入的数据为12的话，那么数据库实际存储数据为00012。</p><p>2、实数类型，包括FLOAT、DOUBLE、DECIMAL。<br>DECIMAL可以用于存储比BIGINT还大的整型，能存储精确的小数。<br>而FLOAT和DOUBLE是有取值范围的，并支持使用标准的浮点进行近似计算。<br>计算时FLOAT和DOUBLE相比DECIMAL效率更高一些，DECIMAL你可以理解成是用字符串进行处理。</p><p>3、字符串类型，包括VARCHAR、CHAR、TEXT、BLOB<br>VARCHAR用于存储可变长字符串，它比定长类型更节省空间。<br>VARCHAR使用额外1或2个字节存储字符串长度。列长度小于255字节时，使用1字节表示，否则使用2字节表示。<br>VARCHAR存储的内容超出设置的长度时，内容会被截断。<br>CHAR是定长的，根据定义的字符串长度分配足够的空间。<br>CHAR会根据需要使用空格进行填充方便比较。<br>CHAR适合存储很短的字符串，或者所有值都接近同一个长度。<br>CHAR存储的内容超出设置的长度时，内容同样会被截断。</p><p>使用策略：<br>对于经常变更的数据来说，CHAR比VARCHAR更好，因为CHAR不容易产生碎片。<br>对于非常短的列，CHAR比VARCHAR在存储空间上更有效率。<br>使用时要注意只分配需要的空间，更长的列排序时会消耗更多内存。<br>尽量避免使用TEXT/BLOB类型，查询时会使用临时表，导致严重的性能开销。</p><p>4、枚举类型（ENUM），把不重复的数据存储为一个预定义的集合。<br>有时可以使用ENUM代替常用的字符串类型。<br>ENUM存储非常紧凑，会把列表值压缩到一个或两个字节。<br>ENUM在内部存储时，其实存的是整数。<br>尽量避免使用数字作为ENUM枚举的常量，因为容易混乱。<br>排序是按照内部存储的整数</p><p>5、日期和时间类型，尽量使用timestamp，空间效率高于datetime，<br>用整数保存时间戳通常不方便处理。<br>如果需要存储微妙，可以使用bigint存储。<br>看到这里，这道真题是不是就比较容易回答了。</p><h2 id="引擎"><a href="#引擎" class="headerlink" title="引擎"></a>引擎</h2><h3 id="MySQL存储引擎MyISAM与InnoDB区别"><a href="#MySQL存储引擎MyISAM与InnoDB区别" class="headerlink" title="MySQL存储引擎MyISAM与InnoDB区别"></a>MySQL存储引擎MyISAM与InnoDB区别</h3><p>存储引擎Storage engine：MySQL中的数据、索引以及其他对象是如何存储的，是一套文件系统的实现。</p><p>常用的存储引擎有以下：</p><ul><li>Innodb引擎：Innodb引擎提供了对数据库ACID事务的支持。并且还提供了行级锁和外键的约束。它的设计的目标就是处理大数据容量的数据库系统。</li><li>MyIASM引擎(原本Mysql的默认引擎)：不提供事务的支持，也不支持行级锁和外键。</li><li>MEMORY引擎：所有的数据都在内存中，数据的处理速度快，但是安全性不高。</li></ul><h3 id="MyISAM索引与InnoDB索引的区别？"><a href="#MyISAM索引与InnoDB索引的区别？" class="headerlink" title="MyISAM索引与InnoDB索引的区别？"></a>MyISAM索引与InnoDB索引的区别？</h3><ul><li>InnoDB索引是聚簇索引，MyISAM索引是非聚簇索引。</li><li>InnoDB的主键索引的叶子节点存储着行数据，因此主键索引非常高效。</li><li>MyISAM索引的叶子节点存储的是行数据地址，需要再寻址一次才能得到数据。</li><li>InnoDB非主键索引的叶子节点存储的是主键和其他带索引的列数据，因此查询时做到覆盖索引会非常高效。</li></ul><h3 id="InnoDB引擎的4大特性"><a href="#InnoDB引擎的4大特性" class="headerlink" title="InnoDB引擎的4大特性"></a>InnoDB引擎的4大特性</h3><ul><li>插入缓冲（insert buffer)</li><li>二次写(double write)</li><li>自适应哈希索引(ahi)</li><li>预读(read ahead)</li></ul><h3 id="存储引擎选择"><a href="#存储引擎选择" class="headerlink" title="存储引擎选择"></a>存储引擎选择</h3><p>如果没有特别的需求，使用默认的<code>Innodb</code>即可。</p><p>MyISAM：以读写插入为主的应用程序，比如博客系统、新闻门户网站。</p><p>Innodb：更新（删除）操作频率也高，或者要保证数据的完整性；并发量高，支持事务和外键。比如OA自动化办公系统。</p><h2 id="索引"><a href="#索引" class="headerlink" title="索引"></a>索引</h2><h3 id="什么是索引？"><a href="#什么是索引？" class="headerlink" title="什么是索引？"></a>什么是索引？</h3><p>索引是一种特殊的文件(InnoDB数据表上的索引是表空间的一个组成部分)，它们包含着对数据表里所有记录的引用指针。</p><p>索引是一种数据结构。数据库索引，是数据库管理系统中一个排序的数据结构，以协助快速查询、更新数据库表中数据。索引的实现通常使用B树及其变种B+树。</p><p>更通俗的说，索引就相当于目录。为了方便查找书中的内容，通过对内容建立索引形成目录。索引是一个文件，它是要占据物理空间的。</p><h3 id="索引有哪些优缺点？"><a href="#索引有哪些优缺点？" class="headerlink" title="索引有哪些优缺点？"></a>索引有哪些优缺点？</h3><p>索引的优点</p><ul><li>可以大大加快数据的检索速度，这也是创建索引的最主要的原因。</li><li>通过使用索引，可以在查询的过程中，使用优化隐藏器，提高系统的性能。</li></ul><p>索引的缺点</p><ul><li>时间方面：创建索引和维护索引要耗费时间，具体地，当对表中的数据进行增加、删除和修改的时候，索引也要动态的维护，会降低增/改/删的执行效率；</li><li>空间方面：索引需要占物理空间。</li></ul><h3 id="索引使用场景（重点）"><a href="#索引使用场景（重点）" class="headerlink" title="索引使用场景（重点）"></a>索引使用场景（重点）</h3><p>where</p><p>order by</p><p>join</p><p>索引覆盖</p><p>如果要查询的字段都建立过索引，那么引擎会直接在索引表中查询而不会访问原始数据（否则只要有一个字段没有建立索引就会做全表扫描），这叫索引覆盖。因此我们需要尽可能的在select后只写必要的查询字段，以增加索引覆盖的几率。</p><p>这里值得注意的是不要想着为每个字段建立索引，因为优先使用索引的优势就在于其体积小。</p><h3 id="索引有哪几种类型？"><a href="#索引有哪几种类型？" class="headerlink" title="索引有哪几种类型？"></a>索引有哪几种类型？</h3><p>主键索引: 数据列不允许重复，不允许为NULL，一个表只能有一个主键。</p><p>唯一索引: 数据列不允许重复，允许为NULL值，一个表允许多个列创建唯一索引。</p><p>可以通过 ALTER TABLE table_name ADD UNIQUE (column); 创建唯一索引</p><p>可以通过 ALTER TABLE table_name ADD UNIQUE (column1,column2); 创建唯一组合索引</p><p>普通索引: 基本的索引类型，没有唯一性的限制，允许为NULL值。</p><p>可以通过ALTER TABLE table_name ADD INDEX index_name (column);创建普通索引</p><p>可以通过ALTER TABLE table_name ADD INDEX index_name(column1, column2, column3);创建组合索引</p><p>全文索引： 是目前搜索引擎使用的一种关键技术。</p><p>可以通过ALTER TABLE table_name ADD FULLTEXT (column);创建全文索引</p><h3 id="创建索引的原则（重中之重）"><a href="#创建索引的原则（重中之重）" class="headerlink" title="创建索引的原则（重中之重）"></a>创建索引的原则（重中之重）</h3><p>索引虽好，但也不是无限制的使用，最好符合一下几个原则</p><p>1） 最左前缀匹配原则，组合索引非常重要的原则，mysql会一直向右匹配直到遇到范围查询(&gt;、&lt;、between、like)就停止匹配，比如a = 1 and b = 2 and c &gt; 3 and d = 4 如果建立(a,b,c,d)顺序的索引，d是用不到索引的，如果建立(a,b,d,c)的索引则都可以用到，a,b,d的顺序可以任意调整。</p><p>2）较频繁作为查询条件的字段才去创建索引</p><p>3）更新频繁字段不适合创建索引</p><p>4）若是不能有效区分数据的列不适合做索引列(如性别，男女未知，最多也就三种，区分度实在太低)</p><p>5）尽量的扩展索引，不要新建索引。比如表中已经有a的索引，现在要加(a,b)的索引，那么只需要修改原来的索引即可。</p><p>6）定义有外键的数据列一定要建立索引。</p><p>7）对于那些查询中很少涉及的列，重复值比较多的列不要建立索引。</p><p>8）对于定义为text、image和bit的数据类型的列不要建立索引。</p><h3 id="什么是聚簇索引？何时使用聚簇索引与非聚簇索引"><a href="#什么是聚簇索引？何时使用聚簇索引与非聚簇索引" class="headerlink" title="什么是聚簇索引？何时使用聚簇索引与非聚簇索引"></a>什么是聚簇索引？何时使用聚簇索引与非聚簇索引</h3><ul><li>聚簇索引：将数据存储与索引放到了一块，找到索引也就找到了数据</li><li>非聚簇索引：将数据存储于索引分开结构，索引结构的叶子节点指向了数据的对应行，myisam通过key_buffer把索引先缓存到内存中，当需要访问数据时（通过索引访问数据），在内存中直接搜索索引，然后通过索引找到磁盘相应数据，这也就是为什么索引不在key buffer命中时，速度慢的原因</li></ul><p>澄清一个概念：innodb中，在聚簇索引之上创建的索引称之为辅助索引，辅助索引访问数据总是需要二次查找，非聚簇索引都是辅助索引，像复合索引、前缀索引、唯一索引，辅助索引叶子节点存储的不再是行的物理位置，而是主键值</p><p>何时使用聚簇索引与非聚簇索引</p><p><img src="https://lixiangbetter.github.io/2020/07/01/MySQL%E6%95%B0%E6%8D%AE%E5%BA%93%E7%AC%94%E8%AE%B0/aHR0cHM6Ly91cGxvYWQtaW1hZ2VzLmppYW5zaHUuaW8vdXBsb2FkX2ltYWdlcy8xMDE1NDQ5OS1kNTNhNWNlOWNlY2YyMmYzLnBuZw.jpeg" alt></p><h3 id="非聚簇索引一定会回表查询吗？"><a href="#非聚簇索引一定会回表查询吗？" class="headerlink" title="非聚簇索引一定会回表查询吗？"></a>非聚簇索引一定会回表查询吗？</h3><p>不一定，这涉及到查询语句所要求的字段是否全部命中了索引，如果全部命中了索引，那么就不必再进行回表查询。</p><p>举个简单的例子，假设我们在员工表的年龄上建立了索引，那么当进行select age from employee where age &lt; 20的查询时，在索引的叶子节点上，已经包含了age信息，不会再次进行回表查询。</p><h3 id="联合索引是什么？为什么需要注意联合索引中的顺序？"><a href="#联合索引是什么？为什么需要注意联合索引中的顺序？" class="headerlink" title="联合索引是什么？为什么需要注意联合索引中的顺序？"></a>联合索引是什么？为什么需要注意联合索引中的顺序？</h3><p>MySQL可以使用多个字段同时建立一个索引，叫做联合索引。在联合索引中，如果想要命中索引，需要按照建立索引时的字段顺序挨个使用，否则无法命中索引。</p><p>具体原因为:</p><p>MySQL使用索引时需要索引有序，假设现在建立了”name，age，school”的联合索引，那么索引的排序为: 先按照name排序，如果name相同，则按照age排序，如果age的值也相等，则按照school进行排序。</p><p>当进行查询时，此时索引仅仅按照name严格有序，因此必须首先使用name字段进行等值查询，之后对于匹配到的列而言，其按照age字段严格有序，此时可以使用age字段用做索引查找，以此类推。因此在建立联合索引的时候应该注意索引列的顺序，一般情况下，将查询需求频繁或者字段选择性高的列放在前面。此外可以根据特例的查询或者表结构进行单独的调整。</p><h2 id="事务"><a href="#事务" class="headerlink" title="事务"></a>事务</h2><h3 id="什么是数据库事务？"><a href="#什么是数据库事务？" class="headerlink" title="什么是数据库事务？"></a>什么是数据库事务？</h3><p>事务是一个不可分割的数据库操作序列，也是数据库并发控制的基本单位，其执行的结果必须使数据库从一种一致性状态变到另一种一致性状态。事务是逻辑上的一组操作，要么都执行，要么都不执行。</p><h3 id="事物的四大特性-ACID-介绍一下"><a href="#事物的四大特性-ACID-介绍一下" class="headerlink" title="事物的四大特性(ACID)介绍一下?"></a>事物的四大特性(ACID)介绍一下?</h3><p>原子性： 事务是最小的执行单位，不允许分割。事务的原子性确保动作要么全部完成，要么完全不起作用；<br>一致性： 执行事务前后，数据保持一致，多个事务对同一个数据读取的结果是相同的；<br>隔离性： 并发访问数据库时，一个用户的事务不被其他事务所干扰，各并发事务之间数据库是独立的；<br>持久性： 一个事务被提交之后。它对数据库中数据的改变是持久的，即使数据库发生故障也不应该对其有任何影响。</p><h3 id="什么是脏读？幻读？不可重复读？"><a href="#什么是脏读？幻读？不可重复读？" class="headerlink" title="什么是脏读？幻读？不可重复读？"></a>什么是脏读？幻读？不可重复读？</h3><ul><li>脏读(Drity Read)：某个事务已更新一份数据，另一个事务在此时读取了同一份数据，由于某些原因，前一个RollBack了操作，则后一个事务所读取的数据就会是不正确的。</li><li>不可重复读(Non-repeatable read):在一个事务的两次查询之中数据不一致，这可能是两次查询过程中间插入了一个事务更新的原有的数据。</li><li>幻读(Phantom Read):在一个事务的两次查询中数据笔数不一致，例如有一个事务查询了几列(Row)数据，而另一个事务却在此时插入了新的几列数据，先前的事务在接下来的查询中，就会发现有几列数据是它先前所没有的。</li></ul><h3 id="什么是事务的隔离级别？MySQL的默认隔离级别是什么？"><a href="#什么是事务的隔离级别？MySQL的默认隔离级别是什么？" class="headerlink" title="什么是事务的隔离级别？MySQL的默认隔离级别是什么？"></a>什么是事务的隔离级别？MySQL的默认隔离级别是什么？</h3><p>SQL 标准定义了四个隔离级别：</p><ul><li>READ-UNCOMMITTED(读取未提交)： 最低的隔离级别，允许读取尚未提交的数据变更，可能会导致脏读、幻读或不可重复读。</li><li>READ-COMMITTED(读取已提交)： 允许读取并发事务已经提交的数据，可以阻止脏读，但是幻读或不可重复读仍有可能发生。</li><li>REPEATABLE-READ(可重复读)： 对同一字段的多次读取结果都是一致的，除非数据是被本身事务自己所修改，可以阻止脏读和不可重复读，但幻读仍有可能发生。</li><li>SERIALIZABLE(可串行化)： 最高的隔离级别，完全服从ACID的隔离级别。所有的事务依次逐个执行，这样事务之间就完全不可能产生干扰，也就是说，该级别可以防止脏读、不可重复读以及幻读。</li></ul><h2 id="锁"><a href="#锁" class="headerlink" title="锁"></a>锁</h2><h3 id="对MySQL的锁了解吗"><a href="#对MySQL的锁了解吗" class="headerlink" title="对MySQL的锁了解吗"></a>对MySQL的锁了解吗</h3><p>当数据库有并发事务的时候，可能会产生数据的不一致，这时候需要一些机制来保证访问的次序，锁机制就是这样的一个机制。</p><h3 id="隔离级别与锁的关系"><a href="#隔离级别与锁的关系" class="headerlink" title="隔离级别与锁的关系"></a>隔离级别与锁的关系</h3><p>在Read Uncommitted级别下，读取数据不需要加共享锁，这样就不会跟被修改的数据上的排他锁冲突</p><p>在Read Committed级别下，读操作需要加共享锁，但是在语句执行完以后释放共享锁；</p><p>在Repeatable Read级别下，读操作需要加共享锁，但是在事务提交之前并不释放共享锁，也就是必须等待事务执行完毕以后才释放共享锁。</p><p>SERIALIZABLE 是限制性最强的隔离级别，因为该级别锁定整个范围的键，并一直持有锁，直到事务完成。</p><h3 id="按照锁的粒度分数据库锁有哪些？锁机制与InnoDB锁算法"><a href="#按照锁的粒度分数据库锁有哪些？锁机制与InnoDB锁算法" class="headerlink" title="按照锁的粒度分数据库锁有哪些？锁机制与InnoDB锁算法"></a>按照锁的粒度分数据库锁有哪些？锁机制与InnoDB锁算法</h3><p>在关系型数据库中，可以<strong>按照锁的粒度把数据库锁分</strong>为行级锁(INNODB引擎)、表级锁(MYISAM引擎)和页级锁(BDB引擎 )。</p><p><strong>MyISAM和InnoDB存储引擎使用的锁：</strong></p><ul><li>MyISAM采用表级锁(table-level locking)。</li><li>InnoDB支持行级锁(row-level locking)和表级锁，默认为行级锁</li></ul><p>行级锁，表级锁和页级锁对比</p><p>行级锁 行级锁是Mysql中锁定粒度最细的一种锁，表示只针对当前操作的行进行加锁。行级锁能大大减少数据库操作的冲突。其加锁粒度最小，但加锁的开销也最大。行级锁分为共享锁 和 排他锁。</p><p>特点：开销大，加锁慢；会出现死锁；锁定粒度最小，发生锁冲突的概率最低，并发度也最高。</p><p>表级锁 表级锁是MySQL中锁定粒度最大的一种锁，表示对当前操作的整张表加锁，它实现简单，资源消耗较少，被大部分MySQL引擎支持。最常使用的MYISAM与INNODB都支持表级锁定。表级锁定分为表共享读锁（共享锁）与表独占写锁（排他锁）。</p><p>特点：开销小，加锁快；不会出现死锁；锁定粒度大，发出锁冲突的概率最高，并发度最低。</p><p>页级锁 页级锁是MySQL中锁定粒度介于行级锁和表级锁中间的一种锁。表级锁速度快，但冲突多，行级冲突少，但速度慢。所以取了折衷的页级，一次锁定相邻的一组记录。</p><p>特点：开销和加锁时间界于表锁和行锁之间；会出现死锁；锁定粒度界于表锁和行锁之间，并发度一般</p><h3 id="从锁的类别上分MySQL都有哪些锁呢？像上面那样子进行锁定岂不是有点阻碍并发效率了"><a href="#从锁的类别上分MySQL都有哪些锁呢？像上面那样子进行锁定岂不是有点阻碍并发效率了" class="headerlink" title="从锁的类别上分MySQL都有哪些锁呢？像上面那样子进行锁定岂不是有点阻碍并发效率了"></a>从锁的类别上分MySQL都有哪些锁呢？像上面那样子进行锁定岂不是有点阻碍并发效率了</h3><p>从锁的类别上来讲，有共享锁和排他锁。</p><p>共享锁: 又叫做读锁。 当用户要进行数据的读取时，对数据加上共享锁。共享锁可以同时加上多个。</p><p>排他锁: 又叫做写锁。 当用户要进行数据的写入时，对数据加上排他锁。排他锁只可以加一个，他和其他的排他锁，共享锁都相斥。</p><p>用上面的例子来说就是用户的行为有两种，一种是来看房，多个用户一起看房是可以接受的。 一种是真正的入住一晚，在这期间，无论是想入住的还是想看房的都不可以。</p><p>锁的粒度取决于具体的存储引擎，InnoDB实现了行级锁，页级锁，表级锁。</p><p>他们的加锁开销从大到小，并发能力也是从大到小。</p><h3 id="MySQL中InnoDB引擎的行锁是怎么实现的？"><a href="#MySQL中InnoDB引擎的行锁是怎么实现的？" class="headerlink" title="MySQL中InnoDB引擎的行锁是怎么实现的？"></a>MySQL中InnoDB引擎的行锁是怎么实现的？</h3><p>InnoDB是基于索引来完成行锁</p><p>例: select * from tab_with_index where id = 1 for update;</p><p>for update 可以根据条件来完成行锁锁定，并且 id 是有索引键的列，如果 id 不是索引键那么InnoDB将完成表锁，并发将无从谈起</p><h3 id="InnoDB存储引擎的锁的算法有三种"><a href="#InnoDB存储引擎的锁的算法有三种" class="headerlink" title="InnoDB存储引擎的锁的算法有三种"></a>InnoDB存储引擎的锁的算法有三种</h3><ul><li>Record lock：单个行记录上的锁</li><li>Gap lock：间隙锁，锁定一个范围，不包括记录本身</li><li>Next-key lock：record+gap 锁定一个范围，包含记录本身</li></ul><p>相关知识点：</p><ul><li>innodb对于行的查询使用next-key lock</li><li>Next-locking keying为了解决Phantom Problem幻读问题</li><li>当查询的索引含有唯一属性时，将next-key lock降级为record key</li><li>Gap锁设计的目的是为了阻止多个事务将记录插入到同一范围内，而这会导致幻读问题的产生</li><li>有两种方式显式关闭gap锁：（除了外键约束和唯一性检查外，其余情况仅使用record lock） A. 将事务隔离级别设置为RC B. 将参数innodb_locks_unsafe_for_binlog设置为1</li></ul><h3 id="什么是死锁？怎么解决？"><a href="#什么是死锁？怎么解决？" class="headerlink" title="什么是死锁？怎么解决？"></a>什么是死锁？怎么解决？</h3><p>死锁是指两个或多个事务在同一资源上相互占用，并请求锁定对方的资源，从而导致恶性循环的现象。</p><p>常见的解决死锁的方法</p><p>1、如果不同程序会并发存取多个表，尽量约定以相同的顺序访问表，可以大大降低死锁机会。</p><p>2、在同一个事务中，尽可能做到一次锁定所需要的所有资源，减少死锁产生概率；</p><p>3、对于非常容易产生死锁的业务部分，可以尝试使用升级锁定颗粒度，通过表级锁定来减少死锁产生的概率；</p><p>如果业务处理不好可以用分布式事务锁或者使用乐观锁</p><h3 id="数据库的乐观锁和悲观锁是什么？怎么实现的？"><a href="#数据库的乐观锁和悲观锁是什么？怎么实现的？" class="headerlink" title="数据库的乐观锁和悲观锁是什么？怎么实现的？"></a>数据库的乐观锁和悲观锁是什么？怎么实现的？</h3><p>数据库管理系统（DBMS）中的并发控制的任务是确保在多个事务同时存取数据库中同一数据时不破坏事务的隔离性和统一性以及数据库的统一性。乐观并发控制（乐观锁）和悲观并发控制（悲观锁）是并发控制主要采用的技术手段。</p><p>悲观锁：假定会发生并发冲突，屏蔽一切可能违反数据完整性的操作。在查询完数据的时候就把事务锁起来，直到提交事务。实现方式：使用数据库中的锁机制</p><p>乐观锁：假设不会发生并发冲突，只在提交操作时检查是否违反数据完整性。在修改数据的时候把事务锁起来，通过version的方式来进行锁定。实现方式：乐一般会使用版本号机制或CAS算法实现。</p><p>两种锁的使用场景</p><p>从上面对两种锁的介绍，我们知道两种锁各有优缺点，不可认为一种好于另一种，像乐观锁适用于写比较少的情况下（多读场景），即冲突真的很少发生的时候，这样可以省去了锁的开销，加大了系统的整个吞吐量。</p><p>但如果是多写的情况，一般会经常产生冲突，这就会导致上层应用会不断的进行retry，这样反倒是降低了性能，所以一般多写的场景下用悲观锁就比较合适。</p><h2 id="视图"><a href="#视图" class="headerlink" title="视图"></a>视图</h2><h3 id="为什么要使用视图？什么是视图？"><a href="#为什么要使用视图？什么是视图？" class="headerlink" title="为什么要使用视图？什么是视图？"></a>为什么要使用视图？什么是视图？</h3><p>为了提高复杂SQL语句的复用性和表操作的安全性，MySQL数据库管理系统提供了视图特性。所谓视图，本质上是一种虚拟表，在物理上是不存在的，其内容与真实的表相似，包含一系列带有名称的列和行数据。但是，视图并不在数据库中以储存的数据值形式存在。行和列数据来自定义视图的查询所引用基本表，并且在具体引用视图时动态生成。</p><p>视图使开发者只关心感兴趣的某些特定数据和所负责的特定任务，只能看到视图中所定义的数据，而不是视图所引用表中的数据，从而提高了数据库中数据的安全性。</p><h3 id="视图有哪些特点？"><a href="#视图有哪些特点？" class="headerlink" title="视图有哪些特点？"></a>视图有哪些特点？</h3><p>视图的特点如下:</p><ul><li>视图的列可以来自不同的表，是表的抽象和在逻辑意义上建立的新关系。</li><li>视图是由基本表(实表)产生的表(虚表)。</li><li>视图的建立和删除不影响基本表。</li><li>对视图内容的更新(添加，删除和修改)直接影响基本表。</li><li>当视图来自多个基本表时，不允许添加和删除数据。</li></ul><p>视图的操作包括创建视图，查看视图，删除视图和修改视图。</p><h3 id="视图的使用场景有哪些？"><a href="#视图的使用场景有哪些？" class="headerlink" title="视图的使用场景有哪些？"></a>视图的使用场景有哪些？</h3><p>视图根本用途：简化sql查询，提高开发效率。如果说还有另外一个用途那就是兼容老的表结构。</p><p>下面是视图的常见使用场景：</p><ul><li>重用SQL语句；</li><li>简化复杂的SQL操作。在编写查询后，可以方便的重用它而不必知道它的基本查询细节；</li><li>使用表的组成部分而不是整个表；</li><li>保护数据。可以给用户授予表的特定部分的访问权限而不是整个表的访问权限；</li><li>更改数据格式和表示。视图可返回与底层表的表示和格式不同的数据。</li></ul><h3 id="视图的优点"><a href="#视图的优点" class="headerlink" title="视图的优点"></a>视图的优点</h3><p>查询简单化。视图能简化用户的操作<br>数据安全性。视图使用户能以多种角度看待同一数据，能够对机密数据提供安全保护<br>逻辑数据独立性。视图对重构数据库提供了一定程度的逻辑独立性</p><h3 id="视图的缺点"><a href="#视图的缺点" class="headerlink" title="视图的缺点"></a>视图的缺点</h3><ol><li><p>性能。数据库必须把视图的查询转化成对基本表的查询，如果这个视图是由一个复杂的多表查询所定义，那么，即使是视图的一个简单查询，数据库也把它变成一个复杂的结合体，需要花费一定的时间。</p></li><li><p>修改限制。当用户试图修改视图的某些行时，数据库必须把它转化为对基本表的某些行的修改。事实上，当从视图中插入或者删除时，情况也是这样。对于简单视图来说，这是很方便的，但是，对于比较复杂的视图，可能是不可修改的</p><p>这些视图有如下特征：1.有UNIQUE等集合操作符的视图。2.有GROUP BY子句的视图。3.有诸如AVG\SUM\MAX等聚合函数的视图。 4.使用DISTINCT关键字的视图。5.连接表的视图（其中有些例外）</p></li></ol><h3 id="什么是游标？"><a href="#什么是游标？" class="headerlink" title="什么是游标？"></a>什么是游标？</h3><p>游标是系统为用户开设的一个数据缓冲区，存放SQL语句的执行结果，每个游标区都有一个名字。用户可以通过游标逐一获取记录并赋给主变量，交由主语言进一步处理。</p><h2 id="存储过程与函数"><a href="#存储过程与函数" class="headerlink" title="存储过程与函数"></a>存储过程与函数</h2><h3 id="什么是存储过程？有哪些优缺点？"><a href="#什么是存储过程？有哪些优缺点？" class="headerlink" title="什么是存储过程？有哪些优缺点？"></a>什么是存储过程？有哪些优缺点？</h3><p>存储过程是一个预编译的SQL语句，优点是允许模块化的设计，就是说只需要创建一次，以后在该程序中就可以调用多次。如果某次操作需要执行多次SQL，使用存储过程比单纯SQL语句执行要快。</p><p>优点</p><p>1）存储过程是预编译过的，执行效率高。</p><p>2）存储过程的代码直接存放于数据库中，通过存储过程名直接调用，减少网络通讯。</p><p>3）安全性高，执行存储过程需要有一定权限的用户。</p><p>4）存储过程可以重复使用，减少数据库开发人员的工作量。</p><p>缺点</p><p>1）调试麻烦，但是用 PL/SQL Developer 调试很方便！弥补这个缺点。</p><p>2）移植问题，数据库端代码当然是与数据库相关的。但是如果是做工程型项目，基本不存在移植问题。</p><p>3）重新编译问题，因为后端代码是运行前编译的，如果带有引用关系的对象发生改变时，受影响的存储过程、包将需要重新编译（不过也可以设置成运行时刻自动编译）。</p><p>4）如果在一个程序系统中大量的使用存储过程，到程序交付使用的时候随着用户需求的增加会导致数据结构的变化，接着就是系统的相关问题了，最后如果用户想维护该系统可以说是很难很难、而且代价是空前的，维护起来更麻烦。</p><h2 id="触发器"><a href="#触发器" class="headerlink" title="触发器"></a>触发器</h2><h3 id="什么是触发器？触发器的使用场景有哪些？"><a href="#什么是触发器？触发器的使用场景有哪些？" class="headerlink" title="什么是触发器？触发器的使用场景有哪些？"></a>什么是触发器？触发器的使用场景有哪些？</h3><p>触发器是用户定义在关系表上的一类由事件驱动的特殊的存储过程。触发器是指一段代码，当触发某个事件时，自动执行这些代码。</p><p>使用场景</p><ul><li>可以通过数据库中的相关表实现级联更改。</li><li>实时监控某张表中的某个字段的更改而需要做出相应的处理。</li><li>例如可以生成某些业务的编号。</li><li>注意不要滥用，否则会造成数据库及应用程序的维护困难。</li></ul><h3 id="MySQL中都有哪些触发器？"><a href="#MySQL中都有哪些触发器？" class="headerlink" title="MySQL中都有哪些触发器？"></a>MySQL中都有哪些触发器？</h3><p>在MySQL数据库中有如下六种触发器：</p><ul><li>Before Insert</li><li>After Insert</li><li>Before Update</li><li>After Update</li><li>Before Delete</li><li>After Delete</li></ul><h2 id="常用SQL语句"><a href="#常用SQL语句" class="headerlink" title="常用SQL语句"></a>常用SQL语句</h2><h3 id="SQL语句主要分为哪几类"><a href="#SQL语句主要分为哪几类" class="headerlink" title="SQL语句主要分为哪几类"></a>SQL语句主要分为哪几类</h3><p>数据定义语言DDL（Data Ddefinition Language）CREATE，DROP，ALTER</p><p>主要为以上操作 即对逻辑结构等有操作的，其中包括表结构，视图和索引。</p><p>数据查询语言DQL（Data Query Language）SELECT</p><p>这个较为好理解 即查询操作，以select关键字。各种简单查询，连接查询等 都属于DQL。</p><p>数据操纵语言DML（Data Manipulation Language）INSERT，UPDATE，DELETE</p><p>主要为以上操作 即对数据进行操作的，对应上面所说的查询操作 DQL与DML共同构建了多数初级程序员常用的增删改查操作。而查询是较为特殊的一种 被划分到DQL中。</p><p>数据控制功能DCL（Data Control Language）GRANT，REVOKE，COMMIT，ROLLBACK</p><p>主要为以上操作 即对数据库安全性完整性等有操作的，可以简单的理解为权限控制等。</p><h3 id="超键、候选键、主键、外键分别是什么？"><a href="#超键、候选键、主键、外键分别是什么？" class="headerlink" title="超键、候选键、主键、外键分别是什么？"></a>超键、候选键、主键、外键分别是什么？</h3><p>超键：在关系中能唯一标识元组的属性集称为关系模式的超键。一个属性可以为作为一个超键，多个属性组合在一起也可以作为一个超键。超键包含候选键和主键。<br>候选键：是最小超键，即没有冗余元素的超键。<br>主键：数据库表中对储存数据对象予以唯一和完整标识的数据列或属性的组合。一个数据列只能有一个主键，且主键的取值不能缺失，即不能为空值（Null）。<br>外键：在一个表中存在的另一个表的主键称此表的外键。</p><h3 id="SQL-约束有哪几种？"><a href="#SQL-约束有哪几种？" class="headerlink" title="SQL 约束有哪几种？"></a>SQL 约束有哪几种？</h3><p>NOT NULL: 用于控制字段的内容一定不能为空（NULL）。<br>UNIQUE: 控件字段内容不能重复，一个表允许有多个 Unique 约束。<br>PRIMARY KEY: 也是用于控件字段内容不能重复，但它在一个表只允许出现一个。<br>FOREIGN KEY: 用于预防破坏表之间连接的动作，也能防止非法数据插入外键列，因为它必须是它指向的那个表中的值之一。<br>CHECK: 用于控制字段的值范围。</p><h3 id="六种关联查询"><a href="#六种关联查询" class="headerlink" title="六种关联查询"></a>六种关联查询</h3><ul><li>内连接（INNER JOIN）</li><li>外连接（LEFT JOIN/RIGHT JOIN）</li><li>联合查询（UNION与UNION ALL）</li><li>全连接（FULL JOIN）</li></ul><h3 id="什么是子查询"><a href="#什么是子查询" class="headerlink" title="什么是子查询"></a>什么是子查询</h3><ol><li>条件：一条SQL语句的查询结果做为另一条查询语句的条件或查询结果</li><li>嵌套：多条SQL语句嵌套使用，内部的SQL查询语句称为子查询。</li></ol><h3 id="mysql中-in-和-exists-区别"><a href="#mysql中-in-和-exists-区别" class="headerlink" title="mysql中 in 和 exists 区别"></a>mysql中 in 和 exists 区别</h3><p>mysql中的in语句是把外表和内表作hash 连接，而exists语句是对外表作loop循环，每次loop循环再对内表进行查询。一直大家都认为exists比in语句的效率要高，这种说法其实是不准确的。这个是要区分环境的。</p><ol><li>如果查询的两个表大小相当，那么用in和exists差别不大。</li><li>如果两个表中一个较小，一个是大表，则子查询表大的用exists，子查询表小的用in。</li><li>not in 和not exists：如果查询语句使用了not in，那么内外表都进行全表扫描，没有用到索引；而not extsts的子查询依然能用到表上的索引。所以无论那个表大，用not exists都比not in要快。</li></ol><h3 id="varchar与char的区别"><a href="#varchar与char的区别" class="headerlink" title="varchar与char的区别"></a>varchar与char的区别</h3><p>总之，结合性能角度（char更快）和节省磁盘空间角度（varchar更小），具体情况还需具体来设计数据库才是妥当的做法。</p><h3 id="varchar-50-中50的涵义"><a href="#varchar-50-中50的涵义" class="headerlink" title="varchar(50)中50的涵义"></a>varchar(50)中50的涵义</h3><p>最多存放50个字符，varchar(50)和(200)存储hello所占空间一样，但后者在排序时会消耗更多内存，因为order by col采用fixed_length计算col长度(memory引擎也一样)。在早期 MySQL 版本中， 50 代表字节数，现在代表字符数。</p><h3 id="int-20-中20的涵义"><a href="#int-20-中20的涵义" class="headerlink" title="int(20)中20的涵义"></a>int(20)中20的涵义</h3><p>是指显示字符的长度。20表示最大显示宽度为20，但仍占4字节存储，存储范围不变；</p><p>不影响内部存储，只是影响带 zerofill 定义的 int 时，前面补多少个 0，易于报表展示</p><h3 id="mysql为什么这么设计"><a href="#mysql为什么这么设计" class="headerlink" title="mysql为什么这么设计"></a>mysql为什么这么设计</h3><p>对大多数应用没有意义，只是规定一些工具用来显示字符的个数；int(1)和int(20)存储和计算均一样；</p><h3 id="mysql中int-10-和char-10-以及varchar-10-的区别"><a href="#mysql中int-10-和char-10-以及varchar-10-的区别" class="headerlink" title="mysql中int(10)和char(10)以及varchar(10)的区别"></a>mysql中int(10)和char(10)以及varchar(10)的区别</h3><ul><li><p>int(10)的10表示显示的数据的长度，不是存储数据的大小；chart(10)和varchar(10)的10表示存储数据的大小，即表示存储多少个字符。</p><p>int(10) 10位的数据长度 9999999999，占32个字节，int型4位<br>char(10) 10位固定字符串，不足补空格 最多10个字符<br>varchar(10) 10位可变字符串，不足补空格 最多10个字符</p></li><li><p>char(10)表示存储定长的10个字符，不足10个就用空格补齐，占用更多的存储空间</p></li><li><p>varchar(10)表示存储10个变长的字符，存储多少个就是多少个，空格也按一个字符存储，这一点是和char(10)的空格不同的，char(10)的空格表示占位不算一个字符</p></li></ul><h3 id="FLOAT和DOUBLE的区别是什么？"><a href="#FLOAT和DOUBLE的区别是什么？" class="headerlink" title="FLOAT和DOUBLE的区别是什么？"></a>FLOAT和DOUBLE的区别是什么？</h3><ul><li>FLOAT类型数据可以存储至多8位十进制数，并在内存中占4字节。</li><li>DOUBLE类型数据可以存储至多18位十进制数，并在内存中占8字节。</li></ul><h3 id="drop、delete与truncate的区别"><a href="#drop、delete与truncate的区别" class="headerlink" title="drop、delete与truncate的区别"></a>drop、delete与truncate的区别</h3><p>因此，在不再需要一张表的时候，用drop；在想删除部分数据行时候，用delete；在保留表而删除所有数据的时候用truncate。</p><h3 id="UNION与UNION-ALL的区别？"><a href="#UNION与UNION-ALL的区别？" class="headerlink" title="UNION与UNION ALL的区别？"></a>UNION与UNION ALL的区别？</h3><ul><li>如果使用UNION ALL，不会合并重复的记录行</li><li>效率 UNION 高于 UNION ALL</li></ul><h2 id="SQL优化"><a href="#SQL优化" class="headerlink" title="SQL优化"></a>SQL优化</h2><h3 id="如何定位及优化SQL语句的性能问题？创建的索引有没有被使用到-或者说怎么才可以知道这条语句运行很慢的原因？"><a href="#如何定位及优化SQL语句的性能问题？创建的索引有没有被使用到-或者说怎么才可以知道这条语句运行很慢的原因？" class="headerlink" title="如何定位及优化SQL语句的性能问题？创建的索引有没有被使用到?或者说怎么才可以知道这条语句运行很慢的原因？"></a>如何定位及优化SQL语句的性能问题？创建的索引有没有被使用到?或者说怎么才可以知道这条语句运行很慢的原因？</h3><p>对于低性能的SQL语句的定位，最重要也是最有效的方法就是使用执行计划，MySQL提供了explain命令来查看语句的执行计划。 我们知道，不管是哪种数据库，或者是哪种数据库引擎，在对一条SQL语句进行执行的过程中都会做很多相关的优化，对于查询语句，最重要的优化方式就是使用索引。 而执行计划，就是显示数据库引擎对于SQL语句的执行的详细情况，其中包含了是否使用索引，使用什么索引，使用的索引的相关信息等。<br>执行计划包含的信息 id 有一组数字组成。表示一个查询中各个子查询的执行顺序;</p><p>id相同执行顺序由上至下。<br>id不同，id值越大优先级越高，越先被执行。<br>id为null时表示一个结果集，不需要使用它查询，常出现在包含union等查询语句中。<br>select_type 每个子查询的查询类型，一些常见的查询类型。<br><strong>table</strong> 查询的数据表，当从衍生表中查数据时会显示 x 表示对应的执行计划id <strong>partitions</strong> 表分区、表创建的时候可以指定通过那个列进行表分区。 </p><p>type(非常重要，可以看到有没有走索引) 访问类型</p><ul><li>ALL 扫描全表数据</li><li>index 遍历索引</li><li>range 索引范围查找</li><li>index_subquery 在子查询中使用 ref</li><li>unique_subquery 在子查询中使用 eq_ref</li><li>ref_or_null 对Null进行索引的优化的 ref</li><li>fulltext 使用全文索引</li><li>ref 使用非唯一索引查找数据</li><li>eq_ref 在join查询中使用PRIMARY KEYorUNIQUE NOT NULL索引关联。</li></ul><p><strong>possible_keys</strong> 可能使用的索引，注意不一定会使用。查询涉及到的字段上若存在索引，则该索引将被列出来。当该列为 NULL时就要考虑当前的SQL是否需要优化了。</p><p>key 显示MySQL在查询中实际使用的索引，若没有使用索引，显示为NULL。</p><p>TIPS:查询中若使用了覆盖索引(覆盖索引：索引的数据覆盖了需要查询的所有数据)，则该索引仅出现在key列表中</p><p>key_length 索引长度</p><p>ref 表示上述表的连接匹配条件，即哪些列或常量被用于查找索引列上的值</p><p>rows 返回估算的结果集数目，并不是一个准确的值。</p><p>extra 的信息非常丰富，常见的有：</p><ul><li>Using index 使用覆盖索引</li><li>Using where 使用了用where子句来过滤结果集</li><li>Using filesort 使用文件排序，使用非索引列进行排序时出现，非常消耗性能，尽量优化。</li><li>Using temporary 使用了临时表 sql优化的目标可以参考阿里开发手册</li></ul><h3 id="SQL的生命周期？"><a href="#SQL的生命周期？" class="headerlink" title="SQL的生命周期？"></a>SQL的生命周期？</h3><ol><li>应用服务器与数据库服务器建立一个连接</li><li>数据库进程拿到请求sql</li><li>解析并生成执行计划，执行</li><li>读取数据到内存并进行逻辑处理</li><li>通过步骤一的连接，发送结果到客户端</li><li>关掉连接，释放资源</li></ol><h3 id="大表数据查询，怎么优化"><a href="#大表数据查询，怎么优化" class="headerlink" title="大表数据查询，怎么优化"></a>大表数据查询，怎么优化</h3><ol><li>优化shema、sql语句+索引；</li><li>第二加缓存，memcached, redis；</li><li>主从复制，读写分离；</li><li>垂直拆分，根据你模块的耦合度，将一个大的系统分为多个小的系统，也就是分布式系统；</li><li>水平切分，针对数据量大的表，这一步最麻烦，最能考验技术水平，要选择一个合理的sharding key, 为了有好的查询效率，表结构也要改动，做一定的冗余，应用也要改，sql中尽量带sharding key，将数据定位到限定的表上去查，而不是扫描全部的表；</li></ol><h3 id="超大分页怎么处理？"><a href="#超大分页怎么处理？" class="headerlink" title="超大分页怎么处理？"></a>超大分页怎么处理？</h3><p>超大的分页一般从两个方向上来解决.</p><ul><li>数据库层面,这也是我们主要集中关注的(虽然收效没那么大),类似于select * from table where age &gt; 20 limit 1000000,10这种查询其实也是有可以优化的余地的. 这条语句需要load1000000数据然后基本上全部丢弃,只取10条当然比较慢. 当时我们可以修改为select * from table where id in (select id from table where age &gt; 20 limit 1000000,10).这样虽然也load了一百万的数据,但是由于索引覆盖,要查询的所有字段都在索引中,所以速度会很快. 同时如果ID连续的好,我们还可以select * from table where id &gt; 1000000 limit 10,效率也是不错的,优化的可能性有许多种,但是核心思想都一样,就是减少load的数据.</li><li>从需求的角度减少这种请求…主要是不做类似的需求(直接跳转到几百万页之后的具体某一页.只允许逐页查看或者按照给定的路线走,这样可预测,可缓存)以及防止ID泄漏且连续被人恶意攻击.<br>解决超大分页,其实主要是靠缓存,可预测性的提前查到内容,缓存至redis等k-V数据库中,直接返回即可.</li></ul><h3 id="mysql-分页"><a href="#mysql-分页" class="headerlink" title="mysql 分页"></a>mysql 分页</h3><p>LIMIT 子句可以被用于强制 SELECT 语句返回指定的记录数。LIMIT 接受一个或两个数字参数。参数必须是一个整数常量。如果给定两个参数，第一个参数指定第一个返回记录行的偏移量，第二个参数指定返回记录行的最大数目。初始记录行的偏移量是 0(而不是 1)</p><h3 id="慢查询日志"><a href="#慢查询日志" class="headerlink" title="慢查询日志"></a>慢查询日志</h3><p>用于记录执行时间超过某个临界值的SQL日志，用于快速定位慢查询，为我们的优化做参考。</p><p>开启慢查询日志</p><p>配置项：slow_query_log</p><p>可以使用show variables like ‘slov_query_log’查看是否开启，如果状态值为OFF，可以使用set GLOBAL slow_query_log = on来开启，它会在datadir下产生一个xxx-slow.log的文件。</p><p>设置临界时间</p><p>配置项：long_query_time</p><p>查看：show VARIABLES like ‘long_query_time’，单位秒</p><p>设置：set long_query_time=0.5</p><p>实操时应该从长时间设置到短的时间，即将最慢的SQL优化掉</p><p>查看日志，一旦SQL超过了我们设置的临界时间就会被记录到xxx-slow.log中</p><h3 id="关心过业务系统里面的sql耗时吗？统计过慢查询吗？对慢查询都怎么优化过？"><a href="#关心过业务系统里面的sql耗时吗？统计过慢查询吗？对慢查询都怎么优化过？" class="headerlink" title="关心过业务系统里面的sql耗时吗？统计过慢查询吗？对慢查询都怎么优化过？"></a>关心过业务系统里面的sql耗时吗？统计过慢查询吗？对慢查询都怎么优化过？</h3><p>在业务系统中，除了使用主键进行的查询，其他的我都会在测试库上测试其耗时，慢查询的统计主要由运维在做，会定期将业务中的慢查询反馈给我们。</p><p>慢查询的优化首先要搞明白慢的原因是什么？ 是查询条件没有命中索引？是load了不需要的数据列？还是数据量太大？</p><p>所以优化也是针对这三个方向来的，</p><ul><li>首先分析语句，看看是否load了额外的数据，可能是查询了多余的行并且抛弃掉了，可能是加载了许多结果中并不需要的列，对语句进行分析以及重写。</li><li>分析语句的执行计划，然后获得其使用索引的情况，之后修改语句或者修改索引，使得语句可以尽可能的命中索引。</li><li>如果对语句的优化已经无法进行，可以考虑表中的数据量是否太大，如果是的话可以进行横向或者纵向的分表。</li></ul><h3 id="为什么要尽量设定一个主键？"><a href="#为什么要尽量设定一个主键？" class="headerlink" title="为什么要尽量设定一个主键？"></a>为什么要尽量设定一个主键？</h3><p>主键是数据库确保数据行在整张表唯一性的保障，即使业务上本张表没有主键，也建议添加一个自增长的ID列作为主键。设定了主键之后，在后续的删改查的时候可能更加快速以及确保操作数据范围安全。</p><h3 id="主键使用自增ID还是UUID？"><a href="#主键使用自增ID还是UUID？" class="headerlink" title="主键使用自增ID还是UUID？"></a>主键使用自增ID还是UUID？</h3><p>推荐使用自增ID，不要使用UUID。</p><p>因为在InnoDB存储引擎中，主键索引是作为聚簇索引存在的，也就是说，主键索引的B+树叶子节点上存储了主键索引以及全部的数据(按照顺序)，如果主键索引是自增ID，那么只需要不断向后排列即可，如果是UUID，由于到来的ID与原来的大小不确定，会造成非常多的数据插入，数据移动，然后导致产生很多的内存碎片，进而造成插入性能的下降。</p><p>总之，在数据量大一些的情况下，用自增主键性能会好一些。</p><p>关于主键是聚簇索引，如果没有主键，InnoDB会选择一个唯一键来作为聚簇索引，如果没有唯一键，会生成一个隐式的主键。</p><h3 id="字段为什么要求定义为not-null？"><a href="#字段为什么要求定义为not-null？" class="headerlink" title="字段为什么要求定义为not null？"></a>字段为什么要求定义为not null？</h3><p>null值会占用更多的字节，且会在程序中造成很多与预期不符的情况。</p><h3 id="如果要存储用户的密码散列，应该使用什么字段进行存储？"><a href="#如果要存储用户的密码散列，应该使用什么字段进行存储？" class="headerlink" title="如果要存储用户的密码散列，应该使用什么字段进行存储？"></a>如果要存储用户的密码散列，应该使用什么字段进行存储？</h3><p>密码散列，盐，用户身份证号等固定长度的字符串应该使用char而不是varchar来存储，这样可以节省空间且提高检索效率。</p><h3 id="优化查询过程中的数据访问"><a href="#优化查询过程中的数据访问" class="headerlink" title="优化查询过程中的数据访问"></a>优化查询过程中的数据访问</h3><ul><li>访问数据太多导致查询性能下降</li><li>确定应用程序是否在检索大量超过需要的数据，可能是太多行或列</li><li>确认MySQL服务器是否在分析大量不必要的数据行</li><li>避免犯如下SQL语句错误</li><li>查询不需要的数据。解决办法：使用limit解决</li><li>多表关联返回全部列。解决办法：指定列名</li><li>总是返回全部列。解决办法：避免使用SELECT *</li><li>重复查询相同的数据。解决办法：可以缓存数据，下次直接读取缓存</li><li>是否在扫描额外的记录。解决办法：</li><li>使用explain进行分析，如果发现查询需要扫描大量的数据，但只返回少数的行，可以通过如下技巧去优化：</li><li>使用索引覆盖扫描，把所有的列都放到索引中，这样存储引擎不需要回表获取对应行就可以返回结果。</li><li>改变数据库和表的结构，修改数据表范式</li><li>重写SQL语句，让优化器可以以更优的方式执行查询。</li></ul><h3 id="优化长难的查询语句"><a href="#优化长难的查询语句" class="headerlink" title="优化长难的查询语句"></a>优化长难的查询语句</h3><ul><li>一个复杂查询还是多个简单查询</li><li>MySQL内部每秒能扫描内存中上百万行数据，相比之下，响应数据给客户端就要慢得多</li><li>使用尽可能小的查询是好的，但是有时将一个大的查询分解为多个小的查询是很有必要的。</li><li>切分查询</li><li>将一个大的查询分为多个小的相同的查询</li><li>一次性删除1000万的数据要比一次删除1万，暂停一会的方案更加损耗服务器开销。</li><li>分解关联查询，让缓存的效率更高。</li><li>执行单个查询可以减少锁的竞争。</li><li>在应用层做关联更容易对数据库进行拆分。</li><li>查询效率会有大幅提升。</li><li>较少冗余记录的查询。</li></ul><h3 id="优化特定类型的查询语句"><a href="#优化特定类型的查询语句" class="headerlink" title="优化特定类型的查询语句"></a>优化特定类型的查询语句</h3><ul><li>count(<em>)会忽略所有的列，直接统计所有列数，不要使用count(列名)</em></li><li><em>MyISAM中，没有任何where条件的count(</em>)非常快。</li><li>当有where条件时，MyISAM的count统计不一定比其它引擎快。</li><li>可以使用explain查询近似值，用近似值替代count(*)</li><li>增加汇总表</li><li>使用缓存</li></ul><h3 id="优化关联查询"><a href="#优化关联查询" class="headerlink" title="优化关联查询"></a>优化关联查询</h3><ul><li>确定ON或者USING子句中是否有索引。</li><li>确保GROUP BY和ORDER BY只有一个表中的列，这样MySQL才有可能使用索引。</li></ul><h3 id="优化子查询"><a href="#优化子查询" class="headerlink" title="优化子查询"></a>优化子查询</h3><ul><li>用关联查询替代</li><li>优化GROUP BY和DISTINCT</li><li>这两种查询据可以使用索引来优化，是最有效的优化方法</li><li>关联查询中，使用标识列分组的效率更高</li><li>如果不需要ORDER BY，进行GROUP BY时加ORDER BY NULL，MySQL不会再进行文件排序。</li><li>WITH ROLLUP超级聚合，可以挪到应用程序处理</li></ul><h3 id="优化LIMIT分页"><a href="#优化LIMIT分页" class="headerlink" title="优化LIMIT分页"></a>优化LIMIT分页</h3><ul><li>LIMIT偏移量大的时候，查询效率较低</li><li>可以记录上次查询的最大ID，下次查询时直接根据该ID来查询</li></ul><h3 id="优化UNION查询"><a href="#优化UNION查询" class="headerlink" title="优化UNION查询"></a>优化UNION查询</h3><p>UNION ALL的效率高于UNION</p><h3 id="优化WHERE子句"><a href="#优化WHERE子句" class="headerlink" title="优化WHERE子句"></a>优化WHERE子句</h3><p>SQL语句优化的一些方法？</p><ol><li>对查询进行优化，应尽量避免全表扫描，首先应考虑在 where 及 order by 涉及的列上建立索引。</li><li>应尽量避免在 where 子句中对字段进行 null 值判断，否则将导致引擎放弃使用索引而进行全表扫描<br>– 可以在num上设置默认值0，确保表中num列没有null值</li><li>应尽量避免在 where 子句中使用!=或&lt;&gt;操作符，否则引擎将放弃使用索引而进行全表扫描。</li><li>应尽量避免在 where 子句中使用or 来连接条件，否则将导致引擎放弃使用索引而进行全表扫描</li><li>in 和 not in 也要慎用，否则会导致全表扫描</li><li>下面的查询也将导致全表扫描：select id from t where name like ‘%李%’若要提高效率，可以考虑全文检索</li><li>如果在 where 子句中使用参数，也会导致全表扫描。因为SQL只有在运行时才会解析局部变量，但优化程序不能将访问计划的选择推迟到运行时；它必须在编译时进行选择。然 而，如果在编译时建立访问计划，变量的值还是未知的，因而无法作为索引选择的输入项。</li><li>应尽量避免在 where 子句中对字段进行表达式操作，这将导致引擎放弃使用索引而进行全表扫描。</li><li>应尽量避免在where子句中对字段进行函数操作，这将导致引擎放弃使用索引而进行全表扫描。</li><li>不要在 where 子句中的“=”左边进行函数、算术运算或其他表达式运算，否则系统将可能无法正确使用索引。</li></ol><h2 id="数据库优化"><a href="#数据库优化" class="headerlink" title="数据库优化"></a>数据库优化</h2><h3 id="为什么要优化"><a href="#为什么要优化" class="headerlink" title="为什么要优化"></a>为什么要优化</h3><ul><li>系统的吞吐量瓶颈往往出现在数据库的访问速度上</li><li>随着应用程序的运行，数据库的中的数据会越来越多，处理时间会相应变慢</li><li>数据是存放在磁盘上的，读写速度无法和内存相比</li></ul><p>优化原则：减少系统瓶颈，减少资源占用，增加系统的反应速度。</p><h3 id="数据库结构优化"><a href="#数据库结构优化" class="headerlink" title="数据库结构优化"></a>数据库结构优化</h3><p>一个好的数据库设计方案对于数据库的性能往往会起到事半功倍的效果。</p><p>需要考虑数据冗余、查询和更新的速度、字段的数据类型是否合理等多方面的内容。</p><p>将字段很多的表分解成多个表</p><p>对于字段较多的表，如果有些字段的使用频率很低，可以将这些字段分离出来形成新表。</p><p>因为当一个表的数据量很大时，会由于使用频率低的字段的存在而变慢。</p><p>增加中间表</p><p>对于需要经常联合查询的表，可以建立中间表以提高查询效率。</p><p>通过建立中间表，将需要通过联合查询的数据插入到中间表中，然后将原来的联合查询改为对中间表的查询。</p><p>增加冗余字段</p><p>设计数据表时应尽量遵循范式理论的规约，尽可能的减少冗余字段，让数据库设计看起来精致、优雅。但是，合理的加入冗余字段可以提高查询速度。</p><p>表的规范化程度越高，表和表之间的关系越多，需要连接查询的情况也就越多，性能也就越差。</p><p>注意：</p><p>冗余字段的值在一个表中修改了，就要想办法在其他表中更新，否则就会导致数据不一致的问题。</p><h3 id="MySQL数据库cpu飙升到500-的话他怎么处理？"><a href="#MySQL数据库cpu飙升到500-的话他怎么处理？" class="headerlink" title="MySQL数据库cpu飙升到500%的话他怎么处理？"></a>MySQL数据库cpu飙升到500%的话他怎么处理？</h3><p>当 cpu 飙升到 500%时，先用操作系统命令 top 命令观察是不是 mysqld 占用导致的，如果不是，找出占用高的进程，并进行相关处理。</p><p>如果是 mysqld 造成的， show processlist，看看里面跑的 session 情况，是不是有消耗资源的 sql 在运行。找出消耗高的 sql，看看执行计划是否准确， index 是否缺失，或者实在是数据量太大造成。</p><p>一般来说，肯定要 kill 掉这些线程(同时观察 cpu 使用率是否下降)，等进行相应的调整(比如说加索引、改 sql、改内存参数)之后，再重新跑这些 SQL。</p><p>也有可能是每个 sql 消耗资源并不多，但是突然之间，有大量的 session 连进来导致 cpu 飙升，这种情况就需要跟应用一起来分析为何连接数会激增，再做出相应的调整，比如说限制连接数等</p><h3 id="大表怎么优化？某个表有近千万数据，CRUD比较慢，如何优化？分库分表了是怎么做的？分表分库了有什么问题？有用到中间件么？他们的原理知道么？"><a href="#大表怎么优化？某个表有近千万数据，CRUD比较慢，如何优化？分库分表了是怎么做的？分表分库了有什么问题？有用到中间件么？他们的原理知道么？" class="headerlink" title="大表怎么优化？某个表有近千万数据，CRUD比较慢，如何优化？分库分表了是怎么做的？分表分库了有什么问题？有用到中间件么？他们的原理知道么？"></a>大表怎么优化？某个表有近千万数据，CRUD比较慢，如何优化？分库分表了是怎么做的？分表分库了有什么问题？有用到中间件么？他们的原理知道么？</h3><p>当MySQL单表记录数过大时，数据库的CRUD性能会明显下降，一些常见的优化措施如下：</p><ol><li><p>限定数据的范围： 务必禁止不带任何限制数据范围条件的查询语句。比如：我们当用户在查询订单历史的时候，我们可以控制在一个月的范围内；</p></li><li><p>读/写分离： 经典的数据库拆分方案，主库负责写，从库负责读；</p></li><li><p>缓存： 使用MySQL的缓存，另外对重量级、更新少的数据可以考虑使用应用级别的缓存；<br>还有就是通过分库分表的方式进行优化，主要有垂直分表和水平分表；</p></li><li><p><strong>垂直分区：</strong></p></li></ol><p><strong>根据数据库里面数据表的相关性进行拆分。</strong> 例如，用户表中既有用户的登录信息又有用户的基本信息，可以将用户表拆分成两个单独的表，甚至放到单独的库做分库。</p><p><strong>简单来说垂直拆分是指数据表列的拆分，把一张列比较多的表拆分为多张表。</strong> 如下图所示，这样来说大家应该就更容易理解了。</p><p><strong>垂直拆分的优点：</strong> 可以使得行数据变小，在查询时减少读取的Block数，减少I/O次数。此外，垂直分区可以简化表的结构，易于维护。</p><p><strong>垂直拆分的缺点：</strong> 主键会出现冗余，需要管理冗余列，并会引起Join操作，可以通过在应用层进行Join来解决。此外，垂直分区会让事务变得更加复杂；</p><h4 id="垂直分表"><a href="#垂直分表" class="headerlink" title="垂直分表"></a>垂直分表</h4><p>把主键和一些列放在一个表，然后把主键和另外的列放在另一个表中</p><p>适用场景</p><ol><li>如果一个表中某些列常用，另外一些列不常用</li><li>可以使数据行变小，一个数据页能存储更多数据，查询时减少I/O次数</li></ol><p>缺点</p><ol><li><p>有些分表的策略基于应用层的逻辑算法，一旦逻辑算法改变，整个分表逻辑都会改变，扩展性较差</p></li><li><p>对于应用层来说，逻辑算法增加开发成本</p></li><li><p>管理冗余列，查询所有数据需要join操作</p></li><li><p><strong>水平分区</strong>：</p></li></ol><p>保持数据表结构不变，通过某种策略存储数据分片。这样每一片数据分散到不同的表或者库中，达到了分布式的目的。 水平拆分可以支撑非常大的数据量。</p><p>水平拆分是指数据表行的拆分，表的行数超过200万行时，就会变慢，这时可以把一张的表的数据拆成多张表来存放。举个例子：我们可以将用户信息表拆分成多个用户信息表，这样就可以避免单一表数据量过大对性能造成影响。</p><p>水平拆分可以支持非常大的数据量。需要注意的一点是:分表仅仅是解决了单一表数据过大的问题，但由于表的数据还是在同一台机器上，其实对于提升MySQL并发能力没有什么意义，所以水平拆分最好分库 。</p><p>水平拆分能够支持非常大的数据量存储，应用端改造也少，但 分片事务难以解决 ，跨界点Join性能较差，逻辑复杂。</p><p>《Java工程师修炼之道》的作者推荐 尽量不要对数据进行分片，因为拆分会带来逻辑、部署、运维的各种复杂度 ，一般的数据表在优化得当的情况下支撑千万以下的数据量是没有太大问题的。如果实在要分片，尽量选择客户端分片架构，这样可以减少一次和中间件的网络I/O。</p><p>水平分表：<br>表很大，分割后可以降低在查询时需要读的数据和索引的页数，同时也降低了索引的层数，提高查询次数</p><p>适用场景</p><ol><li>表中的数据本身就有独立性，例如表中分表记录各个地区的数据或者不同时期的数据，特别是有些数据常用，有些不常用。</li><li>需要把数据存放在多个介质上。</li></ol><p>水平切分的缺点</p><ol><li>给应用增加复杂度，通常查询时需要多个表名，查询所有数据都需UNION操作</li><li>在许多数据库应用中，这种复杂度会超过它带来的优点，查询时会增加读一个索引层的磁盘次数</li></ol><p>下面补充一下数据库分片的两种常见方案：</p><ol><li>客户端代理： 分片逻辑在应用端，封装在jar包中，通过修改或者封装JDBC层来实现。 当当网的 Sharding-JDBC 、阿里的TDDL是两种比较常用的实现。</li><li>中间件代理： 在应用和数据中间加了一个代理层。分片逻辑统一维护在中间件服务中。 我们现在谈的 Mycat 、360的Atlas、网易的DDB等等都是这种架构的实现。</li></ol><p><strong>分库分表后面临的问题</strong></p><ul><li><p>事务支持 分库分表后，就成了分布式事务了。如果依赖数据库本身的分布式事务管理功能去执行事务，将付出高昂的性能代价； 如果由应用程序去协助控制，形成程序逻辑上的事务，又会造成编程方面的负担。</p></li><li><p>跨库join</p><p>只要是进行切分，跨节点Join的问题是不可避免的。但是良好的设计和切分却可以减少此类情况的发生。解决这一问题的普遍做法是分两次查询实现。在第一次查询的结果集中找出关联数据的id,根据这些id发起第二次请求得到关联数据。 分库分表方案产品</p></li><li><p>跨节点的count,order by,group by以及聚合函数问题 这些是一类问题，因为它们都需要基于全部数据集合进行计算。多数的代理都不会自动处理合并工作。解决方案：与解决跨节点join问题的类似，分别在各个节点上得到结果后在应用程序端进行合并。和join不同的是每个结点的查询可以并行执行，因此很多时候它的速度要比单一大表快很多。但如果结果集很大，对应用程序内存的消耗是一个问题。</p></li><li><p>数据迁移，容量规划，扩容等问题 来自淘宝综合业务平台团队，它利用对2的倍数取余具有向前兼容的特性（如对4取余得1的数对2取余也是1）来分配数据，避免了行级别的数据迁移，但是依然需要进行表级别的迁移，同时对扩容规模和分表数量都有限制。总得来说，这些方案都不是十分的理想，多多少少都存在一些缺点，这也从一个侧面反映出了Sharding扩容的难度。</p></li><li><p>ID问题</p></li><li><p>一旦数据库被切分到多个物理结点上，我们将不能再依赖数据库自身的主键生成机制。一方面，某个分区数据库自生成的ID无法保证在全局上是唯一的；另一方面，应用程序在插入数据之前需要先获得ID,以便进行SQL路由. 一些常见的主键生成策略</p><p>UUID 使用UUID作主键是最简单的方案，但是缺点也是非常明显的。由于UUID非常的长，除占用大量存储空间外，最主要的问题是在索引上，在建立索引和基于索引进行查询时都存在性能问题。 Twitter的分布式自增ID算法Snowflake 在分布式系统中，需要生成全局UID的场合还是比较多的，twitter的snowflake解决了这种需求，实现也还是很简单的，除去配置信息，核心代码就是毫秒级时间41位 机器ID 10位 毫秒内序列12位。</p></li><li><p>跨分片的排序分页</p><p>一般来讲，分页时需要按照指定字段进行排序。当排序字段就是分片字段的时候，我们通过分片规则可以比较容易定位到指定的分片，而当排序字段非分片字段的时候，情况就会变得比较复杂了。为了最终结果的准确性，我们需要在不同的分片节点中将数据进行排序并返回，并将不同分片返回的结果集进行汇总和再次排序，最后再返回给用户。</p></li></ul><h3 id="MySQL的复制原理以及流程"><a href="#MySQL的复制原理以及流程" class="headerlink" title="MySQL的复制原理以及流程"></a>MySQL的复制原理以及流程</h3><p>主从复制：将主数据库中的DDL和DML操作通过二进制日志（BINLOG）传输到从数据库上，然后将这些日志重新执行（重做）；从而使得从数据库的数据与主数据库保持一致。</p><p>主从复制的作用</p><ol><li>主数据库出现问题，可以切换到从数据库。</li><li>可以进行数据库层面的读写分离。</li><li>可以在从数据库上进行日常备份。</li></ol><p>MySQL主从复制解决的问题</p><ul><li>数据分布：随意开始或停止复制，并在不同地理位置分布数据备份</li><li>负载均衡：降低单个服务器的压力</li><li>高可用和故障切换：帮助应用程序避免单点失败</li><li>升级测试：可以用更高版本的MySQL作为从库</li></ul><p>MySQL主从复制工作原理</p><ul><li>在主库上把数据更新记录到二进制日志</li><li>从库将主库的日志复制到自己的中继日志</li><li>从库读取中继日志的事件，将其重放到从库数据中</li></ul><p>基本原理流程，3个线程以及之间的关联</p><p>主：binlog线程——记录下所有改变了数据库数据的语句，放进master上的binlog中；</p><p>从：io线程——在使用start slave 之后，负责从master上拉取 binlog 内容，放进自己的relay log中；</p><p>从：sql执行线程——执行relay log中的语句；</p><h3 id="读写分离有哪些解决方案？"><a href="#读写分离有哪些解决方案？" class="headerlink" title="读写分离有哪些解决方案？"></a>读写分离有哪些解决方案？</h3><p>读写分离是依赖于主从复制，而主从复制又是为读写分离服务的。因为主从复制要求slave不能写只能读（如果对slave执行写操作，那么show slave status将会呈现Slave_SQL_Running=NO，此时你需要按照前面提到的手动同步一下slave）。</p><p>方案一</p><p>使用mysql-proxy代理</p><p>优点：直接实现读写分离和负载均衡，不用修改代码，master和slave用一样的帐号，mysql官方不建议实际生产中使用</p><p>缺点：降低性能， 不支持事务</p><p>方案二</p><p>使用AbstractRoutingDataSource+aop+annotation在dao层决定数据源。<br>如果采用了mybatis， 可以将读写分离放在ORM层，比如mybatis可以通过mybatis plugin拦截sql语句，所有的insert/update/delete都访问master库，所有的select 都访问salve库，这样对于dao层都是透明。 plugin实现时可以通过注解或者分析语句是读写方法来选定主从库。不过这样依然有一个问题， 也就是不支持事务， 所以我们还需要重写一下DataSourceTransactionManager， 将read-only的事务扔进读库， 其余的有读有写的扔进写库。</p><p>方案三</p><p>使用AbstractRoutingDataSource+aop+annotation在service层决定数据源，可以支持事务.</p><p>缺点：类内部方法通过this.xx()方式相互调用时，aop不会进行拦截，需进行特殊处理。</p><h3 id="备份计划，mysqldump以及xtranbackup的实现原理"><a href="#备份计划，mysqldump以及xtranbackup的实现原理" class="headerlink" title="备份计划，mysqldump以及xtranbackup的实现原理"></a>备份计划，mysqldump以及xtranbackup的实现原理</h3><p>(1)备份计划</p><p>视库的大小来定，一般来说 100G 内的库，可以考虑使用 mysqldump 来做，因为 mysqldump更加轻巧灵活，备份时间选在业务低峰期，可以每天进行都进行全量备份(mysqldump 备份出来的文件比较小，压缩之后更小)。</p><p>100G 以上的库，可以考虑用 xtranbackup 来做，备份速度明显要比 mysqldump 要快。一般是选择一周一个全备，其余每天进行增量备份，备份时间为业务低峰期。</p><p>(2)备份恢复时间</p><p>物理备份恢复快，逻辑备份恢复慢</p><p>这里跟机器，尤其是硬盘的速率有关系，以下列举几个仅供参考</p><p>20G的2分钟（mysqldump）</p><p>80G的30分钟(mysqldump)</p><p>111G的30分钟（mysqldump)</p><p>288G的3小时（xtra)</p><p>3T的4小时（xtra)</p><p>逻辑导入时间一般是备份时间的5倍以上</p><p>(3)备份恢复失败如何处理</p><p>首先在恢复之前就应该做足准备工作，避免恢复的时候出错。比如说备份之后的有效性检查、权限检查、空间检查等。如果万一报错，再根据报错的提示来进行相应的调整。</p><p>(4)mysqldump和xtrabackup实现原理</p><p>mysqldump</p><p>mysqldump 属于逻辑备份。加入–single-transaction 选项可以进行一致性备份。后台进程会先设置 session 的事务隔离级别为 RR(SET SESSION TRANSACTION ISOLATION LEVELREPEATABLE READ)，之后显式开启一个事务(START TRANSACTION /*!40100 WITH CONSISTENTSNAPSHOT */)，这样就保证了该事务里读到的数据都是事务事务时候的快照。之后再把表的数据读取出来。如果加上–master-data=1 的话，在刚开始的时候还会加一个数据库的读锁(FLUSH TABLES WITH READ LOCK),等开启事务后，再记录下数据库此时 binlog 的位置(showmaster status)，马上解锁，再读取表的数据。等所有的数据都已经导完，就可以结束事务</p><p>Xtrabackup:</p><p>xtrabackup 属于物理备份，直接拷贝表空间文件，同时不断扫描产生的 redo 日志并保存下来。最后完成 innodb 的备份后，会做一个 flush engine logs 的操作(老版本在有 bug，在5.6 上不做此操作会丢数据)，确保所有的 redo log 都已经落盘(涉及到事务的两阶段提交</p><p>概念，因为 xtrabackup 并不拷贝 binlog，所以必须保证所有的 redo log 都落盘，否则可能会丢最后一组提交事务的数据)。这个时间点就是 innodb 完成备份的时间点，数据文件虽然不是一致性的，但是有这段时间的 redo 就可以让数据文件达到一致性(恢复的时候做的事</p><p>情)。然后还需要 flush tables with read lock，把 myisam 等其他引擎的表给备份出来，备份完后解锁。这样就做到了完美的热备。</p><h3 id="数据表损坏的修复方式有哪些？"><a href="#数据表损坏的修复方式有哪些？" class="headerlink" title="数据表损坏的修复方式有哪些？"></a>数据表损坏的修复方式有哪些？</h3><p>使用 myisamchk 来修复，具体步骤：</p><p>1）修复前将mysql服务停止。<br>2）打开命令行方式，然后进入到mysql的/bin目录。<br>3）执行myisamchk –recover 数据库所在路径/*.MYI<br>使用repair table 或者 OPTIMIZE table命令来修复，REPAIR TABLE table_name 修复表 OPTIMIZE TABLE table_name 优化表 REPAIR TABLE 用于修复被破坏的表。 OPTIMIZE TABLE 用于回收闲置的数据库空间，当表上的数据行被删除时，所占据的磁盘空间并没有立即被回收，使用了OPTIMIZE TABLE命令后这些空间将被回收，并且对磁盘上的数据行进行重排（注意：是磁盘上，而非数据库）</p>]]></content>
      
      
      <categories>
          
          <category> java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> mysql </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Redis笔记</title>
      <link href="/2020/06/30/Redis%E7%AC%94%E8%AE%B0/"/>
      <url>/2020/06/30/Redis%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<h1 id="Redis"><a href="#Redis" class="headerlink" title="Redis"></a>Redis</h1><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><h3 id="什么是Redis"><a href="#什么是Redis" class="headerlink" title="什么是Redis"></a>什么是Redis</h3><p>Redis(Remote Dictionary Server) 是一个使用 C 语言编写的，开源的（BSD许可）高性能非关系型（NoSQL）的键值对数据库。</p><p>Redis 可以存储键和五种不同类型的值之间的映射。键的类型只能为字符串，值支持五种数据类型：字符串、列表、集合、散列表、有序集合。</p><p>与传统数据库不同的是 Redis 的数据是存在内存中的，所以读写速度非常快，因此 redis 被广泛应用于缓存方向，每秒可以处理超过 10万次读写操作，是已知性能最快的Key-Value DB。另外，Redis 也经常用来做分布式锁。除此之外，Redis 支持事务 、持久化、LUA脚本、LRU驱动事件、多种集群方案。</p><h3 id="Redis有哪些优缺点"><a href="#Redis有哪些优缺点" class="headerlink" title="Redis有哪些优缺点"></a>Redis有哪些优缺点</h3><p>优点</p><ul><li>读写性能优异， Redis能读的速度是110000次/s，写的速度是81000次/s。</li><li>支持数据持久化，支持AOF和RDB两种持久化方式。</li><li>支持事务，Redis的所有操作都是原子性的，同时Redis还支持对几个操作合并后的原子性执行。</li><li>数据结构丰富，除了支持string类型的value外还支持hash、set、zset、list等数据结构。</li><li>支持主从复制，主机会自动将数据同步到从机，可以进行读写分离。</li></ul><p>缺点</p><ul><li>数据库容量受到物理内存的限制，不能用作海量数据的高性能读写，因此Redis适合的场景主要局限在较小数据量的高性能操作和运算上。</li><li>Redis 不具备自动容错和恢复功能，主机从机的宕机都会导致前端部分读写请求失败，需要等待机器重启或者手动切换前端的IP才能恢复。</li><li>主机宕机，宕机前有部分数据未能及时同步到从机，切换IP后还会引入数据不一致的问题，降低了系统的可用性。</li><li>Redis 较难支持在线扩容，在集群容量达到上限时在线扩容会变得很复杂。为避免这一问题，运维人员在系统上线时必须确保有足够的空间，这对资源造成了很大的浪费。</li></ul><h3 id="为什么要用-Redis-为什么要用缓存"><a href="#为什么要用-Redis-为什么要用缓存" class="headerlink" title="为什么要用 Redis /为什么要用缓存"></a>为什么要用 Redis /为什么要用缓存</h3><p>主要从“高性能”和“高并发”这两点来看待这个问题。</p><p><strong>高性能</strong>：</p><p>假如用户第一次访问数据库中的某些数据。这个过程会比较慢，因为是从硬盘上读取的。将该用户访问的数据存在数缓存中，这样下一次再访问这些数据的时候就可以直接从缓存中获取了。操作缓存就是直接操作内存，所以速度相当快。如果数据库中的对应数据改变的之后，同步改变缓存中相应的数据即可！</p><p><strong>高并发：</strong></p><p>直接操作缓存能够承受的请求是远远大于直接访问数据库的，所以我们可以考虑把数据库中的部分数据转移到缓存中去，这样用户的一部分请求会直接到缓存这里而不用经过数据库。</p><h3 id="为什么要用-Redis-而不用-map-guava-做缓存"><a href="#为什么要用-Redis-而不用-map-guava-做缓存" class="headerlink" title="为什么要用 Redis 而不用 map/guava 做缓存?"></a>为什么要用 Redis 而不用 map/guava 做缓存?</h3><p>缓存分为本地缓存和分布式缓存。以 Java 为例，使用自带的 map 或者 guava 实现的是本地缓存，最主要的特点是轻量以及快速，生命周期随着 jvm 的销毁而结束，并且在多实例的情况下，每个实例都需要各自保存一份缓存，缓存不具有一致性。</p><p>使用 redis 或 memcached 之类的称为分布式缓存，在多实例的情况下，各实例共用一份缓存数据，缓存具有一致性。缺点是需要保持 redis 或 memcached服务的高可用，整个程序架构上较为复杂。</p><h3 id="Redis为什么这么快"><a href="#Redis为什么这么快" class="headerlink" title="Redis为什么这么快"></a>Redis为什么这么快</h3><p>1、完全基于内存，绝大部分请求是纯粹的内存操作，非常快速。数据存在内存中，类似于 HashMap，HashMap 的优势就是查找和操作的时间复杂度都是O(1)；</p><p>2、数据结构简单，对数据操作也简单，Redis 中的数据结构是专门进行设计的；</p><p>3、采用单线程，避免了不必要的上下文切换和竞争条件，也不存在多进程或者多线程导致的切换而消耗 CPU，不用去考虑各种锁的问题，不存在加锁释放锁操作，没有因为可能出现死锁而导致的性能消耗；</p><p>4、使用多路 I/O 复用模型，非阻塞 IO；</p><p>5、使用底层模型不同，它们之间底层实现方式以及与客户端之间通信的应用协议不一样，Redis 直接自己构建了 VM 机制 ，因为一般的系统调用系统函数的话，会浪费一定的时间去移动和请求；</p><h2 id="数据类型"><a href="#数据类型" class="headerlink" title="数据类型"></a>数据类型</h2><h3 id="Redis有哪些数据类型"><a href="#Redis有哪些数据类型" class="headerlink" title="Redis有哪些数据类型"></a>Redis有哪些数据类型</h3><p>Redis主要有5种数据类型，包括String，List，Set，Zset，Hash，满足大部分的使用要求</p><h3 id="Redis的应用场景"><a href="#Redis的应用场景" class="headerlink" title="Redis的应用场景"></a>Redis的应用场景</h3><p>总结一</p><p>计数器、缓存、会话缓存、全页缓存（FPC）、查找表、消息队列(发布/订阅功能)、分布式锁实现</p><p>其它</p><p>Set 可以实现交集、并集等操作，从而实现共同好友等功能。ZSet 可以实现有序性操作，从而实现排行榜等功能。</p><p>总结二</p><p>Redis相比其他缓存，有一个非常大的优势，就是支持多种数据类型。</p><p>数据类型说明string字符串，最简单的k-v存储hashhash格式，value为field和value，适合ID-Detail这样的场景。list简单的list，顺序列表，支持首位或者末尾插入数据set无序list，查找速度快，适合交集、并集、差集处理sorted set有序的set</p><p>其实，通过上面的数据类型的特性，基本就能想到合适的应用场景了。</p><p>string——适合最简单的k-v存储，类似于memcached的存储结构，短信验证码，配置信息等，就用这种类型来存储。</p><p>hash——一般key为ID或者唯一标示，value对应的就是详情了。如商品详情，个人信息详情，新闻详情等。</p><p>list——因为list是有序的，比较适合存储一些有序且数据相对固定的数据。如省市区表、字典表等。因为list是有序的，适合根据写入的时间来排序，如：最新的***，消息队列等。</p><p>set——可以简单的理解为ID-List的模式，如微博中一个人有哪些好友，set最牛的地方在于，可以对两个set提供交集、并集、差集操作。例如：查找两个人共同的好友等。</p><p>Sorted Set——是set的增强版本，增加了一个score参数，自动会根据score的值进行排序。比较适合类似于top 10等不根据插入的时间来排序的数据。</p><p>如上所述，虽然Redis不像关系数据库那么复杂的数据结构，但是，也能适合很多场景，比一般的缓存数据结构要多。了解每种数据结构适合的业务场景，不仅有利于提升开发效率，也能有效利用Redis的性能。</p><h2 id="持久化"><a href="#持久化" class="headerlink" title="持久化"></a>持久化</h2><h3 id="什么是Redis持久化？"><a href="#什么是Redis持久化？" class="headerlink" title="什么是Redis持久化？"></a>什么是Redis持久化？</h3><p>持久化就是把内存的数据写到磁盘中去，防止服务宕机了内存数据丢失。</p><h3 id="Redis-的持久化机制是什么？各自的优缺点？"><a href="#Redis-的持久化机制是什么？各自的优缺点？" class="headerlink" title="Redis 的持久化机制是什么？各自的优缺点？"></a>Redis 的持久化机制是什么？各自的优缺点？</h3><p>Redis 提供两种持久化机制 RDB（默认） 和 AOF 机制:</p><p>RDB：是Redis DataBase缩写快照</p><p>优点：</p><ul><li>1、只有一个文件 dump.rdb，方便持久化。</li><li>2、容灾性好，一个文件可以保存到安全的磁盘。</li><li>3、性能最大化，fork 子进程来完成写操作，让主进程继续处理命令，所以是 IO 最大化。使用单独子进程来进行持久化，主进程不会进行任何 IO 操作，保证了 redis 的高性能</li><li>4.相对于数据集大时，比 AOF 的启动效率更高。</li></ul><p>缺点：</p><ul><li>1、数据安全性低。RDB 是间隔一段时间进行持久化，如果持久化之间 redis 发生故障，会发生数据丢失。所以这种方式更适合数据要求不严谨的时候)</li></ul><p>AOF持久化(即Append Only File持久化)，则是将Redis执行的每次写命令记录到单独的日志文件中，当重启Redis会重新将持久化的日志中文件恢复数据。</p><p>当两种方式同时开启时，数据恢复Redis会优先选择AOF恢复。</p><p>优点：</p><p>1、数据安全，aof 持久化可以配置 appendfsync 属性，有 always，每进行一次 命令操作就记录到 aof 文件中一次。<br>2、通过 append 模式写文件，即使中途服务器宕机，可以通过 redis-check-aof 工具解决数据一致性问题。<br>3、AOF 机制的 rewrite 模式。AOF 文件没被 rewrite 之前（文件过大时会对命令 进行合并重写），可以删除其中的某些命令（比如误操作的 flushall）)<br>缺点：</p><p>1、AOF 文件比 RDB 文件大，且恢复速度慢。<br>2、数据集大的时候，比 rdb 启动效率低。</p><p>优缺点是什么？</p><ul><li>AOF文件比RDB更新频率高，优先使用AOF还原数据。</li><li>AOF比RDB更安全也更大</li><li>RDB性能比AOF好</li><li>如果两个都配了优先加载AOF</li></ul><h3 id="如何选择合适的持久化方式"><a href="#如何选择合适的持久化方式" class="headerlink" title="如何选择合适的持久化方式"></a>如何选择合适的持久化方式</h3><ul><li><p>一般来说， 如果想达到足以媲美PostgreSQL的数据安全性，你应该同时使用两种持久化功能。在这种情况下，当 Redis 重启的时候会优先载入AOF文件来恢复原始的数据，因为在通常情况下AOF文件保存的数据集要比RDB文件保存的数据集要完整。</p></li><li><p>如果你非常关心你的数据， 但仍然可以承受数分钟以内的数据丢失，那么你可以只使用RDB持久化。</p></li><li><p>有很多用户都只使用AOF持久化，但并不推荐这种方式，因为定时生成RDB快照（snapshot）非常便于进行数据库备份， 并且 RDB 恢复数据集的速度也要比AOF恢复的速度要快，除此之外，使用RDB还可以避免AOF程序的bug。</p></li><li><p>如果你只希望你的数据在服务器运行的时候存在，你也可以不使用任何持久化方式。</p></li></ul><h3 id="Redis持久化数据和缓存怎么做扩容？"><a href="#Redis持久化数据和缓存怎么做扩容？" class="headerlink" title="Redis持久化数据和缓存怎么做扩容？"></a>Redis持久化数据和缓存怎么做扩容？</h3><ul><li>如果Redis被当做缓存使用，使用一致性哈希实现动态扩容缩容。</li><li>如果Redis被当做一个持久化存储使用，必须使用固定的keys-to-nodes映射关系，节点的数量一旦确定不能变化。否则的话(即Redis节点需要动态变化的情况），必须使用可以在运行时进行数据再平衡的一套系统，而当前只有Redis集群可以做到这样。</li></ul><h2 id="过期键的删除策略"><a href="#过期键的删除策略" class="headerlink" title="过期键的删除策略"></a>过期键的删除策略</h2><h3 id="Redis的过期键的删除策略"><a href="#Redis的过期键的删除策略" class="headerlink" title="Redis的过期键的删除策略"></a>Redis的过期键的删除策略</h3><p>我们都知道，Redis是key-value数据库，我们可以设置Redis中缓存的key的过期时间。Redis的过期策略就是指当Redis中缓存的key过期了，Redis如何处理。</p><p>过期策略通常有以下三种：</p><ul><li>定时过期：每个设置过期时间的key都需要创建一个定时器，到过期时间就会立即清除。该策略可以立即清除过期的数据，对内存很友好；但是会占用大量的CPU资源去处理过期的数据，从而影响缓存的响应时间和吞吐量。</li><li>惰性过期：只有当访问一个key时，才会判断该key是否已过期，过期则清除。该策略可以最大化地节省CPU资源，却对内存非常不友好。极端情况可能出现大量的过期key没有再次被访问，从而不会被清除，占用大量内存。</li><li>定期过期：每隔一定的时间，会扫描一定数量的数据库的expires字典中一定数量的key，并清除其中已过期的key。该策略是前两者的一个折中方案。通过调整定时扫描的时间间隔和每次扫描的限定耗时，可以在不同情况下使得CPU和内存资源达到最优的平衡效果。<br>(expires字典会保存所有设置了过期时间的key的过期时间数据，其中，key是指向键空间中的某个键的指针，value是该键的毫秒精度的UNIX时间戳表示的过期时间。键空间是指该Redis集群中保存的所有键。)</li></ul><p>Redis中同时使用了惰性过期和定期过期两种过期策略。</p><h3 id="Redis-key的过期时间和永久有效分别怎么设置？"><a href="#Redis-key的过期时间和永久有效分别怎么设置？" class="headerlink" title="Redis key的过期时间和永久有效分别怎么设置？"></a>Redis key的过期时间和永久有效分别怎么设置？</h3><p>EXPIRE和PERSIST命令。</p><h3 id="我们知道通过expire来设置key-的过期时间，那么对过期的数据怎么处理呢"><a href="#我们知道通过expire来设置key-的过期时间，那么对过期的数据怎么处理呢" class="headerlink" title="我们知道通过expire来设置key 的过期时间，那么对过期的数据怎么处理呢?"></a>我们知道通过expire来设置key 的过期时间，那么对过期的数据怎么处理呢?</h3><p>除了缓存服务器自带的缓存失效策略之外（Redis默认的有6中策略可供选择），我们还可以根据具体的业务需求进行自定义的缓存淘汰，常见的策略有两种：</p><ol><li>定时去清理过期的缓存；</li><li>当有用户请求过来时，再判断这个请求所用到的缓存是否过期，过期的话就去底层系统得到新数据并更新缓存。</li></ol><h2 id="内存相关"><a href="#内存相关" class="headerlink" title="内存相关"></a>内存相关</h2><h3 id="MySQL里有2000w数据，redis中只存20w的数据，如何保证redis中的数据都是热点数据"><a href="#MySQL里有2000w数据，redis中只存20w的数据，如何保证redis中的数据都是热点数据" class="headerlink" title="MySQL里有2000w数据，redis中只存20w的数据，如何保证redis中的数据都是热点数据"></a>MySQL里有2000w数据，redis中只存20w的数据，如何保证redis中的数据都是热点数据</h3><p>redis内存数据集大小上升到一定大小的时候，就会施行数据淘汰策略。</p><h3 id="Redis的内存淘汰策略有哪些"><a href="#Redis的内存淘汰策略有哪些" class="headerlink" title="Redis的内存淘汰策略有哪些"></a>Redis的内存淘汰策略有哪些</h3><p>Redis的内存淘汰策略是指在Redis的用于缓存的内存不足时，怎么处理需要新写入且需要申请额外空间的数据。</p><p>全局的键空间选择性移除</p><ul><li>noeviction：当内存不足以容纳新写入数据时，新写入操作会报错。</li><li>allkeys-lru：当内存不足以容纳新写入数据时，在键空间中，移除最近最少使用的key。（这个是最常用的）</li><li>allkeys-random：当内存不足以容纳新写入数据时，在键空间中，随机移除某个key。</li></ul><p>设置过期时间的键空间选择性移除</p><ul><li>volatile-lru：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，移除最近最少使用的key。</li><li>volatile-random：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，随机移除某个key。</li><li>volatile-ttl：当内存不足以容纳新写入数据时，在设置了过期时间的键空间中，有更早过期时间的key优先移除。</li></ul><p>总结</p><p>Redis的内存淘汰策略的选取并不会影响过期的key的处理。内存淘汰策略用于处理内存不足时的需要申请额外空间的数据；过期策略用于处理过期的缓存数据。</p><h3 id="Redis主要消耗什么物理资源？"><a href="#Redis主要消耗什么物理资源？" class="headerlink" title="Redis主要消耗什么物理资源？"></a>Redis主要消耗什么物理资源？</h3><p>内存</p><h3 id="Redis的内存用完了会发生什么？"><a href="#Redis的内存用完了会发生什么？" class="headerlink" title="Redis的内存用完了会发生什么？"></a>Redis的内存用完了会发生什么？</h3><p>如果达到设置的上限，Redis的写命令会返回错误信息（但是读命令还可以正常返回。）或者你可以配置内存淘汰机制，当Redis达到内存上限时会冲刷掉旧的内容。</p><h3 id="Redis如何做内存优化？"><a href="#Redis如何做内存优化？" class="headerlink" title="Redis如何做内存优化？"></a>Redis如何做内存优化？</h3><p>可以好好利用Hash,list,sorted set,set等集合类型数据，因为通常情况下很多小的Key-Value可以用更紧凑的方式存放到一起。尽可能使用散列表（hashes），散列表（是说散列表里面存储的数少）使用的内存非常小，所以你应该尽可能的将你的数据模型抽象到一个散列表里面。比如你的web系统中有一个用户对象，不要为这个用户的名称，姓氏，邮箱，密码设置单独的key，而是应该把这个用户的所有信息存储到一张散列表里面。</p><h2 id="线程模型"><a href="#线程模型" class="headerlink" title="线程模型"></a>线程模型</h2><h3 id="Redis线程模型"><a href="#Redis线程模型" class="headerlink" title="Redis线程模型"></a>Redis线程模型</h3><p>Redis基于Reactor模式开发了网络事件处理器，这个处理器被称为文件事件处理器（file event handler）。它的组成结构为4部分：多个套接字、IO多路复用程序、文件事件分派器、事件处理器。因为文件事件分派器队列的消费是单线程的，所以Redis才叫单线程模型。</p><p>文件事件处理器使用 I/O 多路复用（multiplexing）程序来同时监听多个套接字， 并根据套接字目前执行的任务来为套接字关联不同的事件处理器。<br>当被监听的套接字准备好执行连接应答（accept）、读取（read）、写入（write）、关闭（close）等操作时， 与操作相对应的文件事件就会产生， 这时文件事件处理器就会调用套接字之前关联好的事件处理器来处理这些事件。<br>虽然文件事件处理器以单线程方式运行， 但通过使用 I/O 多路复用程序来监听多个套接字， 文件事件处理器既实现了高性能的网络通信模型， 又可以很好地与 redis 服务器中其他同样以单线程方式运行的模块进行对接， 这保持了 Redis 内部单线程设计的简单性。</p><h2 id="事务"><a href="#事务" class="headerlink" title="事务"></a>事务</h2><h3 id="什么是事务？"><a href="#什么是事务？" class="headerlink" title="什么是事务？"></a>什么是事务？</h3><p>事务是一个单独的隔离操作：事务中的所有命令都会序列化、按顺序地执行。事务在执行的过程中，不会被其他客户端发送来的命令请求所打断。</p><p>事务是一个原子操作：事务中的命令要么全部被执行，要么全部都不执行。</p><h3 id="Redis事务的概念"><a href="#Redis事务的概念" class="headerlink" title="Redis事务的概念"></a>Redis事务的概念</h3><p>Redis 事务的本质是通过MULTI、EXEC、WATCH等一组命令的集合。事务支持一次执行多个命令，一个事务中所有命令都会被序列化。在事务执行过程，会按照顺序串行化执行队列中的命令，其他客户端提交的命令请求不会插入到事务执行命令序列中。</p><p>总结说：redis事务就是一次性、顺序性、排他性的执行一个队列中的一系列命令。</p><h3 id="Redis事务的三个阶段"><a href="#Redis事务的三个阶段" class="headerlink" title="Redis事务的三个阶段"></a>Redis事务的三个阶段</h3><ol><li>事务开始 MULTI</li><li>命令入队</li><li>事务执行 EXEC</li></ol><p>事务执行过程中，如果服务端收到有EXEC、DISCARD、WATCH、MULTI之外的请求，将会把请求放入队列中排队</p><h3 id="Redis事务相关命令"><a href="#Redis事务相关命令" class="headerlink" title="Redis事务相关命令"></a>Redis事务相关命令</h3><p>Redis事务功能是通过MULTI、EXEC、DISCARD和WATCH 四个原语实现的</p><p>Redis会将一个事务中的所有命令序列化，然后按顺序执行。</p><ol><li>redis 不支持回滚，“Redis 在事务失败时不进行回滚，而是继续执行余下的命令”， 所以 Redis 的内部可以保持简单且快速。</li><li>如果在一个事务中的命令出现错误，那么所有的命令都不会执行；</li><li>如果在一个事务中出现运行错误，那么正确的命令会被执行。</li></ol><ul><li>WATCH 命令是一个乐观锁，可以为 Redis 事务提供 check-and-set （CAS）行为。 可以监控一个或多个键，一旦其中有一个键被修改（或删除），之后的事务就不会执行，监控一直持续到EXEC命令。</li><li>MULTI命令用于开启一个事务，它总是返回OK。 MULTI执行之后，客户端可以继续向服务器发送任意多条命令，这些命令不会立即被执行，而是被放到一个队列中，当EXEC命令被调用时，所有队列中的命令才会被执行。</li><li>EXEC：执行所有事务块内的命令。返回事务块内所有命令的返回值，按命令执行的先后顺序排列。 当操作被打断时，返回空值 nil 。<br>通过调用DISCARD，客户端可以清空事务队列，并放弃执行事务， 并且客户端会从事务状态中退出。</li><li>UNWATCH命令可以取消watch对所有key的监控。</li></ul><h3 id="事务管理（ACID）概述"><a href="#事务管理（ACID）概述" class="headerlink" title="事务管理（ACID）概述"></a>事务管理（ACID）概述</h3><p>原子性（Atomicity）<br>原子性是指事务是一个不可分割的工作单位，事务中的操作要么都发生，要么都不发生。</p><p>一致性（Consistency）<br>事务前后数据的完整性必须保持一致。</p><p>隔离性（Isolation）<br>多个事务并发执行时，一个事务的执行不应影响其他事务的执行</p><p>持久性（Durability）<br>持久性是指一个事务一旦被提交，它对数据库中数据的改变就是永久性的，接下来即使数据库发生故障也不应该对其有任何影响</p><p>Redis的事务总是具有ACID中的一致性和隔离性，其他特性是不支持的。当服务器运行在AOF持久化模式下，并且appendfsync选项的值为always时，事务也具有耐久性。</p><h3 id="Redis事务支持隔离性吗"><a href="#Redis事务支持隔离性吗" class="headerlink" title="Redis事务支持隔离性吗"></a>Redis事务支持隔离性吗</h3><p>Redis 是单进程程序，并且它保证在执行事务时，不会对事务进行中断，事务可以运行直到执行完所有事务队列中的命令为止。因此，<strong>Redis 的事务是总是带有隔离性的</strong>。</p><h3 id="Redis事务保证原子性吗，支持回滚吗"><a href="#Redis事务保证原子性吗，支持回滚吗" class="headerlink" title="Redis事务保证原子性吗，支持回滚吗"></a>Redis事务保证原子性吗，支持回滚吗</h3><p>Redis中，单条命令是原子性执行的，但<strong>事务不保证原子性，且没有回滚</strong>。事务中任意命令执行失败，其余的命令仍会被执行。</p><h3 id="Redis事务其他实现"><a href="#Redis事务其他实现" class="headerlink" title="Redis事务其他实现"></a>Redis事务其他实现</h3><ul><li>基于Lua脚本，Redis可以保证脚本内的命令一次性、按顺序地执行，<br>其同时也不提供事务运行错误的回滚，执行过程中如果部分命令运行错误，剩下的命令还是会继续运行完</li><li>基于中间标记变量，通过另外的标记变量来标识事务是否执行完成，读取数据时先读取该标记变量判断是否事务执行完成。但这样会需要额外写代码实现，比较繁琐</li></ul><h2 id="集群方案"><a href="#集群方案" class="headerlink" title="集群方案"></a>集群方案</h2><h3 id="哨兵模式"><a href="#哨兵模式" class="headerlink" title="哨兵模式"></a>哨兵模式</h3><p><img src="https://lixiangbetter.github.io/2020/07/01/Redis%E7%AC%94%E8%AE%B0/%E5%88%9D%E8%AF%86alluxio.png" alt></p><h3 id="官方Redis-Cluster-方案-服务端路由查询"><a href="#官方Redis-Cluster-方案-服务端路由查询" class="headerlink" title="官方Redis Cluster 方案(服务端路由查询)"></a>官方Redis Cluster 方案(服务端路由查询)</h3><p>redis 集群模式的工作原理能说一下么？在集群模式下，redis 的 key 是如何寻址的？分布式寻址都有哪些算法？了解一致性 hash 算法吗？</p><p><strong>简介</strong></p><p>Redis Cluster是一种服务端Sharding技术，3.0版本开始正式提供。Redis Cluster并没有使用一致性hash，而是采用slot(槽)的概念，一共分成16384个槽。将请求发送到任意节点，接收到请求的节点会将查询请求发送到正确的节点上执行</p><h3 id="基于客户端分配"><a href="#基于客户端分配" class="headerlink" title="基于客户端分配"></a>基于客户端分配</h3><p>简介</p><p>Redis Sharding是Redis Cluster出来之前，业界普遍使用的多Redis实例集群方法。其主要思想是采用哈希算法将Redis数据的key进行散列，通过hash函数，特定的key会映射到特定的Redis节点上。Java redis客户端驱动jedis，支持Redis Sharding功能，即ShardedJedis以及结合缓存池的ShardedJedisPool</p><h3 id="基于代理服务器分片"><a href="#基于代理服务器分片" class="headerlink" title="基于代理服务器分片"></a>基于代理服务器分片</h3><p><strong>简介</strong></p><p>客户端发送请求到一个代理组件，代理解析客户端的数据，并将请求转发至正确的节点，最后将结果回复给客户端</p><h3 id="Redis-主从架构"><a href="#Redis-主从架构" class="headerlink" title="Redis 主从架构"></a>Redis 主从架构</h3><p>redis replication -&gt; 主从架构 -&gt; 读写分离 -&gt; 水平扩容支撑读高并发</p><ul><li>redis 采用异步方式复制数据到 slave 节点，不过 redis2.8 开始，slave node 会周期性地确认自己每次复制的数据量；</li><li>一个 master node 是可以配置多个 slave node 的；</li><li>slave node 也可以连接其他的 slave node；</li><li>slave node 做复制的时候，不会 block master node 的正常工作；</li><li>slave node 在做复制的时候，也不会 block 对自己的查询操作，它会用旧的数据集来提供服务；但是复制完成的时候，需要删除旧数据集，加载新数据集，这个时候就会暂停对外服务了；</li><li>slave node 主要用来进行横向扩容，做读写分离，扩容的 slave node 可以提高读的吞吐量。</li></ul><p>注意，如果采用了主从架构，那么建议必须开启 master node 的持久化，不建议用 slave node 作为 master node 的数据热备，因为那样的话，如果你关掉 master 的持久化，可能在 master 宕机重启的时候数据是空的，然后可能一经过复制， slave node 的数据也丢了。</p><p>另外，master 的各种备份方案，也需要做。万一本地的所有文件丢失了，从备份中挑选一份 rdb 去恢复 master，这样才能确保启动的时候，是有数据的，即使采用了后续讲解的高可用机制，slave node 可以自动接管 master node，但也可能 sentinel 还没检测到 master failure，master node 就自动重启了，还是可能导致上面所有的 slave node 数据被清空。</p><p><strong>redis 主从复制的核心原理</strong></p><p>当启动一个 slave node 的时候，它会发送一个 PSYNC 命令给 master node。</p><p>如果这是 slave node 初次连接到 master node，那么会触发一次 full resynchronization 全量复制。此时 master 会启动一个后台线程，开始生成一份 RDB 快照文件，</p><p>同时还会将从客户端 client 新收到的所有写命令缓存在内存中。RDB 文件生成完毕后， master 会将这个 RDB 发送给 slave，slave 会先写入本地磁盘，然后再从本地磁盘加载到内存中，</p><p>接着 master 会将内存中缓存的写命令发送到 slave，slave 也会同步这些数据。</p><p>slave node 如果跟 master node 有网络故障，断开了连接，会自动重连，连接之后 master node 仅会复制给 slave 部分缺少的数据。</p><h3 id="Redis集群的主从复制模型是怎样的？"><a href="#Redis集群的主从复制模型是怎样的？" class="headerlink" title="Redis集群的主从复制模型是怎样的？"></a>Redis集群的主从复制模型是怎样的？</h3><p>为了使在部分节点失败或者大部分节点无法通信的情况下集群仍然可用，所以集群使用了主从复制模型，每个节点都会有N-1个复制品</p><h3 id="生产环境中的-redis-是怎么部署的？"><a href="#生产环境中的-redis-是怎么部署的？" class="headerlink" title="生产环境中的 redis 是怎么部署的？"></a>生产环境中的 redis 是怎么部署的？</h3><p>redis cluster，10 台机器，5 台机器部署了 redis 主实例，另外 5 台机器部署了 redis 的从实例，每个主实例挂了一个从实例，5 个节点对外提供读写服务，每个节点的读写高峰qps可能可以达到每秒 5 万，5 台机器最多是 25 万读写请求/s。</p><p>机器是什么配置？32G 内存+ 8 核 CPU + 1T 磁盘，但是分配给 redis 进程的是10g内存，一般线上生产环境，redis 的内存尽量不要超过 10g，超过 10g 可能会有问题。</p><p>5 台机器对外提供读写，一共有 50g 内存。</p><p>因为每个主实例都挂了一个从实例，所以是高可用的，任何一个主实例宕机，都会自动故障迁移，redis 从实例会自动变成主实例继续提供读写服务。</p><p>你往内存里写的是什么数据？每条数据的大小是多少？商品数据，每条数据是 10kb。100 条数据是 1mb，10 万条数据是 1g。常驻内存的是 200 万条商品数据，占用内存是 20g，仅仅不到总内存的 50%。目前高峰期每秒就是 3500 左右的请求量。</p><p>其实大型的公司，会有基础架构的 team 负责缓存集群的运维。</p><h3 id="说说Redis哈希槽的概念？"><a href="#说说Redis哈希槽的概念？" class="headerlink" title="说说Redis哈希槽的概念？"></a>说说Redis哈希槽的概念？</h3><p>Redis集群没有使用一致性hash,而是引入了哈希槽的概念，Redis集群有16384个哈希槽，每个key通过CRC16校验后对16384取模来决定放置哪个槽，集群的每个节点负责一部分hash槽。</p><h3 id="Redis集群会有写操作丢失吗？为什么？"><a href="#Redis集群会有写操作丢失吗？为什么？" class="headerlink" title="Redis集群会有写操作丢失吗？为什么？"></a>Redis集群会有写操作丢失吗？为什么？</h3><p>Redis并不能保证数据的强一致性，这意味这在实际中集群在特定的条件下可能会丢失写操作。</p><h3 id="Redis集群之间是如何复制的？"><a href="#Redis集群之间是如何复制的？" class="headerlink" title="Redis集群之间是如何复制的？"></a>Redis集群之间是如何复制的？</h3><p>异步复制</p><h3 id="Redis集群最大节点个数是多少？"><a href="#Redis集群最大节点个数是多少？" class="headerlink" title="Redis集群最大节点个数是多少？"></a>Redis集群最大节点个数是多少？</h3><p>16384个</p><h3 id="Redis集群如何选择数据库？"><a href="#Redis集群如何选择数据库？" class="headerlink" title="Redis集群如何选择数据库？"></a>Redis集群如何选择数据库？</h3><p>Redis集群目前无法做数据库选择，默认在0数据库。</p><h2 id="分区"><a href="#分区" class="headerlink" title="分区"></a>分区</h2><h3 id="Redis是单线程的，如何提高多核CPU的利用率？"><a href="#Redis是单线程的，如何提高多核CPU的利用率？" class="headerlink" title="Redis是单线程的，如何提高多核CPU的利用率？"></a>Redis是单线程的，如何提高多核CPU的利用率？</h3><p>可以在同一个服务器部署多个Redis的实例，并把他们当作不同的服务器来使用，在某些时候，无论如何一个服务器是不够的， 所以，如果你想使用多个CPU，你可以考虑一下分片（shard）。</p><h3 id="为什么要做Redis分区？"><a href="#为什么要做Redis分区？" class="headerlink" title="为什么要做Redis分区？"></a>为什么要做Redis分区？</h3><p>分区可以让Redis管理更大的内存，Redis将可以使用所有机器的内存。如果没有分区，你最多只能使用一台机器的内存。分区使Redis的计算能力通过简单地增加计算机得到成倍提升，Redis的网络带宽也会随着计算机和网卡的增加而成倍增长。</p><h3 id="你知道有哪些Redis分区实现方案？"><a href="#你知道有哪些Redis分区实现方案？" class="headerlink" title="你知道有哪些Redis分区实现方案？"></a>你知道有哪些Redis分区实现方案？</h3><ul><li>客户端分区就是在客户端就已经决定数据会被存储到哪个redis节点或者从哪个redis节点读取。大多数客户端已经实现了客户端分区。</li><li>代理分区意味着客户端将请求发送给代理，然后代理决定去哪个节点写数据或者读数据。代理根据分区规则决定请求哪些Redis实例，然后根据Redis的响应结果返回给客户端。redis和memcached的一种代理实现就是Twemproxy</li><li>查询路由(Query routing) 的意思是客户端随机地请求任意一个redis实例，然后由Redis将请求转发给正确的Redis节点。Redis Cluster实现了一种混合形式的查询路由，但并不是直接将请求从一个redis节点转发到另一个redis节点，而是在客户端的帮助下直接redirected到正确的redis节点。</li></ul><h3 id="Redis分区有什么缺点？"><a href="#Redis分区有什么缺点？" class="headerlink" title="Redis分区有什么缺点？"></a>Redis分区有什么缺点？</h3><ul><li>涉及多个key的操作通常不会被支持。例如你不能对两个集合求交集，因为他们可能被存储到不同的Redis实例（实际上这种情况也有办法，但是不能直接使用交集指令）。</li><li>同时操作多个key,则不能使用Redis事务.</li><li>分区使用的粒度是key，不能使用一个非常长的排序key存储一个数据集（The partitioning granularity is the key, so it is not possible to shard a dataset with a single huge key like a very big sorted set）</li><li>当使用分区的时候，数据处理会非常复杂，例如为了备份你必须从不同的Redis实例和主机同时收集RDB / AOF文件。</li><li>分区时动态扩容或缩容可能非常复杂。Redis集群在运行时增加或者删除Redis节点，能做到最大程度对用户透明地数据再平衡，但其他一些客户端分区或者代理分区方法则不支持这种特性。然而，有一种预分片的技术也可以较好的解决这个问题。</li></ul><h2 id="分布式问题"><a href="#分布式问题" class="headerlink" title="分布式问题"></a>分布式问题</h2><h3 id="Redis实现分布式锁"><a href="#Redis实现分布式锁" class="headerlink" title="Redis实现分布式锁"></a>Redis实现分布式锁</h3><p>Redis为单进程单线程模式，采用队列模式将并发访问变成串行访问，且多客户端对Redis的连接并不存在竞争关系Redis中可以使用SETNX命令实现分布式锁。</p><p>当且仅当 key 不存在，将 key 的值设为 value。 若给定的 key 已经存在，则 SETNX 不做任何动作</p><p>SETNX 是『SET if Not eXists』(如果不存在，则 SET)的简写。</p><p>返回值：设置成功，返回 1 。设置失败，返回 0 。</p><h3 id="如何解决-Redis-的并发竞争-Key-问题"><a href="#如何解决-Redis-的并发竞争-Key-问题" class="headerlink" title="如何解决 Redis 的并发竞争 Key 问题"></a>如何解决 Redis 的并发竞争 Key 问题</h3><p>所谓 Redis 的并发竞争 Key 的问题也就是多个系统同时对一个 key 进行操作，但是最后执行的顺序和我们期望的顺序不同，这样也就导致了结果的不同！</p><p>推荐一种方案：分布式锁（zookeeper 和 redis 都可以实现分布式锁）。（如果不存在 Redis 的并发竞争 Key 问题，不要使用分布式锁，这样会影响性能）</p><p>基于zookeeper临时有序节点可以实现的分布式锁。大致思想为：每个客户端对某个方法加锁时，在zookeeper上的与该方法对应的指定节点的目录下，生成一个唯一的瞬时有序节点。 判断是否获取锁的方式很简单，只需要判断有序节点中序号最小的一个。 当释放锁的时候，只需将这个瞬时节点删除即可。同时，其可以避免服务宕机导致的锁无法释放，而产生的死锁问题。完成业务流程后，删除对应的子节点释放锁。</p><h3 id="分布式Redis是前期做还是后期规模上来了再做好？为什么？"><a href="#分布式Redis是前期做还是后期规模上来了再做好？为什么？" class="headerlink" title="分布式Redis是前期做还是后期规模上来了再做好？为什么？"></a>分布式Redis是前期做还是后期规模上来了再做好？为什么？</h3><p>既然Redis是如此的轻量（单实例只使用1M内存），为防止以后的扩容，最好的办法就是一开始就启动较多实例。即便你只有一台服务器，你也可以一开始就让Redis以分布式的方式运行，使用分区，在同一台服务器上启动多个实例。</p><p>一开始就多设置几个Redis实例，例如32或者64个实例，对大多数用户来说这操作起来可能比较麻烦，但是从长久来看做这点牺牲是值得的。</p><p>这样的话，当你的数据不断增长，需要更多的Redis服务器时，你需要做的就是仅仅将Redis实例从一台服务迁移到另外一台服务器而已（而不用考虑重新分区的问题）。一旦你添加了另一台服务器，你需要将你一半的Redis实例从第一台机器迁移到第二台机器。</p><h3 id="什么是-RedLock"><a href="#什么是-RedLock" class="headerlink" title="什么是 RedLock"></a>什么是 RedLock</h3><p>Redis 官方站提出了一种权威的基于 Redis 实现分布式锁的方式名叫 Redlock，此种方式比原先的单节点的方法更安全。它可以保证以下特性：</p><ul><li>安全特性：互斥访问，即永远只有一个 client 能拿到锁</li><li>避免死锁：最终 client 都可能拿到锁，不会出现死锁的情况，即使原本锁住某资源的 client crash 了或者出现了网络分区</li><li>容错性：只要大部分 Redis 节点存活就可以正常提供服务</li></ul><h2 id="缓存异常"><a href="#缓存异常" class="headerlink" title="缓存异常"></a>缓存异常</h2><h3 id="缓存雪崩"><a href="#缓存雪崩" class="headerlink" title="缓存雪崩"></a>缓存雪崩</h3><p>缓存雪崩是指缓存同一时间大面积的失效，所以，后面的请求都会落到数据库上，造成数据库短时间内承受大量请求而崩掉。</p><p>解决方案</p><ul><li>缓存数据的过期时间设置随机，防止同一时间大量数据过期现象发生。</li><li>一般并发量不是特别多的时候，使用最多的解决方案是加锁排队。</li><li>给每一个缓存数据增加相应的缓存标记，记录缓存的是否失效，如果缓存标记失效，则更新数据缓存。</li></ul><h3 id="缓存穿透"><a href="#缓存穿透" class="headerlink" title="缓存穿透"></a>缓存穿透</h3><p>缓存穿透是指缓存和数据库中都没有的数据，导致所有的请求都落到数据库上，造成数据库短时间内承受大量请求而崩掉。</p><p>解决方案</p><ul><li>接口层增加校验，如用户鉴权校验，id做基础校验，id&lt;=0的直接拦截；</li><li>从缓存取不到的数据，在数据库中也没有取到，这时也可以将key-value对写为key-null，缓存有效时间可以设置短点，如30秒（设置太长会导致正常情况也没法使用）。这样可以防止攻击用户反复用同一个id暴力攻击</li><li>采用布隆过滤器，将所有可能存在的数据哈希到一个足够大的 bitmap 中，一个一定不存在的数据会被这个 bitmap 拦截掉，从而避免了对底层存储系统的查询压力</li></ul><p>Bloom-Filter一般用于在大数据量的集合中判定某元素是否存在。</p><h3 id="缓存击穿"><a href="#缓存击穿" class="headerlink" title="缓存击穿"></a>缓存击穿</h3><p>缓存击穿是指缓存中没有但数据库中有的数据（一般是缓存时间到期），这时由于并发用户特别多，同时读缓存没读到数据，又同时去数据库去取数据，引起数据库压力瞬间增大，造成过大压力。和缓存雪崩不同的是，缓存击穿指并发查同一条数据，缓存雪崩是不同数据都过期了，很多数据都查不到从而查数据库。</p><p>解决方案</p><ol><li>设置热点数据永远不过期。</li><li>加互斥锁，互斥锁</li></ol><h3 id="缓存预热"><a href="#缓存预热" class="headerlink" title="缓存预热"></a>缓存预热</h3><p>缓存预热就是系统上线后，将相关的缓存数据直接加载到缓存系统。这样就可以避免在用户请求的时候，先查询数据库，然后再将数据缓存的问题！用户直接查询事先被预热的缓存数据！</p><p>解决方案</p><ol><li><p>直接写个缓存刷新页面，上线时手工操作一下；</p></li><li><p>数据量不大，可以在项目启动的时候自动进行加载；</p></li><li><p>定时刷新缓存；</p></li></ol><h3 id="缓存降级"><a href="#缓存降级" class="headerlink" title="缓存降级"></a>缓存降级</h3><p>当访问量剧增、服务出现问题（如响应时间慢或不响应）或非核心服务影响到核心流程的性能时，仍然需要保证服务还是可用的，即使是有损服务。系统可以根据一些关键数据进行自动降级，也可以配置开关实现人工降级。</p><p>缓存降级的最终目的是保证核心服务可用，即使是有损的。而且有些服务是无法降级的（如加入购物车、结算）。</p><p>在进行降级之前要对系统进行梳理，看看系统是不是可以丢卒保帅；从而梳理出哪些必须誓死保护，哪些可降级；比如可以参考日志级别设置预案：</p><p>一般：比如有些服务偶尔因为网络抖动或者服务正在上线而超时，可以自动降级；</p><p>警告：有些服务在一段时间内成功率有波动（如在95~100%之间），可以自动降级或人工降级，并发送告警；</p><p>错误：比如可用率低于90%，或者数据库连接池被打爆了，或者访问量突然猛增到系统能承受的最大阀值，此时可以根据情况自动降级或者人工降级；</p><p>严重错误：比如因为特殊原因数据错误了，此时需要紧急人工降级。</p><p>服务降级的目的，是为了防止Redis服务故障，导致数据库跟着一起发生雪崩问题。因此，对于不重要的缓存数据，可以采取服务降级策略，例如一个比较常见的做法就是，Redis出现问题，不去数据库查询，而是直接返回默认值给用户。</p><h3 id="热点数据和冷数据"><a href="#热点数据和冷数据" class="headerlink" title="热点数据和冷数据"></a>热点数据和冷数据</h3><p>热点数据，缓存才有价值</p><p>对于冷数据而言，大部分数据可能还没有再次访问到就已经被挤出内存，不仅占用内存，而且价值不大。频繁修改的数据，看情况考虑使用缓存</p><p>对于热点数据，比如我们的某IM产品，生日祝福模块，当天的寿星列表，缓存以后可能读取数十万次。再举个例子，某导航产品，我们将导航信息，缓存以后可能读取数百万次。</p><p>数据更新前至少读取两次，缓存才有意义。这个是最基本的策略，如果缓存还没有起作用就失效了，那就没有太大价值了。</p><p>那存不存在，修改频率很高，但是又不得不考虑缓存的场景呢？有！比如，这个读取接口对数据库的压力很大，但是又是热点数据，这个时候就需要考虑通过缓存手段，减少数据库的压力，比如我们的某助手产品的，点赞数，收藏数，分享数等是非常典型的热点数据，但是又不断变化，此时就需要将数据同步保存到Redis缓存，减少数据库压力。</p><h3 id="缓存热点key"><a href="#缓存热点key" class="headerlink" title="缓存热点key"></a>缓存热点key</h3><p>缓存中的一个Key(比如一个促销商品)，在某个时间点过期的时候，恰好在这个时间点对这个Key有大量的并发请求过来，这些请求发现缓存过期一般都会从后端DB加载数据并回设到缓存，这个时候大并发的请求可能会瞬间把后端DB压垮。</p><p>解决方案</p><p>对缓存查询加锁，如果KEY不存在，就加锁，然后查DB入缓存，然后解锁；其他进程如果发现有锁就等待，然后等解锁后返回数据或者进入DB查询</p><h2 id="常用工具"><a href="#常用工具" class="headerlink" title="常用工具"></a>常用工具</h2><h3 id="Redis支持的Java客户端都有哪些？官方推荐用哪个？"><a href="#Redis支持的Java客户端都有哪些？官方推荐用哪个？" class="headerlink" title="Redis支持的Java客户端都有哪些？官方推荐用哪个？"></a>Redis支持的Java客户端都有哪些？官方推荐用哪个？</h3><p>Redisson、Jedis、lettuce等等，官方推荐使用Redisson。</p><h3 id="Redis和Redisson有什么关系？"><a href="#Redis和Redisson有什么关系？" class="headerlink" title="Redis和Redisson有什么关系？"></a>Redis和Redisson有什么关系？</h3><p>Redisson是一个高级的分布式协调Redis客服端，能帮助用户在分布式环境中轻松实现一些Java的对象 (Bloom filter, BitSet, Set, SetMultimap, ScoredSortedSet, SortedSet, Map, ConcurrentMap, List, ListMultimap, Queue, BlockingQueue, Deque, BlockingDeque, Semaphore, Lock, ReadWriteLock, AtomicLong, CountDownLatch, Publish / Subscribe, HyperLogLog)。</p><h3 id="Jedis与Redisson对比有什么优缺点？"><a href="#Jedis与Redisson对比有什么优缺点？" class="headerlink" title="Jedis与Redisson对比有什么优缺点？"></a>Jedis与Redisson对比有什么优缺点？</h3><p>Jedis是Redis的Java实现的客户端，其API提供了比较全面的Redis命令的支持；Redisson实现了分布式和可扩展的Java数据结构，和Jedis相比，功能较为简单，不支持字符串操作，不支持排序、事务、管道、分区等Redis特性。Redisson的宗旨是促进使用者对Redis的关注分离，从而让使用者能够将精力更集中地放在处理业务逻辑上。</p><h2 id="其他问题"><a href="#其他问题" class="headerlink" title="其他问题"></a>其他问题</h2><h3 id="Redis与Memcached的区别"><a href="#Redis与Memcached的区别" class="headerlink" title="Redis与Memcached的区别"></a>Redis与Memcached的区别</h3><p>两者都是非关系型内存键值数据库，现在公司一般都是用 Redis 来实现缓存，而且 Redis 自身也越来越强大了！Redis 与 Memcached 主要有以下不同：</p><p>(1) memcached所有的值均是简单的字符串，redis作为其替代者，支持更为丰富的数据类型</p><p>(2) redis的速度比memcached快很多</p><p>(3) redis可以持久化其数据</p><h3 id="如何保证缓存与数据库双写时的数据一致性？"><a href="#如何保证缓存与数据库双写时的数据一致性？" class="headerlink" title="如何保证缓存与数据库双写时的数据一致性？"></a>如何保证缓存与数据库双写时的数据一致性？</h3><p>你只要用缓存，就可能会涉及到缓存与数据库双存储双写，你只要是双写，就一定会有数据一致性的问题，那么你如何解决一致性问题？</p><p>一般来说，就是如果你的系统不是严格要求缓存+数据库必须一致性的话，缓存可以稍微的跟数据库偶尔有不一致的情况，最好不要做这个方案，读请求和写请求串行化，串到一个内存队列里去，这样就可以保证一定不会出现不一致的情况</p><p>串行化之后，就会导致系统的吞吐量会大幅度的降低，用比正常情况下多几倍的机器去支撑线上的一个请求。</p><p>还有一种方式就是可能会暂时产生不一致的情况，但是发生的几率特别小，就是<strong>先更新数据库，然后再删除缓存</strong>。</p><h3 id="Redis常见性能问题和解决方案？"><a href="#Redis常见性能问题和解决方案？" class="headerlink" title="Redis常见性能问题和解决方案？"></a>Redis常见性能问题和解决方案？</h3><ol><li>Master最好不要做任何持久化工作，包括内存快照和AOF日志文件，特别是不要启用内存快照做持久化。</li><li>如果数据比较关键，某个Slave开启AOF备份数据，策略为每秒同步一次。</li><li>为了主从复制的速度和连接的稳定性，Slave和Master最好在同一个局域网内。</li><li>尽量避免在压力较大的主库上增加从库</li><li>Master调用BGREWRITEAOF重写AOF文件，AOF在重写的时候会占大量的CPU和内存资源，导致服务load过高，出现短暂服务暂停现象。</li><li>为了Master的稳定性，主从复制不要用图状结构，用单向链表结构更稳定，即主从关系为：Master&lt;–Slave1&lt;–Slave2&lt;–Slave3…，这样的结构也方便解决单点故障问题，实现Slave对Master的替换，也即，如果Master挂了，可以立马启用Slave1做Master，其他不变。</li></ol><h3 id="Redis官方为什么不提供Windows版本？"><a href="#Redis官方为什么不提供Windows版本？" class="headerlink" title="Redis官方为什么不提供Windows版本？"></a>Redis官方为什么不提供Windows版本？</h3><p>因为目前Linux版本已经相当稳定，而且用户量很大，无需开发windows版本，反而会带来兼容性等问题。</p><h3 id="一个字符串类型的值能存储最大容量是多少？"><a href="#一个字符串类型的值能存储最大容量是多少？" class="headerlink" title="一个字符串类型的值能存储最大容量是多少？"></a>一个字符串类型的值能存储最大容量是多少？</h3><p>512M</p><h3 id="Redis如何做大量数据插入？"><a href="#Redis如何做大量数据插入？" class="headerlink" title="Redis如何做大量数据插入？"></a>Redis如何做大量数据插入？</h3><p>Redis2.6开始redis-cli支持一种新的被称之为pipe mode的新模式用于执行大量数据插入工作。</p><h3 id="假如Redis里面有1亿个key，其中有10w个key是以某个固定的已知的前缀开头的，如何将它们全部找出来？"><a href="#假如Redis里面有1亿个key，其中有10w个key是以某个固定的已知的前缀开头的，如何将它们全部找出来？" class="headerlink" title="假如Redis里面有1亿个key，其中有10w个key是以某个固定的已知的前缀开头的，如何将它们全部找出来？"></a>假如Redis里面有1亿个key，其中有10w个key是以某个固定的已知的前缀开头的，如何将它们全部找出来？</h3><p>使用keys指令可以扫出指定模式的key列表。<br>对方接着追问：如果这个redis正在给线上的业务提供服务，那使用keys指令会有什么问题？<br>这个时候你要回答redis关键的一个特性：redis的单线程的。keys指令会导致线程阻塞一段时间，线上服务会停顿，直到指令执行完毕，服务才能恢复。这个时候可以使用scan指令，scan指令可以无阻塞的提取出指定模式的key列表，但是会有一定的重复概率，在客户端做一次去重就可以了，但是整体所花费的时间会比直接用keys指令长。</p><h3 id="使用Redis做过异步队列吗，是如何实现的"><a href="#使用Redis做过异步队列吗，是如何实现的" class="headerlink" title="使用Redis做过异步队列吗，是如何实现的"></a>使用Redis做过异步队列吗，是如何实现的</h3><p>使用list类型保存数据信息，rpush生产消息，lpop消费消息，当lpop没有消息时，可以sleep一段时间，然后再检查有没有信息，如果不想sleep的话，可以使用blpop, 在没有信息的时候，会一直阻塞，直到信息的到来。redis可以通过pub/sub主题订阅模式实现一个生产者，多个消费者，当然也存在一定的缺点，当消费者下线时，生产的消息会丢失。</p><h3 id="Redis如何实现延时队列"><a href="#Redis如何实现延时队列" class="headerlink" title="Redis如何实现延时队列"></a>Redis如何实现延时队列</h3><p>使用sortedset，使用时间戳做score, 消息内容作为key,调用zadd来生产消息，消费者使用zrangbyscore获取n秒之前的数据做轮询处理。</p><h3 id="Redis回收进程如何工作的？"><a href="#Redis回收进程如何工作的？" class="headerlink" title="Redis回收进程如何工作的？"></a>Redis回收进程如何工作的？</h3><ol><li>一个客户端运行了新的命令，添加了新的数据。</li><li>Redis检查内存使用情况，如果大于maxmemory的限制， 则根据设定好的策略进行回收。</li><li>一个新的命令被执行，等等。</li><li>所以我们不断地穿越内存限制的边界，通过不断达到边界然后不断地回收回到边界以下。</li></ol><p>如果一个命令的结果导致大量内存被使用（例如很大的集合的交集保存到一个新的键），不用多久内存限制就会被这个内存使用量超越。</p><h3 id="Redis回收使用的是什么算法？"><a href="#Redis回收使用的是什么算法？" class="headerlink" title="Redis回收使用的是什么算法？"></a>Redis回收使用的是什么算法？</h3><p>LRU算法</p>]]></content>
      
      
      <categories>
          
          <category> java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> redis </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>MyBatis笔记</title>
      <link href="/2020/06/30/MyBatis%E7%AC%94%E8%AE%B0/"/>
      <url>/2020/06/30/MyBatis%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<h1 id="MyBatis笔记"><a href="#MyBatis笔记" class="headerlink" title="MyBatis笔记"></a>MyBatis笔记</h1><h2 id="MyBatis简介"><a href="#MyBatis简介" class="headerlink" title="MyBatis简介"></a>MyBatis简介</h2><h3 id="MyBatis是什么？"><a href="#MyBatis是什么？" class="headerlink" title="MyBatis是什么？"></a>MyBatis是什么？</h3><p>MyBatis 是一款优秀的持久层框架，一个半 ORM（对象关系映射）框架，它支持定制化 SQL、存储过程以及高级映射。MyBatis 避免了几乎所有的 JDBC 代码和手动设置参数以及获取结果集。MyBatis 可以使用简单的 XML 或注解来配置和映射原生类型、接口和 Java 的 POJO（Plain Old Java Objects，普通老式 Java 对象）为数据库中的记录。</p><h3 id="ORM是什么"><a href="#ORM是什么" class="headerlink" title="ORM是什么"></a>ORM是什么</h3><p>ORM（Object Relational Mapping），对象关系映射，是一种为了解决关系型数据库数据与简单Java对象（POJO）的映射关系的技术。简单的说，ORM是通过使用描述对象和数据库之间映射的元数据，将程序中的对象自动持久化到关系型数据库中。</p><h3 id="为什么说Mybatis是半自动ORM映射工具？它与全自动的区别在哪里？"><a href="#为什么说Mybatis是半自动ORM映射工具？它与全自动的区别在哪里？" class="headerlink" title="为什么说Mybatis是半自动ORM映射工具？它与全自动的区别在哪里？"></a>为什么说Mybatis是半自动ORM映射工具？它与全自动的区别在哪里？</h3><p>Hibernate属于全自动ORM映射工具，使用Hibernate查询关联对象或者关联集合对象时，可以根据对象关系模型直接获取，所以它是全自动的。</p><p>而Mybatis在查询关联对象或关联集合对象时，需要手动编写sql来完成，所以，称之为半自动ORM映射工具。</p><h3 id="JDBC编程有哪些不足之处，MyBatis是如何解决这些问题的？"><a href="#JDBC编程有哪些不足之处，MyBatis是如何解决这些问题的？" class="headerlink" title="JDBC编程有哪些不足之处，MyBatis是如何解决这些问题的？"></a>JDBC编程有哪些不足之处，MyBatis是如何解决这些问题的？</h3><p>1、数据库链接创建、释放频繁造成系统资源浪费从而影响系统性能，如果使用数据库连接池可解决此问题。</p><p>解决：在mybatis-config.xml中配置数据链接池，使用连接池管理数据库连接。</p><p>2、Sql语句写在代码中造成代码不易维护，实际应用sql变化的可能较大，sql变动需要改变java代码。</p><p>解决：将Sql语句配置在XXXXmapper.xml文件中与java代码分离。</p><p>3、向sql语句传参数麻烦，因为sql语句的where条件不一定，可能多也可能少，占位符需要和参数一一对应。</p><p>解决： Mybatis自动将java对象映射至sql语句。</p><p>4、对结果集解析麻烦，sql变化导致解析代码变化，且解析前需要遍历，如果能将数据库记录封装成pojo对象解析比较方便。</p><p>解决：Mybatis自动将sql执行结果映射至java对象。</p><h3 id="Mybatis优缺点"><a href="#Mybatis优缺点" class="headerlink" title="Mybatis优缺点"></a>Mybatis优缺点</h3><p>优点</p><p>与传统的数据库访问技术相比，ORM有以下优点：</p><ul><li>基于SQL语句编程，相当灵活，不会对应用程序或者数据库的现有设计造成任何影响，SQL写在XML里，解除sql与程序代码的耦合，便于统一管理；提供XML标签，支持编写动态SQL语句，并可重用</li><li>与JDBC相比，减少了50%以上的代码量，消除了JDBC大量冗余的代码，不需要手动开关连接</li><li>很好的与各种数据库兼容（因为MyBatis使用JDBC来连接数据库，所以只要JDBC支持的数据库MyBatis都支持）</li><li>提供映射标签，支持对象与数据库的ORM字段关系映射；提供对象关系映射标签，支持对象关系组件维护<br>能够与Spring很好的集成</li></ul><p>缺点</p><ul><li>SQL语句的编写工作量较大，尤其当字段多、关联表多时，对开发人员编写SQL语句的功底有一定要求</li><li>SQL语句依赖于数据库，导致数据库移植性差，不能随意更换数据库</li></ul><h3 id="MyBatis框架适用场景"><a href="#MyBatis框架适用场景" class="headerlink" title="MyBatis框架适用场景"></a>MyBatis框架适用场景</h3><ul><li>MyBatis专注于SQL本身，是一个足够灵活的DAO层解决方案。</li><li>对性能的要求很高，或者需求变化较多的项目，如互联网项目，MyBatis将是不错的选择。</li></ul><h3 id="Hibernate-和-MyBatis-的区别"><a href="#Hibernate-和-MyBatis-的区别" class="headerlink" title="Hibernate 和 MyBatis 的区别"></a>Hibernate 和 MyBatis 的区别</h3><p>总结</p><p>MyBatis 是一个小巧、方便、高效、简单、直接、半自动化的持久层框架，</p><p>Hibernate 是一个强大、方便、高效、复杂、间接、全自动化的持久层框架。</p><h2 id="MyBatis的解析和运行原理"><a href="#MyBatis的解析和运行原理" class="headerlink" title="MyBatis的解析和运行原理"></a>MyBatis的解析和运行原理</h2><h3 id="MyBatis编程步骤是什么样的？"><a href="#MyBatis编程步骤是什么样的？" class="headerlink" title="MyBatis编程步骤是什么样的？"></a>MyBatis编程步骤是什么样的？</h3><p>1、 创建SqlSessionFactory</p><p>2、 通过SqlSessionFactory创建SqlSession</p><p>3、 通过sqlsession执行数据库操作</p><p>4、 调用session.commit()提交事务</p><p>5、 调用session.close()关闭会话</p><h3 id="请说说MyBatis的工作原理"><a href="#请说说MyBatis的工作原理" class="headerlink" title="请说说MyBatis的工作原理"></a>请说说MyBatis的工作原理</h3><p><img src="https://lixiangbetter.github.io/2020/06/30/MyBatis%E7%AC%94%E8%AE%B0/aHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL0pvdXJXb24vaW1hZ2UvbWFzdGVyL015QmF0aXMlRTYlQTElODYlRTYlOUUlQjYlRTYlODAlQkIlRTclQkIlOTMvTXlCYXRpcyVFNSVCNyVBNSVFNCVCRCU5QyVFNSU4RSU5RiVFNyU5MCU4Ni5wbmc.jpeg" alt></p><h3 id="MyBatis的功能架构是怎样的"><a href="#MyBatis的功能架构是怎样的" class="headerlink" title="MyBatis的功能架构是怎样的"></a>MyBatis的功能架构是怎样的</h3><p><img src="https://lixiangbetter.github.io/2020/06/30/MyBatis%E7%AC%94%E8%AE%B0/aHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL0pvdXJXb24vaW1hZ2UvbWFzdGVyL015QmF0aXMlRTYlQTElODYlRTYlOUUlQjYlRTYlODAlQkIlRTclQkIlOTMvTXlCYXRpcyVFNSU4QSU5RiVFOCU4MyVCRCVFNiU5RSVCNiVFNiU5RSU4NC5wbmc.jpeg" alt></p><h3 id="为什么需要预编译"><a href="#为什么需要预编译" class="headerlink" title="为什么需要预编译"></a>为什么需要预编译</h3><ol><li>定义：<br>SQL 预编译指的是数据库驱动在发送 SQL 语句和参数给 DBMS 之前对 SQL 语句进行编译，这样 DBMS 执行 SQL 时，就不需要重新编译。</li><li>为什么需要预编译<br>JDBC 中使用对象 PreparedStatement 来抽象预编译语句，使用预编译。预编译阶段可以优化 SQL 的执行。预编译之后的 SQL 多数情况下可以直接执行，DBMS 不需要再次编译，越复杂的SQL，编译的复杂度将越大，预编译阶段可以合并多次操作为一个操作。同时预编译语句对象可以重复利用。把一个 SQL 预编译后产生的 PreparedStatement 对象缓存下来，下次对于同一个SQL，可以直接使用这个缓存的 PreparedState 对象。Mybatis默认情况下，将对所有的 SQL 进行预编译。</li></ol><h3 id="Mybatis都有哪些Executor执行器？它们之间的区别是什么？"><a href="#Mybatis都有哪些Executor执行器？它们之间的区别是什么？" class="headerlink" title="Mybatis都有哪些Executor执行器？它们之间的区别是什么？"></a>Mybatis都有哪些Executor执行器？它们之间的区别是什么？</h3><p>Mybatis有三种基本的Executor执行器，SimpleExecutor、ReuseExecutor、BatchExecutor。</p><p>SimpleExecutor：每执行一次update或select，就开启一个Statement对象，用完立刻关闭Statement对象。</p><p>ReuseExecutor：执行update或select，以sql作为key查找Statement对象，存在就使用，不存在就创建，用完后，不关闭Statement对象，而是放置于Map&lt;String, Statement&gt;内，供下一次使用。简言之，就是重复使用Statement对象。</p><p>BatchExecutor：执行update（没有select，JDBC批处理不支持select），将所有sql都添加到批处理中（addBatch()），等待统一执行（executeBatch()），它缓存了多个Statement对象，每个Statement对象都是addBatch()完毕后，等待逐一执行executeBatch()批处理。与JDBC批处理相同。</p><p>作用范围：Executor的这些特点，都严格限制在SqlSession生命周期范围内。</p><h3 id="Mybatis中如何指定使用哪一种Executor执行器？"><a href="#Mybatis中如何指定使用哪一种Executor执行器？" class="headerlink" title="Mybatis中如何指定使用哪一种Executor执行器？"></a>Mybatis中如何指定使用哪一种Executor执行器？</h3><p>在Mybatis配置文件中，在设置（settings）可以指定默认的ExecutorType执行器类型，也可以手动给DefaultSqlSessionFactory的创建SqlSession的方法传递ExecutorType类型参数，如SqlSession openSession(ExecutorType execType)。</p><h3 id="Mybatis是否支持延迟加载？如果支持，它的实现原理是什么？"><a href="#Mybatis是否支持延迟加载？如果支持，它的实现原理是什么？" class="headerlink" title="Mybatis是否支持延迟加载？如果支持，它的实现原理是什么？"></a>Mybatis是否支持延迟加载？如果支持，它的实现原理是什么？</h3><p>Mybatis仅支持association关联对象和collection关联集合对象的延迟加载，association指的就是一对一，collection指的就是一对多查询。在Mybatis配置文件中，可以配置是否启用延迟加载lazyLoadingEnabled=true|false。</p><p>它的原理是，使用CGLIB创建目标对象的代理对象，当调用目标方法时，进入拦截器方法，比如调用a.getB().getName()，拦截器invoke()方法发现a.getB()是null值，那么就会单独发送事先保存好的查询关联B对象的sql，把B查询上来，然后调用a.setB(b)，于是a的对象b属性就有值了，接着完成a.getB().getName()方法的调用。这就是延迟加载的基本原理。</p><h2 id="映射器"><a href="#映射器" class="headerlink" title="映射器"></a>映射器</h2><h3 id="和-的区别"><a href="#和-的区别" class="headerlink" title="#{}和${}的区别"></a>#{}和${}的区别</h3><ul><li>#{}是占位符，预编译处理；${}是拼接符，字符串替换，没有预编译处理。</li><li>Mybatis在处理#{}时，#{}传入参数是以字符串传入，会将SQL中的#{}替换为?号，调用PreparedStatement的set方法来赋值。</li><li>Mybatis在处理时，是原值传入，就是把{}时，是原值传入，就是把时，是原值传入，就是把{}替换成变量的值，相当于JDBC中的Statement编译</li><li>变量替换后，#{} 对应的变量自动加上单引号 ‘’；变量替换后，${} 对应的变量不会加上单引号 ‘’</li><li>#{} 可以有效的防止SQL注入，提高系统安全性；${} 不能防止SQL 注入</li><li>#{} 的变量替换是在DBMS 中；${} 的变量替换是在 DBMS 外</li></ul><h3 id="模糊查询like语句该怎么写"><a href="#模糊查询like语句该怎么写" class="headerlink" title="模糊查询like语句该怎么写"></a>模糊查询like语句该怎么写</h3><p>（1）’%${question}%’ 可能引起SQL注入，不推荐</p><p>（2）”%”#{question}”%” 注意：因为#{…}解析成sql语句时候，会在变量外侧自动加单引号’ ‘，所以这里 % 需要使用双引号” “，不能使用单引号 ’ ‘，不然会查不到任何结果。</p><p>（3）CONCAT(’%’,#{question},’%’) 使用CONCAT()函数，推荐</p><p>（4）使用bind标签</p><h3 id="在mapper中如何传递多个参数"><a href="#在mapper中如何传递多个参数" class="headerlink" title="在mapper中如何传递多个参数"></a>在mapper中如何传递多个参数</h3><p><strong>方法1：顺序传参法</strong></p><p><strong>方法2：@Param注解传参法</strong> 推荐</p><p><strong>方法3：Map传参法</strong></p><p><strong>方法4：Java Bean传参法</strong> 推荐</p><h3 id="Mybatis如何执行批量操作"><a href="#Mybatis如何执行批量操作" class="headerlink" title="Mybatis如何执行批量操作"></a>Mybatis如何执行批量操作</h3><p><strong>使用foreach标签</strong></p><p><strong>使用ExecutorType.BATCH</strong></p><h3 id="如何获取生成的主键"><a href="#如何获取生成的主键" class="headerlink" title="如何获取生成的主键"></a>如何获取生成的主键</h3><p><strong>对于支持主键自增的数据库（MySQL）</strong></p><p><strong>不支持主键自增的数据库（Oracle）</strong></p><selectkey><h3 id="当实体类中的属性名和表中的字段名不一样-，怎么办"><a href="#当实体类中的属性名和表中的字段名不一样-，怎么办" class="headerlink" title="当实体类中的属性名和表中的字段名不一样 ，怎么办"></a>当实体类中的属性名和表中的字段名不一样 ，怎么办</h3><p>第1种： 通过在查询的SQL语句中定义字段名的别名，让字段名的别名和实体类的属性名一致。</p><p>第2种： 通过 resultmap 来映射字段名和实体类属性名的一一对应的关系。</p><h3 id="Mapper-编写有哪几种方式？"><a href="#Mapper-编写有哪几种方式？" class="headerlink" title="Mapper 编写有哪几种方式？"></a>Mapper 编写有哪几种方式？</h3><p>第三种：使用 mapper 扫描器：</p><p>（1）mapper.xml 文件编写：</p><p>mapper.xml 中的 namespace 为 mapper 接口的地址；</p><p>mapper 接口中的方法名和 mapper.xml 中的定义的 statement 的 id 保持一致；</p><p>如果将 mapper.xml 和 mapper 接口的名称保持一致则不用在 sqlMapConfig.xml中进行配置。</p><p>（2）定义 mapper 接口：</p><p>注意 mapper.xml 的文件名和 mapper 的接口名称保持一致，且放在同一个目录</p><p>（3）配置 mapper 扫描器</p><p>（4）使用扫描器后从 spring 容器中获取 mapper 的实现对象。</p><h3 id="什么是MyBatis的接口绑定？有哪些实现方式？"><a href="#什么是MyBatis的接口绑定？有哪些实现方式？" class="headerlink" title="什么是MyBatis的接口绑定？有哪些实现方式？"></a>什么是MyBatis的接口绑定？有哪些实现方式？</h3><h3 id="使用MyBatis的mapper接口调用时有哪些要求？"><a href="#使用MyBatis的mapper接口调用时有哪些要求？" class="headerlink" title="使用MyBatis的mapper接口调用时有哪些要求？"></a>使用MyBatis的mapper接口调用时有哪些要求？</h3><p>接口绑定，就是在MyBatis中任意定义接口，然后把接口里面的方法和SQL语句绑定，我们直接调用接口方法就可以，这样比起原来了SqlSession提供的方法我们可以有更加灵活的选择和设置。</p><p>接口绑定有两种实现方式</p><p>通过注解绑定，就是在接口的方法上面加上 @Select、@Update等注解，里面包含Sql语句来绑定；</p><p>通过xml里面写SQL来绑定， 在这种情况下，要指定xml映射文件里面的namespace必须为接口的全路径名。当Sql语句比较简单时候，用注解绑定， 当SQL语句比较复杂时候，用xml绑定，一般用xml绑定的比较多。</p><h3 id="使用MyBatis的mapper接口调用时有哪些要求？-1"><a href="#使用MyBatis的mapper接口调用时有哪些要求？-1" class="headerlink" title="使用MyBatis的mapper接口调用时有哪些要求？"></a>使用MyBatis的mapper接口调用时有哪些要求？</h3><p>1、Mapper接口方法名和mapper.xml中定义的每个sql的id相同。</p><p>2、Mapper接口方法的输入参数类型和mapper.xml中定义的每个sql 的parameterType的类型相同。</p><p>3、Mapper接口方法的输出参数类型和mapper.xml中定义的每个sql的resultType的类型相同。</p><p>4、Mapper.xml文件中的namespace即是mapper接口的类路径。</p><h3 id="最佳实践中，通常一个Xml映射文件，都会写一个Dao接口与之对应，请问，这个Dao接口的工作原理是什么？Dao接口里的方法，参数不同时，方法能重载吗"><a href="#最佳实践中，通常一个Xml映射文件，都会写一个Dao接口与之对应，请问，这个Dao接口的工作原理是什么？Dao接口里的方法，参数不同时，方法能重载吗" class="headerlink" title="最佳实践中，通常一个Xml映射文件，都会写一个Dao接口与之对应，请问，这个Dao接口的工作原理是什么？Dao接口里的方法，参数不同时，方法能重载吗"></a>最佳实践中，通常一个Xml映射文件，都会写一个Dao接口与之对应，请问，这个Dao接口的工作原理是什么？Dao接口里的方法，参数不同时，方法能重载吗</h3><p>Mapper接口是没有实现类的，当调用接口方法时，接口全限名+方法名拼接字符串作为key值，可唯一定位一个MappedStatement，举例：com.mybatis3.mappers.StudentDao.findStudentById，可以唯一找到namespace为com.mybatis3.mappers.StudentDao下面id = findStudentById的MappedStatement。在Mybatis中，每一个<select>、<insert>、<update>、<delete>标签，都会被解析为一个MappedStatement对象。</delete></update></insert></select></p><p>Dao接口里的方法，是不能重载的，因为是全限名+方法名的保存和寻找策略。</p><p>Dao接口的工作原理是JDK动态代理，Mybatis运行时会使用JDK动态代理为Dao接口生成代理proxy对象，代理对象proxy会拦截接口方法，转而执行MappedStatement所代表的sql，然后将sql执行结果返回。</p><h3 id="Mybatis的Xml映射文件中，不同的Xml映射文件，id是否可以重复？"><a href="#Mybatis的Xml映射文件中，不同的Xml映射文件，id是否可以重复？" class="headerlink" title="Mybatis的Xml映射文件中，不同的Xml映射文件，id是否可以重复？"></a>Mybatis的Xml映射文件中，不同的Xml映射文件，id是否可以重复？</h3><p>不同的Xml映射文件，如果配置了namespace，那么id可以重复；如果没有配置namespace，那么id不能重复；毕竟namespace不是必须的，只是最佳实践而已。</p><p>原因就是namespace+id是作为Map&lt;String, MappedStatement&gt;的key使用的，如果没有namespace，就剩下id，那么，id重复会导致数据互相覆盖。有了namespace，自然id就可以重复，namespace不同，namespace+id自然也就不同。</p><h3 id="简述Mybatis的Xml映射文件和Mybatis内部数据结构之间的映射关系？"><a href="#简述Mybatis的Xml映射文件和Mybatis内部数据结构之间的映射关系？" class="headerlink" title="简述Mybatis的Xml映射文件和Mybatis内部数据结构之间的映射关系？"></a>简述Mybatis的Xml映射文件和Mybatis内部数据结构之间的映射关系？</h3><p>Mybatis将所有Xml配置信息都封装到All-In-One重量级对象Configuration内部。在Xml映射文件中，<parametermap>标签会被解析为ParameterMap对象，其每个子元素会被解析为ParameterMapping对象。<resultmap>标签会被解析为ResultMap对象，其每个子元素会被解析为ResultMapping对象。每一个<select>、<insert>、<update>、<delete>标签均会被解析为MappedStatement对象，标签内的sql会被解析为BoundSql对象。</delete></update></insert></select></resultmap></parametermap></p><h3 id="Mybatis是如何将sql执行结果封装为目标对象并返回的？都有哪些映射形式？"><a href="#Mybatis是如何将sql执行结果封装为目标对象并返回的？都有哪些映射形式？" class="headerlink" title="Mybatis是如何将sql执行结果封装为目标对象并返回的？都有哪些映射形式？"></a>Mybatis是如何将sql执行结果封装为目标对象并返回的？都有哪些映射形式？</h3><p>第一种是使用<resultmap>标签，逐一定义列名和对象属性名之间的映射关系。</resultmap></p><p>第二种是使用sql列的别名功能，将列别名书写为对象属性名，比如T_NAME AS NAME，对象属性名一般是name，小写，但是列名不区分大小写，Mybatis会忽略列名大小写，智能找到与之对应对象属性名，你甚至可以写成T_NAME AS NaMe，Mybatis一样可以正常工作。</p><p>有了列名与属性名的映射关系后，Mybatis通过反射创建对象，同时使用反射给对象的属性逐一赋值并返回，那些找不到映射关系的属性，是无法完成赋值的。</p><h3 id="Xml映射文件中，除了常见的select-insert-updae-delete标签之外，还有哪些标签？"><a href="#Xml映射文件中，除了常见的select-insert-updae-delete标签之外，还有哪些标签？" class="headerlink" title="Xml映射文件中，除了常见的select|insert|updae|delete标签之外，还有哪些标签？"></a>Xml映射文件中，除了常见的select|insert|updae|delete标签之外，还有哪些标签？</h3><p>还有很多其他的标签，<resultmap>、<parametermap>、<sql>、<include>、<selectkey>，加上动态sql的9个标签，trim|where|set|foreach|if|choose|when|otherwise|bind等，其中<sql>为sql片段标签，通过<include>标签引入sql片段，<selectkey>为不支持自增的主键生成策略标签。</selectkey></include></sql></selectkey></include></sql></parametermap></resultmap></p><h3 id="Mybatis映射文件中，如果A标签通过include引用了B标签的内容，请问，B标签能否定义在A标签的后面，还是说必须定义在A标签的前面？"><a href="#Mybatis映射文件中，如果A标签通过include引用了B标签的内容，请问，B标签能否定义在A标签的后面，还是说必须定义在A标签的前面？" class="headerlink" title="Mybatis映射文件中，如果A标签通过include引用了B标签的内容，请问，B标签能否定义在A标签的后面，还是说必须定义在A标签的前面？"></a>Mybatis映射文件中，如果A标签通过include引用了B标签的内容，请问，B标签能否定义在A标签的后面，还是说必须定义在A标签的前面？</h3><p>虽然Mybatis解析Xml映射文件是按照顺序解析的，但是，被引用的B标签依然可以定义在任何地方，Mybatis都可以正确识别。</p><p>原理是，Mybatis解析A标签，发现A标签引用了B标签，但是B标签尚未解析到，尚不存在，此时，Mybatis会将A标签标记为未解析状态，然后继续解析余下的标签，包含B标签，待所有标签解析完毕，Mybatis会重新解析那些被标记为未解析的标签，此时再解析A标签时，B标签已经存在，A标签也就可以正常解析完成了。</p><h2 id="高级查询"><a href="#高级查询" class="headerlink" title="高级查询"></a>高级查询</h2><h3 id="MyBatis实现一对一，一对多有几种方式，怎么操作的？"><a href="#MyBatis实现一对一，一对多有几种方式，怎么操作的？" class="headerlink" title="MyBatis实现一对一，一对多有几种方式，怎么操作的？"></a>MyBatis实现一对一，一对多有几种方式，怎么操作的？</h3><p>有联合查询和嵌套查询。联合查询是几个表联合查询，只查询一次，通过在resultMap里面的association，collection节点配置一对一，一对多的类就可以完成</p><p>嵌套查询是先查一个表，根据这个表里面的结果的外键id，去再另外一个表里面查询数据，也是通过配置association，collection，但另外一个表的查询通过select节点配置。</p><h3 id="Mybatis是否可以映射Enum枚举类？"><a href="#Mybatis是否可以映射Enum枚举类？" class="headerlink" title="Mybatis是否可以映射Enum枚举类？"></a>Mybatis是否可以映射Enum枚举类？</h3><p>Mybatis可以映射枚举类，不单可以映射枚举类，Mybatis可以映射任何对象到表的一列上。映射方式为自定义一个TypeHandler，实现TypeHandler的setParameter()和getResult()接口方法。</p><p>TypeHandler有两个作用，一是完成从javaType至jdbcType的转换，二是完成jdbcType至javaType的转换，体现为setParameter()和getResult()两个方法，分别代表设置sql问号占位符参数和获取列查询结果。</p><h2 id="动态SQL"><a href="#动态SQL" class="headerlink" title="动态SQL"></a>动态SQL</h2><h3 id="Mybatis动态sql是做什么的？都有哪些动态sql？能简述一下动态sql的执行原理不？"><a href="#Mybatis动态sql是做什么的？都有哪些动态sql？能简述一下动态sql的执行原理不？" class="headerlink" title="Mybatis动态sql是做什么的？都有哪些动态sql？能简述一下动态sql的执行原理不？"></a>Mybatis动态sql是做什么的？都有哪些动态sql？能简述一下动态sql的执行原理不？</h3><p>Mybatis动态sql可以让我们在Xml映射文件内，以标签的形式编写动态sql，完成逻辑判断和动态拼接sql的功能，Mybatis提供了9种动态sql标签trim|where|set|foreach|if|choose|when|otherwise|bind。</p><p>其执行原理为，使用OGNL从sql参数对象中计算表达式的值，根据表达式的值动态拼接sql，以此来完成动态sql的功能。</p><h2 id="插件模块"><a href="#插件模块" class="headerlink" title="插件模块"></a>插件模块</h2><h3 id="Mybatis是如何进行分页的？分页插件的原理是什么？"><a href="#Mybatis是如何进行分页的？分页插件的原理是什么？" class="headerlink" title="Mybatis是如何进行分页的？分页插件的原理是什么？"></a>Mybatis是如何进行分页的？分页插件的原理是什么？</h3><p>Mybatis使用RowBounds对象进行分页，它是针对ResultSet结果集执行的内存分页，而非物理分页，可以在sql内直接书写带有物理分页的参数来完成物理分页功能，也可以使用分页插件来完成物理分页。</p><p>分页插件的基本原理是使用Mybatis提供的插件接口，实现自定义插件，在插件的拦截方法内拦截待执行的sql，然后重写sql，根据dialect方言，添加对应的物理分页语句和物理分页参数。</p><p>举例：select * from student，拦截sql后重写为：select t.* from (select * from student) t limit 0, 10</p><h3 id="简述Mybatis的插件运行原理，以及如何编写一个插件。"><a href="#简述Mybatis的插件运行原理，以及如何编写一个插件。" class="headerlink" title="简述Mybatis的插件运行原理，以及如何编写一个插件。"></a>简述Mybatis的插件运行原理，以及如何编写一个插件。</h3><p>Mybatis仅可以编写针对ParameterHandler、ResultSetHandler、StatementHandler、Executor这4种接口的插件，Mybatis使用JDK的动态代理，为需要拦截的接口生成代理对象以实现接口方法拦截功能，每当执行这4种接口对象的方法时，就会进入拦截方法，具体就是InvocationHandler的invoke()方法，当然，只会拦截那些你指定需要拦截的方法。</p><p>实现Mybatis的Interceptor接口并复写intercept()方法，然后在给插件编写注解，指定要拦截哪一个接口的哪些方法即可，记住，别忘了在配置文件中配置你编写的插件。</p><h2 id="缓存"><a href="#缓存" class="headerlink" title="缓存"></a>缓存</h2><h3 id="Mybatis的一级、二级缓存"><a href="#Mybatis的一级、二级缓存" class="headerlink" title="Mybatis的一级、二级缓存"></a>Mybatis的一级、二级缓存</h3><p>1）一级缓存: 基于 PerpetualCache 的 HashMap 本地缓存，其存储作用域为 Session，当 Session flush 或 close 之后，该 Session 中的所有 Cache 就将清空，默认打开一级缓存。</p><p>2）二级缓存与一级缓存其机制相同，默认也是采用 PerpetualCache，HashMap 存储，不同在于其存储作用域为 Mapper(Namespace)，并且可自定义存储源，如 Ehcache。默认不打开二级缓存，要开启二级缓存，使用二级缓存属性类需要实现Serializable序列化接口(可用来保存对象的状态),可在它的映射文件中配置<cache> ；</cache></p><p>3）对于缓存数据更新机制，当某一个作用域(一级缓存 Session/二级缓存Namespaces)的进行了C/U/D 操作后，默认该作用域下所有 select 中的缓存将被 clear。</p></selectkey>]]></content>
      
      
      <categories>
          
          <category> java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> mybatis </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spring Cloud笔记</title>
      <link href="/2020/06/30/Spring-Cloud%E7%AC%94%E8%AE%B0/"/>
      <url>/2020/06/30/Spring-Cloud%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<h1 id="Spring-Cloud笔记"><a href="#Spring-Cloud笔记" class="headerlink" title="Spring Cloud笔记"></a>Spring Cloud笔记</h1><h2 id="为什么需要学习Spring-Cloud"><a href="#为什么需要学习Spring-Cloud" class="headerlink" title="为什么需要学习Spring Cloud"></a>为什么需要学习Spring Cloud</h2><p>不论是商业应用还是用户应用，在业务初期都很简单，我们通常会把它实现为单体结构的应用。但是，随着业务逐渐发展，产品思想会变得越来越复杂，单体结构的应用也会越来越复杂。这就会给应用带来如下的几个问题：</p><p>代码结构混乱：业务复杂，导致代码量很大，管理会越来越困难。同时，这也会给业务的快速迭代带来巨大挑战；<br>开发效率变低：开发人员同时开发一套代码，很难避免代码冲突。开发过程会伴随着不断解决冲突的过程，这会严重的影响开发效率；<br>排查解决问题成本高：线上业务发现 bug，修复 bug 的过程可能很简单。但是，由于只有一套代码，需要重新编译、打包、上线，成本很高。<br>由于单体结构的应用随着系统复杂度的增高，会暴露出各种各样的问题。近些年来，微服务架构逐渐取代了单体架构，且这种趋势将会越来越流行。Spring Cloud是目前最常用的微服务开发框架，已经在企业级开发中大量的应用。</p><h2 id="什么是Spring-Cloud"><a href="#什么是Spring-Cloud" class="headerlink" title="什么是Spring Cloud"></a>什么是Spring Cloud</h2><p>Spring Cloud是一系列框架的有序集合。它利用Spring Boot的开发便利性巧妙地简化了分布式系统基础设施的开发，如服务发现注册、配置中心、智能路由、消息总线、负载均衡、断路器、数据监控等，都可以用Spring Boot的开发风格做到一键启动和部署。Spring Cloud并没有重复制造轮子，它只是将各家公司开发的比较成熟、经得起实际考验的服务框架组合起来，通过Spring Boot风格进行再封装屏蔽掉了复杂的配置和实现原理，最终给开发者留出了一套简单易懂、易部署和易维护的分布式系统开发工具包。</p><h2 id="设计目标与优缺点"><a href="#设计目标与优缺点" class="headerlink" title="设计目标与优缺点"></a>设计目标与优缺点</h2><h3 id="设计目标"><a href="#设计目标" class="headerlink" title="设计目标"></a>设计目标</h3><p><strong>协调各个微服务，简化分布式系统开发</strong>。</p><p>优缺点<br>微服务的框架那么多比如：dubbo、Kubernetes，为什么就要使用Spring Cloud的呢？</p><p>优点：</p><ul><li>产出于Spring大家族，Spring在企业级开发框架中无人能敌，来头很大，可以保证后续的更新、完善</li><li>组件丰富，功能齐全。Spring Cloud 为微服务架构提供了非常完整的支持。例如、配置管理、服务发现、断路器、微服务网关等；</li><li>Spring Cloud 社区活跃度很高，教程很丰富，遇到问题很容易找到解决方案</li><li>服务拆分粒度更细，耦合度比较低，有利于资源重复利用，有利于提高开发效率</li><li>可以更精准的制定优化服务方案，提高系统的可维护性</li><li>减轻团队的成本，可以并行开发，不用关注其他人怎么开发，先关注自己的开发</li><li>微服务可以是跨平台的，可以用任何一种语言开发</li><li>适于互联网时代，产品迭代周期更短</li></ul><p>缺点：</p><ul><li>微服务过多，治理成本高，不利于维护系统</li><li>分布式系统开发的成本高（容错，分布式事务等）对团队挑战大</li></ul><p>总的来说优点大过于缺点，目前看来Spring Cloud是一套非常完善的分布式框架，目前很多企业开始用微服务、Spring Cloud的优势是显而易见的。因此对于想研究微服务架构的同学来说，学习Spring Cloud是一个不错的选择。</p><h2 id="Spring-Cloud发展前景"><a href="#Spring-Cloud发展前景" class="headerlink" title="Spring Cloud发展前景"></a>Spring Cloud发展前景</h2><p>Spring Cloud对于中小型互联网公司来说是一种福音，因为这类公司往往没有实力或者没有足够的资金投入去开发自己的分布式系统基础设施，使用Spring Cloud一站式解决方案能在从容应对业务发展的同时大大减少开发成本。同时，随着近几年微服务架构和Docker容器概念的火爆，也会让Spring Cloud在未来越来越“云”化的软件开发风格中立有一席之地，尤其是在五花八门的分布式解决方案中提供了标准化的、全站式的技术方案，意义可能会堪比当年Servlet规范的诞生，有效推进服务端软件系统技术水平的进步。</p><h2 id="整体架构"><a href="#整体架构" class="headerlink" title="整体架构"></a>整体架构</h2><p><img src="https://lixiangbetter.github.io/2020/06/30/Spring-MVC%E7%AC%94%E8%AE%B0/20200208211439106.png/Spring-Cloud%E7%AC%94%E8%AE%B0/20191226143921760.png" alt></p><h2 id="主要项目"><a href="#主要项目" class="headerlink" title="主要项目"></a>主要项目</h2><p>Spring Cloud的子项目，大致可分成两类，一类是对现有成熟框架”Spring Boot化”的封装和抽象，也是数量最多的项目；第二类是开发了一部分分布式系统的基础设施的实现，如Spring Cloud Stream扮演的就是kafka, ActiveMQ这样的角色。</p><h3 id="Spring-Cloud-Config"><a href="#Spring-Cloud-Config" class="headerlink" title="Spring Cloud Config"></a>Spring Cloud Config</h3><p>集中配置管理工具，分布式系统中统一的外部配置管理，默认使用Git来存储配置，可以支持客户端配置的刷新及加密、解密操作。</p><h3 id="Spring-Cloud-Netflix"><a href="#Spring-Cloud-Netflix" class="headerlink" title="Spring Cloud Netflix"></a>Spring Cloud Netflix</h3><p>Netflix OSS 开源组件集成，包括Eureka、Hystrix、Ribbon、Feign、Zuul等核心组件。</p><ul><li>Eureka：服务治理组件，包括服务端的注册中心和客户端的服务发现机制；</li><li>Ribbon：负载均衡的服务调用组件，具有多种负载均衡调用策略；</li><li>Hystrix：服务容错组件，实现了断路器模式，为依赖服务的出错和延迟提供了容错能力；</li><li>Feign：基于Ribbon和Hystrix的声明式服务调用组件；</li><li>Zuul：API网关组件，对请求提供路由及过滤功能。</li></ul><h3 id="Spring-Cloud-Bus"><a href="#Spring-Cloud-Bus" class="headerlink" title="Spring Cloud Bus"></a>Spring Cloud Bus</h3><p>用于传播集群状态变化的消息总线，使用轻量级消息代理链接分布式系统中的节点，可以用来动态刷新集群中的服务配置。</p><h3 id="Spring-Cloud-Consul"><a href="#Spring-Cloud-Consul" class="headerlink" title="Spring Cloud Consul"></a>Spring Cloud Consul</h3><p>基于Hashicorp Consul的服务治理组件。</p><h3 id="Spring-Cloud-Security"><a href="#Spring-Cloud-Security" class="headerlink" title="Spring Cloud Security"></a>Spring Cloud Security</h3><p>安全工具包，对Zuul代理中的负载均衡OAuth2客户端及登录认证进行支持。</p><h3 id="Spring-Cloud-Sleuth"><a href="#Spring-Cloud-Sleuth" class="headerlink" title="Spring Cloud Sleuth"></a>Spring Cloud Sleuth</h3><p>Spring Cloud应用程序的分布式请求链路跟踪，支持使用Zipkin、HTrace和基于日志（例如ELK）的跟踪。</p><h3 id="Spring-Cloud-Stream"><a href="#Spring-Cloud-Stream" class="headerlink" title="Spring Cloud Stream"></a>Spring Cloud Stream</h3><p>轻量级事件驱动微服务框架，可以使用简单的声明式模型来发送及接收消息，主要实现为Apache Kafka及RabbitMQ。</p><h3 id="Spring-Cloud-Task"><a href="#Spring-Cloud-Task" class="headerlink" title="Spring Cloud Task"></a>Spring Cloud Task</h3><p>用于快速构建短暂、有限数据处理任务的微服务框架，用于向应用中添加功能性和非功能性的特性。</p><h3 id="Spring-Cloud-Zookeeper"><a href="#Spring-Cloud-Zookeeper" class="headerlink" title="Spring Cloud Zookeeper"></a>Spring Cloud Zookeeper</h3><p>基于Apache Zookeeper的服务治理组件。</p><h3 id="Spring-Cloud-Gateway"><a href="#Spring-Cloud-Gateway" class="headerlink" title="Spring Cloud Gateway"></a>Spring Cloud Gateway</h3><p>API网关组件，对请求提供路由及过滤功能。</p><h3 id="Spring-Cloud-OpenFeign"><a href="#Spring-Cloud-OpenFeign" class="headerlink" title="Spring Cloud OpenFeign"></a>Spring Cloud OpenFeign</h3><p>基于Ribbon和Hystrix的声明式服务调用组件，可以动态创建基于Spring MVC注解的接口实现用于服务调用，在Spring Cloud 2.0中已经取代Feign成为了一等公民。</p><h3 id="Spring-Cloud和SpringBoot版本对应关系"><a href="#Spring-Cloud和SpringBoot版本对应关系" class="headerlink" title="Spring Cloud和SpringBoot版本对应关系"></a>Spring Cloud和SpringBoot版本对应关系</h3><table><thead><tr><th>Spring Cloud Version</th><th>SpringBoot Version</th></tr></thead><tbody><tr><td>Hoxton</td><td>2.2.x</td></tr><tr><td>Greenwich</td><td>2.1.x</td></tr><tr><td>Finchley</td><td>2.0.x</td></tr><tr><td>Edgware</td><td>1.5.x</td></tr><tr><td>Dalston</td><td>1.5.x</td></tr></tbody></table><h2 id="SpringBoot和SpringCloud的区别？"><a href="#SpringBoot和SpringCloud的区别？" class="headerlink" title="SpringBoot和SpringCloud的区别？"></a>SpringBoot和SpringCloud的区别？</h2><p>SpringBoot专注于快速方便的开发单个个体微服务。</p><p>SpringCloud是关注全局的微服务协调整理治理框架，它将SpringBoot开发的一个个单体微服务整合并管理起来，</p><p>为各个微服务之间提供，配置管理、服务发现、断路器、路由、微代理、事件总线、全局锁、决策竞选、分布式会话等等集成服务</p><p>SpringBoot可以离开SpringCloud独立使用开发项目， 但是SpringCloud离不开SpringBoot ，属于依赖的关系</p><p>SpringBoot专注于快速、方便的开发单个微服务个体，SpringCloud关注全局的服务治理框架。</p><h2 id="服务注册和发现是什么意思？Spring-Cloud-如何实现？"><a href="#服务注册和发现是什么意思？Spring-Cloud-如何实现？" class="headerlink" title="服务注册和发现是什么意思？Spring Cloud 如何实现？"></a>服务注册和发现是什么意思？Spring Cloud 如何实现？</h2><p>由于所有服务都在 Eureka 服务器上注册并通过调用 Eureka 服务器完成查找，因此无需处理服务地点的任何更改和处理。</p><h2 id="Spring-Cloud-和dubbo区别"><a href="#Spring-Cloud-和dubbo区别" class="headerlink" title="Spring Cloud 和dubbo区别?"></a>Spring Cloud 和dubbo区别?</h2><p>（1）服务调用方式 dubbo是RPC springcloud Rest Api</p><p>（2）注册中心,dubbo 是zookeeper springcloud是eureka，也可以是zookeeper</p><p>（3）服务网关,dubbo本身没有实现，只能通过其他第三方技术整合，springcloud有Zuul路由网关</p><h2 id="负载平衡的意义什么？"><a href="#负载平衡的意义什么？" class="headerlink" title="负载平衡的意义什么？"></a>负载平衡的意义什么？</h2><p>在计算中，负载平衡可以改善跨计算机，计算机集群，网络链接，中央处理单元或磁盘驱动器等多种计算资源的工作负载分布。负载平衡旨在优化资源使用，最大化吞吐量，最小化响应时间并避免任何单一资源的过载。使用多个组件进行负载平衡而不是单个组件可能会通过冗余来提高可靠性和可用性。负载平衡通常涉及专用软件或硬件，例如多层交换机或域名系统服务器进程。</p><h2 id="什么是-Hystrix？它如何实现容错？"><a href="#什么是-Hystrix？它如何实现容错？" class="headerlink" title="什么是 Hystrix？它如何实现容错？"></a>什么是 Hystrix？它如何实现容错？</h2><p>Hystrix 是一个延迟和容错库，旨在隔离远程系统，服务和第三方库的访问点，当出现故障是不可避免的故障时，停止级联故障并在复杂的分布式系统中实现弹性。</p><p>通常对于使用微服务架构开发的系统，涉及到许多微服务。这些微服务彼此协作。</p><h2 id="什么是-Hystrix-断路器？我们需要它吗？"><a href="#什么是-Hystrix-断路器？我们需要它吗？" class="headerlink" title="什么是 Hystrix 断路器？我们需要它吗？"></a>什么是 Hystrix 断路器？我们需要它吗？</h2><p>由于某些原因，employee-consumer 公开服务会引发异常。在这种情况下使用Hystrix 我们定义了一个回退方法。如果在公开服务中发生异常，则回退方法返回一些默认值。</p><h2 id="什么是-Netflix-Feign？它的优点是什么？"><a href="#什么是-Netflix-Feign？它的优点是什么？" class="headerlink" title="什么是 Netflix Feign？它的优点是什么？"></a>什么是 Netflix Feign？它的优点是什么？</h2><p>Feign 是受到 Retrofit，JAXRS-2.0 和 WebSocket 启发的 java 客户端联编程序。</p><p> Netflix Feign 使呼叫变得更加轻松和清洁</p><h2 id="什么是-Spring-Cloud-Bus？我们需要它吗？"><a href="#什么是-Spring-Cloud-Bus？我们需要它吗？" class="headerlink" title="什么是 Spring Cloud Bus？我们需要它吗？"></a>什么是 Spring Cloud Bus？我们需要它吗？</h2><p>Spring Cloud Bus 提供了跨多个实例刷新配置的功能。</p><h2 id="Spring-Cloud断路器的作用"><a href="#Spring-Cloud断路器的作用" class="headerlink" title="Spring Cloud断路器的作用"></a>Spring Cloud断路器的作用</h2><p>当一个服务调用另一个服务由于网络原因或自身原因出现问题，调用者就会等待被调用者的响应 当更多的服务请求到这些资源导致更多的请求等待，发生连锁效应（雪崩效应）</p><p>断路器有完全打开状态:一段时间内 达到一定的次数无法调用 并且多次监测没有恢复的迹象 断路器完全打开 那么下次请求就不会请求到该服务</p><p>半开:短时间内 有恢复迹象 断路器会将部分请求发给该服务，正常调用时 断路器关闭</p><p>关闭：当服务一直处于正常状态 能正常调用</p><h2 id="什么是Spring-Cloud-Config"><a href="#什么是Spring-Cloud-Config" class="headerlink" title="什么是Spring Cloud Config?"></a>什么是Spring Cloud Config?</h2><p>在分布式系统中，由于服务数量巨多，为了方便服务配置文件统一管理，实时更新，所以需要分布式配置中心组件。</p><p>使用：</p><p>（1）添加pom依赖</p><p>（2）配置文件添加相关配置</p><p>（3）启动类添加注解@EnableConfigServer</p><h2 id="什么是Spring-Cloud-Gateway"><a href="#什么是Spring-Cloud-Gateway" class="headerlink" title="什么是Spring Cloud Gateway?"></a>什么是Spring Cloud Gateway?</h2><p>Spring Cloud Gateway是Spring Cloud官方推出的第二代网关框架，取代Zuul网关。网关作为流量的，在微服务系统中有着非常作用，网关常见的功能有路由转发、权限校验、限流控制等作用。</p>]]></content>
      
      
      <categories>
          
          <category> java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spring cloud </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spring Boot笔记</title>
      <link href="/2020/06/30/Spring-Boot%E7%AC%94%E8%AE%B0/"/>
      <url>/2020/06/30/Spring-Boot%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<h1 id="Spring-Boot笔记"><a href="#Spring-Boot笔记" class="headerlink" title="Spring Boot笔记"></a>Spring Boot笔记</h1><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><h3 id="什么是-Spring-Boot？"><a href="#什么是-Spring-Boot？" class="headerlink" title="什么是 Spring Boot？"></a>什么是 Spring Boot？</h3><p>Spring Boot 是 Spring 开源组织下的子项目，是 Spring 组件一站式解决方案，主要是简化了使用 Spring 的难度，简省了繁重的配置，提供了各种启动器，开发者能快速上手。</p><h3 id="Spring-Boot-有哪些优点？"><a href="#Spring-Boot-有哪些优点？" class="headerlink" title="Spring Boot 有哪些优点？"></a>Spring Boot 有哪些优点？</h3><p>Spring Boot 主要有如下优点：</p><ul><li>容易上手，提升开发效率，为 Spring 开发提供一个更快、更广泛的入门体验。</li><li>开箱即用，远离繁琐的配置。</li><li>提供了一系列大型项目通用的非业务性功能，例如：内嵌服务器、安全管理、运行数据监控、运行状况检查和外部化配置等。</li><li>没有代码生成，也不需要XML配置。</li><li>避免大量的 Maven 导入和各种版本冲突。</li></ul><h3 id="Spring-Boot-的核心注解是哪个？它主要由哪几个注解组成的？"><a href="#Spring-Boot-的核心注解是哪个？它主要由哪几个注解组成的？" class="headerlink" title="Spring Boot 的核心注解是哪个？它主要由哪几个注解组成的？"></a>Spring Boot 的核心注解是哪个？它主要由哪几个注解组成的？</h3><p>启动类上面的注解是@SpringBootApplication，它也是 Spring Boot 的核心注解，主要组合包含了以下 3 个注解：</p><ul><li>@SpringBootConfiguration：组合了 @Configuration 注解，实现配置文件的功能。</li><li>@EnableAutoConfiguration：打开自动配置的功能，也可以关闭某个自动配置的选项，如关闭数据源自动配置功能： @SpringBootApplication(exclude = { DataSourceAutoConfiguration.class })。</li><li>@ComponentScan：Spring组件扫描。</li></ul><h2 id="配置"><a href="#配置" class="headerlink" title="配置"></a>配置</h2><h3 id="什么是-JavaConfig？"><a href="#什么是-JavaConfig？" class="headerlink" title="什么是 JavaConfig？"></a>什么是 JavaConfig？</h3><p>Spring JavaConfig 是 Spring 社区的产品，它提供了配置 Spring IoC 容器的纯Java 方法。因此它有助于避免使用 XML 配置。使用 JavaConfig 的优点在于：</p><p>（1）面向对象的配置。由于配置被定义为 JavaConfig 中的类，因此用户可以充分利用 Java 中的面向对象功能。一个配置类可以继承另一个，重写它的@Bean 方法等。</p><p>（2）减少或消除 XML 配置。基于依赖注入原则的外化配置的好处已被证明。但是，许多开发人员不希望在 XML 和 Java 之间来回切换。JavaConfig 为开发人员提供了一种纯 Java 方法来配置与 XML 配置概念相似的 Spring 容器。从技术角度来讲，只使用 JavaConfig 配置类来配置容器是可行的，但实际上很多人认为将JavaConfig 与 XML 混合匹配是理想的。</p><p>（3）类型安全和重构友好。JavaConfig 提供了一种类型安全的方法来配置 Spring容器。由于 Java 5.0 对泛型的支持，现在可以按类型而不是按名称检索 bean，不需要任何强制转换或基于字符串的查找。</p><h3 id="Spring-Boot-自动配置原理是什么？"><a href="#Spring-Boot-自动配置原理是什么？" class="headerlink" title="Spring Boot 自动配置原理是什么？"></a>Spring Boot 自动配置原理是什么？</h3><p>注解 @EnableAutoConfiguration, @Configuration, @ConditionalOnClass 就是自动配置的核心，</p><p>@EnableAutoConfiguration 给容器导入META-INF/spring.factories 里定义的自动配置类。</p><p>筛选有效的自动配置类。</p><p>每一个自动配置类结合对应的 xxxProperties.java 读取配置文件进行自动配置功能</p><h3 id="你如何理解-Spring-Boot-配置加载顺序？"><a href="#你如何理解-Spring-Boot-配置加载顺序？" class="headerlink" title="你如何理解 Spring Boot 配置加载顺序？"></a>你如何理解 Spring Boot 配置加载顺序？</h3><p>在 Spring Boot 里面，可以使用以下几种方式来加载配置。</p><p>1）properties文件；</p><p>2）YAML文件；</p><p>3）系统环境变量；</p><p>4）命令行参数；</p><p>等等……</p><h3 id="什么是-YAML？"><a href="#什么是-YAML？" class="headerlink" title="什么是 YAML？"></a>什么是 YAML？</h3><p>YAML 是一种人类可读的数据序列化语言。它通常用于配置文件。与属性文件相比，如果我们想要在配置文件中添加复杂的属性，YAML 文件就更加结构化，而且更少混淆。可以看出 YAML 具有分层配置数据。</p><h3 id="YAML-配置的优势在哪里"><a href="#YAML-配置的优势在哪里" class="headerlink" title="YAML 配置的优势在哪里 ?"></a>YAML 配置的优势在哪里 ?</h3><p>YAML 现在可以算是非常流行的一种配置文件格式了，无论是前端还是后端，都可以见到 YAML 配置。那么 YAML 配置和传统的 properties 配置相比到底有哪些优势呢？</p><ul><li>配置有序，在一些特殊的场景下，配置有序很关键</li><li>支持数组，数组中的元素可以是基本数据类型也可以是对象</li><li>简洁</li></ul><p>相比 properties 配置文件，YAML 还有一个缺点，就是不支持 @PropertySource 注解导入自定义的 YAML 配置。</p><h3 id="Spring-Boot-是否可以使用-XML-配置"><a href="#Spring-Boot-是否可以使用-XML-配置" class="headerlink" title="Spring Boot 是否可以使用 XML 配置 ?"></a>Spring Boot 是否可以使用 XML 配置 ?</h3><p>Spring Boot 推荐使用 Java 配置而非 XML 配置，但是 Spring Boot 中也可以使用 XML 配置，通过 @ImportResource 注解可以引入一个 XML 配置。</p><h3 id="spring-boot-核心配置文件是什么？bootstrap-properties-和-application-properties-有何区别"><a href="#spring-boot-核心配置文件是什么？bootstrap-properties-和-application-properties-有何区别" class="headerlink" title="spring boot 核心配置文件是什么？bootstrap.properties 和 application.properties 有何区别 ?"></a>spring boot 核心配置文件是什么？bootstrap.properties 和 application.properties 有何区别 ?</h3><p>单纯做 Spring Boot 开发，可能不太容易遇到 bootstrap.properties 配置文件，但是在结合 Spring Cloud 时，这个配置就会经常遇到了，特别是在需要加载一些远程配置文件的时侯。</p><p>spring boot 核心的两个配置文件：</p><ul><li>bootstrap (. yml 或者 . properties)：boostrap 由父 ApplicationContext 加载的，比 applicaton 优先加载，配置在应用程序上下文的引导阶段生效。一般来说我们在 Spring Cloud Config 或者 Nacos 中会用到它。且 boostrap 里面的属性不能被覆盖；</li><li>application (. yml 或者 . properties)： 由ApplicatonContext 加载，用于 spring boot 项目的自动化配置。</li></ul><h3 id="什么是-Spring-Profiles？"><a href="#什么是-Spring-Profiles？" class="headerlink" title="什么是 Spring Profiles？"></a>什么是 Spring Profiles？</h3><p>Spring Profiles 允许用户根据配置文件（dev，test，prod 等）来注册 bean。因此，当应用程序在开发中运行时，只有某些 bean 可以加载，而在 PRODUCTION中，某些其他 bean 可以加载。假设我们的要求是 Swagger 文档仅适用于 QA 环境，并且禁用所有其他文档。这可以使用配置文件来完成。Spring Boot 使得使用配置文件非常简单。</p><h3 id="如何在自定义端口上运行-Spring-Boot-应用程序？"><a href="#如何在自定义端口上运行-Spring-Boot-应用程序？" class="headerlink" title="如何在自定义端口上运行 Spring Boot 应用程序？"></a>如何在自定义端口上运行 Spring Boot 应用程序？</h3><p>为了在自定义端口上运行 Spring Boot 应用程序，您可以在application.properties 中指定端口。server.port = 8090</p><h2 id="安全"><a href="#安全" class="headerlink" title="安全"></a>安全</h2><h3 id="如何实现-Spring-Boot-应用程序的安全性？"><a href="#如何实现-Spring-Boot-应用程序的安全性？" class="headerlink" title="如何实现 Spring Boot 应用程序的安全性？"></a>如何实现 Spring Boot 应用程序的安全性？</h3><p>为了实现 Spring Boot 的安全性，我们使用 spring-boot-starter-security 依赖项，并且必须添加安全配置。它只需要很少的代码。配置类将必须扩展WebSecurityConfigurerAdapter 并覆盖其方法。</p><h3 id="比较一下-Spring-Security-和-Shiro-各自的优缺点"><a href="#比较一下-Spring-Security-和-Shiro-各自的优缺点" class="headerlink" title="比较一下 Spring Security 和 Shiro 各自的优缺点 ?"></a>比较一下 Spring Security 和 Shiro 各自的优缺点 ?</h3><p>由于 Spring Boot 官方提供了大量的非常方便的开箱即用的 Starter ，包括 Spring Security 的 Starter ，使得在 Spring Boot 中使用 Spring Security 变得更加容易，甚至只需要添加一个依赖就可以保护所有的接口，所以，如果是 Spring Boot 项目，一般选择 Spring Security 。当然这只是一个建议的组合，单纯从技术上来说，无论怎么组合，都是没有问题的。Shiro 和 Spring Security 相比，主要有如下一些特点：</p><ul><li>Spring Security 是一个重量级的安全管理框架；Shiro 则是一个轻量级的安全管理框架</li><li>Spring Security 概念复杂，配置繁琐；Shiro 概念简单、配置简单</li><li>Spring Security 功能强大；Shiro 功能简单</li></ul><h3 id="Spring-Boot-中如何解决跨域问题"><a href="#Spring-Boot-中如何解决跨域问题" class="headerlink" title="Spring Boot 中如何解决跨域问题 ?"></a>Spring Boot 中如何解决跨域问题 ?</h3><p>跨域可以在前端通过 JSONP 来解决，但是 JSONP 只可以发送 GET 请求，无法发送其他类型的请求，在 RESTful 风格的应用中，就显得非常鸡肋，因此我们推荐在后端通过 （CORS，Cross-origin resource sharing） 来解决跨域问题。这种解决方案并非 Spring Boot 特有的，在传统的 SSM 框架中，就可以通过 CORS 来解决跨域问题，只不过之前我们是在 XML 文件中配置 CORS ，现在可以通过实现WebMvcConfigurer接口然后重写addCorsMappings方法解决跨域问题。</p><h3 id="什么是-CSRF-攻击？"><a href="#什么是-CSRF-攻击？" class="headerlink" title="什么是 CSRF 攻击？"></a>什么是 CSRF 攻击？</h3><p>CSRF 代表跨站请求伪造。这是一种攻击，迫使最终用户在当前通过身份验证的Web 应用程序上执行不需要的操作。CSRF 攻击专门针对状态改变请求，而不是数据窃取，因为攻击者无法查看对伪造请求的响应。</p><h2 id="监视器"><a href="#监视器" class="headerlink" title="监视器"></a>监视器</h2><h3 id="Spring-Boot-中的监视器是什么？"><a href="#Spring-Boot-中的监视器是什么？" class="headerlink" title="Spring Boot 中的监视器是什么？"></a>Spring Boot 中的监视器是什么？</h3><p>Spring boot actuator 是 spring 启动框架中的重要功能之一。Spring boot 监视器可帮助您访问生产环境中正在运行的应用程序的当前状态。有几个指标必须在生产环境中进行检查和监控。即使一些外部应用程序可能正在使用这些服务来向相关人员触发警报消息。监视器模块公开了一组可直接作为 HTTP URL 访问的REST 端点来检查状态。</p><h3 id="如何在-Spring-Boot-中禁用-Actuator-端点安全性？"><a href="#如何在-Spring-Boot-中禁用-Actuator-端点安全性？" class="headerlink" title="如何在 Spring Boot 中禁用 Actuator 端点安全性？"></a>如何在 Spring Boot 中禁用 Actuator 端点安全性？</h3><p>默认情况下，所有敏感的 HTTP 端点都是安全的，只有具有 ACTUATOR 角色的用户才能访问它们。安全性是使用标准的 HttpServletRequest.isUserInRole 方法实施的。 我们可以使用来禁用安全性。只有在执行机构端点在防火墙后访问时，才建议禁用安全性。</p><h3 id="我们如何监视所有-Spring-Boot-微服务？"><a href="#我们如何监视所有-Spring-Boot-微服务？" class="headerlink" title="我们如何监视所有 Spring Boot 微服务？"></a>我们如何监视所有 Spring Boot 微服务？</h3><p>Spring Boot 提供监视器端点以监控各个微服务的度量。这些端点对于获取有关应用程序的信息（如它们是否已启动）以及它们的组件（如数据库等）是否正常运行很有帮助。但是，使用监视器的一个主要缺点或困难是，我们必须单独打开应用程序的知识点以了解其状态或健康状况。想象一下涉及 50 个应用程序的微服务，管理员将不得不击中所有 50 个应用程序的执行终端。为了帮助我们处理这种情况，我们将使用位于的开源项目。 它建立在 Spring Boot Actuator 之上，它提供了一个 Web UI，使我们能够可视化多个应用程序的度量。</p><h2 id="整合第三方项目"><a href="#整合第三方项目" class="headerlink" title="整合第三方项目"></a>整合第三方项目</h2><h3 id="什么是-WebSockets？"><a href="#什么是-WebSockets？" class="headerlink" title="什么是 WebSockets？"></a>什么是 WebSockets？</h3><p>WebSocket 是一种计算机通信协议，通过单个 TCP 连接提供全双工通信信道。</p><p>1、WebSocket 是双向的 -使用 WebSocket 客户端或服务器可以发起消息发送。</p><p>2、WebSocket 是全双工的 -客户端和服务器通信是相互独立的。</p><p>3、单个 TCP 连接 -初始连接使用 HTTP，然后将此连接升级到基于套接字的连接。然后这个单一连接用于所有未来的通信</p><p>4、Light -与 http 相比，WebSocket 消息数据交换要轻得多。</p><h3 id="什么是-Spring-Data"><a href="#什么是-Spring-Data" class="headerlink" title="什么是 Spring Data ?"></a>什么是 Spring Data ?</h3><p>Spring Data 是 Spring 的一个子项目。用于简化数据库访问，支持NoSQL 和 关系数据存储。其主要目标是使数据库的访问变得方便快捷。Spring Data 具有如下特点：</p><p>SpringData 项目支持 NoSQL 存储：</p><ol><li>MongoDB （文档数据库）</li><li>Neo4j（图形数据库）</li><li>Redis（键/值存储）</li><li>Hbase（列族数据库）</li></ol><p>SpringData 项目所支持的关系数据存储技术：</p><ol><li>JDBC</li><li>JPA</li></ol><p>Spring Data Jpa 致力于减少数据访问层 (DAO) 的开发量. 开发者唯一要做的，就是声明持久层的接口，其他都交给 Spring Data JPA 来帮你完成！Spring Data JPA 通过规范方法的名字，根据符合规范的名字来确定方法需要实现什么样的逻辑。</p><h3 id="什么是-Spring-Batch？"><a href="#什么是-Spring-Batch？" class="headerlink" title="什么是 Spring Batch？"></a>什么是 Spring Batch？</h3><p>Spring Boot Batch 提供可重用的函数，这些函数在处理大量记录时非常重要，包括日志/跟踪，事务管理，作业处理统计信息，作业重新启动，跳过和资源管理。它还提供了更先进的技术服务和功能，通过优化和分区技术，可以实现极高批量和高性能批处理作业。简单以及复杂的大批量批处理作业可以高度可扩展的方式利用框架处理重要大量的信息。</p><h3 id="什么是-FreeMarker-模板？"><a href="#什么是-FreeMarker-模板？" class="headerlink" title="什么是 FreeMarker 模板？"></a>什么是 FreeMarker 模板？</h3><p>FreeMarker 是一个基于 Java 的模板引擎，最初专注于使用 MVC 软件架构进行动态网页生成。使用 Freemarker 的主要优点是表示层和业务层的完全分离。程序员可以处理应用程序代码，而设计人员可以处理 html 页面设计。最后使用freemarker 可以将这些结合起来，给出最终的输出页面。</p><h3 id="如何集成-Spring-Boot-和-ActiveMQ？"><a href="#如何集成-Spring-Boot-和-ActiveMQ？" class="headerlink" title="如何集成 Spring Boot 和 ActiveMQ？"></a>如何集成 Spring Boot 和 ActiveMQ？</h3><p>对于集成 Spring Boot 和 ActiveMQ，我们使用依赖关系。 它只需要很少的配置，并且不需要样板代码。</p><h3 id="什么是-Apache-Kafka？"><a href="#什么是-Apache-Kafka？" class="headerlink" title="什么是 Apache Kafka？"></a>什么是 Apache Kafka？</h3><p>Apache Kafka 是一个分布式发布 - 订阅消息系统。它是一个可扩展的，容错的发布 - 订阅消息系统，它使我们能够构建分布式应用程序。这是一个 Apache 顶级项目。Kafka 适合离线和在线消息消费。</p><h3 id="什么是-Swagger？你用-Spring-Boot-实现了它吗？"><a href="#什么是-Swagger？你用-Spring-Boot-实现了它吗？" class="headerlink" title="什么是 Swagger？你用 Spring Boot 实现了它吗？"></a>什么是 Swagger？你用 Spring Boot 实现了它吗？</h3><p>Swagger 广泛用于可视化 API，使用 Swagger UI 为前端开发人员提供在线沙箱。Swagger 是用于生成 RESTful Web 服务的可视化表示的工具，规范和完整框架实现。它使文档能够以与服务器相同的速度更新。当通过 Swagger 正确定义时，消费者可以使用最少量的实现逻辑来理解远程服务并与其进行交互。因此，Swagger消除了调用服务时的猜测。</p><h3 id="前后端分离，如何维护接口文档"><a href="#前后端分离，如何维护接口文档" class="headerlink" title="前后端分离，如何维护接口文档 ?"></a>前后端分离，如何维护接口文档 ?</h3><p>前后端分离开发日益流行，大部分情况下，我们都是通过 Spring Boot 做前后端分离开发，前后端分离一定会有接口文档，不然会前后端会深深陷入到扯皮中。一个比较笨的方法就是使用 word 或者 md 来维护接口文档，但是效率太低，接口一变，所有人手上的文档都得变。在 Spring Boot 中，这个问题常见的解决方案是 Swagger ，使用 Swagger 我们可以快速生成一个接口文档网站，接口一旦发生变化，文档就会自动更新，所有开发工程师访问这一个在线网站就可以获取到最新的接口文档，非常方便。</p><h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><h3 id="如何重新加载-Spring-Boot-上的更改，而无需重新启动服务器？Spring-Boot项目如何热部署？"><a href="#如何重新加载-Spring-Boot-上的更改，而无需重新启动服务器？Spring-Boot项目如何热部署？" class="headerlink" title="如何重新加载 Spring Boot 上的更改，而无需重新启动服务器？Spring Boot项目如何热部署？"></a>如何重新加载 Spring Boot 上的更改，而无需重新启动服务器？Spring Boot项目如何热部署？</h3><p>这可以使用 DEV 工具来实现。通过这种依赖关系，您可以节省任何更改，嵌入式tomcat 将重新启动。Spring Boot 有一个开发工具（DevTools）模块，它有助于提高开发人员的生产力。Java 开发人员面临的一个主要挑战是将文件更改自动部署到服务器并自动重启服务器。开发人员可以重新加载 Spring Boot 上的更改，而无需重新启动服务器。这将消除每次手动部署更改的需要。Spring Boot 在发布它的第一个版本时没有这个功能。这是开发人员最需要的功能。DevTools 模块完全满足开发人员的需求。该模块将在生产环境中被禁用。它还提供 H2 数据库控制台以更好地测试应用程序。</p><h3 id="您使用了哪些-starter-maven-依赖项？"><a href="#您使用了哪些-starter-maven-依赖项？" class="headerlink" title="您使用了哪些 starter maven 依赖项？"></a>您使用了哪些 starter maven 依赖项？</h3><p>使用了下面的一些依赖项</p><p>spring-boot-starter-activemq</p><p>spring-boot-starter-security</p><p>这有助于增加更少的依赖关系，并减少版本的冲突。</p><h3 id="Spring-Boot-中的-starter-到底是什么"><a href="#Spring-Boot-中的-starter-到底是什么" class="headerlink" title="Spring Boot 中的 starter 到底是什么 ?"></a>Spring Boot 中的 starter 到底是什么 ?</h3><p>首先，这个 Starter 并非什么新的技术点，基本上还是基于 Spring 已有功能来实现的。首先它提供了一个自动化配置类，一般命名为 XXXAutoConfiguration ，在这个配置类中通过条件注解来决定一个配置是否生效（条件注解就是 Spring 中原本就有的），然后它还会提供一系列的默认配置，也允许开发者根据实际情况自定义相关配置，然后通过类型安全的属性注入将这些配置属性注入进来，新注入的属性会代替掉默认属性。正因为如此，很多第三方框架，我们只需要引入依赖就可以直接使用了。当然，开发者也可以自定义 Starter</p><h3 id="spring-boot-starter-parent-有什么用"><a href="#spring-boot-starter-parent-有什么用" class="headerlink" title="spring-boot-starter-parent 有什么用 ?"></a>spring-boot-starter-parent 有什么用 ?</h3><p>我们都知道，新创建一个 Spring Boot 项目，默认都是有 parent 的，这个 parent 就是 spring-boot-starter-parent ，spring-boot-starter-parent 主要有如下作用：</p><ol><li>定义了 Java 编译版本为 1.8 。</li><li>使用 UTF-8 格式编码。</li><li>继承自 spring-boot-dependencies，这个里边定义了依赖的版本，也正是因为继承了这个依赖，所以我们在写依赖时才不需要写版本号。</li><li>执行打包操作的配置。</li><li>自动化的资源过滤。</li><li>自动化的插件配置。</li><li>针对 application.properties 和 application.yml 的资源过滤，包括通过 profile 定义的不同环境的配置文件，例如 application-dev.properties 和 application-dev.yml。</li></ol><h3 id="Spring-Boot-打成的-jar-和普通的-jar-有什么区别"><a href="#Spring-Boot-打成的-jar-和普通的-jar-有什么区别" class="headerlink" title="Spring Boot 打成的 jar 和普通的 jar 有什么区别 ?"></a>Spring Boot 打成的 jar 和普通的 jar 有什么区别 ?</h3><p>Spring Boot 项目最终打包成的 jar 是可执行 jar ，这种 jar 可以直接通过 java -jar xxx.jar 命令来运行，这种 jar 不可以作为普通的 jar 被其他项目依赖，即使依赖了也无法使用其中的类。</p><p>Spring Boot 的 jar 无法被其他项目依赖，主要还是他和普通 jar 的结构不同。普通的 jar 包，解压后直接就是包名，包里就是我们的代码，而 Spring Boot 打包成的可执行 jar 解压后，在 \BOOT-INF\classes 目录下才是我们的代码，因此无法被直接引用。如果非要引用，可以在 pom.xml 文件中增加配置，将 Spring Boot 项目打包成两个 jar ，一个可执行，一个可引用。</p><h3 id="运行-Spring-Boot-有哪几种方式？"><a href="#运行-Spring-Boot-有哪几种方式？" class="headerlink" title="运行 Spring Boot 有哪几种方式？"></a>运行 Spring Boot 有哪几种方式？</h3><p>1）打包用命令或者放到容器中运行</p><p>2）用 Maven/ Gradle 插件运行</p><p>3）直接执行 main 方法运行</p><h3 id="Spring-Boot-需要独立的容器运行吗？"><a href="#Spring-Boot-需要独立的容器运行吗？" class="headerlink" title="Spring Boot 需要独立的容器运行吗？"></a>Spring Boot 需要独立的容器运行吗？</h3><p>可以不需要，内置了 Tomcat/ Jetty 等容器。</p><h3 id="开启-Spring-Boot-特性有哪几种方式？"><a href="#开启-Spring-Boot-特性有哪几种方式？" class="headerlink" title="开启 Spring Boot 特性有哪几种方式？"></a>开启 Spring Boot 特性有哪几种方式？</h3><p>1）继承spring-boot-starter-parent项目</p><p>2）导入spring-boot-dependencies项目依赖</p><h3 id="如何使用-Spring-Boot-实现异常处理？"><a href="#如何使用-Spring-Boot-实现异常处理？" class="headerlink" title="如何使用 Spring Boot 实现异常处理？"></a>如何使用 Spring Boot 实现异常处理？</h3><p>Spring 提供了一种使用 ControllerAdvice 处理异常的非常有用的方法。 我们通过实现一个 ControlerAdvice 类，来处理控制器类抛出的所有异常。</p><h3 id="如何使用-Spring-Boot-实现分页和排序？"><a href="#如何使用-Spring-Boot-实现分页和排序？" class="headerlink" title="如何使用 Spring Boot 实现分页和排序？"></a>如何使用 Spring Boot 实现分页和排序？</h3><p>使用 Spring Boot 实现分页非常简单。使用 Spring Data-JPA 可以实现将可分页的传递给存储库方法。</p><h3 id="微服务中如何实现-session-共享"><a href="#微服务中如何实现-session-共享" class="headerlink" title="微服务中如何实现 session 共享 ?"></a>微服务中如何实现 session 共享 ?</h3><p>在微服务中，一个完整的项目被拆分成多个不相同的独立的服务，各个服务独立部署在不同的服务器上，各自的 session 被从物理空间上隔离开了，但是经常，我们需要在不同微服务之间共享 session ，常见的方案就是 Spring Session + Redis 来实现 session 共享。将所有微服务的 session 统一保存在 Redis 上，当各个微服务对 session 有相关的读写操作时，都去操作 Redis 上的 session 。这样就实现了 session 共享，Spring Session 基于 Spring 中的代理过滤器实现，使得 session 的同步操作对开发人员而言是透明的，非常简便。</p><h3 id="微服务中如何实现-session-共享-1"><a href="#微服务中如何实现-session-共享-1" class="headerlink" title="微服务中如何实现 session 共享 ?"></a>微服务中如何实现 session 共享 ?</h3><p>在微服务中，一个完整的项目被拆分成多个不相同的独立的服务，各个服务独立部署在不同的服务器上，各自的 session 被从物理空间上隔离开了，但是经常，我们需要在不同微服务之间共享 session ，常见的方案就是 Spring Session + Redis 来实现 session 共享。将所有微服务的 session 统一保存在 Redis 上，当各个微服务对 session 有相关的读写操作时，都去操作 Redis 上的 session 。这样就实现了 session 共享，Spring Session 基于 Spring 中的代理过滤器实现，使得 session 的同步操作对开发人员而言是透明的，非常简便。</p><h3 id="Spring-Boot-中如何实现定时任务"><a href="#Spring-Boot-中如何实现定时任务" class="headerlink" title="Spring Boot 中如何实现定时任务 ?"></a>Spring Boot 中如何实现定时任务 ?</h3><p>定时任务也是一个常见的需求，Spring Boot 中对于定时任务的支持主要还是来自 Spring 框架。</p><p>在 Spring Boot 中使用定时任务主要有两种不同的方式，一个就是使用 Spring 中的 @Scheduled 注解，另一个则是使用第三方框架 Quartz。</p><p>使用 Spring 中的 @Scheduled 的方式主要通过 @Scheduled 注解来实现。</p><p>使用 Quartz ，则按照 Quartz 的方式，定义 Job 和 Trigger 即可。</p>]]></content>
      
      
      <categories>
          
          <category> java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> springboot </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spring MVC笔记</title>
      <link href="/2020/06/30/Spring-MVC%E7%AC%94%E8%AE%B0/"/>
      <url>/2020/06/30/Spring-MVC%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<h1 id="Spring-MVC笔记"><a href="#Spring-MVC笔记" class="headerlink" title="Spring-MVC笔记"></a>Spring-MVC笔记</h1><h2 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h2><h3 id="什么是Spring-MVC？简单介绍下你对Spring-MVC的理解？"><a href="#什么是Spring-MVC？简单介绍下你对Spring-MVC的理解？" class="headerlink" title="什么是Spring MVC？简单介绍下你对Spring MVC的理解？"></a>什么是Spring MVC？简单介绍下你对Spring MVC的理解？</h3><p>Spring MVC是一个基于Java的实现了MVC设计模式的请求驱动类型的轻量级Web框架，通过把模型-视图-控制器分离，将web层进行职责解耦，把复杂的web应用分成逻辑清晰的几部分，简化开发，减少出错，方便组内开发人员之间的配合。</p><h3 id="Spring-MVC的优点"><a href="#Spring-MVC的优点" class="headerlink" title="Spring MVC的优点"></a>Spring MVC的优点</h3><p>（1）可以支持各种视图技术,而不仅仅局限于JSP；</p><p>（2）与Spring框架集成（如IoC容器、AOP等）；</p><p>（3）清晰的角色分配：前端控制器(dispatcherServlet) , 请求到处理器映射（handlerMapping), 处理器适配器（HandlerAdapter), 视图解析器（ViewResolver）。</p><p>（4） 支持各种请求资源的映射策略。</p><h2 id="核心组件"><a href="#核心组件" class="headerlink" title="核心组件"></a>核心组件</h2><h3 id="Spring-MVC的主要组件？"><a href="#Spring-MVC的主要组件？" class="headerlink" title="Spring MVC的主要组件？"></a>Spring MVC的主要组件？</h3><p>（1）前端控制器 DispatcherServlet（不需要程序员开发）</p><p>作用：接收请求、响应结果，相当于转发器，有了DispatcherServlet 就减少了其它组件之间的耦合度。</p><p>（2）处理器映射器HandlerMapping（不需要程序员开发）</p><p>作用：根据请求的URL来查找Handler</p><p>（3）处理器适配器HandlerAdapter</p><p>注意：在编写Handler的时候要按照HandlerAdapter要求的规则去编写，这样适配器HandlerAdapter才可以正确的去执行Handler。</p><p>（4）处理器Handler（需要程序员开发）</p><p>（5）视图解析器 ViewResolver（不需要程序员开发）</p><p>作用：进行视图的解析，根据视图逻辑名解析成真正的视图（view）</p><p>（6）视图View（需要程序员开发jsp）</p><p>View是一个接口， 它的实现类支持不同的视图类型（jsp，freemarker，pdf等等）</p><h3 id="什么是DispatcherServlet"><a href="#什么是DispatcherServlet" class="headerlink" title="什么是DispatcherServlet"></a>什么是DispatcherServlet</h3><p>Spring的MVC框架是围绕DispatcherServlet来设计的，它用来处理所有的HTTP请求和响应。</p><h3 id="什么是Spring-MVC框架的控制器？"><a href="#什么是Spring-MVC框架的控制器？" class="headerlink" title="什么是Spring MVC框架的控制器？"></a>什么是Spring MVC框架的控制器？</h3><p>控制器提供一个访问应用程序的行为，此行为通常通过服务接口实现。控制器解析用户输入并将其转换为一个由视图呈现给用户的模型。Spring用一个非常抽象的方式实现了一个控制层，允许用户创建多种用途的控制器。</p><h3 id="Spring-MVC的控制器是不是单例模式-如果是-有什么问题-怎么解决？"><a href="#Spring-MVC的控制器是不是单例模式-如果是-有什么问题-怎么解决？" class="headerlink" title="Spring MVC的控制器是不是单例模式,如果是,有什么问题,怎么解决？"></a>Spring MVC的控制器是不是单例模式,如果是,有什么问题,怎么解决？</h3><p>答：是单例模式,所以在多线程访问的时候有线程安全问题,不要用同步,会影响性能的,解决方案是在控制器里面不能写字段。</p><h2 id="工作原理"><a href="#工作原理" class="headerlink" title="工作原理"></a>工作原理</h2><h3 id="请描述Spring-MVC的工作流程？描述一下-DispatcherServlet-的工作流程？"><a href="#请描述Spring-MVC的工作流程？描述一下-DispatcherServlet-的工作流程？" class="headerlink" title="请描述Spring MVC的工作流程？描述一下 DispatcherServlet 的工作流程？"></a>请描述Spring MVC的工作流程？描述一下 DispatcherServlet 的工作流程？</h3><p><img src="https://lixiangbetter.github.io/2020/06/30/Spring-MVC%E7%AC%94%E8%AE%B0/20200208211439106.png" alt></p><h2 id="MVC框架"><a href="#MVC框架" class="headerlink" title="MVC框架"></a>MVC框架</h2><h3 id="MVC是什么？MVC设计模式的好处有哪些"><a href="#MVC是什么？MVC设计模式的好处有哪些" class="headerlink" title="MVC是什么？MVC设计模式的好处有哪些"></a>MVC是什么？MVC设计模式的好处有哪些</h3><p>mvc是一种设计模式（设计模式就是日常开发中编写代码的一种好的方法和经验的总结）。模型（model）-视图（view）-控制器（controller），三层架构的设计模式。用于实现前端页面的展现与后端业务数据处理的分离。</p><p>mvc设计模式的好处</p><p>1.分层设计，实现了业务系统各个组件之间的解耦，有利于业务系统的可扩展性，可维护性。</p><p>2.有利于系统的并行开发，提升开发效率。</p><h2 id="常用注解"><a href="#常用注解" class="headerlink" title="常用注解"></a>常用注解</h2><h3 id="注解原理是什么"><a href="#注解原理是什么" class="headerlink" title="注解原理是什么"></a>注解原理是什么</h3><p>注解本质是一个继承了Annotation的特殊接口，其具体实现类是Java运行时生成的动态代理类。我们通过反射获取注解时，返回的是Java运行时生成的动态代理对象。通过代理对象调用自定义注解的方法，会最终调用AnnotationInvocationHandler的invoke方法。该方法会从memberValues这个Map中索引出对应的值。而memberValues的来源是Java常量池。</p><h3 id="Spring-MVC常用的注解有哪些？"><a href="#Spring-MVC常用的注解有哪些？" class="headerlink" title="Spring MVC常用的注解有哪些？"></a>Spring MVC常用的注解有哪些？</h3><p>@RequestMapping：用于处理请求 url 映射的注解，可用于类或方法上。用于类上，则表示类中的所有响应请求的方法都是以该地址作为父路径。</p><p>@RequestBody：注解实现接收http请求的json数据，将json转换为java对象。</p><p>@ResponseBody：注解实现将conreoller方法返回对象转化为json对象响应给客户。</p><h3 id="SpingMvc中的控制器的注解一般用哪个-有没有别的注解可以替代？"><a href="#SpingMvc中的控制器的注解一般用哪个-有没有别的注解可以替代？" class="headerlink" title="SpingMvc中的控制器的注解一般用哪个,有没有别的注解可以替代？"></a>SpingMvc中的控制器的注解一般用哪个,有没有别的注解可以替代？</h3><p>一般用@Controller注解,也可以使用@RestController,@RestController注解相当于@ResponseBody ＋ @Controller,表示是表现层,除此之外，一般不用别的注解代替。</p><h3 id="Controller注解的作用"><a href="#Controller注解的作用" class="headerlink" title="@Controller注解的作用"></a>@Controller注解的作用</h3><p>在Spring MVC 中，控制器Controller 负责处理由DispatcherServlet 分发的请求，它把用户请求的数据经过业务处理层处理之后封装成一个Model ，然后再把该Model 返回给对应的View 进行展示。</p><h3 id="RequestMapping注解的作用"><a href="#RequestMapping注解的作用" class="headerlink" title="@RequestMapping注解的作用"></a>@RequestMapping注解的作用</h3><p>RequestMapping注解有六个属性，下面我们把她分成三类进行说明（下面有相应示例）。</p><p>value， method</p><p>value： 指定请求的实际地址，指定的地址可以是URI Template 模式（后面将会说明）；</p><p>method： 指定请求的method类型， GET、POST、PUT、DELETE等；</p><p>consumes，produces</p><p>consumes： 指定处理请求的提交内容类型（Content-Type），例如application/json, text/html;</p><p>produces: 指定返回的内容类型，仅当request请求头中的(Accept)类型中包含该指定类型才返回；</p><p>params，headers</p><p>params： 指定request中必须包含某些参数值是，才让该方法处理。</p><p>headers： 指定request中必须包含某些指定的header值，才能让该方法处理请求。</p><h3 id="ResponseBody注解的作用"><a href="#ResponseBody注解的作用" class="headerlink" title="@ResponseBody注解的作用"></a>@ResponseBody注解的作用</h3><p>作用： 该注解用于将Controller的方法返回的对象，通过适当的HttpMessageConverter转换为指定格式后，写入到Response对象的body数据区。</p><p>使用时机：返回的数据不是html标签的页面，而是其他某种格式的数据时（如json、xml等）使用；</p><h3 id="PathVariable和-RequestParam的区别"><a href="#PathVariable和-RequestParam的区别" class="headerlink" title="@PathVariable和@RequestParam的区别"></a>@PathVariable和@RequestParam的区别</h3><p>请求路径上有个id的变量值，可以通过@PathVariable来获取 @RequestMapping(value = “/page/{id}”, method = RequestMethod.GET)</p><p>@RequestParam用来获得静态的URL请求入参 spring注解时action里用到。</p><h2 id="其他"><a href="#其他" class="headerlink" title="其他"></a>其他</h2><h3 id="Spring-MVC与Struts2区别"><a href="#Spring-MVC与Struts2区别" class="headerlink" title="Spring MVC与Struts2区别"></a>Spring MVC与Struts2区别</h3><h3 id="Spring-MVC怎么样设定重定向和转发的？"><a href="#Spring-MVC怎么样设定重定向和转发的？" class="headerlink" title="Spring MVC怎么样设定重定向和转发的？"></a>Spring MVC怎么样设定重定向和转发的？</h3><p>（1）转发：在返回值前面加”forward:”，譬如”forward:user.do?name=method4”</p><p>（2）重定向：在返回值前面加”redirect:”，譬如”redirect:<a href="http://www.baidu.com&quot;" target="_blank" rel="noopener">http://www.baidu.com&quot;</a></p><h3 id="Spring-MVC怎么和AJAX相互调用的？"><a href="#Spring-MVC怎么和AJAX相互调用的？" class="headerlink" title="Spring MVC怎么和AJAX相互调用的？"></a>Spring MVC怎么和AJAX相互调用的？</h3><p>通过Jackson框架就可以把Java里面的对象直接转化成Js可以识别的Json对象。具体步骤如下 ：</p><p>（1）加入Jackson.jar</p><p>（2）在配置文件中配置json的映射</p><p>（3）在接受Ajax方法里面可以直接返回Object,List等,但方法前面要加上@ResponseBody注解。</p><h3 id="如何解决POST请求中文乱码问题，GET的又如何处理呢？"><a href="#如何解决POST请求中文乱码问题，GET的又如何处理呢？" class="headerlink" title="如何解决POST请求中文乱码问题，GET的又如何处理呢？"></a>如何解决POST请求中文乱码问题，GET的又如何处理呢？</h3><p>Post:在web.xml中配置一个CharacterEncodingFilter过滤器，设置成utf-8；</p><p>Get:</p><p>①修改tomcat配置文件添加编码与工程编码一致<br>②另外一种方法对参数进行重新编码</p><h3 id="Spring-MVC的异常处理？"><a href="#Spring-MVC的异常处理？" class="headerlink" title="Spring MVC的异常处理？"></a>Spring MVC的异常处理？</h3><p>可以将异常抛给Spring框架，由Spring框架来处理；我们只需要配置简单的异常处理器，在异常处理器中添视图页面即可。</p><h3 id="如果在拦截请求中，我想拦截get方式提交的方法-怎么配置"><a href="#如果在拦截请求中，我想拦截get方式提交的方法-怎么配置" class="headerlink" title="如果在拦截请求中，我想拦截get方式提交的方法,怎么配置"></a>如果在拦截请求中，我想拦截get方式提交的方法,怎么配置</h3><p>method=RequestMethod.GET。</p><h3 id="怎样在方法里面得到Request-或者Session？"><a href="#怎样在方法里面得到Request-或者Session？" class="headerlink" title="怎样在方法里面得到Request,或者Session？"></a>怎样在方法里面得到Request,或者Session？</h3><p>request,Spring MVC就自动把request对象传入。</p><h3 id="如果想在拦截的方法里面得到从前台传入的参数-怎么得到？"><a href="#如果想在拦截的方法里面得到从前台传入的参数-怎么得到？" class="headerlink" title="如果想在拦截的方法里面得到从前台传入的参数,怎么得到？"></a>如果想在拦截的方法里面得到从前台传入的参数,怎么得到？</h3><p>直接在形参里面声明这个参数就可以,但必须名字和传过来的参数一样。</p><h3 id="如果前台有很多个参数传入-并且这些参数都是一个对象的-那么怎么样快速得到这个对象？"><a href="#如果前台有很多个参数传入-并且这些参数都是一个对象的-那么怎么样快速得到这个对象？" class="headerlink" title="如果前台有很多个参数传入,并且这些参数都是一个对象的,那么怎么样快速得到这个对象？"></a>如果前台有很多个参数传入,并且这些参数都是一个对象的,那么怎么样快速得到这个对象？</h3><p>直接在方法中声明这个对象,Spring MVC就自动会把属性赋值到这个对象里面。</p><h3 id="Spring-MVC中函数的返回值是什么？"><a href="#Spring-MVC中函数的返回值是什么？" class="headerlink" title="Spring MVC中函数的返回值是什么？"></a>Spring MVC中函数的返回值是什么？</h3><p>返回值可以有很多类型,有String, ModelAndView。ModelAndView类把视图和数据都合并的一起的，但一般用String比较好。</p><h3 id="Spring-MVC用什么对象从后台向前台传递数据的？"><a href="#Spring-MVC用什么对象从后台向前台传递数据的？" class="headerlink" title="Spring MVC用什么对象从后台向前台传递数据的？"></a>Spring MVC用什么对象从后台向前台传递数据的？</h3><p>通过ModelMap对象,可以在这个对象里面调用put方法,把对象加到里面,前台就可以通过el表达式拿到。</p><h3 id="怎么样把ModelMap里面的数据放入Session里面？"><a href="#怎么样把ModelMap里面的数据放入Session里面？" class="headerlink" title="怎么样把ModelMap里面的数据放入Session里面？"></a>怎么样把ModelMap里面的数据放入Session里面？</h3><p>可以在类上面加上@SessionAttributes注解,里面包含的字符串就是要放入session里面的key。</p><h3 id="Spring-MVC里面拦截器是怎么写的"><a href="#Spring-MVC里面拦截器是怎么写的" class="headerlink" title="Spring MVC里面拦截器是怎么写的"></a>Spring MVC里面拦截器是怎么写的</h3><p>有两种写法,一种是实现HandlerInterceptor接口，另外一种是继承适配器类，接着在接口方法当中，实现处理逻辑；然后在Spring MVC的配置文件中配置拦截器即可</p><h3 id="介绍一下-WebApplicationContext"><a href="#介绍一下-WebApplicationContext" class="headerlink" title="介绍一下 WebApplicationContext"></a>介绍一下 WebApplicationContext</h3><p>WebApplicationContext 继承了ApplicationContext 并增加了一些WEB应用必备的特有功能，它不同于一般的ApplicationContext ，因为它能处理主题，并找到被关联的servlet。</p>]]></content>
      
      
      <categories>
          
          <category> java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spring mvc </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spring笔记</title>
      <link href="/2020/06/29/Spring%E7%AC%94%E8%AE%B0/"/>
      <url>/2020/06/29/Spring%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<h1 id="Spring笔记"><a href="#Spring笔记" class="headerlink" title="Spring笔记"></a>Spring笔记</h1><h2 id="Spring概述（10）"><a href="#Spring概述（10）" class="headerlink" title="Spring概述（10）"></a>Spring概述（10）</h2><h3 id="什么是spring"><a href="#什么是spring" class="headerlink" title="什么是spring?"></a>什么是spring?</h3><p>为了降低Java开发的复杂性，Spring采取了以下4种关键策略</p><ul><li>基于POJO的轻量级和最小侵入性编程；</li><li>通过依赖注入和面向接口实现松耦合；</li><li>基于切面和惯例进行声明式编程；</li><li>通过切面和模板减少样板式代码。</li></ul><h3 id="Spring框架的设计目标，设计理念，和核心是什么"><a href="#Spring框架的设计目标，设计理念，和核心是什么" class="headerlink" title="Spring框架的设计目标，设计理念，和核心是什么"></a>Spring框架的设计目标，设计理念，和核心是什么</h3><p>Spring设计目标：Spring为开发者提供一个一站式轻量级应用开发平台；</p><p>Spring设计理念：在JavaEE开发中，支持POJO和JavaBean开发方式，使应用面向接口开发，充分支持OO（面向对象）设计方法；Spring通过IoC容器实现对象耦合关系的管理，并实现依赖反转，将对象之间的依赖关系交给IoC容器，实现解耦；</p><p>Spring框架的核心：IoC容器和AOP模块。通过IoC容器管理POJO对象以及他们之间的耦合关系；通过AOP以动态非侵入的方式增强服务。</p><p>IoC让相互协作的组件保持松散的耦合，而AOP编程允许你把遍布于应用各层的功能分离出来形成可重用的功能组件。</p><h3 id="Spring的优缺点是什么？"><a href="#Spring的优缺点是什么？" class="headerlink" title="Spring的优缺点是什么？"></a>Spring的优缺点是什么？</h3><p>优点</p><ul><li><p>方便解耦，简化开发</p><p>Spring就是一个大工厂，可以将所有对象的创建和依赖关系的维护，交给Spring管理。</p></li><li><p>AOP编程的支持</p><p>Spring提供面向切面编程，可以方便的实现对程序进行权限拦截、运行监控等功能。</p></li><li><p>声明式事务的支持</p><p>只需要通过配置就可以完成对事务的管理，而无需手动编程。</p></li><li><p>方便程序的测试</p><p>Spring对Junit4支持，可以通过注解方便的测试Spring程序。</p></li><li><p>方便集成各种优秀框架</p><p>Spring不排斥各种优秀的开源框架，其内部提供了对各种优秀框架的直接支持（如：Struts、Hibernate、MyBatis等）。</p></li><li><p>降低JavaEE API的使用难度</p><p>Spring对JavaEE开发中非常难用的一些API（JDBC、JavaMail、远程调用等），都提供了封装，使这些API应用难度大大降低。</p></li></ul><p>缺点</p><p>Spring明明一个很轻量级的框架，却给人感觉大而全<br>Spring依赖反射，反射影响性能<br>使用门槛升高，入门Spring需要较长时间</p><h3 id="Spring有哪些应用场景"><a href="#Spring有哪些应用场景" class="headerlink" title="Spring有哪些应用场景"></a>Spring有哪些应用场景</h3><p>应用场景：JavaEE企业应用开发，包括SSH、SSM等</p><p>Spring价值：</p><ul><li>Spring是非侵入式的框架，目标是使应用程序代码对框架依赖最小化；</li><li>Spring提供一个一致的编程模型，使应用直接使用POJO开发，与运行环境隔离开来；</li><li>Spring推动应用设计风格向面向对象和面向接口开发转变，提高了代码的重用性和可测试性；</li></ul><h3 id="Spring由哪些模块组成？"><a href="#Spring由哪些模块组成？" class="headerlink" title="Spring由哪些模块组成？"></a>Spring由哪些模块组成？</h3><p>Spring 总共大约有 20 个模块， 由 1300 多个不同的文件构成。 而这些组件被分别整合在核心容器（Core Container） 、 AOP（Aspect Oriented Programming）和设备支持（Instrmentation） 、数据访问与集成（Data Access/Integeration） 、 Web、 消息（Messaging） 、 Test等 6 个模块中。</p><ul><li>spring core：提供了框架的基本组成部分，包括控制反转（Inversion of Control，IOC）和依赖注入（Dependency Injection，DI）功能。</li><li>spring beans：提供了BeanFactory，是工厂模式的一个经典实现，Spring将管理对象称为Bean。</li><li>spring context：构建于 core 封装包基础上的 context 封装包，提供了一种框架式的对象访问方法。</li><li>spring jdbc：提供了一个JDBC的抽象层，消除了烦琐的JDBC编码和数据库厂商特有的错误代码解析， 用于简化JDBC。</li><li>spring aop：提供了面向切面的编程实现，让你可以自定义拦截器、切点等。</li><li>spring Web：提供了针对 Web 开发的集成特性，例如文件上传，利用 servlet listeners 进行 ioc 容器初始化和针对 Web 的 ApplicationContext。</li><li>spring test：主要为测试提供支持的，支持使用JUnit或TestNG对Spring组件进行单元测试和集成测试。</li></ul><h3 id="Spring-框架中都用到了哪些设计模式？"><a href="#Spring-框架中都用到了哪些设计模式？" class="headerlink" title="Spring 框架中都用到了哪些设计模式？"></a>Spring 框架中都用到了哪些设计模式？</h3><ul><li>工厂模式：BeanFactory就是简单工厂模式的体现，用来创建对象的实例；</li><li>单例模式：Bean默认为单例模式。</li><li>代理模式：Spring的AOP功能用到了JDK的动态代理和CGLIB字节码生成技术；</li><li>模板方法：用来解决代码重复的问题。比如. RestTemplate, JmsTemplate, JpaTemplate。</li><li>观察者模式：定义对象键一种一对多的依赖关系，当一个对象的状态发生改变时，所有依赖于它的对象都会得到通知被制动更新，如Spring中listener的实现–ApplicationListener。</li></ul><h3 id="详细讲解一下核心容器（spring-context应用上下文-模块"><a href="#详细讲解一下核心容器（spring-context应用上下文-模块" class="headerlink" title="详细讲解一下核心容器（spring context应用上下文) 模块"></a>详细讲解一下核心容器（spring context应用上下文) 模块</h3><h3 id="Spring框架中有哪些不同类型的事件"><a href="#Spring框架中有哪些不同类型的事件" class="headerlink" title="Spring框架中有哪些不同类型的事件"></a>Spring框架中有哪些不同类型的事件</h3><p>Spring 提供了以下5种标准的事件：</p><ul><li>上下文更新事件（ContextRefreshedEvent）：在调用ConfigurableApplicationContext 接口中的refresh()方法时被触发。</li><li>上下文开始事件（ContextStartedEvent）：当容器调用ConfigurableApplicationContext的Start()方法开始/重新开始容器时触发该事件。</li><li>上下文停止事件（ContextStoppedEvent）：当容器调用ConfigurableApplicationContext的Stop()方法停止容器时触发该事件。</li><li>上下文关闭事件（ContextClosedEvent）：当ApplicationContext被关闭时触发该事件。容器被关闭时，其管理的所有单例Bean都被销毁。</li><li>请求处理事件（RequestHandledEvent）：在Web应用中，当一个http请求（request）结束触发该事件。如果一个bean实现了ApplicationListener接口，当一个ApplicationEvent 被发布以后，bean会自动被通知。</li></ul><h3 id="Spring-应用程序有哪些不同组件？"><a href="#Spring-应用程序有哪些不同组件？" class="headerlink" title="Spring 应用程序有哪些不同组件？"></a>Spring 应用程序有哪些不同组件？</h3><p>Spring 应用一般有以下组件：</p><ul><li>接口 - 定义功能。</li><li>Bean 类 - 它包含属性，setter 和 getter 方法，函数等。</li><li>Bean 配置文件 - 包含类的信息以及如何配置它们。</li><li>Spring 面向切面编程（AOP） - 提供面向切面编程的功能。</li><li>用户程序 - 它使用接口。</li></ul><h3 id="使用-Spring-有哪些方式？"><a href="#使用-Spring-有哪些方式？" class="headerlink" title="使用 Spring 有哪些方式？"></a>使用 Spring 有哪些方式？</h3><p>使用 Spring 有以下方式：</p><ul><li>作为一个成熟的 Spring Web 应用程序。</li><li>作为第三方 Web 框架，使用 Spring Frameworks 中间层。</li><li>作为企业级 Java Bean，它可以包装现有的 POJO（Plain Old Java Objects）。</li><li>用于远程使用。</li></ul><h2 id="Spring控制反转-IOC-（13）"><a href="#Spring控制反转-IOC-（13）" class="headerlink" title="Spring控制反转(IOC)（13）"></a>Spring控制反转(IOC)（13）</h2><h3 id="什么是Spring-IOC-容器？"><a href="#什么是Spring-IOC-容器？" class="headerlink" title="什么是Spring IOC 容器？"></a>什么是Spring IOC 容器？</h3><p>Spring IOC 负责创建对象，管理对象（通过依赖注入（DI），装配对象，配置对象，并且管理这些对象的整个生命周期。</p><h3 id="控制反转-IoC-有什么作用"><a href="#控制反转-IoC-有什么作用" class="headerlink" title="控制反转(IoC)有什么作用"></a>控制反转(IoC)有什么作用</h3><ul><li>管理对象的创建和依赖关系的维护。对象的创建并不是一件简单的事，在对象关系比较复杂时，如果依赖关系需要程序猿来维护的话，那是相当头疼的</li><li>解耦，由容器去维护具体的对象</li><li>托管了类的产生过程，比如我们需要在类的产生过程中做一些处理，最直接的例子就是代理，如果有容器程序可以把这部分处理交给容器，应用程序则无需去关心类是如何完成代理的</li></ul><h3 id="IOC的优点是什么？"><a href="#IOC的优点是什么？" class="headerlink" title="IOC的优点是什么？"></a>IOC的优点是什么？</h3><ul><li>IOC 或 依赖注入把应用的代码量降到最低。</li><li>它使应用容易测试，单元测试不再需要单例和JNDI查找机制。</li><li>最小的代价和最小的侵入性使松散耦合得以实现。</li><li>IOC容器支持加载服务时的饿汉式初始化和懒加载。</li></ul><h3 id="Spring-IoC-的实现机制"><a href="#Spring-IoC-的实现机制" class="headerlink" title="Spring IoC 的实现机制"></a>Spring IoC 的实现机制</h3><p>Spring 中的 IoC 的实现原理就是工厂模式加反射机制。</p><h3 id="Spring-的-IoC支持哪些功能"><a href="#Spring-的-IoC支持哪些功能" class="headerlink" title="Spring 的 IoC支持哪些功能"></a>Spring 的 IoC支持哪些功能</h3><p>Spring 的 IoC 设计支持以下功能：</p><ul><li>依赖注入</li><li>依赖检查</li><li>自动装配</li><li>支持集合</li><li>指定初始化方法和销毁方法</li><li>支持回调某些方法（但是需要实现 Spring 接口，略有侵入）</li></ul><p>其中，最重要的就是依赖注入，从 XML 的配置上说，即 ref 标签。对应 Spring RuntimeBeanReference 对象。</p><p>对于 IoC 来说，最重要的就是容器。容器管理着 Bean 的生命周期，控制着 Bean 的依赖注入。</p><h3 id="BeanFactory-和-ApplicationContext有什么区别？"><a href="#BeanFactory-和-ApplicationContext有什么区别？" class="headerlink" title="BeanFactory 和 ApplicationContext有什么区别？"></a>BeanFactory 和 ApplicationContext有什么区别？</h3><h3 id="Spring-如何设计容器的，BeanFactory和ApplicationContext的关系详解"><a href="#Spring-如何设计容器的，BeanFactory和ApplicationContext的关系详解" class="headerlink" title="Spring 如何设计容器的，BeanFactory和ApplicationContext的关系详解"></a>Spring 如何设计容器的，BeanFactory和ApplicationContext的关系详解</h3><p>BeanFactory 简单粗暴，可以理解为就是个 HashMap，Key 是 BeanName，Value 是 Bean 实例。通常只提供注册（put），获取（get）这两个功能。我们可以称之为 <strong>“低级容器”</strong>。</p><p>ApplicationContext 可以称之为 “高级容器”。因为他比 BeanFactory 多了更多的功能。他继承了多个接口。因此具备了更多的功能。例如资源的获取，支持多种消息（例如 JSP tag 的支持），对 BeanFactory 多了工具级别的支持等待。所以你看他的名字，已经不是 BeanFactory 之类的工厂了，而是 “应用上下文”， 代表着整个大容器的所有功能。该接口定义了一个 refresh 方法，此方法是所有阅读 Spring 源码的人的最熟悉的方法，用于刷新整个容器，即重新加载/刷新所有的 bean。</p><p>小结</p><p>说了这么多，不知道你有没有理解Spring IoC？ 这里小结一下：IoC 在 Spring 里，只需要低级容器就可以实现，2 个步骤：</p><p>加载配置文件，解析成 BeanDefinition 放在 Map 里。</p><p>调用 getBean 的时候，从 BeanDefinition 所属的 Map 里，拿出 Class 对象进行实例化，同时，如果有依赖关系，将递归调用 getBean 方法 —— 完成依赖注入。</p><p>上面就是 Spring 低级容器（BeanFactory）的 IoC。</p><p>至于高级容器 ApplicationContext，他包含了低级容器的功能，当他执行 refresh 模板方法的时候，将刷新整个容器的 Bean。同时其作为高级容器，包含了太多的功能。一句话，他不仅仅是 IoC。他支持不同信息源头，支持 BeanFactory 工具类，支持层级容器，支持访问文件资源，支持事件发布通知，支持接口回调等等。</p><h3 id="ApplicationContext通常的实现是什么？"><a href="#ApplicationContext通常的实现是什么？" class="headerlink" title="ApplicationContext通常的实现是什么？"></a>ApplicationContext通常的实现是什么？</h3><p>FileSystemXmlApplicationContext ：此容器从一个XML文件中加载beans的定义，XML Bean 配置文件的全路径名必须提供给它的构造函数。</p><p>ClassPathXmlApplicationContext：此容器也从一个XML文件中加载beans的定义，这里，你需要正确设置classpath因为这个容器将在classpath里找bean配置。</p><p>WebXmlApplicationContext：此容器加载一个XML文件，此文件定义了一个WEB应用的所有bean。</p><h3 id="什么是Spring的依赖注入？"><a href="#什么是Spring的依赖注入？" class="headerlink" title="什么是Spring的依赖注入？"></a>什么是Spring的依赖注入？</h3><p>依赖注入：相对于IoC而言，依赖注入(DI)更加准确地描述了IoC的设计理念。所谓依赖注入（Dependency Injection），即组件之间的依赖关系由容器在应用系统运行期来决定，也就是由容器动态地将某种依赖关系的目标对象实例注入到应用系统中的各个关联的组件之中。组件不做定位查询，只提供普通的Java方法让容器去决定依赖关系。</p><h3 id="依赖注入的基本原则"><a href="#依赖注入的基本原则" class="headerlink" title="依赖注入的基本原则"></a>依赖注入的基本原则</h3><p>依赖注入的基本原则是：应用组件不应该负责查找资源或者其他依赖的协作对象。配置对象的工作应该由IoC容器负责，“查找资源”的逻辑应该从应用组件的代码中抽取出来，交给IoC容器负责。容器全权负责组件的装配，它会把符合依赖关系的对象通过属性（JavaBean中的setter）或者是构造器传递给需要的对象。</p><h3 id="依赖注入有什么优势"><a href="#依赖注入有什么优势" class="headerlink" title="依赖注入有什么优势"></a>依赖注入有什么优势</h3><p>依赖注入之所以更流行是因为它是一种更可取的方式：让容器全权负责依赖查询，受管组件只需要暴露JavaBean的setter方法或者带参数的构造器或者接口，使容器可以在初始化时组装对象的依赖关系。其与依赖查找方式相比，主要优势为：</p><ul><li>查找定位操作与应用代码完全无关。</li><li>不依赖于容器的API，可以很容易地在任何容器以外使用应用对象。</li><li>不需要特殊的接口，绝大多数对象可以做到完全不必依赖容器。</li></ul><h3 id="有哪些不同类型的依赖注入实现方式？"><a href="#有哪些不同类型的依赖注入实现方式？" class="headerlink" title="有哪些不同类型的依赖注入实现方式？"></a>有哪些不同类型的依赖注入实现方式？</h3><p>依赖注入是时下最流行的IoC实现方式，依赖注入分为接口注入（Interface Injection），Setter方法注入（Setter Injection）和构造器注入（Constructor Injection）三种方式。其中接口注入由于在灵活性和易用性比较差，现在从Spring4开始已被废弃。</p><p>构造器依赖注入：构造器依赖注入通过容器触发一个类的构造器来实现的，该类有一系列参数，每个参数代表一个对其他类的依赖。</p><p>Setter方法注入：Setter方法注入是容器通过调用无参构造器或无参static工厂 方法实例化bean之后，调用该bean的setter方法，即实现了基于setter的依赖注入。</p><h3 id="构造器依赖注入和-Setter方法注入的区别"><a href="#构造器依赖注入和-Setter方法注入的区别" class="headerlink" title="构造器依赖注入和 Setter方法注入的区别"></a>构造器依赖注入和 Setter方法注入的区别</h3><table><thead><tr><th><strong>构造函数注入</strong></th><th><strong>setter</strong> <strong>注入</strong></th></tr></thead><tbody><tr><td>没有部分注入</td><td>有部分注入</td></tr><tr><td>不会覆盖 setter 属性</td><td>会覆盖 setter 属性</td></tr><tr><td>任意修改都会创建一个新实例</td><td>任意修改不会创建一个新实例</td></tr><tr><td>适用于设置很多属性</td><td>适用于设置少量属性</td></tr></tbody></table><p>两种依赖方式都可以使用，构造器注入和Setter方法注入。最好的解决方案是用构造器参数实现强制依赖，setter方法实现可选依赖。</p><h2 id="Spring-Beans（19）"><a href="#Spring-Beans（19）" class="headerlink" title="Spring Beans（19）"></a>Spring Beans（19）</h2><h3 id="什么是Spring-beans？"><a href="#什么是Spring-beans？" class="headerlink" title="什么是Spring beans？"></a>什么是Spring beans？</h3><p>Spring beans 是那些形成Spring应用的主干的java对象。它们被Spring IOC容器初始化，装配，和管理。这些beans通过容器中配置的元数据创建。比如，以XML文件中 的形式定义。</p><h3 id="一个-Spring-Bean-定义-包含什么？"><a href="#一个-Spring-Bean-定义-包含什么？" class="headerlink" title="一个 Spring Bean 定义 包含什么？"></a>一个 Spring Bean 定义 包含什么？</h3><p>一个Spring Bean 的定义包含容器必知的所有配置元数据，包括如何创建一个bean，它的生命周期详情及它的依赖。</p><h3 id="如何给Spring-容器提供配置元数据？Spring有几种配置方式"><a href="#如何给Spring-容器提供配置元数据？Spring有几种配置方式" class="headerlink" title="如何给Spring 容器提供配置元数据？Spring有几种配置方式"></a>如何给Spring 容器提供配置元数据？Spring有几种配置方式</h3><p>这里有三种重要的方法给Spring 容器提供配置元数据。</p><ul><li>XML配置文件。</li><li>基于注解的配置。</li><li>基于java的配置。</li></ul><h3 id="Spring配置文件包含了哪些信息"><a href="#Spring配置文件包含了哪些信息" class="headerlink" title="Spring配置文件包含了哪些信息"></a>Spring配置文件包含了哪些信息</h3><p>Spring配置文件是个XML 文件，这个文件包含了类信息，描述了如何配置它们，以及如何相互调用。</p><h3 id="Spring基于xml注入bean的几种方式"><a href="#Spring基于xml注入bean的几种方式" class="headerlink" title="Spring基于xml注入bean的几种方式"></a>Spring基于xml注入bean的几种方式</h3><ol><li>Set方法注入；</li><li>构造器注入：①通过index设置参数的位置；②通过type设置参数类型；</li><li>静态工厂注入；</li><li>实例工厂；</li></ol><h3 id="你怎样定义类的作用域？"><a href="#你怎样定义类的作用域？" class="headerlink" title="你怎样定义类的作用域？"></a>你怎样定义类的作用域？</h3><p>当定义一个 在Spring里，我们还能给这个bean声明一个作用域。它可以通过bean 定义中的scope属性来定义。如，当Spring要在需要的时候每次生产一个新的bean实例，bean的scope属性被指定为prototype。另一方面，一个bean每次使用的时候必须返回同一个实例，这个bean的scope 属性 必须设为 singleton。</p><h3 id="解释Spring支持的几种bean的作用域"><a href="#解释Spring支持的几种bean的作用域" class="headerlink" title="解释Spring支持的几种bean的作用域"></a>解释Spring支持的几种bean的作用域</h3><p>Spring框架支持以下五种bean的作用域：</p><ul><li>singleton : bean在每个Spring ioc 容器中只有一个实例。</li><li>prototype：一个bean的定义可以有多个实例。</li><li>request：每次http请求都会创建一个bean，该作用域仅在基于web的Spring ApplicationContext情形下有效。</li><li>session：在一个HTTP Session中，一个bean定义对应一个实例。该作用域仅在基于web的Spring ApplicationContext情形下有效。</li><li>global-session：在一个全局的HTTP Session中，一个bean定义对应一个实例。该作用域仅在基于web的Spring ApplicationContext情形下有效。</li></ul><p>注意： 缺省的Spring bean 的作用域是Singleton。使用 prototype 作用域需要慎重的思考，因为频繁创建和销毁 bean 会带来很大的性能开销。</p><h3 id="Spring框架中的单例bean是线程安全的吗？"><a href="#Spring框架中的单例bean是线程安全的吗？" class="headerlink" title="Spring框架中的单例bean是线程安全的吗？"></a>Spring框架中的单例bean是线程安全的吗？</h3><p>不是，Spring框架中的单例bean不是线程安全的。</p><p>spring 中的 bean 默认是单例模式，spring 框架并没有对单例 bean 进行多线程的封装处理。</p><p>实际上大部分时候 spring bean 无状态的（比如 dao 类），所有某种程度上来说 bean 也是安全的，但如果 bean 有状态的话（比如 view model 对象），那就要开发者自己去保证线程安全了，最简单的就是改变 bean 的作用域，把“singleton”变更为“prototype”，这样请求 bean 相当于 new Bean()了，所以就可以保证线程安全了。</p><ul><li>有状态就是有数据存储功能。</li><li>无状态就是不会保存数据。</li></ul><h3 id="Spring如何处理线程并发问题？"><a href="#Spring如何处理线程并发问题？" class="headerlink" title="Spring如何处理线程并发问题？"></a>Spring如何处理线程并发问题？</h3><p>在一般情况下，只有无状态的Bean才可以在多线程环境下共享，在Spring中，绝大部分Bean都可以声明为singleton作用域，因为Spring对一些Bean中非线程安全状态采用ThreadLocal进行处理，解决线程安全问题。</p><p>ThreadLocal和线程同步机制都是为了解决多线程中相同变量的访问冲突问题。同步机制采用了“时间换空间”的方式，仅提供一份变量，不同的线程在访问前需要获取锁，没获得锁的线程则需要排队。而ThreadLocal采用了“空间换时间”的方式。</p><p>ThreadLocal会为每一个线程提供一个独立的变量副本，从而隔离了多个线程对数据的访问冲突。因为每一个线程都拥有自己的变量副本，从而也就没有必要对该变量进行同步了。ThreadLocal提供了线程安全的共享对象，在编写多线程代码时，可以把不安全的变量封装进ThreadLocal。</p><h3 id="解释Spring框架中bean的生命周期"><a href="#解释Spring框架中bean的生命周期" class="headerlink" title="解释Spring框架中bean的生命周期"></a>解释Spring框架中bean的生命周期</h3><h3 id="哪些是重要的bean生命周期方法？-你能重载它们吗？"><a href="#哪些是重要的bean生命周期方法？-你能重载它们吗？" class="headerlink" title="哪些是重要的bean生命周期方法？ 你能重载它们吗？"></a>哪些是重要的bean生命周期方法？ 你能重载它们吗？</h3><p>有两个重要的bean 生命周期方法，第一个是setup ， 它是在容器加载bean的时候被调用。第二个方法是 teardown 它是在容器卸载类的时候被调用。</p><p>bean 标签有两个重要的属性（init-method和destroy-method）。用它们你可以自己定制初始化和注销方法。它们也有相应的注解（@PostConstruct和@PreDestroy）。</p><h3 id="什么是Spring的内部bean？什么是Spring-inner-beans？"><a href="#什么是Spring的内部bean？什么是Spring-inner-beans？" class="headerlink" title="什么是Spring的内部bean？什么是Spring inner beans？"></a>什么是Spring的内部bean？什么是Spring inner beans？</h3><p>在Spring框架中，当一个bean仅被用作另一个bean的属性时，它能被声明为一个内部bean。内部bean可以用setter注入“属性”和构造方法注入“构造参数”的方式来实现，内部bean通常是匿名的，它们的Scope一般是prototype。</p><h3 id="在-Spring中如何注入一个java集合？"><a href="#在-Spring中如何注入一个java集合？" class="headerlink" title="在 Spring中如何注入一个java集合？"></a>在 Spring中如何注入一个java集合？</h3><p>Spring提供以下几种集合的配置元素：</p><p>类型用于注入一列值，允许有相同的值。</p><p>类型用于注入一组值，不允许有相同的值。</p><p>类型用于注入一组键值对，键和值都可以为任意类型。</p><p>类型用于注入一组键值对，键和值都只能为String类型。</p><h3 id="什么是bean装配？"><a href="#什么是bean装配？" class="headerlink" title="什么是bean装配？"></a>什么是bean装配？</h3><p>装配，或bean 装配是指在Spring 容器中把bean组装到一起，前提是容器需要知道bean的依赖关系，如何通过依赖注入来把它们装配到一起。</p><h3 id="什么是bean的自动装配？"><a href="#什么是bean的自动装配？" class="headerlink" title="什么是bean的自动装配？"></a>什么是bean的自动装配？</h3><p>在Spring框架中，在配置文件中设定bean的依赖关系是一个很好的机制，Spring 容器能够自动装配相互合作的bean，这意味着容器不需要和配置，能通过Bean工厂自动处理bean之间的协作。这意味着 Spring可以通过向Bean Factory中注入的方式自动搞定bean之间的依赖关系。自动装配可以设置在每个bean上，也可以设定在特定的bean上。</p><h3 id="解释不同方式的自动装配，spring-自动装配-bean-有哪些方式？"><a href="#解释不同方式的自动装配，spring-自动装配-bean-有哪些方式？" class="headerlink" title="解释不同方式的自动装配，spring 自动装配 bean 有哪些方式？"></a>解释不同方式的自动装配，spring 自动装配 bean 有哪些方式？</h3><p>在spring中，对象无需自己查找或创建与其关联的其他对象，由容器负责把需要相互协作的对象引用赋予各个对象，使用autowire来配置自动装载模式。</p><p>在Spring框架xml配置中共有5种自动装配：</p><ul><li>no：默认的方式是不进行自动装配的，通过手工设置ref属性来进行装配bean。</li><li>byName：通过bean的名称进行自动装配，如果一个bean的 property 与另一bean 的name 相同，就进行自动装配。</li><li>byType：通过参数的数据类型进行自动装配。</li><li>constructor：利用构造函数进行装配，并且构造函数的参数通过byType进行装配。</li><li>autodetect：自动探测，如果有构造方法，通过 construct的方式自动装配，否则使用 byType的方式自动装配。</li></ul><h3 id="使用-Autowired注解自动装配的过程是怎样的？"><a href="#使用-Autowired注解自动装配的过程是怎样的？" class="headerlink" title="使用@Autowired注解自动装配的过程是怎样的？"></a>使用@Autowired注解自动装配的过程是怎样的？</h3><p>使用@Autowired注解来自动装配指定的bean。在使用@Autowired注解之前需要在Spring配置文件进行配置，&lt;context:annotation-config /&gt;。</p><p>在启动spring IoC时，容器自动装载了一个AutowiredAnnotationBeanPostProcessor后置处理器，当容器扫描到@Autowied、@Resource或@Inject时，就会在IoC容器自动查找需要的bean，并装配给该对象的属性。在使用@Autowired时，首先在容器中查询对应类型的bean：</p><ul><li>如果查询结果刚好为一个，就将该bean装配给@Autowired指定的数据；</li><li>如果查询的结果不止一个，那么@Autowired会根据名称来查找；</li><li>如果上述查找的结果为空，那么会抛出异常。解决方法时，使用required=false。</li></ul><h3 id="自动装配有哪些局限性？"><a href="#自动装配有哪些局限性？" class="headerlink" title="自动装配有哪些局限性？"></a>自动装配有哪些局限性？</h3><p>自动装配的局限性是：</p><p><strong>重写</strong>：你仍需用 和 配置来定义依赖，意味着总要重写自动装配。</p><p><strong>基本数据类型</strong>：你不能自动装配简单的属性，如基本数据类型，String字符串，和类。</p><p><strong>模糊特性</strong>：自动装配不如显式装配精确，如果有可能，建议使用显式装配。</p><h3 id="你可以在Spring中注入一个null-和一个空字符串吗？"><a href="#你可以在Spring中注入一个null-和一个空字符串吗？" class="headerlink" title="你可以在Spring中注入一个null 和一个空字符串吗？"></a>你可以在Spring中注入一个null 和一个空字符串吗？</h3><p>可以。</p><h2 id="Spring注解（8）"><a href="#Spring注解（8）" class="headerlink" title="Spring注解（8）"></a>Spring注解（8）</h2><h3 id="什么是基于Java的Spring注解配置-给一些注解的例子"><a href="#什么是基于Java的Spring注解配置-给一些注解的例子" class="headerlink" title="什么是基于Java的Spring注解配置? 给一些注解的例子"></a>什么是基于Java的Spring注解配置? 给一些注解的例子</h3><p>基于Java的配置，允许你在少量的Java注解的帮助下，进行你的大部分Spring配置而非通过XML文件。</p><h3 id="怎样开启注解装配？"><a href="#怎样开启注解装配？" class="headerlink" title="怎样开启注解装配？"></a>怎样开启注解装配？</h3><p>注解装配在默认情况下是不开启的，为了使用注解装配，我们必须在Spring配置文件中配置<a href="context:annotation-config" target="_blank" rel="noopener">context:annotation-config</a>元素。</p><h3 id="Component-Controller-Repository-Service-有何区别？"><a href="#Component-Controller-Repository-Service-有何区别？" class="headerlink" title="@Component, @Controller, @Repository, @Service 有何区别？"></a>@Component, @Controller, @Repository, @Service 有何区别？</h3><p>@Component：这将 java 类标记为 bean。它是任何 Spring 管理组件的通用构造型。spring 的组件扫描机制现在可以将其拾取并将其拉入应用程序环境中。</p><p>@Controller：这将一个类标记为 Spring Web MVC 控制器。标有它的 Bean 会自动导入到 IoC 容器中。</p><p>@Service：此注解是组件注解的特化。它不会对 @Component 注解提供任何其他行为。您可以在服务层类中使用 @Service 而不是 @Component，因为它以更好的方式指定了意图。</p><p>@Repository：这个注解是具有类似用途和功能的 @Component 注解的特化。它为 DAO 提供了额外的好处。它将 DAO 导入 IoC 容器，并使未经检查的异常有资格转换为 Spring DataAccessException。</p><h3 id="Required-注解有什么作用"><a href="#Required-注解有什么作用" class="headerlink" title="@Required 注解有什么作用"></a>@Required 注解有什么作用</h3><p>这个注解表明bean的属性必须在配置的时候设置，通过一个bean定义的显式的属性值或通过自动装配，若@Required注解的bean属性未被设置，容器将抛出BeanInitializationException。示例：</p><h3 id="Autowired-注解有什么作用"><a href="#Autowired-注解有什么作用" class="headerlink" title="@Autowired 注解有什么作用"></a>@Autowired 注解有什么作用</h3><p>@Autowired默认是按照类型装配注入的，默认情况下它要求依赖对象必须存在（可以设置它required属性为false）。@Autowired 注解提供了更细粒度的控制，包括在何处以及如何完成自动装配。</p><h3 id="Autowired和-Resource之间的区别"><a href="#Autowired和-Resource之间的区别" class="headerlink" title="@Autowired和@Resource之间的区别"></a>@Autowired和@Resource之间的区别</h3><p>@Autowired可用于：构造函数、成员变量、Setter方法</p><p>@Autowired和@Resource之间的区别</p><ul><li>@Autowired默认是按照类型装配注入的，默认情况下它要求依赖对象必须存在（可以设置它required属性为false）。</li><li>@Resource默认是按照名称来装配注入的，只有当找不到与名称匹配的bean才会按照类型来装配注入。</li></ul><h3 id="Qualifier-注解有什么作用"><a href="#Qualifier-注解有什么作用" class="headerlink" title="@Qualifier 注解有什么作用"></a>@Qualifier 注解有什么作用</h3><p>当您创建多个相同类型的 bean 并希望仅使用属性装配其中一个 bean 时，您可以使用@Qualifier 注解和 @Autowired 通过指定应该装配哪个确切的 bean 来消除歧义。</p><h3 id="RequestMapping-注解有什么用？"><a href="#RequestMapping-注解有什么用？" class="headerlink" title="@RequestMapping 注解有什么用？"></a>@RequestMapping 注解有什么用？</h3><p>@RequestMapping 注解用于将特定 HTTP 请求方法映射到将处理相应请求的控制器中的特定类/方法。此注释可应用于两个级别：</p><ul><li>类级别：映射请求的 URL</li><li>方法级别：映射 URL 以及 HTTP 请求方法</li></ul><h2 id="Spring数据访问（14）"><a href="#Spring数据访问（14）" class="headerlink" title="Spring数据访问（14）"></a>Spring数据访问（14）</h2><h3 id="解释对象-关系映射集成模块"><a href="#解释对象-关系映射集成模块" class="headerlink" title="解释对象/关系映射集成模块"></a>解释对象/关系映射集成模块</h3><p>Spring 通过提供ORM模块，支持我们在直接JDBC之上使用一个对象/关系映射映射(ORM)工具，Spring 支持集成主流的ORM框架，如Hiberate，JDO和 iBATIS，JPA，TopLink，JDO，OJB 。Spring的事务管理同样支持以上所有ORM框架及JDBC。</p><h3 id="在Spring框架中如何更有效地使用JDBC？"><a href="#在Spring框架中如何更有效地使用JDBC？" class="headerlink" title="在Spring框架中如何更有效地使用JDBC？"></a>在Spring框架中如何更有效地使用JDBC？</h3><p>使用Spring JDBC 框架，资源管理和错误处理的代价都会被减轻。所以开发者只需写statements 和 queries从数据存取数据，JDBC也可以在Spring框架提供的模板类的帮助下更有效地被使用，这个模板叫JdbcTemplate</p><h3 id="解释JDBC抽象和DAO模块"><a href="#解释JDBC抽象和DAO模块" class="headerlink" title="解释JDBC抽象和DAO模块"></a>解释JDBC抽象和DAO模块</h3><p>通过使用JDBC抽象和DAO模块，保证数据库代码的简洁，并能避免数据库资源错误关闭导致的问题，它在各种不同的数据库的错误信息之上，提供了一个统一的异常访问层。它还利用Spring的AOP 模块给Spring应用中的对象提供事务管理服务。</p><h3 id="spring-DAO-有什么用？"><a href="#spring-DAO-有什么用？" class="headerlink" title="spring DAO 有什么用？"></a>spring DAO 有什么用？</h3><p>Spring DAO（数据访问对象） 使得 JDBC，Hibernate 或 JDO 这样的数据访问技术更容易以一种统一的方式工作。这使得用户容易在持久性技术之间切换。它还允许您在编写代码时，无需考虑捕获每种技术不同的异常。</p><h3 id="spring-JDBC-API-中存在哪些类？"><a href="#spring-JDBC-API-中存在哪些类？" class="headerlink" title="spring JDBC API 中存在哪些类？"></a>spring JDBC API 中存在哪些类？</h3><p>JdbcTemplate</p><p>SimpleJdbcTemplate</p><p>NamedParameterJdbcTemplate</p><p>SimpleJdbcInsert</p><p>SimpleJdbcCall</p><h3 id="JdbcTemplate是什么"><a href="#JdbcTemplate是什么" class="headerlink" title="JdbcTemplate是什么"></a>JdbcTemplate是什么</h3><p>JdbcTemplate 类提供了很多便利的方法解决诸如把数据库数据转变成基本数据类型或对象，执行写好的或可调用的数据库操作语句，提供自定义的数据错误处理。</p><h3 id="使用Spring通过什么方式访问Hibernate？使用-Spring-访问-Hibernate-的方法有哪些？"><a href="#使用Spring通过什么方式访问Hibernate？使用-Spring-访问-Hibernate-的方法有哪些？" class="headerlink" title="使用Spring通过什么方式访问Hibernate？使用 Spring 访问 Hibernate 的方法有哪些？"></a>使用Spring通过什么方式访问Hibernate？使用 Spring 访问 Hibernate 的方法有哪些？</h3><p>在Spring中有两种方式访问Hibernate：</p><ul><li>使用 Hibernate 模板和回调进行控制反转</li><li>扩展 HibernateDAOSupport 并应用 AOP 拦截器节点</li></ul><h3 id="如何通过HibernateDaoSupport将Spring和Hibernate结合起来？"><a href="#如何通过HibernateDaoSupport将Spring和Hibernate结合起来？" class="headerlink" title="如何通过HibernateDaoSupport将Spring和Hibernate结合起来？"></a>如何通过HibernateDaoSupport将Spring和Hibernate结合起来？</h3><p>用Spring的 SessionFactory 调用 LocalSessionFactory。集成过程分三步：</p><ul><li>配置the Hibernate SessionFactory</li><li>继承HibernateDaoSupport实现一个DAO</li><li>在AOP支持的事务中装配</li></ul><h3 id="Spring支持的事务管理类型，-spring-事务实现方式有哪些？"><a href="#Spring支持的事务管理类型，-spring-事务实现方式有哪些？" class="headerlink" title="Spring支持的事务管理类型， spring 事务实现方式有哪些？"></a>Spring支持的事务管理类型， spring 事务实现方式有哪些？</h3><p>Spring支持两种类型的事务管理：</p><p><strong>编程式事务管理</strong>：这意味你通过编程的方式管理事务，给你带来极大的灵活性，但是难维护。</p><p><strong>声明式事务管理</strong>：这意味着你可以将业务代码和事务管理分离，你只需用注解和XML配置来管理事务。</p><h3 id="Spring事务的实现方式和实现原理"><a href="#Spring事务的实现方式和实现原理" class="headerlink" title="Spring事务的实现方式和实现原理"></a>Spring事务的实现方式和实现原理</h3><p>Spring事务的本质其实就是数据库对事务的支持，没有数据库的事务支持，spring是无法提供事务功能的。真正的数据库层的事务提交和回滚是通过binlog或者redo log实现的。</p><h3 id="说一下Spring的事务传播行为"><a href="#说一下Spring的事务传播行为" class="headerlink" title="说一下Spring的事务传播行为"></a>说一下Spring的事务传播行为</h3><p>① PROPAGATION_REQUIRED：如果当前没有事务，就创建一个新事务，如果当前存在事务，就加入该事务，该设置是最常用的设置。</p><p>② PROPAGATION_SUPPORTS：支持当前事务，如果当前存在事务，就加入该事务，如果当前不存在事务，就以非事务执行。</p><p>③ PROPAGATION_MANDATORY：支持当前事务，如果当前存在事务，就加入该事务，如果当前不存在事务，就抛出异常。</p><p>④ PROPAGATION_REQUIRES_NEW：创建新事务，无论当前存不存在事务，都创建新事务。</p><p>⑤ PROPAGATION_NOT_SUPPORTED：以非事务方式执行操作，如果当前存在事务，就把当前事务挂起。</p><p>⑥ PROPAGATION_NEVER：以非事务方式执行，如果当前存在事务，则抛出异常。</p><p>⑦ PROPAGATION_NESTED：如果当前存在事务，则在嵌套事务内执行。如果当前没有事务，则按REQUIRED属性执行。</p><h3 id="说一下-spring-的事务隔离？"><a href="#说一下-spring-的事务隔离？" class="headerlink" title="说一下 spring 的事务隔离？"></a>说一下 spring 的事务隔离？</h3><p>spring 有五大隔离级别，默认值为 ISOLATION_DEFAULT（使用数据库的设置），其他四个隔离级别和数据库的隔离级别一致：</p><ul><li>ISOLATION_DEFAULT：用底层数据库的设置隔离级别，数据库设置的是什么我就用什么；</li><li>ISOLATION_READ_UNCOMMITTED：未提交读，最低隔离级别、事务未提交前，就可被其他事务读取（会出现幻读、脏读、不可重复读）；</li><li>ISOLATION_READ_COMMITTED：提交读，一个事务提交后才能被其他事务读取到（会造成幻读、不可重复读），SQL server 的默认级别；</li><li>ISOLATION_REPEATABLE_READ：可重复读，保证多次读取同一个数据时，其值都和事务开始时候的内容是一致，禁止读取到别的事务未提交的数据（会造成幻读），MySQL 的默认级别；</li><li>ISOLATION_SERIALIZABLE：序列化，代价最高最可靠的隔离级别，该隔离级别能防止脏读、不可重复读、幻读。</li></ul><p>脏读 ：表示一个事务能够读取另一个事务中还未提交的数据。比如，某个事务尝试插入记录 A，此时该事务还未提交，然后另一个事务尝试读取到了记录 A。</p><p>不可重复读 ：是指在一个事务内，多次读同一数据。</p><p>幻读 ：指同一个事务内多次查询返回的结果集不一样。比如同一个事务 A 第一次查询时候有 n 条记录，但是第二次同等条件下查询却有 n+1 条记录，这就好像产生了幻觉。发生幻读的原因也是另外一个事务新增或者删除或者修改了第一个事务结果集里面的数据，同一个记录的数据内容被修改了，所有数据行的记录就变多或者变少了。</p><h3 id="Spring框架的事务管理有哪些优点？"><a href="#Spring框架的事务管理有哪些优点？" class="headerlink" title="Spring框架的事务管理有哪些优点？"></a>Spring框架的事务管理有哪些优点？</h3><ul><li>为不同的事务API 如 JTA，JDBC，Hibernate，JPA 和JDO，提供一个不变的编程模式。</li><li>为编程式事务管理提供了一套简单的API而不是一些复杂的事务API</li><li>支持声明式事务管理。</li><li>和Spring各种数据访问抽象层很好得集成。</li></ul><h3 id="你更倾向用那种事务管理类型？"><a href="#你更倾向用那种事务管理类型？" class="headerlink" title="你更倾向用那种事务管理类型？"></a>你更倾向用那种事务管理类型？</h3><p>大多数Spring框架的用户选择声明式事务管理，因为它对应用代码的影响最小，因此更符合一个无侵入的轻量级容器的思想。声明式事务管理要优于编程式事务管理，虽然比编程式事务管理（这种方式允许你通过代码控制事务）少了一点灵活性。唯一不足地方是，最细粒度只能作用到方法级别，无法做到像编程式事务那样可以作用到代码块级别。</p><h2 id="Spring面向切面编程-AOP-（13）"><a href="#Spring面向切面编程-AOP-（13）" class="headerlink" title="Spring面向切面编程(AOP)（13）"></a>Spring面向切面编程(AOP)（13）</h2><h3 id="什么是AOP"><a href="#什么是AOP" class="headerlink" title="什么是AOP"></a>什么是AOP</h3><p>OOP(Object-Oriented Programming)面向对象编程，允许开发者定义纵向的关系，但并适用于定义横向的关系，导致了大量代码的重复，而不利于各个模块的重用。</p><p>AOP(Aspect-Oriented Programming)，一般称为面向切面编程，作为面向对象的一种补充，用于将那些与业务无关，但却对多个对象产生影响的公共行为和逻辑，抽取并封装为一个可重用的模块，这个模块被命名为“切面”（Aspect），减少系统中的重复代码，降低了模块间的耦合度，同时提高了系统的可维护性。可用于权限认证、日志、事务处理等。</p><h3 id="Spring-AOP-and-AspectJ-AOP-有什么区别？AOP-有哪些实现方式？"><a href="#Spring-AOP-and-AspectJ-AOP-有什么区别？AOP-有哪些实现方式？" class="headerlink" title="Spring AOP and AspectJ AOP 有什么区别？AOP 有哪些实现方式？"></a>Spring AOP and AspectJ AOP 有什么区别？AOP 有哪些实现方式？</h3><p>AOP实现的关键在于 代理模式，AOP代理主要分为静态代理和动态代理。静态代理的代表为AspectJ；动态代理则以Spring AOP为代表。</p><p>（1）AspectJ是静态代理的增强，所谓静态代理，就是AOP框架会在编译阶段生成AOP代理类，因此也称为编译时增强，他会在编译阶段将AspectJ(切面)织入到Java字节码中，运行的时候就是增强之后的AOP对象。</p><p>（2）Spring AOP使用的动态代理，所谓的动态代理就是说AOP框架不会去修改字节码，而是每次运行时在内存中临时为方法生成一个AOP对象，这个AOP对象包含了目标对象的全部方法，并且在特定的切点做了增强处理，并回调原对象的方法。</p><h3 id="JDK动态代理和CGLIB动态代理的区别"><a href="#JDK动态代理和CGLIB动态代理的区别" class="headerlink" title="JDK动态代理和CGLIB动态代理的区别"></a>JDK动态代理和CGLIB动态代理的区别</h3><p>Spring AOP中的动态代理主要有两种方式，JDK动态代理和CGLIB动态代理：</p><ul><li>JDK动态代理只提供接口的代理，不支持类的代理。核心InvocationHandler接口和Proxy类，InvocationHandler 通过invoke()方法反射来调用目标类中的代码，动态地将横切逻辑和业务编织在一起；接着，Proxy利用 InvocationHandler动态创建一个符合某一接口的的实例, 生成目标类的代理对象。</li><li>如果代理类没有实现 InvocationHandler 接口，那么Spring AOP会选择使用CGLIB来动态代理目标类。CGLIB（Code Generation Library），是一个代码生成的类库，可以在运行时动态的生成指定类的一个子类对象，并覆盖其中特定方法并添加增强代码，从而实现AOP。CGLIB是通过继承的方式做的动态代理，因此如果某个类被标记为final，那么它是无法使用CGLIB做动态代理的。</li></ul><p>静态代理与动态代理区别在于生成AOP代理对象的时机不同，相对来说AspectJ的静态代理方式具有更好的性能，但是AspectJ需要特定的编译器进行处理，而Spring AOP则无需特定的编译器处理。</p><h3 id="如何理解-Spring-中的代理？"><a href="#如何理解-Spring-中的代理？" class="headerlink" title="如何理解 Spring 中的代理？"></a>如何理解 Spring 中的代理？</h3><p>将 Advice 应用于目标对象后创建的对象称为代理。在客户端对象的情况下，目标对象和代理对象是相同的。</p><p>Advice + Target Object = Proxy</p><h3 id="解释一下Spring-AOP里面的几个名词"><a href="#解释一下Spring-AOP里面的几个名词" class="headerlink" title="解释一下Spring AOP里面的几个名词"></a>解释一下Spring AOP里面的几个名词</h3><p>（1）切面（Aspect）：切面是通知和切点的结合。通知和切点共同定义了切面的全部内容。 在Spring AOP中，切面可以使用通用类（基于模式的风格） 或者在普通类中以 @AspectJ 注解来实现。</p><p>（2）连接点（Join point）：指方法，在Spring AOP中，一个连接点 总是 代表一个方法的执行。 应用可能有数以千计的时机应用通知。这些时机被称为连接点。连接点是在应用执行过程中能够插入切面的一个点。这个点可以是调用方法时、抛出异常时、甚至修改一个字段时。切面代码可以利用这些点插入到应用的正常流程之中，并添加新的行为。</p><p>（3）通知（Advice）：在AOP术语中，切面的工作被称为通知。</p><p>（4）切入点（Pointcut）：切点的定义会匹配通知所要织入的一个或多个连接点。我们通常使用明确的类和方法名称，或是利用正则表达式定义所匹配的类和方法名称来指定这些切点。</p><p>（5）引入（Introduction）：引入允许我们向现有类添加新方法或属性。</p><p>（6）目标对象（Target Object）： 被一个或者多个切面（aspect）所通知（advise）的对象。它通常是一个代理对象。也有人把它叫做 被通知（adviced） 对象。 既然Spring AOP是通过运行时代理实现的，这个对象永远是一个 被代理（proxied） 对象。</p><p>（7）织入（Weaving）：织入是把切面应用到目标对象并创建新的代理对象的过程。在目标对象的生命周期里有多少个点可以进行织入：</p><ul><li>编译期：切面在目标类编译时被织入。AspectJ的织入编译器是以这种方式织入切面的。</li><li>类加载期：切面在目标类加载到JVM时被织入。需要特殊的类加载器，它可以在目标类被引入应用之前增强该目标类的字节码。AspectJ5的加载时织入就支持以这种方式织入切面。</li><li>运行期：切面在应用运行的某个时刻被织入。一般情况下，在织入切面时，AOP容器会为目标对象动态地创建一个代理对象。SpringAOP就是以这种方式织入切面。</li></ul><h3 id="Spring在运行时通知对象"><a href="#Spring在运行时通知对象" class="headerlink" title="Spring在运行时通知对象"></a>Spring在运行时通知对象</h3><p>通过在代理类中包裹切面，Spring在运行期把切面织入到Spring管理的bean中。代理封装了目标类，并拦截被通知方法的调用，再把调用转发给真正的目标bean。当代理拦截到方法调用时，在调用目标bean方法之前，会执行切面逻辑。</p><p>直到应用需要被代理的bean时，Spring才创建代理对象。如果使用的是ApplicationContext的话，在ApplicationContext从BeanFactory中加载所有bean的时候，Spring才会创建被代理的对象。因为Spring运行时才创建代理对象，所以我们不需要特殊的编译器来织入SpringAOP的切面。</p><h3 id="Spring只支持方法级别的连接点"><a href="#Spring只支持方法级别的连接点" class="headerlink" title="Spring只支持方法级别的连接点"></a>Spring只支持方法级别的连接点</h3><p>因为Spring基于动态代理，所以Spring只支持方法连接点。Spring缺少对字段连接点的支持，而且它不支持构造器连接点。方法之外的连接点拦截功能，我们可以利用Aspect来补充。</p><h3 id="在Spring-AOP-中，关注点和横切关注的区别是什么？在-spring-aop-中-concern-和-cross-cutting-concern-的不同之处"><a href="#在Spring-AOP-中，关注点和横切关注的区别是什么？在-spring-aop-中-concern-和-cross-cutting-concern-的不同之处" class="headerlink" title="在Spring AOP 中，关注点和横切关注的区别是什么？在 spring aop 中 concern 和 cross-cutting concern 的不同之处"></a>在Spring AOP 中，关注点和横切关注的区别是什么？在 spring aop 中 concern 和 cross-cutting concern 的不同之处</h3><p>关注点（concern）是应用中一个模块的行为，一个关注点可能会被定义成一个我们想实现的一个功能。</p><p>横切关注点（cross-cutting concern）是一个关注点，此关注点是整个应用都会使用的功能，并影响整个应用，比如日志，安全和数据传输，几乎应用的每个模块都需要的功能。因此这些都属于横切关注点。</p><h3 id="Spring通知有哪些类型？"><a href="#Spring通知有哪些类型？" class="headerlink" title="Spring通知有哪些类型？"></a>Spring通知有哪些类型？</h3><p>Spring切面可以应用5种类型的通知：</p><ul><li>前置通知（Before）：在目标方法被调用之前调用通知功能；</li><li>后置通知（After）：在目标方法完成之后调用通知，此时不会关心方法的输出是什么；</li><li>返回通知（After-returning ）：在目标方法成功执行之后调用通知；</li><li>异常通知（After-throwing）：在目标方法抛出异常后调用通知；</li><li>环绕通知（Around）：通知包裹了被通知的方法，在被通知的方法调用之前和调用之后执行自定义的行为。</li></ul><h3 id="什么是切面-Aspect？"><a href="#什么是切面-Aspect？" class="headerlink" title="什么是切面 Aspect？"></a>什么是切面 Aspect？</h3><p>aspect 由 pointcount 和 advice 组成，切面是通知和切点的结合。 它既包含了横切逻辑的定义, 也包括了连接点的定义. Spring AOP 就是负责实施切面的框架, 它将切面所定义的横切逻辑编织到切面所指定的连接点中.<br>AOP 的工作重心在于如何将增强编织目标对象的连接点上, 这里包含两个工作:</p><ul><li>如何通过 pointcut 和 advice 定位到特定的 joinpoint 上</li><li>如何在 advice 中编写切面代码.</li></ul><p>可以简单地认为, 使用 @Aspect 注解的类就是切面.</p><h3 id="解释基于XML-Schema方式的切面实现"><a href="#解释基于XML-Schema方式的切面实现" class="headerlink" title="解释基于XML Schema方式的切面实现"></a>解释基于XML Schema方式的切面实现</h3><p>在这种情况下，切面由常规类以及基于XML的配置实现。</p><h3 id="解释基于注解的切面实现"><a href="#解释基于注解的切面实现" class="headerlink" title="解释基于注解的切面实现"></a>解释基于注解的切面实现</h3><p>在这种情况下(基于@AspectJ的实现)，涉及到的切面声明的风格与带有java5标注的普通java类一致。</p><h3 id="有几种不同类型的自动代理？"><a href="#有几种不同类型的自动代理？" class="headerlink" title="有几种不同类型的自动代理？"></a>有几种不同类型的自动代理？</h3><p>BeanNameAutoProxyCreator</p><p>DefaultAdvisorAutoProxyCreator</p><p>Metadata autoproxying</p>]]></content>
      
      
      <categories>
          
          <category> java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spring </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>JVM笔记</title>
      <link href="/2020/06/29/JVM%E7%AC%94%E8%AE%B0/"/>
      <url>/2020/06/29/JVM%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<h1 id="JVM笔记"><a href="#JVM笔记" class="headerlink" title="JVM笔记"></a>JVM笔记</h1><h2 id="Java内存区域"><a href="#Java内存区域" class="headerlink" title="Java内存区域"></a>Java内存区域</h2><h3 id="说一下-JVM-的主要组成部分及其作用？"><a href="#说一下-JVM-的主要组成部分及其作用？" class="headerlink" title="说一下 JVM 的主要组成部分及其作用？"></a>说一下 JVM 的主要组成部分及其作用？</h3><p>JVM包含两个子系统和两个组件，两个子系统为Class loader(类装载)、Execution engine(执行引擎)；两个组件为Runtime data area(运行时数据区)、Native Interface(本地接口)。</p><p>Class loader(类装载)：根据给定的全限定名类名(如：java.lang.Object)来装载class文件到Runtime data area中的method area。</p><p>Execution engine（执行引擎）：执行classes中的指令。</p><p>Native Interface(本地接口)：与native libraries交互，是其它编程语言交互的接口。</p><p>Runtime data area(运行时数据区域)：这就是我们常说的JVM的内存。</p><p>作用 ：首先通过编译器把 Java 代码转换成字节码，类加载器（ClassLoader）再把字节码加载到内存中，将其放在运行时数据区（Runtime data area）的方法区内，而字节码文件只是 JVM 的一套指令集规范，并不能直接交给底层操作系统去执行，因此需要特定的命令解析器执行引擎（Execution Engine），将字节码翻译成底层系统指令，再交由 CPU 去执行，而这个过程中需要调用其他语言的本地库接口（Native Interface）来实现整个程序的功能。</p><p><strong>下面是Java程序运行机制详细说明</strong></p><p>Java程序运行机制步骤</p><ul><li>首先利用IDE集成开发工具编写Java源代码，源文件的后缀为.java；</li><li>再利用编译器(javac命令)将源代码编译成字节码文件，字节码文件的后缀名为.class；</li><li>运行字节码的工作是由解释器(java命令)来完成的。</li></ul><h3 id="说一下-JVM-运行时数据区"><a href="#说一下-JVM-运行时数据区" class="headerlink" title="说一下 JVM 运行时数据区"></a>说一下 JVM 运行时数据区</h3><p>不同虚拟机的运行时数据区可能略微有所不同，但都会遵从 Java 虚拟机规范， Java 虚拟机规范规定的区域分为以下 5 个部分：</p><ul><li>程序计数器（Program Counter Register）：当前线程所执行的字节码的行号指示器，字节码解析器的工作是通过改变这个计数器的值，来选取下一条需要执行的字节码指令，分支、循环、跳转、异常处理、线程恢复等基础功能，都需要依赖这个计数器来完成；</li><li>Java 虚拟机栈（Java Virtual Machine Stacks）：用于存储局部变量表、操作数栈、动态链接、方法出口等信息；</li><li>本地方法栈（Native Method Stack）：与虚拟机栈的作用是一样的，只不过虚拟机栈是服务 Java 方法的，而本地方法栈是为虚拟机调用 Native 方法服务的；</li><li>Java 堆（Java Heap）：Java 虚拟机中内存最大的一块，是被所有线程共享的，几乎所有的对象实例都在这里分配内存；</li><li>方法区（Methed Area）：用于存储已被虚拟机加载的类信息、常量、静态变量、即时编译后的代码等数据。</li></ul><h3 id="深拷贝和浅拷贝"><a href="#深拷贝和浅拷贝" class="headerlink" title="深拷贝和浅拷贝"></a>深拷贝和浅拷贝</h3><p>浅拷贝（shallowCopy）只是增加了一个指针指向已存在的内存地址，</p><p>深拷贝（deepCopy）是增加了一个指针并且申请了一个新的内存，使这个增加的指针指向这个新的内存，</p><p>使用深拷贝的情况下，释放内存的时候不会因为出现浅拷贝时释放同一个内存的错误。</p><p>浅复制：仅仅是指向被复制的内存地址，如果原地址发生改变，那么浅复制出来的对象也会相应的改变。</p><p>深复制：在计算机中开辟一块新的内存地址用于存放复制的对象。</p><h3 id="说一下堆栈的区别？"><a href="#说一下堆栈的区别？" class="headerlink" title="说一下堆栈的区别？"></a>说一下堆栈的区别？</h3><p>物理地址</p><p>堆的物理地址分配对对象是不连续的。因此性能慢些。在GC的时候也要考虑到不连续的分配，所以有各种算法。比如，标记-消除，复制，标记-压缩，分代（即新生代使用复制算法，老年代使用标记——压缩）</p><p>栈使用的是数据结构中的栈，先进后出的原则，物理地址分配是连续的。所以性能快。</p><p>内存分别</p><p>堆因为是不连续的，所以分配的内存是在运行期确认的，因此大小不固定。一般堆大小远远大于栈。</p><p>栈是连续的，所以分配的内存大小要在编译期就确认，大小是固定的。</p><p>存放的内容</p><p>堆存放的是对象的实例和数组。因此该区更关注的是数据的存储</p><p>栈存放：局部变量，操作数栈，返回结果。该区更关注的是程序方法的执行。</p><p>程序的可见度</p><p>堆对于整个应用程序都是共享、可见的。</p><p>栈只对于线程是可见的。所以也是线程私有。他的生命周期和线程相同。</p><h3 id="队列和栈是什么？有什么区别？"><a href="#队列和栈是什么？有什么区别？" class="headerlink" title="队列和栈是什么？有什么区别？"></a>队列和栈是什么？有什么区别？</h3><p>队列和栈都是被用来预存储数据的。</p><ul><li>操作的名称不同。队列的插入称为入队，队列的删除称为出队。栈的插入称为进栈，栈的删除称为出栈。</li><li>可操作的方式不同。队列是在队尾入队，队头出队，即两边都可操作。而栈的进栈和出栈都是在栈顶进行的，无法对栈底直接进行操作。</li><li>操作的方法不同。队列是先进先出（FIFO），即队列的修改是依先进先出的原则进行的。新来的成员总是加入队尾（不能从中间插入），每次离开的成员总是队列头上（不允许中途离队）。而栈为后进先出（LIFO）,即每次删除（出栈）的总是当前栈中最新的元素，即最后插入（进栈）的元素，而最先插入的被放在栈的底部，要到最后才能删除。</li></ul><h2 id="HotSpot虚拟机对象探秘"><a href="#HotSpot虚拟机对象探秘" class="headerlink" title="HotSpot虚拟机对象探秘"></a>HotSpot虚拟机对象探秘</h2><h3 id="对象的创建"><a href="#对象的创建" class="headerlink" title="对象的创建"></a>对象的创建</h3><table><thead><tr><th>使用new关键字</th><th>调用了构造函数</th></tr></thead><tbody><tr><td>使用Class的newInstance方法</td><td>调用了构造函数</td></tr><tr><td>使用Constructor类的newInstance方法</td><td>调用了构造函数</td></tr><tr><td>使用clone方法</td><td>没有调用构造函数</td></tr><tr><td>使用反序列化</td><td>没有调用构造函数</td></tr></tbody></table><h3 id="为对象分配内存"><a href="#为对象分配内存" class="headerlink" title="为对象分配内存"></a>为对象分配内存</h3><p><img src="https://lixiangbetter.github.io/2020/06/29/JVM%E7%AC%94%E8%AE%B0/20200103213812259.png" alt></p><h3 id="处理并发安全问题"><a href="#处理并发安全问题" class="headerlink" title="处理并发安全问题"></a>处理并发安全问题</h3><p><img src="https://lixiangbetter.github.io/2020/06/29/JVM%E7%AC%94%E8%AE%B0/20200103213833317.png" alt></p><h3 id="对象的访问定位"><a href="#对象的访问定位" class="headerlink" title="对象的访问定位"></a>对象的访问定位</h3><p><code>Java</code>程序需要通过 <code>JVM</code> 栈上的引用访问堆中的具体对象。对象的访问方式取决于 <code>JVM</code> 虚拟机的实现。目前主流的访问方式有 <strong>句柄</strong> 和 <strong>直接指针</strong> 两种方式。</p><h2 id="内存溢出异常"><a href="#内存溢出异常" class="headerlink" title="内存溢出异常"></a>内存溢出异常</h2><h3 id="Java会存在内存泄漏吗？请简单描述"><a href="#Java会存在内存泄漏吗？请简单描述" class="headerlink" title="Java会存在内存泄漏吗？请简单描述"></a>Java会存在内存泄漏吗？请简单描述</h3><p>内存泄漏是指不再被使用的对象或者变量一直被占据在内存中。理论上来说，Java是有GC垃圾回收机制的，也就是说，不再被使用的对象，会被GC自动回收掉，自动从内存中清除。</p><p>但是，即使这样，Java也还是存在着内存泄漏的情况，java导致内存泄露的原因很明确：长生命周期的对象持有短生命周期对象的引用就很可能发生内存泄露，尽管短生命周期对象已经不再需要，但是因为长生命周期对象持有它的引用而导致不能被回收，这就是java中内存泄露的发生场景。</p><h2 id="垃圾收集器"><a href="#垃圾收集器" class="headerlink" title="垃圾收集器"></a>垃圾收集器</h2><h3 id="简述Java垃圾回收机制"><a href="#简述Java垃圾回收机制" class="headerlink" title="简述Java垃圾回收机制"></a>简述Java垃圾回收机制</h3><p>在java中，程序员是不需要显示的去释放一个对象的内存的，而是由虚拟机自行执行。在JVM中，有一个垃圾回收线程，它是低优先级的，在正常情况下是不会执行的，只有在虚拟机空闲或者当前堆内存不足时，才会触发执行，扫面那些没有被任何引用的对象，并将它们添加到要回收的集合中，进行回收。</p><h3 id="GC是什么？为什么要GC"><a href="#GC是什么？为什么要GC" class="headerlink" title="GC是什么？为什么要GC"></a>GC是什么？为什么要GC</h3><p>GC 是垃圾收集的意思（Gabage Collection）,内存处理是编程人员容易出现问题的地方，忘记或者错误的内存</p><p>回收会导致程序或系统的不稳定甚至崩溃，Java 提供的 GC 功能可以自动监测对象是否超过作用域从而达到自动</p><p>回收内存的目的，Java 语言没有提供释放已分配内存的显示操作方法。</p><h3 id="垃圾回收的优点和原理。并考虑2种回收机制"><a href="#垃圾回收的优点和原理。并考虑2种回收机制" class="headerlink" title="垃圾回收的优点和原理。并考虑2种回收机制"></a>垃圾回收的优点和原理。并考虑2种回收机制</h3><p>java语言最显著的特点就是引入了垃圾回收机制，它使java程序员在编写程序时不再考虑内存管理的问题。</p><p>由于有这个垃圾回收机制，java中的对象不再有“作用域”的概念，只有引用的对象才有“作用域”。</p><p>垃圾回收机制有效的防止了内存泄露，可以有效的使用可使用的内存。</p><p>垃圾回收器通常作为一个单独的低级别的线程运行，在不可预知的情况下对内存堆中已经死亡的或很长时间没有用过的对象进行清除和回收。</p><p>程序员不能实时的对某个对象或所有对象调用垃圾回收器进行垃圾回收。</p><p>垃圾回收有分代复制垃圾回收、标记垃圾回收、增量垃圾回收。</p><h3 id="垃圾回收器的基本原理是什么？垃圾回收器可以马上回收内存吗？有什么办法主动通知虚拟机进行垃圾回收？"><a href="#垃圾回收器的基本原理是什么？垃圾回收器可以马上回收内存吗？有什么办法主动通知虚拟机进行垃圾回收？" class="headerlink" title="垃圾回收器的基本原理是什么？垃圾回收器可以马上回收内存吗？有什么办法主动通知虚拟机进行垃圾回收？"></a>垃圾回收器的基本原理是什么？垃圾回收器可以马上回收内存吗？有什么办法主动通知虚拟机进行垃圾回收？</h3><p>对于GC来说，当程序员创建对象时，GC就开始监控这个对象的地址、大小以及使用情况。</p><p>通常，GC采用有向图的方式记录和管理堆(heap)中的所有对象。通过这种方式确定哪些对象是”可达的”，哪些对象是”不可达的”。当GC确定一些对象为”不可达”时，GC就有责任回收这些内存空间。</p><p>可以。程序员可以手动执行System.gc()，通知GC运行，但是Java语言规范并不保证GC一定会执行。</p><h3 id="Java-中都有哪些引用类型？"><a href="#Java-中都有哪些引用类型？" class="headerlink" title="Java 中都有哪些引用类型？"></a>Java 中都有哪些引用类型？</h3><p>强引用：发生 gc 的时候不会被回收。<br>软引用：有用但不是必须的对象，在发生内存溢出之前会被回收。<br>弱引用：有用但不是必须的对象，在下一次GC时会被回收。<br>虚引用（幽灵引用/幻影引用）：无法通过虚引用获得对象，用 PhantomReference 实现虚引用，虚引用的用途是在 gc 时返回一个通知。</p><h3 id="怎么判断对象是否可以被回收？"><a href="#怎么判断对象是否可以被回收？" class="headerlink" title="怎么判断对象是否可以被回收？"></a>怎么判断对象是否可以被回收？</h3><p>垃圾收集器在做垃圾回收的时候，首先需要判定的就是哪些内存是需要被回收的，哪些对象是「存活」的，是不可以被回收的；哪些对象已经「死掉」了，需要被回收。</p><p>一般有两种方法来判断：</p><ul><li>引用计数器法：为每个对象创建一个引用计数，有对象引用时计数器 +1，引用被释放时计数 -1，当计数器为 0 时就可以被回收。它有一个缺点不能解决循环引用的问题；</li><li>可达性分析算法：从 GC Roots 开始向下搜索，搜索所走过的路径称为引用链。当一个对象到 GC Roots 没有任何引用链相连时，则证明此对象是可以被回收的。</li></ul><h3 id="在Java中，对象什么时候可以被垃圾回收"><a href="#在Java中，对象什么时候可以被垃圾回收" class="headerlink" title="在Java中，对象什么时候可以被垃圾回收"></a>在Java中，对象什么时候可以被垃圾回收</h3><p>当对象对当前使用这个对象的应用程序变得不可触及的时候，这个对象就可以被回收了。<br>垃圾回收不会发生在永久代，如果永久代满了或者是超过了临界值，会触发完全垃圾回收(Full GC)。如果你仔细查看垃圾收集器的输出信息，就会发现永久代也是被回收的。这就是为什么正确的永久代大小对避免Full GC是非常重要的原因。</p><h3 id="JVM中的永久代中会发生垃圾回收吗"><a href="#JVM中的永久代中会发生垃圾回收吗" class="headerlink" title="JVM中的永久代中会发生垃圾回收吗"></a>JVM中的永久代中会发生垃圾回收吗</h3><p>垃圾回收不会发生在永久代，如果永久代满了或者是超过了临界值，会触发完全垃圾回收(Full GC)。如果你仔细查看垃圾收集器的输出信息，就会发现永久代也是被回收的。这就是为什么正确的永久代大小对避免Full GC是非常重要的原因。请参考下Java8：从永久代到元数据区<br>(译者注：Java8中已经移除了永久代，新加了一个叫做元数据区的native内存区)</p><h3 id="说一下-JVM-有哪些垃圾回收算法？"><a href="#说一下-JVM-有哪些垃圾回收算法？" class="headerlink" title="说一下 JVM 有哪些垃圾回收算法？"></a>说一下 JVM 有哪些垃圾回收算法？</h3><ul><li>标记-清除算法：标记无用对象，然后进行清除回收。缺点：效率不高，无法清除垃圾碎片。</li><li>复制算法：按照容量划分二个大小相等的内存区域，当一块用完的时候将活着的对象复制到另一块上，然后再把已使用的内存空间一次清理掉。缺点：内存使用率不高，只有原来的一半。</li><li>标记-整理算法：标记无用对象，让所有存活的对象都向一端移动，然后直接清除掉端边界以外的内存。</li><li>分代算法：根据对象存活周期的不同将内存划分为几块，一般是新生代和老年代，新生代基本采用复制算法，老年代采用标记整理算法。</li></ul><h4 id="标记-清除算法"><a href="#标记-清除算法" class="headerlink" title="标记-清除算法"></a>标记-清除算法</h4><p>标记-清除算法（Mark-Sweep）是一种常见的基础垃圾收集算法，它将垃圾收集分为两个阶段：</p><ul><li>标记阶段：标记出可以回收的对象。</li><li>清除阶段：回收被标记的对象所占用的空间。</li></ul><h4 id="复制算法"><a href="#复制算法" class="headerlink" title="复制算法"></a>复制算法</h4><p><strong>优点</strong>：按顺序分配内存即可，实现简单、运行高效，不用考虑内存碎片。</p><h4 id="标记-整理算法"><a href="#标记-整理算法" class="headerlink" title="标记-整理算法"></a>标记-整理算法</h4><h4 id="分代收集算法"><a href="#分代收集算法" class="headerlink" title="分代收集算法"></a>分代收集算法</h4><h3 id="说一下-JVM-有哪些垃圾回收器？"><a href="#说一下-JVM-有哪些垃圾回收器？" class="headerlink" title="说一下 JVM 有哪些垃圾回收器？"></a>说一下 JVM 有哪些垃圾回收器？</h3><ul><li><p>Serial收集器（复制算法): 新生代单线程收集器，标记和清理都是单线程，优点是简单高效；</p></li><li><p>ParNew收集器 (复制算法): 新生代收并行集器，实际上是Serial收集器的多线程版本，在多核CPU环境下有着比Serial更好的表现；</p></li><li><p>Parallel Scavenge收集器 (复制算法): 新生代并行收集器，追求高吞吐量，高效利用 CPU。吞吐量 = 用户线程时间/(用户线程时间+GC线程时间)，高吞吐量可以高效率的利用CPU时间，尽快完成程序的运算任务，适合后台应用等对交互相应要求不高的场景；</p></li><li><p>Serial Old收集器 (标记-整理算法): 老年代单线程收集器，Serial收集器的老年代版本；</p></li><li><p>Parallel Old收集器 (标记-整理算法)： 老年代并行收集器，吞吐量优先，Parallel Scavenge收集器的老年代版本；</p></li><li><p>CMS(Concurrent Mark Sweep)收集器（标记-清除算法）： 老年代并行收集器，以获取最短回收停顿时间为目标的收集器，具有高并发、低停顿的特点，追求最短GC回收停顿时间。</p></li><li><p>G1(Garbage First)收集器 (标记-整理算法)： Java堆并行收集器，G1收集器是JDK1.7提供的一个新收集器，G1收集器基于“标记-整理”算法实现，也就是说不会产生内存碎片。此外，G1收集器不同于之前的收集器的一个重要特点是：G1回收的范围是整个Java堆(包括新生代，老年代)，而前六种收集器回收的范围仅限于新生代或老年代。</p></li></ul><h3 id="详细介绍一下-CMS-垃圾回收器？"><a href="#详细介绍一下-CMS-垃圾回收器？" class="headerlink" title="详细介绍一下 CMS 垃圾回收器？"></a>详细介绍一下 CMS 垃圾回收器？</h3><p>CMS 是英文 Concurrent Mark-Sweep 的简称，是以牺牲吞吐量为代价来获得最短回收停顿时间的垃圾回收器。对于要求服务器响应速度的应用上，这种垃圾回收器非常适合。在启动 JVM 的参数加上“-XX:+UseConcMarkSweepGC”来指定使用 CMS 垃圾回收器。</p><p>CMS 使用的是标记-清除的算法实现的，所以在 gc 的时候回产生大量的内存碎片，当剩余内存不能满足程序运行要求时，系统将会出现 Concurrent Mode Failure，临时 CMS 会采用 Serial Old 回收器进行垃圾清除，此时的性能将会被降低。</p><h3 id="新生代垃圾回收器和老年代垃圾回收器都有哪些？有什么区别？"><a href="#新生代垃圾回收器和老年代垃圾回收器都有哪些？有什么区别？" class="headerlink" title="新生代垃圾回收器和老年代垃圾回收器都有哪些？有什么区别？"></a>新生代垃圾回收器和老年代垃圾回收器都有哪些？有什么区别？</h3><ul><li>新生代回收器：Serial、ParNew、Parallel Scavenge</li><li>老年代回收器：Serial Old、Parallel Old、CMS</li><li>整堆回收器：G1</li></ul><h3 id="简述分代垃圾回收器是怎么工作的？"><a href="#简述分代垃圾回收器是怎么工作的？" class="headerlink" title="简述分代垃圾回收器是怎么工作的？"></a>简述分代垃圾回收器是怎么工作的？</h3><p>分代回收器有两个分区：老生代和新生代，新生代默认的空间占比总空间的 1/3，老生代的默认占比是 2/3。</p><p>新生代使用的是复制算法，新生代里有 3 个分区：Eden、To Survivor、From Survivor，它们的默认占比是 8:1:1，它的执行流程如下：</p><p>每次在 From Survivor 到 To Survivor 移动时都存活的对象，年龄就 +1，当年龄到达 15（默认配置是 15）时，升级为老生代。大对象也会直接进入老生代。</p><p>老生代当空间占用到达某个值之后就会触发全局垃圾收回，一般使用标记整理的执行算法。以上这些循环往复就构成了整个分代垃圾回收的整体执行流程。</p><h2 id="内存分配策略"><a href="#内存分配策略" class="headerlink" title="内存分配策略"></a>内存分配策略</h2><h3 id="简述java内存分配与回收策率以及Minor-GC和Major-GC"><a href="#简述java内存分配与回收策率以及Minor-GC和Major-GC" class="headerlink" title="简述java内存分配与回收策率以及Minor GC和Major GC"></a>简述java内存分配与回收策率以及Minor GC和Major GC</h3><p>所谓自动内存管理，最终要解决的也就是内存分配和内存回收两个问题。前面我们介绍了内存回收，这里我们再来聊聊内存分配。</p><p>对象的内存分配通常是在 Java 堆上分配（随着虚拟机优化技术的诞生，某些场景下也会在栈上分配，后面会详细介绍），对象主要分配在新生代的 Eden 区，如果启动了本地线程缓冲，将按照线程优先在 TLAB 上分配。少数情况下也会直接在老年代上分配。总的来说分配规则不是百分百固定的，其细节取决于哪一种垃圾收集器组合以及虚拟机相关参数有关，但是虚拟机对于内存的分配还是会遵循以下几种「普世」规则：</p><h4 id="对象优先在-Eden-区分配"><a href="#对象优先在-Eden-区分配" class="headerlink" title="对象优先在 Eden 区分配"></a>对象优先在 Eden 区分配</h4><p>多数情况，对象都在新生代 Eden 区分配。当 Eden 区分配没有足够的空间进行分配时，虚拟机将会发起一次 Minor GC。如果本次 GC 后还是没有足够的空间，则将启用分配担保机制在老年代中分配内存。</p><p>这里我们提到 Minor GC，如果你仔细观察过 GC 日常，通常我们还能从日志中发现 Major GC/Full GC。</p><p>Minor GC 是指发生在新生代的 GC，因为 Java 对象大多都是朝生夕死，所有 Minor GC 非常频繁，一般回收速度也非常快；<br>Major GC/Full GC 是指发生在老年代的 GC，出现了 Major GC 通常会伴随至少一次 Minor GC。Major GC 的速度通常会比 Minor GC 慢 10 倍以上。</p><h4 id="大对象直接进入老年代"><a href="#大对象直接进入老年代" class="headerlink" title="大对象直接进入老年代"></a>大对象直接进入老年代</h4><p>所谓大对象是指需要大量连续内存空间的对象，频繁出现大对象是致命的，会导致在内存还有不少空间的情况下提前触发 GC 以获取足够的连续空间来安置新对象。</p><p>前面我们介绍过新生代使用的是标记-清除算法来处理垃圾回收的，如果大对象直接在新生代分配就会导致 Eden 区和两个 Survivor 区之间发生大量的内存复制。因此对于大对象都会直接在老年代进行分配。</p><h4 id="长期存活对象将进入老年代"><a href="#长期存活对象将进入老年代" class="headerlink" title="长期存活对象将进入老年代"></a>长期存活对象将进入老年代</h4><p>虚拟机采用分代收集的思想来管理内存，那么内存回收时就必须判断哪些对象应该放在新生代，哪些对象应该放在老年代。因此虚拟机给每个对象定义了一个对象年龄的计数器，如果对象在 Eden 区出生，并且能够被 Survivor 容纳，将被移动到 Survivor 空间中，这时设置对象年龄为 1。对象在 Survivor 区中每「熬过」一次 Minor GC 年龄就加 1，当年龄达到一定程度（默认 15） 就会被晋升到老年代。</p><h2 id="虚拟机类加载机制"><a href="#虚拟机类加载机制" class="headerlink" title="虚拟机类加载机制"></a>虚拟机类加载机制</h2><h3 id="简述java类加载机制"><a href="#简述java类加载机制" class="headerlink" title="简述java类加载机制?"></a>简述java类加载机制?</h3><p>虚拟机把描述类的数据从Class文件加载到内存，并对数据进行校验，解析和初始化，最终形成可以被虚拟机直接使用的java类型。</p><h3 id="描述一下JVM加载Class文件的原理机制"><a href="#描述一下JVM加载Class文件的原理机制" class="headerlink" title="描述一下JVM加载Class文件的原理机制"></a>描述一下JVM加载Class文件的原理机制</h3><p>Java中的所有类，都需要由类加载器装载到JVM中才能运行。类加载器本身也是一个类，而它的工作就是把class文件从硬盘读取到内存中。在写程序的时候，我们几乎不需要关心类的加载，因为这些都是隐式装载的，除非我们有特殊的用法，像是反射，就需要显式的加载所需要的类。</p><p>类装载方式，有两种 ：</p><p>1.隐式装载， 程序在运行过程中当碰到通过new 等方式生成对象时，隐式调用类装载器加载对应的类到jvm中，</p><p>2.显式装载， 通过class.forname()等方法，显式加载需要的类</p><p>Java类的加载是动态的，它并不会一次性将所有类全部加载后再运行，而是保证程序运行的基础类(像是基类)完全加载到jvm中，至于其他类，则在需要的时候才加载。这当然就是为了节省内存开销。</p><h3 id="什么是类加载器，类加载器有哪些"><a href="#什么是类加载器，类加载器有哪些" class="headerlink" title="什么是类加载器，类加载器有哪些?"></a>什么是类加载器，类加载器有哪些?</h3><p>实现通过类的权限定名获取该类的二进制字节流的代码块叫做类加载器。</p><p>主要有一下四种类加载器:</p><ul><li>启动类加载器(Bootstrap ClassLoader)用来加载java核心类库，无法被java程序直接引用。</li><li>扩展类加载器(extensions class loader):它用来加载 Java 的扩展库。Java 虚拟机的实现会提供一个扩展库目录。该类加载器在此目录里面查找并加载 Java 类。</li><li>系统类加载器（system class loader）：它根据 Java 应用的类路径（CLASSPATH）来加载 Java 类。一般来说，Java 应用的类都是由它来完成加载的。可以通过 ClassLoader.getSystemClassLoader()来获取它。</li><li>用户自定义类加载器，通过继承 java.lang.ClassLoader类的方式实现。</li></ul><h3 id="说一下类装载的执行过程？"><a href="#说一下类装载的执行过程？" class="headerlink" title="说一下类装载的执行过程？"></a>说一下类装载的执行过程？</h3><p>类装载分为以下 5 个步骤：</p><p>加载：根据查找路径找到相应的 class 文件然后导入；<br>验证：检查加载的 class 文件的正确性；<br>准备：给类中的静态变量分配内存空间；<br>解析：虚拟机将常量池中的符号引用替换成直接引用的过程。符号引用就理解为一个标示，而在直接引用直接指向内存中的地址；<br>初始化：对静态变量和静态代码块执行初始化工作。</p><h3 id="什么是双亲委派模型？"><a href="#什么是双亲委派模型？" class="headerlink" title="什么是双亲委派模型？"></a>什么是双亲委派模型？</h3><p>在介绍双亲委派模型之前先说下类加载器。对于任意一个类，都需要由加载它的类加载器和这个类本身一同确立在 JVM 中的唯一性，每一个类加载器，都有一个独立的类名称空间。类加载器就是根据指定全限定名称将 class 文件加载到 JVM 内存，然后再转化为 class 对象。</p><p>双亲委派模型：如果一个类加载器收到了类加载的请求，它首先不会自己去加载这个类，而是把这个请求委派给父类加载器去完成，每一层的类加载器都是如此，这样所有的加载请求都会被传送到顶层的启动类加载器中，只有当父加载无法完成加载请求（它的搜索范围中没找到所需的类）时，子加载器才会尝试去加载类。</p><p>当一个类收到了类加载请求时，不会自己先去加载这个类，而是将其委派给父类，由父类去加载，如果此时父类不能加载，反馈给子类，由子类去完成类的加载。</p><h2 id="JVM调优"><a href="#JVM调优" class="headerlink" title="JVM调优"></a>JVM调优</h2><h3 id="说一下-JVM-调优的工具？"><a href="#说一下-JVM-调优的工具？" class="headerlink" title="说一下 JVM 调优的工具？"></a>说一下 JVM 调优的工具？</h3><p>JDK 自带了很多监控工具，都位于 JDK 的 bin 目录下，其中最常用的是 jconsole 和 jvisualvm 这两款视图监控工具。</p><ul><li>jconsole：用于对 JVM 中的内存、线程和类等进行监控；</li><li>jvisualvm：JDK 自带的全能分析工具，可以分析：内存快照、线程快照、程序死锁、监控内存的变化、gc 变化等。</li></ul><h3 id="常用的-JVM-调优的参数都有哪些？"><a href="#常用的-JVM-调优的参数都有哪些？" class="headerlink" title="常用的 JVM 调优的参数都有哪些？"></a>常用的 JVM 调优的参数都有哪些？</h3><ul><li>-Xms2g：初始化推大小为 2g；</li><li>-Xmx2g：堆最大内存为 2g；</li><li>-XX:NewRatio=4：设置年轻的和老年代的内存比例为 1:4；</li><li>-XX:SurvivorRatio=8：设置新生代 Eden 和 Survivor 比例为 8:2；</li><li>–XX:+UseParNewGC：指定使用 ParNew + Serial Old 垃圾回收器组合；</li><li>-XX:+UseParallelOldGC：指定使用 ParNew + ParNew Old 垃圾回收器组合；</li><li>-XX:+UseConcMarkSweepGC：指定使用 CMS + Serial Old 垃圾回收器组合；</li><li>-XX:+PrintGC：开启打印 gc 信息；</li><li>-XX:+PrintGCDetails：打印 gc 详细信息。</li></ul>]]></content>
      
      
      <categories>
          
          <category> java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> jvm </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>并发编程笔记</title>
      <link href="/2020/06/21/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E7%AC%94%E8%AE%B0/"/>
      <url>/2020/06/21/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<h1 id="并发编程笔记"><a href="#并发编程笔记" class="headerlink" title="并发编程笔记"></a>并发编程笔记</h1><h2 id="基础知识"><a href="#基础知识" class="headerlink" title="基础知识"></a>基础知识</h2><h3 id="并发编程的优缺点"><a href="#并发编程的优缺点" class="headerlink" title="并发编程的优缺点"></a>并发编程的优缺点</h3><h4 id="为什么要使用并发编程（并发编程的优点）"><a href="#为什么要使用并发编程（并发编程的优点）" class="headerlink" title="为什么要使用并发编程（并发编程的优点）"></a>为什么要使用并发编程（并发编程的优点）</h4><ul><li>充分利用多核CPU的计算能力：通过并发编程的形式可以将多核CPU的计算能力发挥到极致，性能得到提升</li><li>方便进行业务拆分，提升系统并发能力和性能：在特殊的业务场景下，先天的就适合于并发编程。现在的系统动不动就要求百万级甚至千万级的并发量，而多线程并发编程正是开发高并发系统的基础，利用好多线程机制可以大大提高系统整体的并发能力以及性能。面对复杂业务模型，并行程序会比串行程序更适应业务需求，而并发编程更能吻合这种业务拆分 。</li></ul><h4 id="并发编程有什么缺点"><a href="#并发编程有什么缺点" class="headerlink" title="并发编程有什么缺点"></a>并发编程有什么缺点</h4><p>并发编程的目的就是为了能提高程序的执行效率，提高程序运行速度，但是并发编程并不总是能提高程序运行速度的，而且并发编程可能会遇到很多问题，比如<strong>：内存泄漏、上下文切换、线程安全、死锁</strong>等问题。</p><h4 id="并发编程三要素是什么？在-Java-程序中怎么保证多线程的运行安全？"><a href="#并发编程三要素是什么？在-Java-程序中怎么保证多线程的运行安全？" class="headerlink" title="并发编程三要素是什么？在 Java 程序中怎么保证多线程的运行安全？"></a>并发编程三要素是什么？在 Java 程序中怎么保证多线程的运行安全？</h4><p>并发编程三要素（线程的安全性问题体现在）：</p><p>原子性：原子，即一个不可再被分割的颗粒。原子性指的是一个或多个操作要么全部执行成功要么全部执行失败。</p><p>可见性：一个线程对共享变量的修改,另一个线程能够立刻看到。（synchronized,volatile）</p><p>有序性：程序执行的顺序按照代码的先后顺序执行。（处理器可能会对指令进行重排序）</p><p>出现线程安全问题的原因：</p><ul><li><p>线程切换带来的原子性问题</p></li><li><p>缓存导致的可见性问题</p></li><li><p>编译优化带来的有序性问题</p></li></ul><p>解决办法：</p><ul><li>JDK Atomic开头的原子类、synchronized、LOCK，可以解决原子性问题</li><li>synchronized、volatile、LOCK，可以解决可见性问题</li><li>Happens-Before 规则可以解决有序性问题</li></ul><h4 id="并行和并发有什么区别？"><a href="#并行和并发有什么区别？" class="headerlink" title="并行和并发有什么区别？"></a>并行和并发有什么区别？</h4><ul><li>并发：多个任务在同一个 CPU 核上，按细分的时间片轮流(交替)执行，从逻辑上来看那些任务是同时执行。</li><li>并行：单位时间内，多个处理器或多核处理器同时处理多个任务，是真正意义上的“同时进行”。</li><li>串行：有n个任务，由一个线程按顺序执行。由于任务、方法都在一个线程执行所以不存在线程不安全情况，也就不存在临界区的问题。</li></ul><h4 id="什么是多线程，多线程的优劣？"><a href="#什么是多线程，多线程的优劣？" class="headerlink" title="什么是多线程，多线程的优劣？"></a>什么是多线程，多线程的优劣？</h4><p>多线程：多线程是指程序中包含多个执行流，即在一个程序中可以同时运行多个不同的线程来执行不同的任务。</p><p>多线程的好处：</p><p>可以提高 CPU 的利用率。在多线程程序中，一个线程必须等待的时候，CPU 可以运行其它的线程而不是等待，这样就大大提高了程序的效率。也就是说允许单个程序创建多个并行执行的线程来完成各自的任务。</p><p>多线程的劣势：</p><ul><li>线程也是程序，所以线程需要占用内存，线程越多占用内存也越多；</li><li>多线程需要协调和管理，所以需要 CPU 时间跟踪线程；</li><li>线程之间对共享资源的访问会相互影响，必须解决竞用共享资源的问题。</li></ul><h3 id="线程和进程区别"><a href="#线程和进程区别" class="headerlink" title="线程和进程区别"></a>线程和进程区别</h3><h4 id="什么是线程和进程"><a href="#什么是线程和进程" class="headerlink" title="什么是线程和进程?"></a>什么是线程和进程?</h4><p>进程</p><p>一个在内存中运行的应用程序。每个进程都有自己独立的一块内存空间，一个进程可以有多个线程，比如在Windows系统中，一个运行的xx.exe就是一个进程。</p><p>线程</p><p>进程中的一个执行任务（控制单元），负责当前进程中程序的执行。一个进程至少有一个线程，一个进程可以运行多个线程，多个线程可共享数据。</p><h4 id="进程与线程的区别"><a href="#进程与线程的区别" class="headerlink" title="进程与线程的区别"></a>进程与线程的区别</h4><p>线程具有许多传统进程所具有的特征，故又称为轻型进程(Light—Weight Process)或进程元；而把传统的进程称为重型进程(Heavy—Weight Process)，它相当于只有一个线程的任务。在引入了线程的操作系统中，通常一个进程都有若干个线程，至少包含一个线程。</p><p>根本区别：进程是操作系统资源分配的基本单位，而线程是处理器任务调度和执行的基本单位</p><p>资源开销：每个进程都有独立的代码和数据空间（程序上下文），程序之间的切换会有较大的开销；线程可以看做轻量级的进程，同一类线程共享代码和数据空间，每个线程都有自己独立的运行栈和程序计数器（PC），线程之间切换的开销小。</p><p>包含关系：如果一个进程内有多个线程，则执行过程不是一条线的，而是多条线（线程）共同完成的；线程是进程的一部分，所以线程也被称为轻权进程或者轻量级进程。</p><p>内存分配：同一进程的线程共享本进程的地址空间和资源，而进程之间的地址空间和资源是相互独立的</p><p>影响关系：一个进程崩溃后，在保护模式下不会对其他进程产生影响，但是一个线程崩溃整个进程都死掉。所以多进程要比多线程健壮。</p><p>执行过程：每个独立的进程有程序运行的入口、顺序执行序列和程序出口。但是线程不能独立执行，必须依存在应用程序中，由应用程序提供多个线程执行控制，两者均可并发执行</p><h4 id="什么是上下文切换"><a href="#什么是上下文切换" class="headerlink" title="什么是上下文切换?"></a>什么是上下文切换?</h4><p>多线程编程中一般线程的个数都大于 CPU 核心的个数，而一个 CPU 核心在任意时刻只能被一个线程使用，为了让这些线程都能得到有效执行，CPU 采取的策略是为每个线程分配时间片并轮转的形式。当一个线程的时间片用完的时候就会重新处于就绪状态让给其他线程使用，这个过程就属于一次上下文切换。</p><p>概括来说就是：当前任务在执行完 CPU 时间片切换到另一个任务之前会先保存自己的状态，以便下次再切换回这个任务时，可以再加载这个任务的状态。任务从保存到再加载的过程就是一次上下文切换。</p><p>上下文切换通常是计算密集型的。也就是说，它需要相当可观的处理器时间，在每秒几十上百次的切换中，每次切换都需要纳秒量级的时间。所以，上下文切换对系统来说意味着消耗大量的 CPU 时间，事实上，可能是操作系统中时间消耗最大的操作。</p><p>Linux 相比与其他操作系统（包括其他类 Unix 系统）有很多的优点，其中有一项就是，其上下文切换和模式切换的时间消耗非常少。</p><h4 id="守护线程和用户线程有什么区别呢？"><a href="#守护线程和用户线程有什么区别呢？" class="headerlink" title="守护线程和用户线程有什么区别呢？"></a>守护线程和用户线程有什么区别呢？</h4><p>守护线程和用户线程</p><p>用户 (User) 线程：运行在前台，执行具体的任务，如程序的主线程、连接网络的子线程等都是用户线程<br>守护 (Daemon) 线程：运行在后台，为其他前台线程服务。也可以说守护线程是 JVM 中非守护线程的 “佣人”。一旦所有用户线程都结束运行，守护线程会随 JVM 一起结束工作<br>main 函数所在的线程就是一个用户线程啊，main 函数启动的同时在 JVM 内部同时还启动了好多守护线程，比如垃圾回收线程。</p><p>比较明显的区别之一是用户线程结束，JVM 退出，不管这个时候有没有守护线程运行。而守护线程不会影响 JVM 的退出。</p><p>注意事项：</p><ul><li>setDaemon(true)必须在start()方法前执行，否则会抛出 IllegalThreadStateException 异常</li><li>在守护线程中产生的新线程也是守护线程</li><li>不是所有的任务都可以分配给守护线程来执行，比如读写操作或者计算逻辑</li><li>守护 (Daemon) 线程中不能依靠 finally 块的内容来确保执行关闭或清理资源的逻辑。因为我们上面也说过了一旦所有用户线程都结束运行，守护线程会随 JVM 一起结束工作，所以守护 (Daemon) 线程中的 finally 语句块可能无法被执行。</li></ul><h4 id="如何在-Windows-和-Linux-上查找哪个线程cpu利用率最高？"><a href="#如何在-Windows-和-Linux-上查找哪个线程cpu利用率最高？" class="headerlink" title="如何在 Windows 和 Linux 上查找哪个线程cpu利用率最高？"></a>如何在 Windows 和 Linux 上查找哪个线程cpu利用率最高？</h4><p>windows上面用任务管理器看，linux下可以用 top 这个工具看。</p><p>找出cpu耗用厉害的进程pid， 终端执行top命令，然后按下shift+p 查找出cpu利用最厉害的pid号<br>根据上面第一步拿到的pid号，top -H -p pid 。然后按下shift+p，查找出cpu利用率最厉害的线程号，比如top -H -p 1328<br>将获取到的线程号转换成16进制，去百度转换一下就行<br>使用jstack工具将进程信息打印输出，jstack pid号 &gt; /tmp/t.dat，比如jstack 31365 &gt; /tmp/t.dat<br>编辑/tmp/t.dat文件，查找线程号对应的信息</p><h4 id="什么是线程死锁"><a href="#什么是线程死锁" class="headerlink" title="什么是线程死锁"></a>什么是线程死锁</h4><p>死锁是指两个或两个以上的进程（线程）在执行过程中，由于竞争资源或者由于彼此通信而造成的一种阻塞的现象，若无外力作用，它们都将无法推进下去。此时称系统处于死锁状态或系统产生了死锁，这些永远在互相等待的进程（线程）称为死锁进程（线程）。</p><p>多个线程同时被阻塞，它们中的一个或者全部都在等待某个资源被释放。由于线程被无限期地阻塞，因此程序不可能正常终止。</p><h4 id="形成死锁的四个必要条件是什么"><a href="#形成死锁的四个必要条件是什么" class="headerlink" title="形成死锁的四个必要条件是什么"></a>形成死锁的四个必要条件是什么</h4><ul><li>互斥条件：线程(进程)对于所分配到的资源具有排它性，即一个资源只能被一个线程(进程)占用，直到被该线程(进程)释放</li><li>请求与保持条件：一个线程(进程)因请求被占用资源而发生阻塞时，对已获得的资源保持不放。</li><li>不剥夺条件：线程(进程)已获得的资源在末使用完之前不能被其他线程强行剥夺，只有自己使用完毕后才释放资源。</li><li>循环等待条件：当发生死锁时，所等待的线程(进程)必定会形成一个环路（类似于死循环），造成永久阻塞</li></ul><h4 id="如何避免线程死锁"><a href="#如何避免线程死锁" class="headerlink" title="如何避免线程死锁"></a>如何避免线程死锁</h4><p>我们只要破坏产生死锁的四个条件中的其中一个就可以了。</p><p><strong>破坏互斥条件</strong></p><p>没有办法破坏</p><p><strong>破坏请求与保持条件</strong></p><p>一次性申请所有的资源。</p><p><strong>破坏不剥夺条件</strong></p><p>占用部分资源的线程进一步申请其他资源时，如果申请不到，可以主动释放它占有的资源。</p><p><strong>破坏循环等待条件</strong></p><p>靠按序申请资源来预防。按某一顺序申请资源，释放资源则反序释放。破坏循环等待条件。</p><h3 id="创建线程的四种方式"><a href="#创建线程的四种方式" class="headerlink" title="创建线程的四种方式"></a>创建线程的四种方式</h3><h4 id="创建线程有哪几种方式？"><a href="#创建线程有哪几种方式？" class="headerlink" title="创建线程有哪几种方式？"></a>创建线程有哪几种方式？</h4><p>创建线程有四种方式：</p><p>继承 Thread 类；<br>实现 Runnable 接口；<br>实现 Callable 接口；<br>使用 Executors 工具类创建线程池</p><h4 id="说一下-runnable-和-callable-有什么区别？"><a href="#说一下-runnable-和-callable-有什么区别？" class="headerlink" title="说一下 runnable 和 callable 有什么区别？"></a>说一下 runnable 和 callable 有什么区别？</h4><p>相同点</p><ul><li>都是接口</li><li>都可以编写多线程程序</li><li>都采用Thread.start()启动线程</li></ul><p>主要区别</p><ul><li>Runnable 接口 run 方法无返回值；Callable 接口 call 方法有返回值，是个泛型，和Future、FutureTask配合可以用来获取异步执行的结果</li><li>Runnable 接口 run 方法只能抛出运行时异常，且无法捕获处理；Callable 接口 call 方法允许抛出异常，可以获取异常信息</li></ul><p>注：Callalbe接口支持返回执行结果，需要调用FutureTask.get()得到，此方法会阻塞主进程的继续往下执行，如果不调用不会阻塞。</p><h4 id="线程的-run-和-start-有什么区别？"><a href="#线程的-run-和-start-有什么区别？" class="headerlink" title="线程的 run()和 start()有什么区别？"></a>线程的 run()和 start()有什么区别？</h4><p>每个线程都是通过某个特定Thread对象所对应的方法run()来完成其操作的，run()方法称为线程体。通过调用Thread类的start()方法来启动一个线程。</p><p>start() 方法用于启动线程，run() 方法用于执行线程的运行时代码。run() 可以重复调用，而 start() 只能调用一次。</p><p>start()方法来启动一个线程，真正实现了多线程运行。调用start()方法无需等待run方法体代码执行完毕，可以直接继续执行其他的代码； 此时线程是处于就绪状态，并没有运行。 然后通过此Thread类调用方法run()来完成其运行状态， run()方法运行结束， 此线程终止。然后CPU再调度其它线程。</p><p>run()方法是在本线程里的，只是线程里的一个函数，而不是多线程的。 如果直接调用run()，其实就相当于是调用了一个普通函数而已，直接待用run()方法必须等待run()方法执行完毕才能执行下面的代码，所以执行路径还是只有一条，根本就没有线程的特征，所以在多线程执行时要使用start()方法而不是run()方法。</p><h4 id="为什么我们调用-start-方法时会执行-run-方法，为什么我们不能直接调用-run-方法？"><a href="#为什么我们调用-start-方法时会执行-run-方法，为什么我们不能直接调用-run-方法？" class="headerlink" title="为什么我们调用 start() 方法时会执行 run() 方法，为什么我们不能直接调用 run() 方法？"></a>为什么我们调用 start() 方法时会执行 run() 方法，为什么我们不能直接调用 run() 方法？</h4><p>new 一个 Thread，线程进入了新建状态。调用 start() 方法，会启动一个线程并使线程进入了就绪状态，当分配到时间片后就可以开始运行了。 start() 会执行线程的相应准备工作，然后自动执行 run() 方法的内容，这是真正的多线程工作。</p><p>而直接执行 run() 方法，会把 run 方法当成一个 main 线程下的普通方法去执行，并不会在某个线程中执行它，所以这并不是多线程工作。</p><p>调用 start 方法方可启动线程并使线程进入就绪状态，而 run 方法只是 thread 的一个普通方法调用，还是在主线程里执行。</p><h4 id="什么是-Callable-和-Future"><a href="#什么是-Callable-和-Future" class="headerlink" title="什么是 Callable 和 Future?"></a>什么是 Callable 和 Future?</h4><p>Callable 接口类似于 Runnable，从名字就可以看出来了，但是 Runnable 不会返回结果，并且无法抛出返回结果的异常，而 Callable 功能更强大一些，被线程执行后，可以返回值，这个返回值可以被 Future 拿到，也就是说，Future 可以拿到异步执行任务的返回值。</p><p>Future 接口表示异步任务，是一个可能还没有完成的异步任务的结果。所以说 Callable用于产生结果，Future 用于获取结果。</p><h4 id="什么是-FutureTask"><a href="#什么是-FutureTask" class="headerlink" title="什么是 FutureTask"></a>什么是 FutureTask</h4><p>FutureTask 表示一个异步运算的任务。FutureTask 里面可以传入一个 Callable 的具体实现类，可以对这个异步运算的任务的结果进行等待获取、判断是否已经完成、取消任务等操作。只有当运算完成的时候结果才能取回，如果运算尚未完成 get 方法将会阻塞。一个 FutureTask 对象可以对调用了 Callable 和 Runnable 的对象进行包装，由于 FutureTask 也是Runnable 接口的实现类，所以 FutureTask 也可以放入线程池中。</p><h3 id="线程的状态和基本操作"><a href="#线程的状态和基本操作" class="headerlink" title="线程的状态和基本操作"></a>线程的状态和基本操作</h3><h4 id="说说线程的生命周期及五种基本状态？"><a href="#说说线程的生命周期及五种基本状态？" class="headerlink" title="说说线程的生命周期及五种基本状态？"></a>说说线程的生命周期及五种基本状态？</h4><p><img src="https://lixiangbetter.github.io/2020/06/21/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E7%AC%94%E8%AE%B0/aHR0cHM6Ly91c2VyLWdvbGQtY2RuLnhpdHUuaW8vMjAxNy8xMi8xNS8xNjA1OWNjOTFlZThlZmIz.jpeg" alt="not found"></p><ol><li><p>新建(new)：新创建了一个线程对象。</p></li><li><p>可运行(runnable)：线程对象创建后，当调用线程对象的 start()方法，该线程处于就绪状态，等待被线程调度选中，获取cpu的使用权。</p></li><li><p>运行(running)：可运行状态(runnable)的线程获得了cpu时间片（timeslice），执行程序代码。注：就绪状态是进入到运行状态的唯一入口，也就是说，线程要想进入运行状态执行，首先必须处于就绪状态中；</p></li><li><p>阻塞(block)：处于运行状态中的线程由于某种原因，暂时放弃对 CPU的使用权，停止执行，此时进入阻塞状态，直到其进入到就绪状态，才有机会再次被 CPU 调用以进入到运行状态。</p><p>阻塞的情况分三种：<br>(一). 等待阻塞：运行状态中的线程执行 wait()方法，JVM会把该线程放入等待队列(waitting queue)中，使本线程进入到等待阻塞状态；<br>(二). 同步阻塞：线程在获取 synchronized 同步锁失败(因为锁被其它线程所占用)，，则JVM会把该线程放入锁池(lock pool)中，线程会进入同步阻塞状态；<br>(三). 其他阻塞: 通过调用线程的 sleep()或 join()或发出了 I/O 请求时，线程会进入到阻塞状态。当 sleep()状态超时、join()等待线程终止或者超时、或者 I/O 处理完毕时，线程重新转入就绪状态。</p></li><li><p>死亡(dead)：线程run()、main()方法执行结束，或者因异常退出了run()方法，则该线程结束生命周期。死亡的线程不可再次复生。</p></li></ol><h4 id="Java-中用到的线程调度算法是什么？"><a href="#Java-中用到的线程调度算法是什么？" class="headerlink" title="Java 中用到的线程调度算法是什么？"></a>Java 中用到的线程调度算法是什么？</h4><p>计算机通常只有一个 CPU，在任意时刻只能执行一条机器指令，每个线程只有获得CPU 的使用权才能执行指令。所谓多线程的并发运行，其实是指从宏观上看，各个线程轮流获得 CPU 的使用权，分别执行各自的任务。在运行池中，会有多个处于就绪状态的线程在等待 CPU，JAVA 虚拟机的一项任务就是负责线程的调度，线程调度是指按照特定机制为多个线程分配 CPU 的使用权。</p><p>有两种调度模型：分时调度模型和抢占式调度模型。</p><p>分时调度模型是指让所有的线程轮流获得 cpu 的使用权，并且平均分配每个线程占用的 CPU 的时间片这个也比较好理解。</p><p>Java虚拟机采用抢占式调度模型，是指优先让可运行池中优先级高的线程占用CPU，如果可运行池中的线程优先级相同，那么就随机选择一个线程，使其占用CPU。处于运行状态的线程会一直运行，直至它不得不放弃 CPU。</p><h4 id="线程的调度策略"><a href="#线程的调度策略" class="headerlink" title="线程的调度策略"></a>线程的调度策略</h4><p>线程调度器选择优先级最高的线程运行，但是，如果发生以下情况，就会终止线程的运行：</p><p>（1）线程体中调用了 yield 方法让出了对 cpu 的占用权利</p><p>（2）线程体中调用了 sleep 方法使线程进入睡眠状态</p><p>（3）线程由于 IO 操作受到阻塞</p><p>（4）另外一个更高优先级线程出现</p><p>（5）在支持时间片的系统中，该线程的时间片用完</p><h4 id="什么是线程调度器-Thread-Scheduler-和时间分片-Time-Slicing-？"><a href="#什么是线程调度器-Thread-Scheduler-和时间分片-Time-Slicing-？" class="headerlink" title="什么是线程调度器(Thread Scheduler)和时间分片(Time Slicing )？"></a>什么是线程调度器(Thread Scheduler)和时间分片(Time Slicing )？</h4><p>线程调度器是一个操作系统服务，它负责为 Runnable 状态的线程分配 CPU 时间。一旦我们创建一个线程并启动它，它的执行便依赖于线程调度器的实现。</p><p>时间分片是指将可用的 CPU 时间分配给可用的 Runnable 线程的过程。分配 CPU 时间可以基于线程优先级或者线程等待的时间。</p><p>线程调度并不受到 Java 虚拟机控制，所以由应用程序来控制它是更好的选择（也就是说不要让你的程序依赖于线程的优先级）。</p><h4 id="请说出与线程同步以及线程调度相关的方法。"><a href="#请说出与线程同步以及线程调度相关的方法。" class="headerlink" title="请说出与线程同步以及线程调度相关的方法。"></a>请说出与线程同步以及线程调度相关的方法。</h4><p>（1） wait()：使一个线程处于等待（阻塞）状态，并且释放所持有的对象的锁；</p><p>（2）sleep()：使一个正在运行的线程处于睡眠状态，是一个静态方法，调用此方法要处理 InterruptedException 异常；</p><p>（3）notify()：唤醒一个处于等待状态的线程，当然在调用此方法的时候，并不能确切的唤醒某一个等待状态的线程，而是由 JVM 确定唤醒哪个线程，而且与优先级无关；</p><p>（4）notityAll()：唤醒所有处于等待状态的线程，该方法并不是将对象的锁给所有线程，而是让它们竞争，只有获得锁的线程才能进入就绪状态；</p><h4 id="sleep-和-wait-有什么区别？"><a href="#sleep-和-wait-有什么区别？" class="headerlink" title="sleep() 和 wait() 有什么区别？"></a>sleep() 和 wait() 有什么区别？</h4><p>两者都可以暂停线程的执行</p><ul><li>类的不同：sleep() 是 Thread线程类的静态方法，wait() 是 Object类的方法。</li><li>是否释放锁：sleep() 不释放锁；wait() 释放锁。</li><li>用途不同：Wait 通常被用于线程间交互/通信，sleep 通常被用于暂停执行。</li><li>用法不同：wait() 方法被调用后，线程不会自动苏醒，需要别的线程调用同一个对象上的 notify() 或者 notifyAll() 方法。sleep() 方法执行完成后，线程会自动苏醒。或者可以使用wait(long timeout)超时后线程会自动苏醒。</li></ul><h4 id="你是如何调用-wait-方法的？使用-if-块还是循环？为什么？"><a href="#你是如何调用-wait-方法的？使用-if-块还是循环？为什么？" class="headerlink" title="你是如何调用 wait() 方法的？使用 if 块还是循环？为什么？"></a>你是如何调用 wait() 方法的？使用 if 块还是循环？为什么？</h4><p>处于等待状态的线程可能会收到错误警报和伪唤醒，如果不在循环中检查等待条件，程序就会在没有满足结束条件的情况下退出。</p><p>wait() 方法应该在循环调用，因为当线程获取到 CPU 开始执行的时候，其他条件可能还没有满足，所以在处理前，循环检测条件是否满足会更好。</p><h4 id="为什么线程通信的方法-wait-notify-和-notifyAll-被定义在-Object-类里？"><a href="#为什么线程通信的方法-wait-notify-和-notifyAll-被定义在-Object-类里？" class="headerlink" title="为什么线程通信的方法 wait(), notify()和 notifyAll()被定义在 Object 类里？"></a>为什么线程通信的方法 wait(), notify()和 notifyAll()被定义在 Object 类里？</h4><p>Java中，任何对象都可以作为锁，并且 wait()，notify()等方法用于等待对象的锁或者唤醒线程，在 Java 的线程中并没有可供任何对象使用的锁，所以任意对象调用方法一定定义在Object类中。</p><p>wait(), notify()和 notifyAll()这些方法在同步代码块中调用</p><p>有的人会说，既然是线程放弃对象锁，那也可以把wait()定义在Thread类里面啊，新定义的线程继承于Thread类，也不需要重新定义wait()方法的实现。然而，这样做有一个非常大的问题，一个线程完全可以持有很多锁，你一个线程放弃锁的时候，到底要放弃哪个锁？当然了，这种设计并不是不能实现，只是管理起来更加复杂。</p><p>综上所述，wait()、notify()和notifyAll()方法要定义在Object类中。</p><h4 id="为什么-wait-notify-和-notifyAll-必须在同步方法或者同步块中被调用？"><a href="#为什么-wait-notify-和-notifyAll-必须在同步方法或者同步块中被调用？" class="headerlink" title="为什么 wait(), notify()和 notifyAll()必须在同步方法或者同步块中被调用？"></a>为什么 wait(), notify()和 notifyAll()必须在同步方法或者同步块中被调用？</h4><p>当一个线程需要调用对象的 wait()方法的时候，这个线程必须拥有该对象的锁，接着它就会释放这个对象锁并进入等待状态直到其他线程调用这个对象上的 notify()方法。同样的，当一个线程需要调用对象的 notify()方法时，它会释放这个对象的锁，以便其他在等待的线程就可以得到这个对象锁。由于所有的这些方法都需要线程持有对象的锁，这样就只能通过同步来实现，所以他们只能在同步方法或者同步块中被调用。</p><h4 id="Thread-类中的-yield-方法有什么作用？"><a href="#Thread-类中的-yield-方法有什么作用？" class="headerlink" title="Thread 类中的 yield 方法有什么作用？"></a>Thread 类中的 yield 方法有什么作用？</h4><p>使当前线程从执行状态（运行状态）变为可执行态（就绪状态）。</p><p>当前线程到了就绪状态，那么接下来哪个线程会从就绪状态变成执行状态呢？可能是当前线程，也可能是其他线程，看系统的分配了。</p><h4 id="为什么-Thread-类的-sleep-和-yield-方法是静态的？"><a href="#为什么-Thread-类的-sleep-和-yield-方法是静态的？" class="headerlink" title="为什么 Thread 类的 sleep()和 yield ()方法是静态的？"></a>为什么 Thread 类的 sleep()和 yield ()方法是静态的？</h4><p>Thread 类的 sleep()和 yield()方法将在当前正在执行的线程上运行。所以在其他处于等待状态的线程上调用这些方法是没有意义的。这就是为什么这些方法是静态的。它们可以在当前正在执行的线程中工作，并避免程序员错误的认为可以在其他非运行线程调用这些方法。</p><h4 id="线程的-sleep-方法和-yield-方法有什么区别？"><a href="#线程的-sleep-方法和-yield-方法有什么区别？" class="headerlink" title="线程的 sleep()方法和 yield()方法有什么区别？"></a>线程的 sleep()方法和 yield()方法有什么区别？</h4><p>（1） sleep()方法给其他线程运行机会时不考虑线程的优先级，因此会给低优先级的线程以运行的机会；yield()方法只会给相同优先级或更高优先级的线程以运行的机会；</p><p>（2） 线程执行 sleep()方法后转入阻塞（blocked）状态，而执行 yield()方法后转入就绪（ready）状态；</p><p>（3）sleep()方法声明抛出 InterruptedException，而 yield()方法没有声明任何异常；</p><p>（4）sleep()方法比 yield()方法（跟操作系统 CPU 调度相关）具有更好的可移植性，通常不建议使用yield()方法来控制并发线程的执行。</p><h4 id="如何停止一个正在运行的线程？"><a href="#如何停止一个正在运行的线程？" class="headerlink" title="如何停止一个正在运行的线程？"></a>如何停止一个正在运行的线程？</h4><p>在java中有以下3种方法可以终止正在运行的线程：</p><ol><li>使用退出标志，使线程正常退出，也就是当run方法完成后线程终止。</li><li>使用stop方法强行终止，但是不推荐这个方法，因为stop和suspend及resume一样都是过期作废的方法。</li><li>使用interrupt方法中断线程。</li></ol><h4 id="Java-中-interrupted-和-isInterrupted-方法的区别？"><a href="#Java-中-interrupted-和-isInterrupted-方法的区别？" class="headerlink" title="Java 中 interrupted 和 isInterrupted 方法的区别？"></a>Java 中 interrupted 和 isInterrupted 方法的区别？</h4><p>interrupt：用于中断线程。调用该方法的线程的状态为将被置为”中断”状态。</p><p>注意：线程中断仅仅是置线程的中断状态位，不会停止线程。需要用户自己去监视线程的状态为并做处理。支持线程中断的方法（也就是线程中断后会抛出interruptedException 的方法）就是在监视线程的中断状态，一旦线程的中断状态被置为“中断状态”，就会抛出中断异常。</p><p>interrupted：是静态方法，查看当前中断信号是true还是false并且清除中断信号。如果一个线程被中断了，第一次调用 interrupted 则返回 true，第二次和后面的就返回 false 了。</p><p>isInterrupted：查看当前中断信号是true还是false</p><h4 id="什么是阻塞式方法？"><a href="#什么是阻塞式方法？" class="headerlink" title="什么是阻塞式方法？"></a>什么是阻塞式方法？</h4><p>阻塞式方法是指程序会一直等待该方法完成期间不做其他事情，ServerSocket 的accept()方法就是一直等待客户端连接。这里的阻塞是指调用结果返回之前，当前线程会被挂起，直到得到结果之后才会返回。此外，还有异步和非阻塞式方法在任务完成前就返回。</p><h4 id="Java-中你怎样唤醒一个阻塞的线程？"><a href="#Java-中你怎样唤醒一个阻塞的线程？" class="headerlink" title="Java 中你怎样唤醒一个阻塞的线程？"></a>Java 中你怎样唤醒一个阻塞的线程？</h4><p>首先 ，wait()、notify() 方法是针对对象的，调用任意对象的 wait()方法都将导致线程阻塞，阻塞的同时也将释放该对象的锁，相应地，调用任意对象的 notify()方法则将随机解除该对象阻塞的线程，但它需要重新获取该对象的锁，直到获取成功才能往下执行；</p><p>其次，wait、notify 方法必须在 synchronized 块或方法中被调用，并且要保证同步块或方法的锁对象与调用 wait、notify 方法的对象是同一个，如此一来在调用 wait 之前当前线程就已经成功获取某对象的锁，执行 wait 阻塞后当前线程就将之前获取的对象锁释放。</p><h4 id="notify-和-notifyAll-有什么区别？"><a href="#notify-和-notifyAll-有什么区别？" class="headerlink" title="notify() 和 notifyAll() 有什么区别？"></a>notify() 和 notifyAll() 有什么区别？</h4><p>如果线程调用了对象的 wait()方法，那么线程便会处于该对象的等待池中，等待池中的线程不会去竞争该对象的锁。</p><p>notifyAll() 会唤醒所有的线程，notify() 只会唤醒一个线程。</p><p>notifyAll() 调用后，会将全部线程由等待池移到锁池，然后参与锁的竞争，竞争成功则继续执行，如果不成功则留在锁池等待锁被释放后再次参与竞争。而 notify()只会唤醒一个线程，具体唤醒哪一个线程由虚拟机控制。</p><h4 id="如何在两个线程间共享数据？"><a href="#如何在两个线程间共享数据？" class="headerlink" title="如何在两个线程间共享数据？"></a>如何在两个线程间共享数据？</h4><p>在两个线程间共享变量即可实现共享。</p><p>一般来说，共享变量要求变量本身是线程安全的，然后在线程内使用的时候，如果有对共享变量的复合操作，那么也得保证复合操作的线程安全性。</p><h4 id="Java-如何实现多线程之间的通讯和协作？"><a href="#Java-如何实现多线程之间的通讯和协作？" class="headerlink" title="Java 如何实现多线程之间的通讯和协作？"></a>Java 如何实现多线程之间的通讯和协作？</h4><p>Java中线程通信协作的最常见的两种方式：</p><p>一.syncrhoized加锁的线程的Object类的wait()/notify()/notifyAll()</p><p>二.ReentrantLock类加锁的线程的Condition类的await()/signal()/signalAll()</p><p>线程间直接的数据交换：</p><p>三.通过管道进行线程间通信：1）字节流；2）字符流</p><h4 id="同步方法和同步块，哪个是更好的选择？"><a href="#同步方法和同步块，哪个是更好的选择？" class="headerlink" title="同步方法和同步块，哪个是更好的选择？"></a>同步方法和同步块，哪个是更好的选择？</h4><p>同步块是更好的选择，因为它不会锁住整个对象（当然你也可以让它锁住整个对象）。同步方法会锁住整个对象，哪怕这个类中有多个不相关联的同步块，这通常会导致他们停止执行并需要等待获得这个对象上的锁。</p><p>同步块更要符合开放调用的原则，只在需要锁住的代码块锁住相应的对象，这样从侧面来说也可以避免死锁。</p><p>请知道一条原则：同步的范围越小越好。</p><h4 id="什么是线程同步和线程互斥，有哪几种实现方式？"><a href="#什么是线程同步和线程互斥，有哪几种实现方式？" class="headerlink" title="什么是线程同步和线程互斥，有哪几种实现方式？"></a>什么是线程同步和线程互斥，有哪几种实现方式？</h4><p>实现线程同步的方法</p><p>同步代码方法：sychronized 关键字修饰的方法</p><p>同步代码块：sychronized 关键字修饰的代码块</p><p>使用特殊变量域volatile实现线程同步：volatile关键字为域变量的访问提供了一种免锁机制</p><p>使用重入锁实现线程同步：reentrantlock类是可冲入、互斥、实现了lock接口的锁他与sychronized方法具有相同的基本行为和语义</p><h4 id="在监视器-Monitor-内部，是如何做线程同步的？程序应该做哪种级别的同步？"><a href="#在监视器-Monitor-内部，是如何做线程同步的？程序应该做哪种级别的同步？" class="headerlink" title="在监视器(Monitor)内部，是如何做线程同步的？程序应该做哪种级别的同步？"></a>在监视器(Monitor)内部，是如何做线程同步的？程序应该做哪种级别的同步？</h4><p>在 java 虚拟机中，每个对象( Object 和 class )通过某种逻辑关联监视器,每个监视器和一个对象引用相关联，为了实现监视器的互斥功能，每个对象都关联着一把锁。</p><p>一旦方法或者代码块被 synchronized 修饰，那么这个部分就放入了监视器的监视区域，确保一次只能有一个线程执行该部分的代码，线程在获取锁之前不允许执行该部分的代码</p><p>另外 java 还提供了显式监视器( Lock )和隐式监视器( synchronized )两种锁方案</p><h4 id="如果你提交任务时，线程池队列已满，这时会发生什么"><a href="#如果你提交任务时，线程池队列已满，这时会发生什么" class="headerlink" title="如果你提交任务时，线程池队列已满，这时会发生什么"></a>如果你提交任务时，线程池队列已满，这时会发生什么</h4><p>这里区分一下：</p><p>（1）如果使用的是无界队列 LinkedBlockingQueue，也就是无界队列的话，没关系，继续添加任务到阻塞队列中等待执行，因为 LinkedBlockingQueue 可以近乎认为是一个无穷大的队列，可以无限存放任务</p><p>（2）如果使用的是有界队列比如 ArrayBlockingQueue，任务首先会被添加到ArrayBlockingQueue 中，ArrayBlockingQueue 满了，会根据maximumPoolSize 的值增加线程数量，如果增加了线程数量还是处理不过来，ArrayBlockingQueue 继续满，那么则会使用拒绝策略RejectedExecutionHandler 处理满了的任务，默认是 AbortPolicy</p><h4 id="什么叫线程安全？servlet-是线程安全吗"><a href="#什么叫线程安全？servlet-是线程安全吗" class="headerlink" title="什么叫线程安全？servlet 是线程安全吗?"></a>什么叫线程安全？servlet 是线程安全吗?</h4><p>线程安全是编程中的术语，指某个方法在多线程环境中被调用时，能够正确地处理多个线程之间的共享变量，使程序功能正确完成。</p><p>Servlet 不是线程安全的，servlet 是单实例多线程的，当多个线程同时访问同一个方法，是不能保证共享变量的线程安全性的。</p><p>Struts2 的 action 是多实例多线程的，是线程安全的，每个请求过来都会 new 一个新的 action 分配给这个请求，请求完成后销毁。</p><p>SpringMVC 的 Controller 是线程安全的吗？不是的，和 Servlet 类似的处理流程。</p><p>Struts2 好处是不用考虑线程安全问题；Servlet 和 SpringMVC 需要考虑线程安全问题，但是性能可以提升不用处理太多的 gc，可以使用 ThreadLocal 来处理多线程的问题。</p><h4 id="在-Java-程序中怎么保证多线程的运行安全？"><a href="#在-Java-程序中怎么保证多线程的运行安全？" class="headerlink" title="在 Java 程序中怎么保证多线程的运行安全？"></a>在 Java 程序中怎么保证多线程的运行安全？</h4><ul><li>方法一：使用安全类，比如 java.util.concurrent 下的类，使用原子类AtomicInteger</li><li>方法二：使用自动锁 synchronized。</li><li>方法三：使用手动锁 Lock。</li></ul><h4 id="你对线程优先级的理解是什么？"><a href="#你对线程优先级的理解是什么？" class="headerlink" title="你对线程优先级的理解是什么？"></a>你对线程优先级的理解是什么？</h4><p>每一个线程都是有优先级的，一般来说，高优先级的线程在运行时会具有优先权，但这依赖于线程调度的实现，这个实现是和操作系统相关的(OS dependent)。我们可以定义线程的优先级，但是这并不能保证高优先级的线程会在低优先级的线程前执行。线程优先级是一个 int 变量(从 1-10)，1 代表最低优先级，10 代表最高优先级。</p><p>Java 的线程优先级调度会委托给操作系统去处理，所以与具体的操作系统优先级有关，如非特别需要，一般无需设置线程优先级。</p><h4 id="线程类的构造方法、静态块是被哪个线程调用的"><a href="#线程类的构造方法、静态块是被哪个线程调用的" class="headerlink" title="线程类的构造方法、静态块是被哪个线程调用的"></a>线程类的构造方法、静态块是被哪个线程调用的</h4><p>这是一个非常刁钻和狡猾的问题。请记住：线程类的构造方法、静态块是被 new这个线程类所在的线程所调用的，而 run 方法里面的代码才是被线程自身所调用的。</p><p>如果说上面的说法让你感到困惑，那么我举个例子，假设 Thread2 中 new 了Thread1，main 函数中 new 了 Thread2，那么：</p><p>（1）Thread2 的构造方法、静态块是 main 线程调用的，Thread2 的 run()方法是Thread2 自己调用的</p><p>（2）Thread1 的构造方法、静态块是 Thread2 调用的，Thread1 的 run()方法是Thread1 自己调用的</p><h4 id="Java-中怎么获取一份线程-dump-文件？你如何在-Java-中获取线程堆栈？"><a href="#Java-中怎么获取一份线程-dump-文件？你如何在-Java-中获取线程堆栈？" class="headerlink" title="Java 中怎么获取一份线程 dump 文件？你如何在 Java 中获取线程堆栈？"></a>Java 中怎么获取一份线程 dump 文件？你如何在 Java 中获取线程堆栈？</h4><p>Dump文件是进程的内存镜像。可以把程序的执行状态通过调试器保存到dump文件中。</p><p>在 Linux 下，你可以通过命令 kill -3 PID （Java 进程的进程 ID）来获取 Java应用的 dump 文件。</p><p>在 Windows 下，你可以按下 Ctrl + Break 来获取。这样 JVM 就会将线程的 dump 文件打印到标准输出或错误文件中，它可能打印在控制台或者日志文件中，具体位置依赖应用的配置。</p><h4 id="一个线程运行时发生异常会怎样？"><a href="#一个线程运行时发生异常会怎样？" class="headerlink" title="一个线程运行时发生异常会怎样？"></a>一个线程运行时发生异常会怎样？</h4><p>如果异常没有被捕获该线程将会停止执行。Thread.UncaughtExceptionHandler是用于处理未捕获异常造成线程突然中断情况的一个内嵌接口。当一个未捕获异常将造成线程中断的时候，JVM 会使用 Thread.getUncaughtExceptionHandler()来查询线程的 UncaughtExceptionHandler 并将线程和异常作为参数传递给 handler 的 uncaughtException()方法进行处理。</p><h4 id="Java-线程数过多会造成什么异常？"><a href="#Java-线程数过多会造成什么异常？" class="headerlink" title="Java 线程数过多会造成什么异常？"></a>Java 线程数过多会造成什么异常？</h4><ul><li><p>线程的生命周期开销非常高</p></li><li><p>消耗过多的 CPU</p><p>资源如果可运行的线程数量多于可用处理器的数量，那么有线程将会被闲置。大量空闲的线程会占用许多内存，给垃圾回收器带来压力，而且大量的线程在竞争 CPU资源时还将产生其他性能的开销</p></li><li><p>降低稳定性JVM</p><p>在可创建线程的数量上存在一个限制，这个限制值将随着平台的不同而不同，并且承受着多个因素制约，包括 JVM 的启动参数、Thread 构造函数中请求栈的大小，以及底层操作系统对线程的限制等。如果破坏了这些限制，那么可能抛出OutOfMemoryError 异常。</p></li></ul><h2 id="并发理论"><a href="#并发理论" class="headerlink" title="并发理论"></a>并发理论</h2><h3 id="Java内存模型"><a href="#Java内存模型" class="headerlink" title="Java内存模型"></a>Java内存模型</h3><h4 id="Java中垃圾回收有什么目的？什么时候进行垃圾回收？"><a href="#Java中垃圾回收有什么目的？什么时候进行垃圾回收？" class="headerlink" title="Java中垃圾回收有什么目的？什么时候进行垃圾回收？"></a>Java中垃圾回收有什么目的？什么时候进行垃圾回收？</h4><p>垃圾回收是在内存中存在没有引用的对象或超过作用域的对象时进行的。</p><p>垃圾回收的目的是识别并且丢弃应用不再使用的对象来释放和重用资源。</p><h4 id="如果对象的引用被置为null，垃圾收集器是否会立即释放对象占用的内存？"><a href="#如果对象的引用被置为null，垃圾收集器是否会立即释放对象占用的内存？" class="headerlink" title="如果对象的引用被置为null，垃圾收集器是否会立即释放对象占用的内存？"></a>如果对象的引用被置为null，垃圾收集器是否会立即释放对象占用的内存？</h4><p>不会，在下一个垃圾回调周期中，这个对象将是被可回收的。</p><p>也就是说并不会立即被垃圾收集器立刻回收，而是在下一次垃圾回收时才会释放其占用的内存。</p><h4 id="finalize-方法什么时候被调用？析构函数-finalization-的目的是什么？"><a href="#finalize-方法什么时候被调用？析构函数-finalization-的目的是什么？" class="headerlink" title="finalize()方法什么时候被调用？析构函数(finalization)的目的是什么？"></a>finalize()方法什么时候被调用？析构函数(finalization)的目的是什么？</h4><p>1）垃圾回收器（garbage colector）决定回收某对象时，就会运行该对象的finalize()方法；<br>finalize是Object类的一个方法，该方法在Object类中的声明protected void finalize() throws Throwable { }<br>在垃圾回收器执行时会调用被回收对象的finalize()方法，可以覆盖此方法来实现对其资源的回收。注意：一旦垃圾回收器准备释放对象占用的内存，将首先调用该对象的finalize()方法，并且下一次垃圾回收动作发生时，才真正回收对象占用的内存空间</p><p>2）GC本来就是内存回收了，应用还需要在finalization做什么呢？ 答案是大部分时候，什么都不用做(也就是不需要重载)。只有在某些很特殊的情况下，比如你调用了一些native的方法(一般是C写的)，可以要在finaliztion里去调用C的释放函数。</p><h3 id="重排序与数据依赖性"><a href="#重排序与数据依赖性" class="headerlink" title="重排序与数据依赖性"></a>重排序与数据依赖性</h3><h4 id="为什么代码会重排序？"><a href="#为什么代码会重排序？" class="headerlink" title="为什么代码会重排序？"></a>为什么代码会重排序？</h4><p>在执行程序时，为了提供性能，处理器和编译器常常会对指令进行重排序，但是不能随意重排序，不是你想怎么排序就怎么排序，它需要满足以下两个条件：</p><p>在单线程环境下不能改变程序运行的结果；</p><p>存在数据依赖关系的不允许重排序</p><p>需要注意的是：重排序不会影响单线程环境的执行结果，但是会破坏多线程的执行语义。</p><h3 id="as-if-serial规则和happens-before规则的区别"><a href="#as-if-serial规则和happens-before规则的区别" class="headerlink" title="as-if-serial规则和happens-before规则的区别"></a>as-if-serial规则和happens-before规则的区别</h3><ul><li>as-if-serial语义保证单线程内程序的执行结果不被改变，happens-before关系保证正确同步的多线程程序的执行结果不被改变。</li><li>as-if-serial语义给编写单线程程序的程序员创造了一个幻境：单线程程序是按程序的顺序来执行的。happens-before关系给编写正确同步的多线程程序的程序员创造了一个幻境：正确同步的多线程程序是按happens-before指定的顺序来执行的。</li><li>as-if-serial语义和happens-before这么做的目的，都是为了在不改变程序执行结果的前提下，尽可能地提高程序执行的并行度。</li></ul><h2 id="并发关键字"><a href="#并发关键字" class="headerlink" title="并发关键字"></a>并发关键字</h2><h3 id="synchronized"><a href="#synchronized" class="headerlink" title="synchronized"></a>synchronized</h3><h4 id="synchronized-的作用？"><a href="#synchronized-的作用？" class="headerlink" title="synchronized 的作用？"></a>synchronized 的作用？</h4><p>在 Java 中，synchronized 关键字是用来控制线程同步的，就是在多线程的环境下，控制 synchronized 代码段不被多个线程同时执行。synchronized 可以修饰类、方法、变量。</p><p>另外，在 Java 早期版本中，synchronized属于重量级锁，效率低下，因为监视器锁（monitor）是依赖于底层的操作系统的 Mutex Lock 来实现的，Java 的线程是映射到操作系统的原生线程之上的。如果要挂起或者唤醒一个线程，都需要操作系统帮忙完成，而操作系统实现线程之间的切换时需要从用户态转换到内核态，这个状态之间的转换需要相对比较长的时间，时间成本相对较高，这也是为什么早期的 synchronized 效率低的原因。庆幸的是在 Java 6 之后 Java 官方对从 JVM 层面对synchronized 较大优化，所以现在的 synchronized 锁效率也优化得很不错了。JDK1.6对锁的实现引入了大量的优化，如自旋锁、适应性自旋锁、锁消除、锁粗化、偏向锁、轻量级锁等技术来减少锁操作的开销。</p><h4 id="说说自己是怎么使用-synchronized-关键字，在项目中用到了吗"><a href="#说说自己是怎么使用-synchronized-关键字，在项目中用到了吗" class="headerlink" title="说说自己是怎么使用 synchronized 关键字，在项目中用到了吗"></a>说说自己是怎么使用 synchronized 关键字，在项目中用到了吗</h4><p>synchronized关键字最主要的三种使用方式：</p><p>修饰实例方法: 作用于当前对象实例加锁，进入同步代码前要获得当前对象实例的锁<br>修饰静态方法: 也就是给当前类加锁，会作用于类的所有对象实例，因为静态成员不属于任何一个实例对象，是类成员（ static 表明这是该类的一个静态资源，不管new了多少个对象，只有一份）。所以如果一个线程A调用一个实例对象的非静态 synchronized 方法，而线程B需要调用这个实例对象所属类的静态 synchronized 方法，是允许的，不会发生互斥现象，因为访问静态 synchronized 方法占用的锁是当前类的锁，而访问非静态 synchronized 方法占用的锁是当前实例对象锁。<br>修饰代码块: 指定加锁对象，对给定对象加锁，进入同步代码库前要获得给定对象的锁。<br>总结： synchronized 关键字加到 static 静态方法和 synchronized(class)代码块上都是是给 Class 类上锁。synchronized 关键字加到实例方法上是给对象实例上锁。尽量不要使用 synchronized(String a) 因为JVM中，字符串常量池具有缓存功能！</p><p>uniqueInstance 采用 volatile 关键字修饰也是很有必要的， uniqueInstance = new Singleton(); 这段代码其实是分为三步执行：</p><ol><li>为 uniqueInstance 分配内存空间</li><li>初始化 uniqueInstance</li><li>将 uniqueInstance 指向分配的内存地址</li></ol><p>使用 volatile 可以禁止 JVM 的指令重排，保证在多线程环境下也能正常运行。</p><h4 id="说一下-synchronized-底层实现原理？"><a href="#说一下-synchronized-底层实现原理？" class="headerlink" title="说一下 synchronized 底层实现原理？"></a>说一下 synchronized 底层实现原理？</h4><p>可以看出在执行同步代码块之前之后都有一个monitor字样，其中前面的是monitorenter，后面的是离开monitorexit，不难想象一个线程也执行同步代码块，首先要获取锁，而获取锁的过程就是monitorenter ，在执行完代码块之后，要释放锁，释放锁就是执行monitorexit指令。</p><p>为什么会有两个monitorexit呢？</p><p>这个主要是防止在同步代码块中线程因异常退出，而锁没有得到释放，这必然会造成死锁（等待的线程永远获取不到锁）。因此最后一个monitorexit是保证在异常情况下，锁也可以得到释放，避免死锁。<br>仅有ACC_SYNCHRONIZED这么一个标志，该标记表明线程进入该方法时，需要monitorenter，退出该方法时需要monitorexit。</p><p>synchronized可重入的原理</p><p>重入锁是指一个线程获取到该锁之后，该线程可以继续获得该锁。底层原理维护一个计数器，当线程获取该锁时，计数器加一，再次获得该锁时继续加一，释放锁时，计数器减一，当计数器值为0时，表明该锁未被任何线程所持有，其它线程可以竞争获取锁。</p><h4 id="什么是自旋"><a href="#什么是自旋" class="headerlink" title="什么是自旋"></a>什么是自旋</h4><p>很多 synchronized 里面的代码只是一些很简单的代码，执行时间非常快，此时等待的线程都加锁可能是一种不太值得的操作，因为线程阻塞涉及到用户态和内核态切换的问题。既然 synchronized 里面的代码执行得非常快，不妨让等待锁的线程不要被阻塞，而是在 synchronized 的边界做忙循环，这就是自旋。如果做了多次循环发现还没有获得锁，再阻塞，这样可能是一种更好的策略。</p><h4 id="多线程中-synchronized-锁升级的原理是什么？"><a href="#多线程中-synchronized-锁升级的原理是什么？" class="headerlink" title="多线程中 synchronized 锁升级的原理是什么？"></a>多线程中 synchronized 锁升级的原理是什么？</h4><p>synchronized 锁升级原理：在锁对象的对象头里面有一个 thread id 字段，在第一次访问的时候 threadid 为空，jvm 让其持有偏向锁，并将 threadid 设置为其线程 id，再次进入的时候会先判断 threadid 是否与其线程 id 一致，如果一致则可以直接使用此对象，如果不一致，则升级偏向锁为轻量级锁，通过自旋循环一定次数来获取锁，执行一定次数之后，如果还没有正常获取到要使用的对象，此时就会把锁从轻量级升级为重量级锁，此过程就构成了 synchronized 锁的升级。</p><p>锁的升级的目的：锁升级是为了减低了锁带来的性能消耗。在 Java 6 之后优化 synchronized 的实现方式，使用了偏向锁升级为轻量级锁再升级到重量级锁的方式，从而减低了锁带来的性能消耗。</p><h4 id="线程-B-怎么知道线程-A-修改了变量"><a href="#线程-B-怎么知道线程-A-修改了变量" class="headerlink" title="线程 B 怎么知道线程 A 修改了变量"></a>线程 B 怎么知道线程 A 修改了变量</h4><p>（1）volatile 修饰变量</p><p>（2）synchronized 修饰修改变量的方法</p><p>（3）wait/notify</p><p>（4）while 轮询</p><h4 id="当一个线程进入一个对象的-synchronized-方法-A-之后，其它线程是否可进入此对象的-synchronized-方法-B？"><a href="#当一个线程进入一个对象的-synchronized-方法-A-之后，其它线程是否可进入此对象的-synchronized-方法-B？" class="headerlink" title="当一个线程进入一个对象的 synchronized 方法 A 之后，其它线程是否可进入此对象的 synchronized 方法 B？"></a>当一个线程进入一个对象的 synchronized 方法 A 之后，其它线程是否可进入此对象的 synchronized 方法 B？</h4><p>不能。其它线程只能访问该对象的非同步方法，同步方法则不能进入。因为非静态方法上的 synchronized 修饰符要求执行方法时要获得对象的锁，如果已经进入A 方法说明对象锁已经被取走，那么试图进入 B 方法的线程就只能在等锁池（注意不是等待池哦）中等待对象的锁。</p><h4 id="synchronized、volatile、CAS-比较"><a href="#synchronized、volatile、CAS-比较" class="headerlink" title="synchronized、volatile、CAS 比较"></a>synchronized、volatile、CAS 比较</h4><p>（1）synchronized 是悲观锁，属于抢占式，会引起其他线程阻塞。</p><p>（2）volatile 提供多线程共享变量可见性和禁止指令重排序优化。</p><p>（3）CAS 是基于冲突检测的乐观锁（非阻塞）</p><h4 id="synchronized-和-Lock-有什么区别？"><a href="#synchronized-和-Lock-有什么区别？" class="headerlink" title="synchronized 和 Lock 有什么区别？"></a>synchronized 和 Lock 有什么区别？</h4><ul><li>首先synchronized是Java内置关键字，在JVM层面，Lock是个Java类；</li><li>synchronized 可以给类、方法、代码块加锁；而 lock 只能给代码块加锁。</li><li>synchronized 不需要手动获取锁和释放锁，使用简单，发生异常会自动释放锁，不会造成死锁；而 lock 需要自己加锁和释放锁，如果使用不当没有 unLock()去释放锁就会造成死锁。</li><li>通过 Lock 可以知道有没有成功获取锁，而 synchronized 却无法办到。</li></ul><h4 id="synchronized-和-ReentrantLock-区别是什么？"><a href="#synchronized-和-ReentrantLock-区别是什么？" class="headerlink" title="synchronized 和 ReentrantLock 区别是什么？"></a>synchronized 和 ReentrantLock 区别是什么？</h4><p>synchronized 是和 if、else、for、while 一样的关键字，ReentrantLock 是类，这是二者的本质区别。既然 ReentrantLock 是类，那么它就提供了比synchronized 更多更灵活的特性，可以被继承、可以有方法、可以有各种各样的类变量</p><p>synchronized 早期的实现比较低效，对比 ReentrantLock，大多数场景性能都相差较大，但是在 Java 6 中对 synchronized 进行了非常多的改进。</p><p>相同点：两者都是可重入锁</p><p>两者都是可重入锁。“可重入锁”概念是：自己可以再次获取自己的内部锁。比如一个线程获得了某个对象的锁，此时这个对象锁还没有释放，当其再次想要获取这个对象的锁的时候还是可以获取的，如果不可锁重入的话，就会造成死锁。同一个线程每次获取锁，锁的计数器都自增1，所以要等到锁的计数器下降为0时才能释放锁。</p><p>主要区别如下：</p><ul><li>ReentrantLock 使用起来比较灵活，但是必须有释放锁的配合动作；</li><li>ReentrantLock 必须手动获取与释放锁，而 synchronized 不需要手动释放和开启锁；</li><li>ReentrantLock 只适用于代码块锁，而 synchronized 可以修饰类、方法、变量等。</li><li>二者的锁机制其实也是不一样的。ReentrantLock 底层调用的是 Unsafe 的park 方法加锁，synchronized 操作的应该是对象头中 mark word</li></ul><p>Java中每一个对象都可以作为锁，这是synchronized实现同步的基础：</p><ul><li>普通同步方法，锁是当前实例对象</li><li>静态同步方法，锁是当前类的class对象</li><li>同步方法块，锁是括号里面的对象</li></ul><h3 id="volatile"><a href="#volatile" class="headerlink" title="volatile"></a>volatile</h3><h4 id="volatile-关键字的作用"><a href="#volatile-关键字的作用" class="headerlink" title="volatile 关键字的作用"></a>volatile 关键字的作用</h4><p>对于可见性，Java 提供了 volatile 关键字来保证可见性和禁止指令重排。 volatile 提供 happens-before 的保证，确保一个线程的修改能对其他线程是可见的。当一个共享变量被 volatile 修饰时，它会保证修改的值会立即被更新到主存，当有其他线程需要读取时，它会去内存中读取新值。</p><p>从实践角度而言，volatile 的一个重要作用就是和 CAS 结合，保证了原子性，详细的可以参见 java.util.concurrent.atomic 包下的类，比如 AtomicInteger。</p><p>volatile 常用于多线程环境下的单次操作(单次读或者单次写)。</p><h4 id="Java-中能创建-volatile-数组吗？"><a href="#Java-中能创建-volatile-数组吗？" class="headerlink" title="Java 中能创建 volatile 数组吗？"></a>Java 中能创建 volatile 数组吗？</h4><p>能，Java 中可以创建 volatile 类型数组，不过只是一个指向数组的引用，而不是整个数组。意思是，如果改变引用指向的数组，将会受到 volatile 的保护，但是如果多个线程同时改变数组的元素，volatile 标示符就不能起到之前的保护作用了。</p><h4 id="volatile-变量和-atomic-变量有什么不同？"><a href="#volatile-变量和-atomic-变量有什么不同？" class="headerlink" title="volatile 变量和 atomic 变量有什么不同？"></a>volatile 变量和 atomic 变量有什么不同？</h4><p>volatile 变量可以确保先行关系，即写操作会发生在后续的读操作之前, 但它并不能保证原子性。例如用 volatile 修饰 count 变量，那么 count++ 操作就不是原子性的。</p><p>而 AtomicInteger 类提供的 atomic 方法可以让这种操作具有原子性如getAndIncrement()方法会原子性的进行增量操作把当前值加一，其它数据类型和引用变量也可以进行相似操作。</p><h4 id="volatile-能使得一个非原子操作变成原子操作吗？"><a href="#volatile-能使得一个非原子操作变成原子操作吗？" class="headerlink" title="volatile 能使得一个非原子操作变成原子操作吗？"></a>volatile 能使得一个非原子操作变成原子操作吗？</h4><p>关键字volatile的主要作用是使变量在多个线程间可见，但无法保证原子性，对于多个线程访问同一个实例变量需要加锁进行同步。</p><p>虽然volatile只能保证可见性不能保证原子性，但用volatile修饰long和double可以保证其操作原子性。</p><p>所以从Oracle Java Spec里面可以看到：</p><p>对于64位的long和double，如果没有被volatile修饰，那么对其操作可以不是原子的。在操作的时候，可以分成两步，每次对32位操作。<br>如果使用volatile修饰long和double，那么其读写都是原子操作<br>对于64位的引用地址的读写，都是原子操作<br>在实现JVM时，可以自由选择是否把读写long和double作为原子操作<br>推荐JVM实现为原子操作</p><h4 id="volatile-修饰符的有过什么实践？"><a href="#volatile-修饰符的有过什么实践？" class="headerlink" title="volatile 修饰符的有过什么实践？"></a>volatile 修饰符的有过什么实践？</h4><p>Double-Check单例模式</p><p>是否 Lazy 初始化：是</p><p>是否多线程安全：是</p><p>实现难度：较复杂</p><h4 id="synchronized-和-volatile-的区别是什么？"><a href="#synchronized-和-volatile-的区别是什么？" class="headerlink" title="synchronized 和 volatile 的区别是什么？"></a>synchronized 和 volatile 的区别是什么？</h4><p>synchronized 表示只有一个线程可以获取作用对象的锁，执行代码，阻塞其他线程。</p><p>volatile 表示变量在 CPU 的寄存器中是不确定的，必须从主存中读取。保证多线程环境下变量的可见性；禁止指令重排序。</p><h3 id="final"><a href="#final" class="headerlink" title="final"></a>final</h3><h4 id="什么是不可变对象，它对写并发应用有什么帮助？"><a href="#什么是不可变对象，它对写并发应用有什么帮助？" class="headerlink" title="什么是不可变对象，它对写并发应用有什么帮助？"></a>什么是不可变对象，它对写并发应用有什么帮助？</h4><p>不可变对象(Immutable Objects)即对象一旦被创建它的状态（对象的数据，也即对象属性值）就不能改变，反之即为可变对象(Mutable Objects)。</p><p>不可变对象保证了对象的内存可见性，对不可变对象的读取不需要进行额外的同步手段，提升了代码执行效率。</p><h2 id="Lock体系"><a href="#Lock体系" class="headerlink" title="Lock体系"></a>Lock体系</h2><h3 id="Lock简介与初识AQS"><a href="#Lock简介与初识AQS" class="headerlink" title="Lock简介与初识AQS"></a>Lock简介与初识AQS</h3><h4 id="Java-Concurrency-API-中的-Lock-接口-Lock-interface-是什么？对比同步它有什么优势？"><a href="#Java-Concurrency-API-中的-Lock-接口-Lock-interface-是什么？对比同步它有什么优势？" class="headerlink" title="Java Concurrency API 中的 Lock 接口(Lock interface)是什么？对比同步它有什么优势？"></a>Java Concurrency API 中的 Lock 接口(Lock interface)是什么？对比同步它有什么优势？</h4><p>Lock 接口比同步方法和同步块提供了更具扩展性的锁操作。他们允许更灵活的结构，可以具有完全不同的性质，并且可以支持多个相关类的条件对象。</p><p>它的优势有：</p><p>（1）可以使锁更公平</p><p>（2）可以使线程在等待锁的时候响应中断</p><p>（3）可以让线程尝试获取锁，并在无法获取锁的时候立即返回或者等待一段时间</p><p>（4）可以在不同的范围，以不同的顺序获取和释放锁</p><p>整体上来说 Lock 是 synchronized 的扩展版，Lock 提供了无条件的、可轮询的(tryLock 方法)、定时的(tryLock 带参方法)、可中断的(lockInterruptibly)、可多条件队列的(newCondition 方法)锁操作。另外 Lock 的实现类基本都支持非公平锁(默认)和公平锁，synchronized 只支持非公平锁，当然，在大部分情况下，非公平锁是高效的选择。</p><h4 id="乐观锁和悲观锁的理解及如何实现，有哪些实现方式？"><a href="#乐观锁和悲观锁的理解及如何实现，有哪些实现方式？" class="headerlink" title="乐观锁和悲观锁的理解及如何实现，有哪些实现方式？"></a>乐观锁和悲观锁的理解及如何实现，有哪些实现方式？</h4><p>悲观锁：总是假设最坏的情况，每次去拿数据的时候都认为别人会修改，所以每次在拿数据的时候都会上锁，这样别人想拿这个数据就会阻塞直到它拿到锁。传统的关系型数据库里边就用到了很多这种锁机制，比如行锁，表锁等，读锁，写锁等，都是在做操作之前先上锁。再比如 Java 里面的同步原语 synchronized 关键字的实现也是悲观锁。</p><p>乐观锁：顾名思义，就是很乐观，每次去拿数据的时候都认为别人不会修改，所以不会上锁，但是在更新的时候会判断一下在此期间别人有没有去更新这个数据，可以使用版本号等机制。乐观锁适用于多读的应用类型，这样可以提高吞吐量，像数据库提供的类似于 write_condition 机制，其实都是提供的乐观锁。在 Java中 java.util.concurrent.atomic 包下面的原子变量类就是使用了乐观锁的一种实现方式 CAS 实现的。</p><p>乐观锁的实现方式：</p><p>1、使用版本标识来确定读到的数据与提交时的数据是否一致。提交后修改版本标识，不一致时可以采取丢弃和再次尝试的策略。</p><p>2、java 中的 Compare and Swap 即 CAS ，当多个线程尝试使用 CAS 同时更新同一个变量时，只有其中一个线程能更新变量的值，而其它线程都失败，失败的线程并不会被挂起，而是被告知这次竞争中失败，并可以再次尝试。 CAS 操作中包含三个操作数 —— 需要读写的内存位置（V）、进行比较的预期原值（A）和拟写入的新值(B)。如果内存位置 V 的值与预期原值 A 相匹配，那么处理器会自动将该位置值更新为新值 B。否则处理器不做任何操作。</p><h4 id="什么是-CAS"><a href="#什么是-CAS" class="headerlink" title="什么是 CAS"></a>什么是 CAS</h4><p>CAS 是 compare and swap 的缩写，即我们所说的比较交换。</p><h4 id="CAS-的会产生什么问题？"><a href="#CAS-的会产生什么问题？" class="headerlink" title="CAS 的会产生什么问题？"></a>CAS 的会产生什么问题？</h4><p>1、ABA 问题：</p><p>比如说一个线程 one 从内存位置 V 中取出 A，这时候另一个线程 two 也从内存中取出 A，并且 two 进行了一些操作变成了 B，然后 two 又将 V 位置的数据变成 A，这时候线程 one 进行 CAS 操作发现内存中仍然是 A，然后 one 操作成功。尽管线程 one 的 CAS 操作成功，但可能存在潜藏的问题。从 Java1.5 开始 JDK 的 atomic包里提供了一个类 AtomicStampedReference 来解决 ABA 问题。</p><p>2、循环时间长开销大</p><p>3、只能保证一个共享变量的原子操作</p><h4 id="什么是死锁？"><a href="#什么是死锁？" class="headerlink" title="什么是死锁？"></a>什么是死锁？</h4><p>当线程 A 持有独占锁a，并尝试去获取独占锁 b 的同时，线程 B 持有独占锁 b，并尝试获取独占锁 a 的情况下，就会发生 AB 两个线程由于互相持有对方需要的锁，而发生的阻塞现象，我们称为死锁。</p><h4 id="产生死锁的条件是什么？怎么防止死锁？"><a href="#产生死锁的条件是什么？怎么防止死锁？" class="headerlink" title="产生死锁的条件是什么？怎么防止死锁？"></a>产生死锁的条件是什么？怎么防止死锁？</h4><p>产生死锁的必要条件：</p><p>1、互斥条件：所谓互斥就是进程在某一时间内独占资源。</p><p>2、请求与保持条件：一个进程因请求资源而阻塞时，对已获得的资源保持不放。</p><p>3、不剥夺条件：进程已获得资源，在末使用完之前，不能强行剥夺。</p><p>4、循环等待条件：若干进程之间形成一种头尾相接的循环等待资源关系。</p><p>这四个条件是死锁的必要条件，只要系统发生死锁，这些条件必然成立，而只要上述条件之 一不满足，就不会发生死锁。</p><p>理解了死锁的原因，尤其是产生死锁的四个必要条件，就可以最大可能地避免、预防和解除死锁。</p><p>防止死锁可以采用以下的方法：</p><ul><li>尽量使用 tryLock(long timeout, TimeUnit unit)的方法(ReentrantLock、ReentrantReadWriteLock)，设置超时时间，超时可以退出防止死锁。</li><li>尽量使用 Java. util. concurrent 并发类代替自己手写锁。</li><li>尽量降低锁的使用粒度，尽量不要几个功能用同一把锁。</li><li>尽量减少同步的代码块。</li></ul><h4 id="死锁与活锁的区别，死锁与饥饿的区别？"><a href="#死锁与活锁的区别，死锁与饥饿的区别？" class="headerlink" title="死锁与活锁的区别，死锁与饥饿的区别？"></a>死锁与活锁的区别，死锁与饥饿的区别？</h4><p>死锁：是指两个或两个以上的进程（或线程）在执行过程中，因争夺资源而造成的一种互相等待的现象，若无外力作用，它们都将无法推进下去。</p><p>活锁：任务或者执行者没有被阻塞，由于某些条件没有满足，导致一直重复尝试，失败，尝试，失败。</p><p>活锁和死锁的区别在于，处于活锁的实体是在不断的改变状态，这就是所谓的“活”， 而处于死锁的实体表现为等待；活锁有可能自行解开，死锁则不能。</p><p>饥饿：一个或者多个线程因为种种原因无法获得所需要的资源，导致一直无法执行的状态。</p><p>Java 中导致饥饿的原因：</p><p>1、高优先级线程吞噬所有的低优先级线程的 CPU 时间。</p><p>2、线程被永久堵塞在一个等待进入同步块的状态，因为其他线程总是能在它之前持续地对该同步块进行访问。</p><p>3、线程在等待一个本身也处于永久等待完成的对象(比如调用这个对象的 wait 方法)，因为其他线程总是被持续地获得唤醒。</p><h4 id="多线程锁的升级原理是什么？"><a href="#多线程锁的升级原理是什么？" class="headerlink" title="多线程锁的升级原理是什么？"></a>多线程锁的升级原理是什么？</h4><p>在Java中，锁共有4种状态，级别从低到高依次为：无状态锁，偏向锁，轻量级锁和重量级锁状态，这几个状态会随着竞争情况逐渐升级。锁可以升级但不能降级。</p><h3 id="AQS-AbstractQueuedSynchronizer-详解与源码分析"><a href="#AQS-AbstractQueuedSynchronizer-详解与源码分析" class="headerlink" title="AQS(AbstractQueuedSynchronizer)详解与源码分析"></a>AQS(AbstractQueuedSynchronizer)详解与源码分析</h3><h4 id="AQS-介绍"><a href="#AQS-介绍" class="headerlink" title="AQS 介绍"></a>AQS 介绍</h4><p>AQS是一个用来构建锁和同步器的框架，使用AQS能简单且高效地构造出应用广泛的大量的同步器，比如我们提到的ReentrantLock，Semaphore，其他的诸如ReentrantReadWriteLock，SynchronousQueue，FutureTask等等皆是基于AQS的。当然，我们自己也能利用AQS非常轻松容易地构造出符合我们自己需求的同步器。</p><h4 id="AQS-原理分析"><a href="#AQS-原理分析" class="headerlink" title="AQS 原理分析"></a>AQS 原理分析</h4><p>AQS 原理概览</p><p>AQS核心思想是，如果被请求的共享资源空闲，则将当前请求资源的线程设置为有效的工作线程，并且将共享资源设置为锁定状态。如果被请求的共享资源被占用，那么就需要一套线程阻塞等待以及被唤醒时锁分配的机制，这个机制AQS是用CLH队列锁实现的，即将暂时获取不到锁的线程加入到队列中。</p><p><img src="https://lixiangbetter.github.io/2020/06/21/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E7%AC%94%E8%AE%B0/aHR0cHM6Ly91c2VyLWdvbGQtY2RuLnhpdHUuaW8vMjAxOS8xMS8yNS8xNmVhMDQ3Njc4NGNkMzJi.jpeg" alt></p><p><strong>AQS 对资源的共享方式</strong></p><p>AQS定义两种资源共享方式</p><ul><li>Exclusive（独占）：只有一个线程能执行，如ReentrantLock。又可分为公平锁和非公平锁：<ul><li>公平锁：按照线程在队列中的排队顺序，先到者先拿到锁</li><li>非公平锁：当线程要获取锁时，无视队列顺序直接去抢锁，谁抢到就是谁的</li></ul></li><li>Share（共享）：多个线程可同时执行，如Semaphore/CountDownLatch。Semaphore、CountDownLatch、 CyclicBarrier、ReadWriteLock 我们都会在后面讲到。</li></ul><h3 id="ReentrantLock-重入锁-实现原理与公平锁非公平锁区别"><a href="#ReentrantLock-重入锁-实现原理与公平锁非公平锁区别" class="headerlink" title="ReentrantLock(重入锁)实现原理与公平锁非公平锁区别"></a>ReentrantLock(重入锁)实现原理与公平锁非公平锁区别</h3><h4 id="什么是可重入锁（ReentrantLock）？"><a href="#什么是可重入锁（ReentrantLock）？" class="headerlink" title="什么是可重入锁（ReentrantLock）？"></a>什么是可重入锁（ReentrantLock）？</h4><p>ReentrantLock重入锁，是实现Lock接口的一个类，也是在实际编程中使用频率很高的一个锁，支持重入性，表示能够对共享资源能够重复加锁，即当前线程获取该锁再次获取不会被阻塞。</p><p>ReentrantLock支持两种锁：<strong>公平锁</strong>和<strong>非公平锁</strong>。<strong>何谓公平性，是针对获取锁而言的，如果一个锁是公平的，那么锁的获取顺序就应该符合请求上的绝对时间顺序，满足FIFO</strong>。</p><h3 id="读写锁ReentrantReadWriteLock源码分析"><a href="#读写锁ReentrantReadWriteLock源码分析" class="headerlink" title="读写锁ReentrantReadWriteLock源码分析"></a>读写锁ReentrantReadWriteLock源码分析</h3><h4 id="ReadWriteLock-是什么"><a href="#ReadWriteLock-是什么" class="headerlink" title="ReadWriteLock 是什么"></a>ReadWriteLock 是什么</h4><p>ReentrantReadWriteLock 是 ReadWriteLock 接口的一个具体实现，实现了读写的分离，读锁是共享的，写锁是独占的，读和读之间不会互斥，读和写、写和读、写和写之间才会互斥，提升了读写的性能。</p><p>而读写锁有以下三个重要的特性：</p><p>（1）公平选择性：支持非公平（默认）和公平的锁获取方式，吞吐量还是非公平优于公平。</p><p>（2）重进入：读锁和写锁都支持线程重进入。</p><p>（3）锁降级：遵循获取写锁、获取读锁再释放写锁的次序，写锁能够降级成为读锁。</p><h2 id="并发容器"><a href="#并发容器" class="headerlink" title="并发容器"></a>并发容器</h2><h3 id="并发容器之ConcurrentHashMap详解-JDK1-8版本-与源码分析"><a href="#并发容器之ConcurrentHashMap详解-JDK1-8版本-与源码分析" class="headerlink" title="并发容器之ConcurrentHashMap详解(JDK1.8版本)与源码分析"></a>并发容器之ConcurrentHashMap详解(JDK1.8版本)与源码分析</h3><h4 id="什么是ConcurrentHashMap？"><a href="#什么是ConcurrentHashMap？" class="headerlink" title="什么是ConcurrentHashMap？"></a>什么是ConcurrentHashMap？</h4><p>JDK 1.6版本关键要素：</p><p>segment继承了ReentrantLock充当锁的角色，为每一个segment提供了线程安全的保障；</p><p>segment维护了哈希散列表的若干个桶，每个桶由HashEntry构成的链表。</p><p>JDK1.8后，ConcurrentHashMap抛弃了原有的Segment 分段锁，而采用了 CAS + synchronized 来保证并发安全性。</p><h4 id="Java-中-ConcurrentHashMap-的并发度是什么？"><a href="#Java-中-ConcurrentHashMap-的并发度是什么？" class="headerlink" title="Java 中 ConcurrentHashMap 的并发度是什么？"></a>Java 中 ConcurrentHashMap 的并发度是什么？</h4><p>ConcurrentHashMap 把实际 map 划分成若干部分来实现它的可扩展性和线程安全。这种划分是使用并发度获得的，它是 ConcurrentHashMap 类构造函数的一个可选参数，默认值为 16，这样在多线程情况下就能避免争用。</p><h4 id="什么是并发容器的实现？"><a href="#什么是并发容器的实现？" class="headerlink" title="什么是并发容器的实现？"></a>什么是并发容器的实现？</h4><p>并发容器使用了与同步容器完全不同的加锁策略来提供更高的并发性和伸缩性，例如在 ConcurrentHashMap 中采用了一种粒度更细的加锁机制，可以称为分段锁，在这种锁机制下，允许任意数量的读线程并发地访问 map，并且执行读操作的线程和写操作的线程也可以并发的访问 map，同时允许一定数量的写操作线程并发地修改 map，所以它可以在并发环境下实现更高的吞吐量。</p><h4 id="Java-中的同步集合与并发集合有什么区别？"><a href="#Java-中的同步集合与并发集合有什么区别？" class="headerlink" title="Java 中的同步集合与并发集合有什么区别？"></a>Java 中的同步集合与并发集合有什么区别？</h4><p>同步集合与并发集合都为多线程和并发提供了合适的线程安全的集合，不过并发集合的可扩展性更高。在 Java1.5 之前程序员们只有同步集合来用且在多线程并发的时候会导致争用，阻碍了系统的扩展性。Java5 介绍了并发集合像ConcurrentHashMap，不仅提供线程安全还用锁分离和内部分区等现代技术提高了可扩展性。</p><h4 id="SynchronizedMap-和-ConcurrentHashMap-有什么区别？"><a href="#SynchronizedMap-和-ConcurrentHashMap-有什么区别？" class="headerlink" title="SynchronizedMap 和 ConcurrentHashMap 有什么区别？"></a>SynchronizedMap 和 ConcurrentHashMap 有什么区别？</h4><p>SynchronizedMap 一次锁住整张表来保证线程安全，所以每次只能有一个线程来访为 map。</p><p>ConcurrentHashMap 使用分段锁来保证在多线程下的性能。</p><h3 id="并发容器之CopyOnWriteArrayList详解"><a href="#并发容器之CopyOnWriteArrayList详解" class="headerlink" title="并发容器之CopyOnWriteArrayList详解"></a>并发容器之CopyOnWriteArrayList详解</h3><h4 id="CopyOnWriteArrayList-是什么，可以用于什么应用场景？有哪些优缺点？"><a href="#CopyOnWriteArrayList-是什么，可以用于什么应用场景？有哪些优缺点？" class="headerlink" title="CopyOnWriteArrayList 是什么，可以用于什么应用场景？有哪些优缺点？"></a>CopyOnWriteArrayList 是什么，可以用于什么应用场景？有哪些优缺点？</h4><p>CopyOnWriteArrayList 的使用场景</p><p>通过源码分析，我们看出它的优缺点比较明显，所以使用场景也就比较明显。就是合适读多写少的场景。</p><h3 id="并发容器之ThreadLocal详解"><a href="#并发容器之ThreadLocal详解" class="headerlink" title="并发容器之ThreadLocal详解"></a>并发容器之ThreadLocal详解</h3><h4 id="ThreadLocal-是什么？有哪些使用场景？"><a href="#ThreadLocal-是什么？有哪些使用场景？" class="headerlink" title="ThreadLocal 是什么？有哪些使用场景？"></a>ThreadLocal 是什么？有哪些使用场景？</h4><p>ThreadLocal 是一个本地线程副本变量工具类，在每个线程中都创建了一个 ThreadLocalMap 对象，简单说 ThreadLocal 就是一种以空间换时间的做法，每个线程可以访问自己内部 ThreadLocalMap 对象内的 value。通过这种方式，避免资源在多线程间共享。</p><h4 id="什么是线程局部变量？"><a href="#什么是线程局部变量？" class="headerlink" title="什么是线程局部变量？"></a>什么是线程局部变量？</h4><p>线程局部变量是局限于线程内部的变量，属于线程自身所有，不在多个线程间共享。</p><h3 id="ThreadLocal内存泄漏分析与解决方案"><a href="#ThreadLocal内存泄漏分析与解决方案" class="headerlink" title="ThreadLocal内存泄漏分析与解决方案"></a>ThreadLocal内存泄漏分析与解决方案</h3><h4 id="ThreadLocal造成内存泄漏的原因？"><a href="#ThreadLocal造成内存泄漏的原因？" class="headerlink" title="ThreadLocal造成内存泄漏的原因？"></a>ThreadLocal造成内存泄漏的原因？</h4><p>ThreadLocalMap实现中已经考虑了这种情况，在调用 <code>set()</code>、<code>get()</code>、<code>remove()</code> 方法的时候，会清理掉 key 为 null 的记录。使用完 <code>ThreadLocal</code>方法后 最好手动调用<code>remove()</code>方法</p><h4 id="ThreadLocal内存泄漏解决方案？"><a href="#ThreadLocal内存泄漏解决方案？" class="headerlink" title="ThreadLocal内存泄漏解决方案？"></a>ThreadLocal内存泄漏解决方案？</h4><ul><li>每次使用完ThreadLocal，都调用它的remove()方法，清除数据。</li><li>在使用线程池的情况下，没有及时清理ThreadLocal，不仅是内存泄漏的问题，更严重的是可能导致业务逻辑出现问题。所以，使用ThreadLocal就跟加锁完要解锁一样，用完就清理。</li></ul><h3 id="并发容器之BlockingQueue详解"><a href="#并发容器之BlockingQueue详解" class="headerlink" title="并发容器之BlockingQueue详解"></a>并发容器之BlockingQueue详解</h3><h4 id="什么是阻塞队列？阻塞队列的实现原理是什么？如何使用阻塞队列来实现生产者-消费者模型？"><a href="#什么是阻塞队列？阻塞队列的实现原理是什么？如何使用阻塞队列来实现生产者-消费者模型？" class="headerlink" title="什么是阻塞队列？阻塞队列的实现原理是什么？如何使用阻塞队列来实现生产者-消费者模型？"></a>什么是阻塞队列？阻塞队列的实现原理是什么？如何使用阻塞队列来实现生产者-消费者模型？</h4><p>阻塞队列（BlockingQueue）是一个支持两个附加操作的队列。</p><p>这两个附加的操作是：在队列为空时，获取元素的线程会等待队列变为非空。当队列满时，存储元素的线程会等待队列可用。</p><p>阻塞队列常用于生产者和消费者的场景，生产者是往队列里添加元素的线程，消费者是从队列里拿元素的线程。阻塞队列就是生产者存放元素的容器，而消费者也只从容器里拿元素。</p><p>阻塞队列使用最经典的场景就是 socket 客户端数据的读取和解析，读取数据的线程不断将数据放入队列，然后解析线程不断从队列取数据解析。</p><h3 id="并发容器之ConcurrentLinkedQueue详解与源码分析"><a href="#并发容器之ConcurrentLinkedQueue详解与源码分析" class="headerlink" title="并发容器之ConcurrentLinkedQueue详解与源码分析"></a>并发容器之ConcurrentLinkedQueue详解与源码分析</h3><h3 id="并发容器之ArrayBlockingQueue与LinkedBlockingQueue详解"><a href="#并发容器之ArrayBlockingQueue与LinkedBlockingQueue详解" class="headerlink" title="并发容器之ArrayBlockingQueue与LinkedBlockingQueue详解"></a>并发容器之ArrayBlockingQueue与LinkedBlockingQueue详解</h3><h2 id="线程池"><a href="#线程池" class="headerlink" title="线程池"></a>线程池</h2><h3 id="Executors类创建四种常见线程池"><a href="#Executors类创建四种常见线程池" class="headerlink" title="Executors类创建四种常见线程池"></a>Executors类创建四种常见线程池</h3><h4 id="什么是线程池？有哪几种创建方式？"><a href="#什么是线程池？有哪几种创建方式？" class="headerlink" title="什么是线程池？有哪几种创建方式？"></a>什么是线程池？有哪几种创建方式？</h4><p>池化技术相比大家已经屡见不鲜了，线程池、数据库连接池、Http 连接池等等都是对这个思想的应用。池化技术的思想主要是为了减少每次获取资源的消耗，提高对资源的利用率。</p><p>（1）newSingleThreadExecutor</p><p>（2）newFixedThreadPool</p><p>（3） newCachedThreadPool</p><p>（4）newScheduledThreadPoo</p><h4 id="线程池有什么优点？"><a href="#线程池有什么优点？" class="headerlink" title="线程池有什么优点？"></a>线程池有什么优点？</h4><ul><li><p>降低资源消耗：重用存在的线程，减少对象创建销毁的开销。</p></li><li><p>提高响应速度。可有效的控制最大并发线程数，提高系统资源的使用率，同时避免过多资源竞争，避免堵塞。当任务到达时，任务可以不需要的等到线程创建就能立即执行。</p></li><li><p>提高线程的可管理性。线程是稀缺资源，如果无限制的创建，不仅会消耗系统资源，还会降低系统的稳定性，使用线程池可以进行统一的分配，调优和监控。</p></li><li><p>附加功能：提供定时执行、定期执行、单线程、并发数控制等功能。</p></li></ul><h4 id="线程池都有哪些状态？"><a href="#线程池都有哪些状态？" class="headerlink" title="线程池都有哪些状态？"></a>线程池都有哪些状态？</h4><ul><li>RUNNING：这是最正常的状态，接受新的任务，处理等待队列中的任务。</li><li>SHUTDOWN：不接受新的任务提交，但是会继续处理等待队列中的任务。</li><li>STOP：不接受新的任务提交，不再处理等待队列中的任务，中断正在执行任务的线程。</li><li>TIDYING：所有的任务都销毁了，workCount 为 0，线程池的状态在转换为 TIDYING 状态时，会执行钩子方法 terminated()。</li><li>TERMINATED：terminated()方法结束后，线程池的状态就会变成这个。</li></ul><h4 id="什么是-Executor-框架？为什么使用-Executor-框架？"><a href="#什么是-Executor-框架？为什么使用-Executor-框架？" class="headerlink" title="什么是 Executor 框架？为什么使用 Executor 框架？"></a>什么是 Executor 框架？为什么使用 Executor 框架？</h4><p>Executor 框架是一个根据一组执行策略调用，调度，执行和控制的异步任务的框架。</p><p>每次执行任务创建线程 new Thread()比较消耗性能，创建一个线程是比较耗时、耗资源的，而且无限制的创建线程会引起应用程序内存溢出。</p><p>所以创建一个线程池是个更好的的解决方案，因为可以限制线程的数量并且可以回收再利用这些线程。利用Executors 框架可以非常方便的创建一个线程池。</p><h4 id="在-Java-中-Executor-和-Executors-的区别？"><a href="#在-Java-中-Executor-和-Executors-的区别？" class="headerlink" title="在 Java 中 Executor 和 Executors 的区别？"></a>在 Java 中 Executor 和 Executors 的区别？</h4><ul><li><p>Executors 工具类的不同方法按照我们的需求创建了不同的线程池，来满足业务的需求。</p></li><li><p>Executor 接口对象能执行我们的线程任务。</p></li><li><p>ExecutorService 接口继承了 Executor 接口并进行了扩展，提供了更多的方法我们能获得任务执行的状态并且可以获取任务的返回值。</p></li><li><p>使用 ThreadPoolExecutor 可以创建自定义线程池。</p></li><li><p>Future 表示异步计算的结果，他提供了检查计算是否完成的方法，以等待计算的完成，并可以使用 get()方法获取计算的结果。</p></li></ul><h4 id="线程池中-submit-和-execute-方法有什么区别？"><a href="#线程池中-submit-和-execute-方法有什么区别？" class="headerlink" title="线程池中 submit() 和 execute() 方法有什么区别？"></a>线程池中 submit() 和 execute() 方法有什么区别？</h4><p>接收参数：execute()只能执行 Runnable 类型的任务。submit()可以执行 Runnable 和 Callable 类型的任务。</p><p>返回值：submit()方法可以返回持有计算结果的 Future 对象，而execute()没有</p><p>异常处理：submit()方便Exception处理</p><h4 id="什么是线程组，为什么在-Java-中不推荐使用？"><a href="#什么是线程组，为什么在-Java-中不推荐使用？" class="headerlink" title="什么是线程组，为什么在 Java 中不推荐使用？"></a>什么是线程组，为什么在 Java 中不推荐使用？</h4><p>ThreadGroup 类，可以把线程归属到某一个线程组中，线程组中可以有线程对象，也可以有线程组，组中还可以有线程，这样的组织结构有点类似于树的形式。</p><p>线程组和线程池是两个不同的概念，他们的作用完全不同，前者是为了方便线程的管理，后者是为了管理线程的生命周期，复用线程，减少创建销毁线程的开销。</p><p>为什么不推荐使用线程组？因为使用有很多的安全隐患吧，没有具体追究，如果需要使用，推荐使用线程池。</p><h3 id="线程池之ThreadPoolExecutor详解"><a href="#线程池之ThreadPoolExecutor详解" class="headerlink" title="线程池之ThreadPoolExecutor详解"></a>线程池之ThreadPoolExecutor详解</h3><h4 id="Executors和ThreaPoolExecutor创建线程池的区别"><a href="#Executors和ThreaPoolExecutor创建线程池的区别" class="headerlink" title="Executors和ThreaPoolExecutor创建线程池的区别"></a>Executors和ThreaPoolExecutor创建线程池的区别</h4><p>Executors 各个方法的弊端：</p><ul><li>newFixedThreadPool 和 newSingleThreadExecutor:<br>主要问题是堆积的请求处理队列可能会耗费非常大的内存，甚至 OOM。</li><li>newCachedThreadPool 和 newScheduledThreadPool:<br>主要问题是线程数最大数是 Integer.MAX_VALUE，可能会创建数量非常多的线程，甚至 OOM。</li></ul><h4 id="你知道怎么创建线程池吗？"><a href="#你知道怎么创建线程池吗？" class="headerlink" title="你知道怎么创建线程池吗？"></a>你知道怎么创建线程池吗？</h4><p>创建线程池的方式有多种，这里你只需要答 ThreadPoolExecutor 即可。</p><p>ThreadPoolExecutor() 是最原始的线程池创建，也是阿里巴巴 Java 开发手册中明确规范的创建线程池的方式。</p><h4 id="ThreadPoolExecutor构造函数重要参数分析"><a href="#ThreadPoolExecutor构造函数重要参数分析" class="headerlink" title="ThreadPoolExecutor构造函数重要参数分析"></a>ThreadPoolExecutor构造函数重要参数分析</h4><p>ThreadPoolExecutor 3 个最重要的参数：</p><ul><li>corePoolSize ：核心线程数，线程数定义了最小可以同时运行的线程数量。</li><li>maximumPoolSize ：线程池中允许存在的工作线程的最大数量</li><li>workQueue：当新任务来的时候会先判断当前运行的线程数量是否达到核心线程数，如果达到的话，任务就会被存放在队列中。</li></ul><p>ThreadPoolExecutor其他常见参数:</p><ul><li>keepAliveTime：线程池中的线程数量大于 corePoolSize 的时候，如果这时没有新的任务提交，核心线程外的线程不会立即销毁，而是会等待，直到等待的时间超过了 keepAliveTime才会被回收销毁；</li><li>unit ：keepAliveTime 参数的时间单位。</li><li>threadFactory：为线程池提供创建新线程的线程工厂</li><li>handler ：线程池任务队列超过 maxinumPoolSize 之后的拒绝策略</li></ul><h4 id="ThreadPoolExecutor饱和策略"><a href="#ThreadPoolExecutor饱和策略" class="headerlink" title="ThreadPoolExecutor饱和策略"></a>ThreadPoolExecutor饱和策略</h4><p>线程池实现原理</p><p><img src="https://lixiangbetter.github.io/2020/06/21/%E5%B9%B6%E5%8F%91%E7%BC%96%E7%A8%8B%E7%AC%94%E8%AE%B0/aHR0cHM6Ly91c2VyLWdvbGQtY2RuLnhpdHUuaW8vMjAxOS8xMS8yNS8xNmVhMDQ3NjEyOTVlNzY2.jpeg" alt></p><h3 id="线程池之ScheduledThreadPoolExecutor详解"><a href="#线程池之ScheduledThreadPoolExecutor详解" class="headerlink" title="线程池之ScheduledThreadPoolExecutor详解"></a>线程池之ScheduledThreadPoolExecutor详解</h3><h3 id="FutureTask详解"><a href="#FutureTask详解" class="headerlink" title="FutureTask详解"></a>FutureTask详解</h3><h2 id="原子操作类"><a href="#原子操作类" class="headerlink" title="原子操作类"></a>原子操作类</h2><h4 id="什么是原子操作？在-Java-Concurrency-API-中有哪些原子类-atomic-classes-？"><a href="#什么是原子操作？在-Java-Concurrency-API-中有哪些原子类-atomic-classes-？" class="headerlink" title="什么是原子操作？在 Java Concurrency API 中有哪些原子类(atomic classes)？"></a>什么是原子操作？在 Java Concurrency API 中有哪些原子类(atomic classes)？</h4><p>原子操作（atomic operation）意为”不可被中断的一个或一系列操作” 。</p><p>在 Java 中可以通过锁和循环 CAS 的方式来实现原子操作。 </p><p>原子类：AtomicBoolean，AtomicInteger，AtomicLong，AtomicReference</p><p>原子数组：AtomicIntegerArray，AtomicLongArray，AtomicReferenceArray</p><p>原子属性更新器：AtomicLongFieldUpdater，AtomicIntegerFieldUpdater，AtomicReferenceFieldUpdater</p><p>解决 ABA 问题的原子类：AtomicMarkableReference（通过引入一个 boolean来反映中间有没有变过），AtomicStampedReference（通过引入一个 int 来累加来反映中间有没有变过）</p><h4 id="说一下-atomic-的原理？"><a href="#说一下-atomic-的原理？" class="headerlink" title="说一下 atomic 的原理？"></a>说一下 atomic 的原理？</h4><p>Atomic包中的类基本的特性就是在多线程环境下，当有多个线程同时对单个（包括基本类型及引用类型）变量进行操作时，具有排他性，即当多个线程同时对该变量的值进行更新时，仅有一个线程能成功，而未成功的线程可以向自旋锁一样，继续尝试，一直等到执行成功。</p><h2 id="并发工具"><a href="#并发工具" class="headerlink" title="并发工具"></a>并发工具</h2><h3 id="并发工具之CountDownLatch与CyclicBarrier"><a href="#并发工具之CountDownLatch与CyclicBarrier" class="headerlink" title="并发工具之CountDownLatch与CyclicBarrier"></a>并发工具之CountDownLatch与CyclicBarrier</h3><h4 id="在-Java-中-CycliBarriar-和-CountdownLatch-有什么区别？"><a href="#在-Java-中-CycliBarriar-和-CountdownLatch-有什么区别？" class="headerlink" title="在 Java 中 CycliBarriar 和 CountdownLatch 有什么区别？"></a>在 Java 中 CycliBarriar 和 CountdownLatch 有什么区别？</h4><p>CountDownLatch与CyclicBarrier都是用于控制并发的工具类，都可以理解成维护的就是一个计数器，但是这两者还是各有不同侧重点的：</p><ul><li><p>CountDownLatch一般用于某个线程A等待若干个其他线程执行完任务之后，它才执行；而CyclicBarrier一般用于一组线程互相等待至某个状态，然后这一组线程再同时执行；CountDownLatch强调一个线程等多个线程完成某件事情。CyclicBarrier是多个线程互等，等大家都完成，再携手共进。</p></li><li><p>调用CountDownLatch的countDown方法后，当前线程并不会阻塞，会继续往下执行；而调用CyclicBarrier的await方法，会阻塞当前线程，直到CyclicBarrier指定的线程全部都到达了指定点的时候，才能继续往下执行；</p></li><li><p>CountDownLatch方法比较少，操作比较简单，而CyclicBarrier提供的方法更多，比如能够通过getNumberWaiting()，isBroken()这些方法获取当前多个线程的状态，并且CyclicBarrier的构造方法可以传入barrierAction，指定当所有线程都到达时执行的业务功能；</p></li><li><p>CountDownLatch是不能复用的，而CyclicLatch是可以复用的。</p></li></ul><h3 id="并发工具之Semaphore与Exchanger"><a href="#并发工具之Semaphore与Exchanger" class="headerlink" title="并发工具之Semaphore与Exchanger"></a>并发工具之Semaphore与Exchanger</h3><h4 id="Semaphore-有什么作用"><a href="#Semaphore-有什么作用" class="headerlink" title="Semaphore 有什么作用"></a>Semaphore 有什么作用</h4><p>Semaphore 就是一个信号量，它的作用是限制某段代码块的并发数。Semaphore有一个构造函数，可以传入一个 int 型整数 n，表示某段代码最多只有 n 个线程可以访问，如果超出了 n，那么请等待，等到某个线程执行完毕这段代码块，下一个线程再进入。由此可以看出如果 Semaphore 构造函数中传入的 int 型整数 n=1，相当于变成了一个 synchronized 了。</p><p>Semaphore(信号量)-允许多个线程同时访问： synchronized 和 ReentrantLock 都是一次只允许一个线程访问某个资源，Semaphore(信号量)可以指定多个线程同时访问某个资源。</p><h4 id="什么是线程间交换数据的工具Exchanger"><a href="#什么是线程间交换数据的工具Exchanger" class="headerlink" title="什么是线程间交换数据的工具Exchanger"></a>什么是线程间交换数据的工具Exchanger</h4><p>Exchanger是一个用于线程间协作的工具类，用于两个线程间交换数据。它提供了一个交换的同步点，在这个同步点两个线程能够交换数据。交换数据是通过exchange方法来实现的，如果一个线程先执行exchange方法，那么它会同步等待另一个线程也执行exchange方法，这个时候两个线程就都达到了同步点，两个线程就可以交换数据。</p><h4 id="常用的并发工具类有哪些？"><a href="#常用的并发工具类有哪些？" class="headerlink" title="常用的并发工具类有哪些？"></a>常用的并发工具类有哪些？</h4><ul><li>Semaphore(信号量)-允许多个线程同时访问： synchronized 和 ReentrantLock 都是一次只允许一个线程访问某个资源，Semaphore(信号量)可以指定多个线程同时访问某个资源。</li><li>CountDownLatch(倒计时器)： CountDownLatch是一个同步工具类，用来协调多个线程之间的同步。这个工具通常用来控制线程等待，它可以让某一个线程等待直到倒计时结束，再开始执行。</li><li>CyclicBarrier(循环栅栏)： CyclicBarrier 和 CountDownLatch 非常类似，它也可以实现线程间的技术等待，但是它的功能比 CountDownLatch 更加复杂和强大。主要应用场景和 CountDownLatch 类似。CyclicBarrier 的字面意思是可循环使用（Cyclic）的屏障（Barrier）。它要做的事情是，让一组线程到达一个屏障（也可以叫同步点）时被阻塞，直到最后一个线程到达屏障时，屏障才会开门，所有被屏障拦截的线程才会继续干活。CyclicBarrier默认的构造方法是 CyclicBarrier(int parties)，其参数表示屏障拦截的线程数量，每个线程调用await()方法告诉 CyclicBarrier 我已经到达了屏障，然后当前线程被阻塞。</li></ul>]]></content>
      
      
      <categories>
          
          <category> java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> juc </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Java异常笔记</title>
      <link href="/2020/06/20/Java%E5%BC%82%E5%B8%B8%E7%AC%94%E8%AE%B0/"/>
      <url>/2020/06/20/Java%E5%BC%82%E5%B8%B8%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<h1 id="Java异常笔记"><a href="#Java异常笔记" class="headerlink" title="Java异常笔记"></a>Java异常笔记</h1><h2 id="Java异常架构与异常关键字"><a href="#Java异常架构与异常关键字" class="headerlink" title="Java异常架构与异常关键字"></a>Java异常架构与异常关键字</h2><h3 id="Java异常简介"><a href="#Java异常简介" class="headerlink" title="Java异常简介"></a>Java异常简介</h3><p>Java异常是Java提供的一种识别及响应错误的一致性机制。<br>Java异常机制可以使程序中异常处理代码和正常业务代码分离，保证程序代码更加优雅，并提高程序健壮性。在有效使用异常的情况下，异常能清晰的回答what, where, why这3个问题：异常类型回答了“什么”被抛出，异常堆栈跟踪回答了“在哪”抛出，异常信息回答了“为什么”会抛出。</p><h3 id="Java异常架构"><a href="#Java异常架构" class="headerlink" title="Java异常架构"></a>Java异常架构</h3><p><img src="https://lixiangbetter.github.io/2020/06/20/Java%E5%BC%82%E5%B8%B8%E7%AC%94%E8%AE%B0/20200314173417278.png" alt></p><h4 id="1-Throwable"><a href="#1-Throwable" class="headerlink" title="1. Throwable"></a>1. Throwable</h4><p>Throwable 是 Java 语言中所有错误与异常的超类。</p><p>Throwable 包含两个子类：Error（错误）和 Exception（异常），它们通常用于指示发生了异常情况。</p><p>Throwable 包含了其线程创建时线程执行堆栈的快照，它提供了 printStackTrace() 等接口用于获取堆栈跟踪数据等信息。</p><h4 id="2-Error（错误）"><a href="#2-Error（错误）" class="headerlink" title="2. Error（错误）"></a>2. Error（错误）</h4><p>定义：Error 类及其子类。程序中无法处理的错误，表示运行应用程序中出现了严重的错误。</p><p>特点：此类错误一般表示代码运行时 JVM 出现问题。通常有 Virtual MachineError（虚拟机运行错误）、NoClassDefFoundError（类定义错误）等。比如 OutOfMemoryError：内存不足错误；StackOverflowError：栈溢出错误。此类错误发生时，JVM 将终止线程。</p><p>这些错误是不受检异常，非代码性错误。因此，当此类错误发生时，应用程序不应该去处理此类错误。按照Java惯例，我们是不应该实现任何新的Error子类的！</p><h4 id="3-Exception（异常）"><a href="#3-Exception（异常）" class="headerlink" title="3. Exception（异常）"></a>3. Exception（异常）</h4><p>程序本身可以捕获并且可以处理的异常。Exception 这种异常又分为两类：运行时异常和编译时异常。</p><p>运行时异常<br>定义：RuntimeException 类及其子类，表示 JVM 在运行期间可能出现的异常。</p><p>特点：Java 编译器不会检查它。也就是说，当程序中可能出现这类异常时，倘若既”没有通过throws声明抛出它”，也”没有用try-catch语句捕获它”，还是会编译通过。比如NullPointerException空指针异常、ArrayIndexOutBoundException数组下标越界异常、ClassCastException类型转换异常、ArithmeticExecption算术异常。此类异常属于不受检异常，一般是由程序逻辑错误引起的，在程序中可以选择捕获处理，也可以不处理。虽然 Java 编译器不会检查运行时异常，但是我们也可以通过 throws 进行声明抛出，也可以通过 try-catch 对它进行捕获处理。如果产生运行时异常，则需要通过修改代码来进行避免。例如，若会发生除数为零的情况，则需要通过代码避免该情况的发生！</p><p>RuntimeException 异常会由 Java 虚拟机自动抛出并自动捕获（就算我们没写异常捕获语句运行时也会抛出错误！！），此类异常的出现绝大数情况是代码本身有问题应该从逻辑上去解决并改进代码。</p><h5 id="编译时异常"><a href="#编译时异常" class="headerlink" title="编译时异常"></a>编译时异常</h5><p>定义: Exception 中除 RuntimeException 及其子类之外的异常。</p><p>特点: Java 编译器会检查它。如果程序中出现此类异常，比如 ClassNotFoundException（没有找到指定的类异常），IOException（IO流异常），要么通过throws进行声明抛出，要么通过try-catch进行捕获处理，否则不能通过编译。在程序中，通常不会自定义该类异常，而是直接使用系统提供的异常类。该异常我们必须手动在代码里添加捕获语句来处理该异常。</p><h4 id="4-受检异常与非受检异常"><a href="#4-受检异常与非受检异常" class="headerlink" title="4. 受检异常与非受检异常"></a>4. 受检异常与非受检异常</h4><p>Java 的所有异常可以分为受检异常（checked exception）和非受检异常（unchecked exception）。</p><p>受检异常<br>编译器要求必须处理的异常。正确的程序在运行过程中，经常容易出现的、符合预期的异常情况。一旦发生此类异常，就必须采用某种方式进行处理。除 RuntimeException 及其子类外，其他的 Exception 异常都属于受检异常。编译器会检查此类异常，也就是说当编译器检查到应用中的某处可能会此类异常时，将会提示你处理本异常——要么使用try-catch捕获，要么使用方法签名中用 throws 关键字抛出，否则编译不通过。</p><p>非受检异常<br>编译器不会进行检查并且不要求必须处理的异常，也就说当程序中出现此类异常时，即使我们没有try-catch捕获它，也没有使用throws抛出该异常，编译也会正常通过。该类异常包括运行时异常（RuntimeException极其子类）和错误（Error）。</p><h3 id="Java异常关键字"><a href="#Java异常关键字" class="headerlink" title="Java异常关键字"></a>Java异常关键字</h3><ul><li>try – 用于监听。将要被监听的代码(可能抛出异常的代码)放在try语句块之内，当try语句块内发生异常时，异常就被抛出。</li><li>catch – 用于捕获异常。catch用来捕获try语句块中发生的异常。</li><li>finally – finally语句块总是会被执行。它主要用于回收在try块里打开的物力资源(如数据库连接、网络连接和磁盘文件)。只有finally块，执行完成之后，才会回来执行try或者catch块中的return或者throw语句，如果finally中使用了return或者throw等终止方法的语句，则就不会跳回执行，直接停止。</li><li>throw – 用于抛出异常。</li><li>throws – 用在方法签名中，用于声明该方法可能抛出的异常。</li></ul><h2 id="Java异常处理"><a href="#Java异常处理" class="headerlink" title="Java异常处理"></a>Java异常处理</h2><p><img src="https://lixiangbetter.github.io/2020/06/20/Java%E5%BC%82%E5%B8%B8%E7%AC%94%E8%AE%B0/aHR0cHM6Ly91c2VyLWdvbGQtY2RuLnhpdHUuaW8vMjAxOS8xMS8xMC8xNmU1NWYyYzMyMWQ5MDlk.jpeg" alt></p><p>Java 通过面向对象的方法进行异常处理，一旦方法抛出异常，系统自动根据该异常对象寻找合适异常处理器（Exception Handler）来处理该异常，把各种不同的异常进行分类，并提供了良好的接口。在 Java 中，每个异常都是一个对象，它是 Throwable 类或其子类的实例。当一个方法出现异常后便抛出一个异常对象，该对象中包含有异常信息，调用这个对象的方法可以捕获到这个异常并可以对其进行处理。Java 的异常处理是通过 5 个关键词来实现的：try、catch、throw、throws 和 finally。</p><p>在Java应用中，异常的处理机制分为声明异常，抛出异常和捕获异常。</p><h3 id="声明异常"><a href="#声明异常" class="headerlink" title="声明异常"></a>声明异常</h3><p>通常，应该捕获那些知道如何处理的异常，将不知道如何处理的异常继续传递下去。传递异常可以在方法签名处使用 throws 关键字声明可能会抛出的异常。</p><p>注意</p><ul><li>非检查异常（Error、RuntimeException 或它们的子类）不可使用 throws 关键字来声明要抛出的异常。</li><li>一个方法出现编译时异常，就需要 try-catch/ throws 处理，否则会导致编译错误。</li></ul><h3 id="抛出异常"><a href="#抛出异常" class="headerlink" title="抛出异常"></a>抛出异常</h3><p>如果你觉得解决不了某些异常问题，且不需要调用者处理，那么你可以抛出异常。</p><p>throw关键字作用是在方法内部抛出一个<code>Throwable</code>类型的异常。任何Java代码都可以通过throw语句抛出异常。</p><h3 id="捕获异常"><a href="#捕获异常" class="headerlink" title="捕获异常"></a>捕获异常</h3><p>程序通常在运行之前不报错，但是运行后可能会出现某些未知的错误，但是还不想直接抛出到上一级，那么就需要通过try…catch…的形式进行异常捕获，之后根据不同的异常情况来进行相应的处理。</p><h3 id="如何选择异常类型"><a href="#如何选择异常类型" class="headerlink" title="如何选择异常类型"></a>如何选择异常类型</h3><p><img src="https://lixiangbetter.github.io/2020/06/20/Java%E5%BC%82%E5%B8%B8%E7%AC%94%E8%AE%B0/20200314173209267.png" alt></p><h3 id="常见异常处理方式"><a href="#常见异常处理方式" class="headerlink" title="常见异常处理方式"></a>常见异常处理方式</h3><h4 id="直接抛出异常"><a href="#直接抛出异常" class="headerlink" title="直接抛出异常"></a>直接抛出异常</h4><p>通常，应该捕获那些知道如何处理的异常，将不知道如何处理的异常继续传递下去。传递异常可以在方法签名处使用 <strong>throws</strong> 关键字声明可能会抛出的异常。</p><h4 id="封装异常再抛出"><a href="#封装异常再抛出" class="headerlink" title="封装异常再抛出"></a>封装异常再抛出</h4><p>有时我们会从 catch 中抛出一个异常，目的是为了改变异常的类型。多用于在多系统集成时，当某个子系统故障，异常类型可能有多种，可以用统一的异常类型向外暴露，不需暴露太多内部异常细节。</p><h4 id="捕获异常-1"><a href="#捕获异常-1" class="headerlink" title="捕获异常"></a>捕获异常</h4><p>在一个 try-catch 语句块中可以捕获多个异常类型，并对不同类型的异常做出不同的处理</p><p>同一个 catch 也可以捕获多种类型异常，用 | 隔开</p><h4 id="自定义异常"><a href="#自定义异常" class="headerlink" title="自定义异常"></a>自定义异常</h4><h4 id="try-with-resource"><a href="#try-with-resource" class="headerlink" title="try-with-resource"></a>try-with-resource</h4><p>JAVA 7 提供了更优雅的方式来实现资源的自动释放，自动释放的资源需要是实现了 AutoCloseable 接口的类。</p><h2 id="Java异常常见面试题"><a href="#Java异常常见面试题" class="headerlink" title="Java异常常见面试题"></a>Java异常常见面试题</h2><h3 id="1-Error-和-Exception-区别是什么？"><a href="#1-Error-和-Exception-区别是什么？" class="headerlink" title="1. Error 和 Exception 区别是什么？"></a>1. Error 和 Exception 区别是什么？</h3><p>Error 类型的错误通常为虚拟机相关错误，如系统崩溃，内存不足，堆栈溢出等，编译器不会对这类错误进行检测，JAVA 应用程序也不应对这类错误进行捕获，一旦这类错误发生，通常应用程序会被终止，仅靠应用程序本身无法恢复；</p><p>Exception 类的错误是可以在应用程序中进行捕获并处理的，通常遇到这种错误，应对其进行处理，使应用程序可以继续正常运行。</p><h3 id="2-运行时异常和一般异常-受检异常-区别是什么？"><a href="#2-运行时异常和一般异常-受检异常-区别是什么？" class="headerlink" title="2. 运行时异常和一般异常(受检异常)区别是什么？"></a>2. 运行时异常和一般异常(受检异常)区别是什么？</h3><p>运行时异常包括 RuntimeException 类及其子类，表示 JVM 在运行期间可能出现的异常。 Java 编译器不会检查运行时异常。</p><p>受检异常是Exception 中除 RuntimeException 及其子类之外的异常。 Java 编译器会检查受检异常。</p><p>RuntimeException异常和受检异常之间的区别：是否强制要求调用者必须处理此异常，如果强制要求调用者必须进行处理，那么就使用受检异常，否则就选择非受检异常(RuntimeException)。一般来讲，如果没有特殊的要求，我们建议使用RuntimeException异常。</p><h3 id="3-JVM-是如何处理异常的？"><a href="#3-JVM-是如何处理异常的？" class="headerlink" title="3. JVM 是如何处理异常的？"></a>3. JVM 是如何处理异常的？</h3><p>在一个方法中如果发生异常，这个方法会创建一个异常对象，并转交给 JVM，该异常对象包含异常名称，异常描述以及异常发生时应用程序的状态。创建异常对象并转交给 JVM 的过程称为抛出异常。可能有一系列的方法调用，最终才进入抛出异常的方法，这一系列方法调用的有序列表叫做调用栈。</p><p>JVM 会顺着调用栈去查找看是否有可以处理异常的代码，如果有，则调用异常处理代码。当 JVM 发现可以处理异常的代码时，会把发生的异常传递给它。如果 JVM 没有找到可以处理该异常的代码块，JVM 就会将该异常转交给默认的异常处理器（默认处理器为 JVM 的一部分），默认异常处理器打印出异常信息并终止应用程序。</p><h3 id="4-throw-和-throws-的区别是什么？"><a href="#4-throw-和-throws-的区别是什么？" class="headerlink" title="4. throw 和 throws 的区别是什么？"></a>4. throw 和 throws 的区别是什么？</h3><p>throws 关键字在方法上声明该方法要拋出的异常，或者在方法内部通过 throw 拋出异常对象。</p><p>throws 关键字和 throw 关键字在使用上的几点区别如下：</p><ul><li>throw 关键字用在方法内部，只能用于抛出一种异常，用来抛出方法或代码块中的异常，受查异常和非受查异常都可以被抛出。</li><li>throws 关键字用在方法声明上，可以抛出多个异常，用来标识该方法可能抛出的异常列表。一个方法用 throws 标识了可能抛出的异常列表，调用该方法的方法中必须包含可处理异常的代码，否则也要在方法签名中用 throws 关键字声明相应的异常。</li></ul><h3 id="5-final、finally、finalize-有什么区别？"><a href="#5-final、finally、finalize-有什么区别？" class="headerlink" title="5. final、finally、finalize 有什么区别？"></a>5. final、finally、finalize 有什么区别？</h3><ul><li>final可以修饰类、变量、方法，修饰类表示该类不能被继承、修饰方法表示该方法不能被重写、修饰变量表示该变量是一个常量不能被重新赋值。</li><li>finally一般作用在try-catch代码块中，在处理异常的时候，通常我们将一定要执行的代码方法finally代码块中，表示不管是否出现异常，该代码块都会执行，一般用来存放一些关闭资源的代码。</li><li>finalize是一个方法，属于Object类的一个方法，而Object类是所有类的父类，Java 中允许使用 finalize()方法在垃圾收集器将对象从内存中清除出去之前做必要的清理工作。</li></ul><h3 id="6-NoClassDefFoundError-和-ClassNotFoundException-区别？"><a href="#6-NoClassDefFoundError-和-ClassNotFoundException-区别？" class="headerlink" title="6. NoClassDefFoundError 和 ClassNotFoundException 区别？"></a>6. NoClassDefFoundError 和 ClassNotFoundException 区别？</h3><p>NoClassDefFoundError 是一个 Error 类型的异常，是由 JVM 引起的，不应该尝试捕获这个异常。</p><p>引起该异常的原因是 JVM 或 ClassLoader 尝试加载某类时在内存中找不到该类的定义，该动作发生在运行期间，即编译时该类存在，但是在运行时却找不到了，可能是变异后被删除了等原因导致；</p><p>ClassNotFoundException 是一个受查异常，需要显式地使用 try-catch 对其进行捕获和处理，或在方法签名中用 throws 关键字进行声明。当使用 Class.forName, ClassLoader.loadClass 或 ClassLoader.findSystemClass 动态加载类到内存的时候，通过传入的类路径参数没有找到该类，就会抛出该异常；另一种抛出该异常的可能原因是某个类已经由一个类加载器加载至内存中，另一个加载器又尝试去加载它。</p><h3 id="7-try-catch-finally-中哪个部分可以省略？"><a href="#7-try-catch-finally-中哪个部分可以省略？" class="headerlink" title="7. try-catch-finally 中哪个部分可以省略？"></a>7. try-catch-finally 中哪个部分可以省略？</h3><p>catch 可以省略</p><h3 id="8-try-catch-finally-中，如果-catch-中-return-了，finally-还会执行吗？"><a href="#8-try-catch-finally-中，如果-catch-中-return-了，finally-还会执行吗？" class="headerlink" title="8. try-catch-finally 中，如果 catch 中 return 了，finally 还会执行吗？"></a>8. try-catch-finally 中，如果 catch 中 return 了，finally 还会执行吗？</h3><p>答：会执行，在 return 前执行。</p><h3 id="9-类-ExampleA-继承-Exception，类-ExampleB-继承ExampleA。"><a href="#9-类-ExampleA-继承-Exception，类-ExampleB-继承ExampleA。" class="headerlink" title="9. 类 ExampleA 继承 Exception，类 ExampleB 继承ExampleA。"></a>9. 类 ExampleA 继承 Exception，类 ExampleB 继承ExampleA。</h3><h3 id="10-常见的-RuntimeException-有哪些？"><a href="#10-常见的-RuntimeException-有哪些？" class="headerlink" title="10. 常见的 RuntimeException 有哪些？"></a>10. 常见的 RuntimeException 有哪些？</h3><p>ClassCastException(类转换异常)<br>IndexOutOfBoundsException(数组越界)<br>NullPointerException(空指针)<br>ArrayStoreException(数据存储异常，操作数组时类型不一致)<br>还有IO操作的BufferOverflowException异常</p><h3 id="11-Java常见异常有哪些"><a href="#11-Java常见异常有哪些" class="headerlink" title="11. Java常见异常有哪些"></a>11. Java常见异常有哪些</h3><p>java.lang.IllegalAccessError：违法访问错误</p><p>java.lang.InstantiationError：实例化错误</p><p>java.lang.OutOfMemoryError：内存不足错误</p><p>java.lang.StackOverflowError：堆栈溢出错误</p><p>java.lang.ClassCastException：类转型异常</p><p>java.lang.ClassNotFoundException：找不到类异常</p><p>java.lang.ArithmeticException：算术条件异常</p><p>java.lang.ArrayIndexOutOfBoundsException：数组索引越界异常</p><p>java.lang.IndexOutOfBoundsException：索引越界异常</p><p>java.lang.InstantiationException：实例化异常</p><p>java.lang.NoSuchFieldException：属性不存在异常</p><p>java.lang.NoSuchMethodException：方法不存在异常</p><p>java.lang.NullPointerException：空指针异常</p><p>java.lang.NumberFormatException：数字格式异常</p><p>java.lang.StringIndexOutOfBoundsException：字符串索引越界异常</p><h2 id="Java异常处理最佳实践"><a href="#Java异常处理最佳实践" class="headerlink" title="Java异常处理最佳实践"></a>Java异常处理最佳实践</h2><h3 id="1-在-finally-块中清理资源或者使用-try-with-resource-语句"><a href="#1-在-finally-块中清理资源或者使用-try-with-resource-语句" class="headerlink" title="1. 在 finally 块中清理资源或者使用 try-with-resource 语句"></a>1. 在 finally 块中清理资源或者使用 try-with-resource 语句</h3><h3 id="2-优先明确的异常"><a href="#2-优先明确的异常" class="headerlink" title="2. 优先明确的异常"></a>2. 优先明确的异常</h3><h3 id="3-对异常进行文档说明"><a href="#3-对异常进行文档说明" class="headerlink" title="3. 对异常进行文档说明"></a>3. 对异常进行文档说明</h3><h3 id="4-使用描述性消息抛出异常"><a href="#4-使用描述性消息抛出异常" class="headerlink" title="4. 使用描述性消息抛出异常"></a>4. 使用描述性消息抛出异常</h3><h3 id="5-优先捕获最具体的异常"><a href="#5-优先捕获最具体的异常" class="headerlink" title="5. 优先捕获最具体的异常"></a>5. 优先捕获最具体的异常</h3><h3 id="6-不要捕获-Throwable-类"><a href="#6-不要捕获-Throwable-类" class="headerlink" title="6. 不要捕获 Throwable 类"></a>6. 不要捕获 Throwable 类</h3><h3 id="7-不要忽略异常"><a href="#7-不要忽略异常" class="headerlink" title="7. 不要忽略异常"></a>7. 不要忽略异常</h3><h3 id="8-不要记录并抛出异常"><a href="#8-不要记录并抛出异常" class="headerlink" title="8. 不要记录并抛出异常"></a>8. 不要记录并抛出异常</h3><h3 id="9-包装异常时不要抛弃原始的异常"><a href="#9-包装异常时不要抛弃原始的异常" class="headerlink" title="9. 包装异常时不要抛弃原始的异常"></a>9. 包装异常时不要抛弃原始的异常</h3><h3 id="10-不要使用异常控制程序的流程"><a href="#10-不要使用异常控制程序的流程" class="headerlink" title="10. 不要使用异常控制程序的流程"></a>10. 不要使用异常控制程序的流程</h3><h3 id="11-使用标准异常"><a href="#11-使用标准异常" class="headerlink" title="11. 使用标准异常"></a>11. 使用标准异常</h3><h3 id="12-异常会影响性能"><a href="#12-异常会影响性能" class="headerlink" title="12. 异常会影响性能"></a>12. 异常会影响性能</h3><h3 id="13-总结"><a href="#13-总结" class="headerlink" title="13. 总结"></a>13. 总结</h3><h3 id="异常处理-阿里巴巴Java开发手册"><a href="#异常处理-阿里巴巴Java开发手册" class="headerlink" title="异常处理-阿里巴巴Java开发手册"></a>异常处理-阿里巴巴Java开发手册</h3><ol><li>【强制】Java 类库中定义的可以通过预检查方式规避的RuntimeException异常不应该通过catch 的方式来处理，比如：NullPointerException，IndexOutOfBoundsException等等。 说明：无法通过预检查的异常除外，比如，在解析字符串形式的数字时，可能存在数字格式错误，不得不通过catch NumberFormatException来实现。 正例：if (obj != null) {…} 反例：try { obj.method(); } catch (NullPointerException e) {…}</li><li>【强制】异常不要用来做流程控制，条件控制。 说明：异常设计的初衷是解决程序运行中的各种意外情况，且异常的处理效率比条件判断方式要低很多。</li><li>【强制】catch时请分清稳定代码和非稳定代码，稳定代码指的是无论如何不会出错的代码。对于非稳定代码的catch尽可能进行区分异常类型，再做对应的异常处理。 说明：对大段代码进行try-catch，使程序无法根据不同的异常做出正确的应激反应，也不利于定位问题，这是一种不负责任的表现。 正例：用户注册的场景中，如果用户输入非法字符，或用户名称已存在，或用户输入密码过于简单，在程序上作出分门别类的判断，并提示给用户。</li><li>【强制】捕获异常是为了处理它，不要捕获了却什么都不处理而抛弃之，如果不想处理它，请将该异常抛给它的调用者。最外层的业务使用者，必须处理异常，将其转化为用户可以理解的内容。</li><li>【强制】有try块放到了事务代码中，catch异常后，如果需要回滚事务，一定要注意手动回滚事务。</li><li>【强制】finally块必须对资源对象、流对象进行关闭，有异常也要做try-catch。 说明：如果JDK7及以上，可以使用try-with-resources方式。</li><li>【强制】不要在finally块中使用return。 说明：try块中的return语句执行成功后，并不马上返回，而是继续执行finally块中的语句，如果此处存在return语句，则在此直接返回，无情丢弃掉try块中的返回点。</li><li>【强制】捕获异常与抛异常，必须是完全匹配，或者捕获异常是抛异常的父类。 说明：如果预期对方抛的是绣球，实际接到的是铅球，就会产生意外情况。</li><li>【强制】在调用RPC、二方包、或动态生成类的相关方法时，捕捉异常必须使用Throwable类来进行拦截。 说明：通过反射机制来调用方法，如果找不到方法，抛出NoSuchMethodException。什么情况会抛出NoSuchMethodError呢？二方包在类冲突时，仲裁机制可能导致引入非预期的版本使类的方法签名不匹配，或者在字节码修改框架（比如：ASM）动态创建或修改类时，修改了相应的方法签名。这些情况，即使代码编译期是正确的，但在代码运行期时，会抛出NoSuchMethodError。</li><li>【推荐】方法的返回值可以为null，不强制返回空集合，或者空对象等，必须添加注释充分说明什么情况下会返回null值。 说明：本手册明确防止NPE是调用者的责任。即使被调用方法返回空集合或者空对象，对调用者来说，也并非高枕无忧，必须考虑到远程调用失败、序列化失败、运行时异常等场景返回null的情况。</li><li>【推荐】防止NPE，是程序员的基本修养，注意NPE产生的场景： 1） 返回类型为基本数据类型，return包装数据类型的对象时，自动拆箱有可能产生NPE。 反例：public int f() { return Integer对象}， 如果为null，自动解箱抛NPE。 2） 数据库的查询结果可能为null。 3） 集合里的元素即使isNotEmpty，取出的数据元素也可能为null。 4） 远程调用返回对象时，一律要求进行空指针判断，防止NPE。 5） 对于Session中获取的数据，建议进行NPE检查，避免空指针。 6） 级联调用obj.getA().getB().getC()；一连串调用，易产生NPE。<br>正例：使用JDK8的Optional类来防止NPE问题。</li><li>【推荐】定义时区分unchecked / checked 异常，避免直接抛出new RuntimeException()，更不允许抛出Exception或者Throwable，应使用有业务含义的自定义异常。推荐业界已定义过的自定义异常，如：DAOException / ServiceException等。</li><li>【参考】对于公司外的http/api开放接口必须使用“错误码”；而应用内部推荐异常抛出；跨应用间RPC调用优先考虑使用Result方式，封装isSuccess()方法、“错误码”、“错误简短信息”。 说明：关于RPC方法返回方式使用Result方式的理由： 1）使用抛异常返回方式，调用方如果没有捕获到就会产生运行时错误。 2）如果不加栈信息，只是new自定义异常，加入自己的理解的error message，对于调用端解决问题的帮助不会太多。如果加了栈信息，在频繁调用出错的情况下，数据序列化和传输的性能损耗也是问题。</li><li>【参考】避免出现重复的代码（Don’t Repeat Yourself），即DRY原则。 说明：随意复制和粘贴代码，必然会导致代码的重复，在以后需要修改时，需要修改所有的副本，容易遗漏。必要时抽取共性方法，或者抽象公共类，甚至是组件化。 正例：一个类中有多个public方法，都需要进行数行相同的参数校验操作，这个时候请抽取：<br>private boolean checkParam(DTO dto) {…}</li></ol>]]></content>
      
      
      <categories>
          
          <category> java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> exception </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Java网络编程笔记</title>
      <link href="/2020/06/20/Java%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%E7%AC%94%E8%AE%B0/"/>
      <url>/2020/06/20/Java%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<h1 id="Java网络编程笔记"><a href="#Java网络编程笔记" class="headerlink" title="Java网络编程笔记"></a>Java网络编程笔记</h1><h2 id="计算机网络体系结构"><a href="#计算机网络体系结构" class="headerlink" title="计算机网络体系结构"></a>计算机网络体系结构</h2><h3 id="网络协议是什么？"><a href="#网络协议是什么？" class="headerlink" title="网络协议是什么？"></a>网络协议是什么？</h3><p>在计算机网络要做到有条不紊地交换数据，就必须遵守一些事先约定好的规则，比如交换数据的格式、是否需要发送一个应答信息。这些规则被称为网络协议。</p><h3 id="为什么要对网络协议分层？"><a href="#为什么要对网络协议分层？" class="headerlink" title="为什么要对网络协议分层？"></a>为什么要对网络协议分层？</h3><ul><li>简化问题难度和复杂度。由于各层之间独立，我们可以分割大问题为小问题。</li><li>灵活性好。当其中一层的技术变化时，只要层间接口关系保持不变，其他层不受影响。</li><li>易于实现和维护。</li><li>促进标准化工作。分开后，每层功能可以相对简单地被描述。</li></ul><p><img src="https://lixiangbetter.github.io/2020/06/20/Java%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%E7%AC%94%E8%AE%B0/20200316173310511.png" alt></p><h2 id="TCP-IP-协议族"><a href="#TCP-IP-协议族" class="headerlink" title="TCP/IP 协议族"></a>TCP/IP 协议族</h2><h3 id="应用层"><a href="#应用层" class="headerlink" title="应用层"></a>应用层</h3><p>应用层( application-layer ）的任务是通过应用进程间的交互来完成特定网络应用。应用层协议定义的是应用进程（进程：主机中正在运行的程序）间的通信和交互的规则。</p><p>对于不同的网络应用需要不同的应用层协议。在互联网中应用层协议很多，如域名系统 DNS，支持万维网应用的 HTTP 协议，支持电子邮件的 SMTP 协议等等。</p><h3 id="运输层"><a href="#运输层" class="headerlink" title="运输层"></a>运输层</h3><p>运输层(transport layer)的主要任务就是负责向两台主机进程之间的通信提供通用的数据传输服务。应用进程利用该服务传送应用层报文。</p><p>运输层主要使用一下两种协议</p><p>传输控制协议-TCP：提供面向连接的，可靠的数据传输服务。<br>用户数据协议-UDP：提供无连接的，尽最大努力的数据传输服务（不保证数据传输的可靠性）。</p><h3 id="网络层"><a href="#网络层" class="headerlink" title="网络层"></a>网络层</h3><p>网络层的任务就是选择合适的网间路由和交换结点，确保计算机通信的数据及时传送。在发送数据时，网络层把运输层产生的报文段或用户数据报封装成分组和包进行传送。在 TCP/IP 体系结构中，由于网络层使用 IP 协议，因此分组也叫 IP 数据报 ，简称数据报。</p><p>互联网是由大量的异构（heterogeneous）网络通过路由器（router）相互连接起来的。互联网使用的网络层协议是无连接的网际协议（Intert Prococol）和许多路由选择协议，因此互联网的网络层也叫做网际层或 IP 层。</p><h3 id="数据链路层"><a href="#数据链路层" class="headerlink" title="数据链路层"></a>数据链路层</h3><p>数据链路层(data link layer)通常简称为链路层。两台主机之间的数据传输，总是在一段一段的链路上传送的，这就需要使用专门的链路层的协议。</p><p>在两个相邻节点之间传送数据时，数据链路层将网络层交下来的 IP 数据报组装成帧，在两个相邻节点间的链路上传送帧。每一帧包括数据和必要的控制信息（如同步信息，地址信息，差错控制等）。</p><p>在接收数据时，控制信息使接收端能够知道一个帧从哪个比特开始和到哪个比特结束。</p><p><img src="https://lixiangbetter.github.io/2020/06/20/Java%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%E7%AC%94%E8%AE%B0/aHR0cHM6Ly91c2VyLWdvbGQtY2RuLnhpdHUuaW8vMjAxOS81LzkvMTZhOWM5Y2Q1MjNlMDU5OQ.jpeg" alt></p><h3 id="物理层"><a href="#物理层" class="headerlink" title="物理层"></a>物理层</h3><p>在物理层上所传送的数据单位是比特。 物理层(physical layer)的作用是实现相邻计算机节点之间比特流的透明传送，尽可能屏蔽掉具体传输介质和物理设备的差异。使其上面的数据链路层不必考虑网络的具体传输介质是什么。“透明传送比特流”表示经实际电路传送后的比特流没有发生变化，对传送的比特流来说，这个电路好像是看不见的。</p><h3 id="TCP-IP-协议族-1"><a href="#TCP-IP-协议族-1" class="headerlink" title="TCP/IP 协议族"></a>TCP/IP 协议族</h3><p><img src="https://lixiangbetter.github.io/2020/06/20/Java%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%E7%AC%94%E8%AE%B0/aHR0cHM6Ly91c2VyLWdvbGQtY2RuLnhpdHUuaW8vMjAxOS80LzcvMTY5ZjY5NjZjMjRhZjM0NQ.jpeg" alt></p><h2 id="TCP的三次握手四次挥手"><a href="#TCP的三次握手四次挥手" class="headerlink" title="TCP的三次握手四次挥手"></a>TCP的三次握手四次挥手</h2><h3 id="三次握手"><a href="#三次握手" class="headerlink" title="三次握手"></a>三次握手</h3><p>我让信使运输一份信件给对方，对方收到了，那么他就知道了我的发件能力和他的收件能力是可以的。</p><p>于是他给我回信，我若收到了，我便知我的发件能力和他的收件能力是可以的，并且他的发件能力和我的收件能力是可以。</p><p>然而此时他还不知道他的发件能力和我的收件能力到底可不可以，于是我最后回馈一次，他若收到了，他便清楚了他的发件能力和我的收件能力是可以的。</p><p><img src="https://lixiangbetter.github.io/2020/06/20/Java%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%E7%AC%94%E8%AE%B0/aHR0cHM6Ly91c2VyLWdvbGQtY2RuLnhpdHUuaW8vMjAyMC8xLzcvMTZmN2UwM2IxZWE1MDdlOA.jpeg" alt></p><h3 id="四次挥手"><a href="#四次挥手" class="headerlink" title="四次挥手"></a>四次挥手</h3><p><img src="https://lixiangbetter.github.io/2020/06/20/Java%E7%BD%91%E7%BB%9C%E7%BC%96%E7%A8%8B%E7%AC%94%E8%AE%B0/aHR0cHM6Ly91c2VyLWdvbGQtY2RuLnhpdHUuaW8vMjAyMC8xLzcvMTZmN2UwM2IyMWEwN2YwYw.jpeg" alt></p><p>第一次挥手：当客户端的数据都传输完成后，客户端向服务端发出连接释放报文(当然数据没发完时也可以发送连接释放报文并停止发送数据)，需要注意的是客户端发出FIN报文段后只是不能发数据了，但是还可以正常收数据；<br>第二次挥手：服务端收到客户端发的FIN报文后给客户端回复确认报文，此时服务端处于关闭等待状态，而不是立马给客户端发FIN报文，这个状态还要持续一段时间，因为服务端可能还有数据没发完。<br>第三次挥手：服务端将最后数据(比如50个字节)发送完毕后就向客户端发出连接释放报文，<br>第四次挥手：客户端收到服务端发的FIN报文后，向服务端发出确认报文，注意客户端发出确认报文后不是立马释放TCP连接，而是要经过2MSL(最长报文段寿命的2倍时长)后才释放TCP连接。而服务端一旦收到客户端发出的确认报文就会立马释放TCP连接，所以服务端结束TCP连接的时间要比客户端早一些。</p><h2 id="常见面试题"><a href="#常见面试题" class="headerlink" title="常见面试题"></a>常见面试题</h2><h3 id="为什么TCP连接的时候是3次？2次不可以吗？"><a href="#为什么TCP连接的时候是3次？2次不可以吗？" class="headerlink" title="为什么TCP连接的时候是3次？2次不可以吗？"></a>为什么TCP连接的时候是3次？2次不可以吗？</h3><p>因为需要考虑连接时丢包的问题，如果只握手2次，第二次握手时如果服务端发给客户端的确认报文段丢失，此时服务端已经准备好了收发数(可以理解服务端已经连接成功)据，而客户端一直没收到服务端的确认报文，所以客户端就不知道服务端是否已经准备好了(可以理解为客户端未连接成功)，这种情况下客户端不会给服务端发数据，也会忽略服务端发过来的数据。</p><p>如果是三次握手，即便发生丢包也不会有问题，比如如果第三次握手客户端发的确认ack报文丢失，服务端在一段时间内没有收到确认ack报文的话就会重新进行第二次握手，也就是服务端会重发SYN报文段，客户端收到重发的报文段后会再次给服务端发送确认ack报文。</p><h3 id="为什么TCP连接的时候是3次，关闭的时候却是4次？"><a href="#为什么TCP连接的时候是3次，关闭的时候却是4次？" class="headerlink" title="为什么TCP连接的时候是3次，关闭的时候却是4次？"></a>为什么TCP连接的时候是3次，关闭的时候却是4次？</h3><p>因为只有在客户端和服务端都没有数据要发送的时候才能断开TCP。而客户端发出FIN报文时只能保证客户端没有数据发了，服务端还有没有数据发客户端是不知道的。而服务端收到客户端的FIN报文后只能先回复客户端一个确认报文来告诉客户端我服务端已经收到你的FIN报文了，但我服务端还有一些数据没发完，等这些数据发完了服务端才能给客户端发FIN报文(所以不能一次性将确认报文和FIN报文发给客户端，就是这里多出来了一次)。</p><h3 id="为什么客户端发出第四次挥手的确认报文后要等2MSL的时间才能释放TCP连接？"><a href="#为什么客户端发出第四次挥手的确认报文后要等2MSL的时间才能释放TCP连接？" class="headerlink" title="为什么客户端发出第四次挥手的确认报文后要等2MSL的时间才能释放TCP连接？"></a>为什么客户端发出第四次挥手的确认报文后要等2MSL的时间才能释放TCP连接？</h3><p>这里同样是要考虑丢包的问题，如果第四次挥手的报文丢失，服务端没收到确认ack报文就会重发第三次挥手的报文，这样报文一去一回最长时间就是2MSL，所以需要等这么长时间来确认服务端确实已经收到了。</p><h3 id="如果已经建立了连接，但是客户端突然出现故障了怎么办？"><a href="#如果已经建立了连接，但是客户端突然出现故障了怎么办？" class="headerlink" title="如果已经建立了连接，但是客户端突然出现故障了怎么办？"></a>如果已经建立了连接，但是客户端突然出现故障了怎么办？</h3><p>TCP设有一个保活计时器，客户端如果出现故障，服务器不能一直等下去，白白浪费资源。服务器每收到一次客户端的请求后都会重新复位这个计时器，时间通常是设置为2小时，若两小时还没有收到客户端的任何数据，服务器就会发送一个探测报文段，以后每隔75秒钟发送一次。若一连发送10个探测报文仍然没反应，服务器就认为客户端出了故障，接着就关闭连接。</p><h3 id="什么是HTTP，HTTP-与-HTTPS-的区别"><a href="#什么是HTTP，HTTP-与-HTTPS-的区别" class="headerlink" title="什么是HTTP，HTTP 与 HTTPS 的区别"></a>什么是HTTP，HTTP 与 HTTPS 的区别</h3><p>HTTP 是一个在计算机世界里专门在两点之间传输文字、图片、音频、视频等超文本数据的约定和规范</p><p><strong>HTTPS是添加了加密和认证机制的 HTTP</strong></p><h3 id="常用HTTP状态码"><a href="#常用HTTP状态码" class="headerlink" title="常用HTTP状态码"></a>常用HTTP状态码</h3><ul><li>1XX    Informational（信息性状态码） 接受的请求正在处理</li><li>2XX    Success（成功状态码） 请求正常处理完毕</li><li>3XX    Redirection（重定向状态码） 需要进行附加操作以完成请求</li><li>4XX    Client Error（客户端错误状态码） 服务器无法处理请求</li><li>5XX    Server Error（服务器错误状态码） 服务器处理请求出错</li></ul><h3 id="GET和POST区别"><a href="#GET和POST区别" class="headerlink" title="GET和POST区别"></a>GET和POST区别</h3><ul><li>GET：从服务器上获取数据，也就是所谓的查，仅仅是获取服务器资源，不进行修改。</li><li>POST：向服务器提交数据，这就涉及到了数据的更新，也就是更改服务器的数据。</li><li>PUT：英文含义是放置，也就是向服务器新添加数据，就是所谓的增。</li><li>DELETE：从字面意思也能看出，这种方式就是删除服务器数据的过程。</li></ul><ol><li>Get是不安全的，因为在传输过程，数据被放在请求的URL中；Post的所有操作对用户来说都是不可见的。 但是这种做法也不时绝对的，大部分人的做法也是按照上面的说法来的，但是也可以在get请求加上 request body，给 post请求带上 URL 参数。</li><li>Get请求提交的url中的数据最多只能是2048字节，这个限制是浏览器或者服务器给添加的，http协议并没有对url长度进行限制，目的是为了保证服务器和浏览器能够正常运行，防止有人恶意发送请求。Post请求则没有大小限制。</li><li>Get限制Form表单的数据集的值必须为ASCII字符；而Post支持整个ISO10646字符集。</li><li>Get执行效率却比Post方法好。Get是form提交的默认方法。</li></ol><p>GET产生一个TCP数据包；POST产生两个TCP数据包。</p><p>对于GET方式的请求，浏览器会把http header和data一并发送出去，服务器响应200（返回数据）；</p><p>而对于POST，浏览器先发送header，服务器响应100 continue，浏览器再发送data，服务器响应200 ok（返回数据）。</p><h3 id="什么是对称加密与非对称加密"><a href="#什么是对称加密与非对称加密" class="headerlink" title="什么是对称加密与非对称加密"></a>什么是对称加密与非对称加密</h3><p>对称密钥加密是指加密和解密使用同一个密钥的方式，这种方式存在的最大问题就是密钥发送问题，即如何安全地将密钥发给对方；</p><p>而非对称加密是指使用一对非对称密钥，即公钥和私钥，公钥可以随意发布，但私钥只有自己知道。发送密文的一方使用对方的公钥进行加密处理，对方接收到加密信息后，使用自己的私钥进行解密。<br>由于非对称加密的方式不需要发送用来解密的私钥，所以可以保证安全性；但是和对称加密比起来，非常的慢</p><h3 id="什么是HTTP2"><a href="#什么是HTTP2" class="headerlink" title="什么是HTTP2"></a>什么是HTTP2</h3><p>HTTP2 可以提高了网页的性能。</p><p>在 HTTP1 中浏览器限制了同一个域名下的请求数量（Chrome 下一般是六个），当在请求很多资源的时候，由于队头阻塞当浏览器达到最大请求数量时，剩余的资源需等待当前的六个请求完成后才能发起请求。</p><p>HTTP2 中引入了多路复用的技术，这个技术可以只通过一个 TCP 连接就可以传输所有的请求数据。多路复用可以绕过浏览器限制同一个域名下的请求数量的问题，进而提高了网页的性能。</p><h3 id="Session、Cookie和Token的主要区别"><a href="#Session、Cookie和Token的主要区别" class="headerlink" title="Session、Cookie和Token的主要区别"></a>Session、Cookie和Token的主要区别</h3><p>什么是cookie</p><p>cookie是由Web服务器保存在用户浏览器上的小文件（key-value格式），包含用户相关的信息。</p><p>什么是session</p><p>session是依赖Cookie实现的。session是服务器端对象</p><p>session 是浏览器和服务器会话过程中，服务器分配的一块储存空间。服务器默认为浏览器在cookie中设置 sessionid，浏览器在向服务器请求过程中传输 cookie 包含 sessionid ，服务器根据 sessionid 获取出会话中存储的信息，然后确定会话的身份信息。</p><p>cookie与session区别</p><ul><li>存储位置与安全性：cookie客户端，安全性较差，session服务器，安全性相对更高；</li><li>存储空间：单个cookie保存的数据不能超过4K，很多浏览器都限制一个站点最多保存20个cookie，session无此限制</li><li>占用服务器资源：session一定时间内保存在服务器上，当访问增多，占用服务器性能，考虑到服务器性能方面，应当使用cookie。</li></ul><p>什么是Token</p><p>Token的引入：Token是在客户端频繁向服务端请求数据，服务端频繁的去数据库查询用户名和密码并进行对比，判断用户名和密码正确与否，并作出相应提示，在这样的背景下，Token便应运而生。</p><p>Token的定义：Token是服务端生成的一串字符串，以作客户端进行请求的一个令牌，当第一次登录后，服务器生成一个Token便将此Token返回给客户端，以后客户端只需带上这个Token前来请求数据即可，无需再次带上用户名和密码。</p><p>使用Token的目的：Token的目的是为了减轻服务器的压力，减少频繁的查询数据库，使服务器更加健壮。</p><p>Token 是在服务端产生的。如果前端使用用户名/密码向服务端请求认证，服务端认证成功，那么在服务端会返回 Token 给前端。前端可以在每次请求的时候带上 Token 证明自己的合法地位</p><p><strong>session与token区别</strong></p><ul><li>session机制存在服务器压力增大，CSRF跨站伪造请求攻击，扩展性不强等问题；</li><li>session存储在服务器端，token存储在客户端</li><li>token提供认证和授权功能，作为身份认证，token安全性比session好；</li><li>session这种会话存储方式方式只适用于客户端代码和服务端代码运行在同一台服务器上，token适用于项目级的前后端分离（前后端代码运行在不同的服务器下）</li></ul><h3 id="Servlet是线程安全的吗"><a href="#Servlet是线程安全的吗" class="headerlink" title="Servlet是线程安全的吗"></a>Servlet是线程安全的吗</h3><p><strong>Servlet不是线程安全的，多线程并发的读写会导致数据不同步的问题。</strong></p><h3 id="Servlet接口中有哪些方法及Servlet生命周期探秘"><a href="#Servlet接口中有哪些方法及Servlet生命周期探秘" class="headerlink" title="Servlet接口中有哪些方法及Servlet生命周期探秘"></a>Servlet接口中有哪些方法及Servlet生命周期探秘</h3><p><strong>Servlet</strong>主要负责接收用户请求<strong>HttpServletRequest</strong>，在<strong>doGet()</strong>，<strong>doPost()\</strong>中做相应的处理，并将回应*<em>HttpServletResponse*</em>反馈给用户。Servlet可以设置初始化参数，供Servlet内部使用。</p><p>Servlet接口定义了5个方法，其中前三个方法与Servlet生命周期相关：</p><ul><li>void init(ServletConfig config) throws ServletException</li><li>void service(ServletRequest req, ServletResponse resp) throws ServletException, java.io.IOException</li><li>void destory()</li><li>java.lang.String getServletInfo()</li><li>ServletConfig getServletConfig()</li></ul><p><strong>生命周期：</strong></p><p>Web容器加载Servlet并将其实例化后，Servlet生命周期开始，容器运行其init()方法进行Servlet的初始化；</p><p>请求到达时调用Servlet的service()方法，service()方法会根据需要调用与请求对应的doGet或doPost等方法；</p><p>当服务器关闭或项目被卸载时服务器会将Servlet实例销毁，此时会调用Servlet的destroy()方法。</p><p>init方法和destory方法只会执行一次，service方法客户端每次请求Servlet都会执行。</p><h3 id="如果客户端禁止-cookie-能实现-session-还能用吗？"><a href="#如果客户端禁止-cookie-能实现-session-还能用吗？" class="headerlink" title="如果客户端禁止 cookie 能实现 session 还能用吗？"></a>如果客户端禁止 cookie 能实现 session 还能用吗？</h3><p>Cookie 与 Session，一般认为是两个独立的东西，Session采用的是在服务器端保持状态的方案，而Cookie采用的是在客户端保持状态的方案。</p><p>但为什么禁用Cookie就不能得到Session呢？因为Session是用Session ID来确定当前对话所对应的服务器Session，而Session ID是通过Cookie来传递的，禁用Cookie相当于失去了Session ID，也就得不到Session了。</p><p>假定用户关闭Cookie的情况下使用Session，其实现途径有以下几种：</p><ul><li>手动通过URL传值、隐藏表单传递Session ID。</li><li>用文件、数据库等形式保存Session ID，在跨页过程中手动调用。</li></ul>]]></content>
      
      
      <categories>
          
          <category> java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 网络编程 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Java基础知识笔记</title>
      <link href="/2020/06/20/Java%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E7%AC%94%E8%AE%B0/"/>
      <url>/2020/06/20/Java%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<h1 id="Java基础知识笔记"><a href="#Java基础知识笔记" class="headerlink" title="Java基础知识笔记"></a>Java基础知识笔记</h1><h2 id="Java概述"><a href="#Java概述" class="headerlink" title="Java概述"></a>Java概述</h2><h3 id="何为编程"><a href="#何为编程" class="headerlink" title="何为编程"></a>何为编程</h3><p>编程就是让计算机为解决某个问题而使用某种程序设计语言编写程序代码，并最终得到结果的过程。</p><h3 id="什么是Java"><a href="#什么是Java" class="headerlink" title="什么是Java"></a>什么是Java</h3><p>Java 面向对象编程语言，吸收C++语言优点，还摒弃了C++难以理解的多继承、指针等概念，因此Java语言具有功能强大和简单易用两个特征。</p><h3 id="jdk1-5之后的三大版本"><a href="#jdk1-5之后的三大版本" class="headerlink" title="jdk1.5之后的三大版本"></a>jdk1.5之后的三大版本</h3><ul><li>Java SE（J2SE，Java 2 Platform Standard Edition，标准版）<br>Java SE 以前称为 J2SE。它允许开发和部署在桌面、服务器、嵌入式环境和实时环境中使用的 Java 应用程序。Java SE 为Java EE和Java ME提供基础。</li><li>Java EE（J2EE，Java 2 Platform Enterprise Edition，企业版）<br>Java EE 以前称为 J2EE。企业版本帮助开发和部署可移植、健壮、可伸缩且安全的服务器端Java 应用程序。Java EE 是在 Java SE 的基础上构建的，它提供 Web 服务、组件模型、管理和通信 API，可以用来实现企业级的面向服务体系结构（service-oriented architecture，SOA）和 Web2.0应用程序。2018年2月，Eclipse 宣布正式将 JavaEE 更名为 JakartaEE</li><li>Java ME（J2ME，Java 2 Platform Micro Edition，微型版）<br>Java ME 以前称为 J2ME。Java ME 为在移动设备和嵌入式设备（比如手机、PDA、电视机顶盒和打印机）上运行的应用程序提供一个健壮且灵活的环境。Java ME 包括灵活的用户界面、健壮的安全模型、许多内置的网络协议以及对可以动态下载的连网和离线应用程序的丰富支持。基于 Java ME 规范的应用程序只需编写一次，就可以用于许多设备，而且可以利用每个设备的本机功能。</li></ul><h3 id="JVM、JRE和JDK的关系"><a href="#JVM、JRE和JDK的关系" class="headerlink" title="JVM、JRE和JDK的关系"></a>JVM、JRE和JDK的关系</h3><p><img src="https://lixiangbetter.github.io/2020/06/20/Java%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E7%AC%94%E8%AE%B0/aHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL0pvdXJXb24vaW1hZ2UvbWFzdGVyL0phdmElRTclQUUlODAlRTQlQkIlOEIvSlZNJkpSRSZKREslRTUlODUlQjMlRTclQjMlQkIlRTUlOUIlQkUucG5n.jpeg" alt="not found"></p><h3 id="什么是跨平台性？原理是什么"><a href="#什么是跨平台性？原理是什么" class="headerlink" title="什么是跨平台性？原理是什么"></a>什么是跨平台性？原理是什么</h3><p>实现原理：Java程序是通过java虚拟机在系统平台上运行的，只要该系统可以安装相应的java虚拟机，该系统就可以运行java程序。</p><h3 id="Java语言有哪些特点简单易学（Java语言的语法与C语言和C-语言很接近）"><a href="#Java语言有哪些特点简单易学（Java语言的语法与C语言和C-语言很接近）" class="headerlink" title="Java语言有哪些特点简单易学（Java语言的语法与C语言和C++语言很接近）"></a>Java语言有哪些特点简单易学（Java语言的语法与C语言和C++语言很接近）</h3><p>面向对象（封装，继承，多态）</p><p>平台无关性（Java虚拟机实现平台无关性）</p><p>支持网络编程并且很方便（Java语言诞生本身就是为简化网络编程设计的）</p><p>支持多线程（多线程机制使应用程序在同一时间并行执行多项任）</p><p>健壮性（Java语言的强类型机制、异常处理、垃圾的自动收集等）</p><p>安全性</p><h3 id="什么是字节码？采用字节码的最大好处是什么"><a href="#什么是字节码？采用字节码的最大好处是什么" class="headerlink" title="什么是字节码？采用字节码的最大好处是什么"></a>什么是字节码？采用字节码的最大好处是什么</h3><p><strong>字节码</strong>：Java源代码经过虚拟机编译器编译后产生的文件（即扩展为.class的文件），它不面向任何特定的处理器，只面向虚拟机。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">Java源代码----&gt;编译器----&gt;jvm可执行的Java字节码(即虚拟指令)----&gt;jvm----&gt;jvm中解释器-----&gt;机器可执行的二进制机器码----&gt;程序运行。</span><br></pre></td></tr></table></figure><h3 id="什么是Java程序的主类？应用程序和小程序的主类有何不同？"><a href="#什么是Java程序的主类？应用程序和小程序的主类有何不同？" class="headerlink" title="什么是Java程序的主类？应用程序和小程序的主类有何不同？"></a>什么是Java程序的主类？应用程序和小程序的主类有何不同？</h3><p>一个主类。包含main()方法，不一定public；Java小程序中，主类继承自系统类JApplet或Applet，必须为public；主类是Java程序执行的入口点。</p><h3 id="Java和C-的区别"><a href="#Java和C-的区别" class="headerlink" title="Java和C++的区别"></a>Java和C++的区别</h3><ul><li>都是面向对象的语言，都支持封装、继承和多态</li><li>Java不提供指针来直接访问内存，程序内存更加安全</li><li>Java的类是单继承的，C++支持多重继承；虽然Java的类不可以多继承，但是接口可以多继承。</li><li>Java有自动内存管理机制，不需要程序员手动释放无用内存</li></ul><h3 id="Oracle-JDK-和-OpenJDK-的对比"><a href="#Oracle-JDK-和-OpenJDK-的对比" class="headerlink" title="Oracle JDK 和 OpenJDK 的对比"></a>Oracle JDK 和 OpenJDK 的对比</h3><ul><li>Oracle JDK每三年发布一次，OpenJDK每三个月发布一次；</li><li>OpenJDK 完全开源，而Oracle JDK是OpenJDK的一个实现，不是完全开源的；</li><li>Oracle JDK 比 OpenJDK 更稳定。O</li><li>在响应性和JVM性能方面，Oracle JDK与OpenJDK相比提供了更好的性能；</li><li>Oracle JDK不会为即将发布的版本提供长期支持，用户每次都必须通过更新到最新版本获得支持来获取最新版本；</li><li>Oracle JDK根据二进制代码许可协议获得许可，而OpenJDK根据GPL v2许可获得许可。</li></ul><h2 id="基础语法"><a href="#基础语法" class="headerlink" title="基础语法"></a>基础语法</h2><h3 id="数据类型"><a href="#数据类型" class="headerlink" title="数据类型"></a>数据类型</h3><h4 id="Java有哪些数据类型"><a href="#Java有哪些数据类型" class="headerlink" title="Java有哪些数据类型"></a>Java有哪些数据类型</h4><p>定义：Java语言是强类型语言，对于每一种数据都定义了明确的具体的数据类型，在内存中分配了不同大小的内存空间。</p><p>分类</p><ul><li>基本数据类型<ul><li>数值型<ul><li>整数类型(byte,short,int,long)</li><li>浮点类型(float,double)</li></ul></li><li>字符型(char)</li><li>布尔型(boolean)</li></ul></li><li>引用数据类型<ul><li>类(class)</li><li>接口(interface)</li><li>数组([])</li></ul></li></ul><h4 id="switch-是否能作用在-byte-上，是否能作用在-long-上，是否能作用在-String-上"><a href="#switch-是否能作用在-byte-上，是否能作用在-long-上，是否能作用在-String-上" class="headerlink" title="switch 是否能作用在 byte 上，是否能作用在 long 上，是否能作用在 String 上"></a>switch 是否能作用在 byte 上，是否能作用在 long 上，是否能作用在 String 上</h4><p>长整型（long）在目前所有的版本中都是不可以的。</p><h4 id="用最有效率的方法计算-2-乘以-8"><a href="#用最有效率的方法计算-2-乘以-8" class="headerlink" title="用最有效率的方法计算 2 乘以 8"></a>用最有效率的方法计算 2 乘以 8</h4><p>2 &lt;&lt; 3</p><h4 id="Math-round-11-5-等于多少？Math-round-11-5-等于多少"><a href="#Math-round-11-5-等于多少？Math-round-11-5-等于多少" class="headerlink" title="Math.round(11.5) 等于多少？Math.round(-11.5)等于多少"></a>Math.round(11.5) 等于多少？Math.round(-11.5)等于多少</h4><p>四舍五入的原理是在参数上加 0.5 然后进行下取整。</p><h4 id="float-f-3-4-是否正确"><a href="#float-f-3-4-是否正确" class="headerlink" title="float f=3.4;是否正确"></a>float f=3.4;是否正确</h4><p>不正确。精度损失；强制类型转换float f =(float)3.4; 或者写成 float f =3.4F；</p><h4 id="short-s1-1-s1-s1-1-有错吗-short-s1-1-s1-1-有错吗"><a href="#short-s1-1-s1-s1-1-有错吗-short-s1-1-s1-1-有错吗" class="headerlink" title="short s1 = 1; s1 = s1 + 1;有错吗?short s1 = 1; s1 += 1;有错吗"></a>short s1 = 1; s1 = s1 + 1;有错吗?short s1 = 1; s1 += 1;有错吗</h4><p>对于 short s1 = 1; s1 = s1 + 1;由于 1 是 int 类型，因此 s1+1 运算结果也是 int型，需要强制转换类型才能赋值给 short 型。</p><p>而 short s1 = 1; s1 += 1;可以正确编译，因为 s1+= 1;相当于 s1 = (short(s1 + 1);其中有隐含的强制类型转换。</p><h3 id="编码"><a href="#编码" class="headerlink" title="编码"></a>编码</h3><h4 id="Java语言采用何种编码方案？有何特点？"><a href="#Java语言采用何种编码方案？有何特点？" class="headerlink" title="Java语言采用何种编码方案？有何特点？"></a>Java语言采用何种编码方案？有何特点？</h4><p>Java语言采用Unicode编码标准，Unicode（标准码），它为每个字符制订了一个唯一的数值，因此在任何的语言，平台，程序都可以放心的使用。</p><h3 id="注释"><a href="#注释" class="headerlink" title="注释"></a>注释</h3><h4 id="什么Java注释"><a href="#什么Java注释" class="headerlink" title="什么Java注释"></a>什么Java注释</h4><p><strong>定义</strong>：用于解释说明程序的文字</p><h3 id="访问修饰符"><a href="#访问修饰符" class="headerlink" title="访问修饰符"></a>访问修饰符</h3><p><img src="https://lixiangbetter.github.io/2020/06/20/Java%E5%9F%BA%E7%A1%80%E7%9F%A5%E8%AF%86%E7%AC%94%E8%AE%B0/aHR0cHM6Ly9yYXcuZ2l0aHVidXNlcmNvbnRlbnQuY29tL0pvdXJXb24vaW1hZ2UvbWFzdGVyL0phdmElRTUlOUYlQkElRTclQTElODAlRTglQUYlQUQlRTYlQjMlOTUvSmF2YSVFOCVBRSVCRiVFOSU5NyVBRSVFNCVCRiVBRSVFOSVBNSVCMCVFNyVBQyVBNi5wbmc.jpeg" alt="not found"></p><h3 id="运算符"><a href="#运算符" class="headerlink" title="运算符"></a>运算符</h3><h4 id="amp-和-amp-amp-的区别"><a href="#amp-和-amp-amp-的区别" class="headerlink" title="&amp;和&amp;&amp;的区别"></a>&amp;和&amp;&amp;的区别</h4><p>&amp;运算符有两种用法：(1)按位与；(2)逻辑与。</p><p>&amp;&amp;运算符是短路与运算。</p><h3 id="关键字"><a href="#关键字" class="headerlink" title="关键字"></a>关键字</h3><h4 id="Java-有没有-goto"><a href="#Java-有没有-goto" class="headerlink" title="Java 有没有 goto"></a>Java 有没有 goto</h4><p>goto 是 Java 中的保留字，在目前版本的 Java 中没有使用。</p><h4 id="final-有什么用？"><a href="#final-有什么用？" class="headerlink" title="final 有什么用？"></a>final 有什么用？</h4><ul><li>被final修饰的类不可以被继承</li><li>被final修饰的方法不可以被重写</li><li>被final修饰的变量不可以被改变，被final修饰不可变的是变量的引用，而不是引用指向的内容，引用指向的内容是可以改变的</li></ul><h4 id="final-finally-finalize区别"><a href="#final-finally-finalize区别" class="headerlink" title="final finally finalize区别"></a>final finally finalize区别</h4><ul><li>final可以修饰类、变量、方法</li><li>finally处理异常的时候，将一定要执行的代码方法finally代码块</li><li>finalize是一个方法，属于Object类的一个方法，而Object类是所有类的父类，该方法一般由垃圾回收器来调<br>用，当我们调用System.gc() 方法的时候，由垃圾回收器调用finalize()，回收垃圾，一个对象是否可回收的<br>最后判断。</li></ul><h4 id="this关键字的用法"><a href="#this关键字的用法" class="headerlink" title="this关键字的用法"></a>this关键字的用法</h4><p>1.普通的直接引用，this相当于是指向当前对象本身。</p><p>2.形参与成员名字重名，用this来区分：</p><p>3.引用本类的构造函数</p><h4 id="super关键字的用法"><a href="#super关键字的用法" class="headerlink" title="super关键字的用法"></a>super关键字的用法</h4><p>1.普通的直接引用</p><p>2.子类成员变量或方法与父类中同名时，用super进行区分</p><p>3.引用父类构造函数</p><h4 id="this与super的区别"><a href="#this与super的区别" class="headerlink" title="this与super的区别"></a>this与super的区别</h4><ul><li>super:　它引用当前对象的直接父类中的成员</li><li>this：它代表当前对象名</li><li>super()在子类中调用父类的构造方法，this()在本类内调用本类的其它构造方</li><li>super()和this()均需放在构造方法内第一行。</li><li>尽管可以用this调用一个构造器，但却不能调用两个。</li><li>this和super不能同时出现在一个构造函数里面</li><li>this()和super()都指的是对象，所以，均不可以在static环境中使用。包括：static变量,static方法，static语句块。</li><li>从本质上讲，this是一个指向本对象的指针, 然而super是一个Java关键字。</li></ul><h4 id="static存在的主要意义"><a href="#static存在的主要意义" class="headerlink" title="static存在的主要意义"></a>static存在的主要意义</h4><p>static的主要意义是在于创建独立于具体对象的域变量或者方法。以致于即使没有创建对象，也能使用属性和调用方法。为什么说static块可以用来优化程序性能，是因为它的特性:<strong>只会在类加载的时候执行一次</strong>。将一些只需要进行一次的初始化操作都放在static代码块中进行。</p><h4 id="static的独特之处"><a href="#static的独特之处" class="headerlink" title="static的独特之处"></a>static的独特之处</h4><p>1、被static修饰的变量或者方法是独立于该类的任何对象，也就是说，这些变量和方法不属于任何一个实例对象，而是被类的实例对象所共享。</p><p>2、在该类被第一次加载的时候，就会去加载被static修饰的部分，而且只在类第一次使用时加载并进行初始化</p><p>3、static变量值在类加载的时候分配空间，以后创建类对象的时候不会重新分配。</p><p>4、被static修饰的变量或者方法是优先于对象存在的，也就是说当一个类加载完毕之后，即便没有创建对象，也可以去访问。</p><h4 id="static应用场景"><a href="#static应用场景" class="headerlink" title="static应用场景"></a>static应用场景</h4><p>因为static是被类的实例对象所共享，因此如果<strong>某个成员变量是被所有对象所共享的，那么这个成员变量就应该定义为静态变量</strong>。</p><p>1、修饰成员变量 2、修饰成员方法 3、静态代码块 4、修饰类【只能修饰内部类也就是静态内部类】 5、静态导包</p><h4 id="static注意事项"><a href="#static注意事项" class="headerlink" title="static注意事项"></a>static注意事项</h4><p>1、静态只能访问静态。 2、非静态既可以访问非静态的，也可以访问静态的。</p><h3 id="流程控制语句"><a href="#流程控制语句" class="headerlink" title="流程控制语句"></a>流程控制语句</h3><h4 id="break-continue-return-的区别及作用"><a href="#break-continue-return-的区别及作用" class="headerlink" title="break ,continue ,return 的区别及作用"></a>break ,continue ,return 的区别及作用</h4><p>break 结束当前的循环体</p><p>continue 结束正在执行的循环 进入下一个循环条件</p><p>return 结束当前的方法 直接返回</p><h4 id="在-Java-中，如何跳出当前的多重嵌套循环"><a href="#在-Java-中，如何跳出当前的多重嵌套循环" class="headerlink" title="在 Java 中，如何跳出当前的多重嵌套循环"></a>在 Java 中，如何跳出当前的多重嵌套循环</h4><p>标号+break 语句，即可跳出外层循环</p><h2 id="面向对象"><a href="#面向对象" class="headerlink" title="面向对象"></a>面向对象</h2><h3 id="面向对象概述"><a href="#面向对象概述" class="headerlink" title="面向对象概述"></a>面向对象概述</h3><h4 id="面向对象和面向过程的区别"><a href="#面向对象和面向过程的区别" class="headerlink" title="面向对象和面向过程的区别"></a>面向对象和面向过程的区别</h4><p>面向过程：</p><p>优点：性能比面向对象高，因为类调用时需要实例化，开销比较大，比较消耗资源;比如单片机、嵌入式开发、Linux/Unix等一般采用面向过程开发，性能是最重要的因素。</p><p>缺点：没有面向对象易维护、易复用、易扩展</p><p>面向对象：</p><p>优点：易维护、易复用、易扩展，由于面向对象有封装、继承、多态性的特性，可以设计出低耦合的系统，使系统更加灵活、更加易于维护</p><p>缺点：性能比面向过程低</p><p>面向对象的底层其实还是面向过程，把面向过程抽象成类，然后封装，方便我们使用的就是面向对象了。</p><h3 id="面向对象三大特性"><a href="#面向对象三大特性" class="headerlink" title="面向对象三大特性"></a>面向对象三大特性</h3><h4 id="面向对象的特征有哪些方面"><a href="#面向对象的特征有哪些方面" class="headerlink" title="面向对象的特征有哪些方面"></a>面向对象的特征有哪些方面</h4><p>抽象：抽象是将一类对象的共同特征总结出来构造类的过程，包括数据抽象和行为抽象两方面。抽象只关注对象有哪些属性和行为，并不关注这些行为的细节是什么。</p><p>其中Java 面向对象编程三大特性：封装 继承 多态</p><p>封装：隐藏对象的属性和实现细节，仅对外提供公共访问方式，将变化隔离，便于使用，提高复用性和安全性。</p><p>继承：继承是使用已存在的类的定义作为基础建立新类的技术，新类的定义可以增加新的数据或新的功能，也可以用父类的功能，但不能选择性地继承父类。通过使用继承可以提高代码复用性。继承是多态的前提。</p><p>关于继承如下 3 点请记住：</p><p>子类拥有父类非 private 的属性和方法。</p><p>子类可以拥有自己属性和方法，即子类可以对父类进行扩展。</p><p>子类可以用自己的方式实现父类的方法。</p><p>多态性：父类或接口定义的引用变量可以指向子类或具体实现类的实例对象。提高了程序的拓展性。</p><p>在Java中有两种形式可以实现多态：继承（多个子类对同一方法的重写）和接口（实现接口并覆盖接口中同一方法）。</p><p>方法重载（overload）实现的是编译时的多态性（也称为前绑定），而方法重写（override）实现的是运行时的多态性（也称为后绑定）。</p><p>一个引用变量到底会指向哪个类的实例对象，该引用变量发出的方法调用到底是哪个类中实现的方法，必须在由程序运行期间才能决定。运行时的多态是面向对象最精髓的东西，要实现多态需要做两件事：</p><p>方法重写（子类继承父类并重写父类中已有的或抽象的方法）；<br>对象转型（用父类型引用子类型对象，这样同样的引用调用同样的方法就会根据子类对象的不同而表现出不同的行为）。</p><h4 id="什么是多态机制？Java语言是如何实现多态的？"><a href="#什么是多态机制？Java语言是如何实现多态的？" class="headerlink" title="什么是多态机制？Java语言是如何实现多态的？"></a>什么是多态机制？Java语言是如何实现多态的？</h4><p>所谓多态就是指程序中定义的引用变量所指向的具体类型和通过该引用变量发出的方法调用在编程时并不确定，而是在程序运行期间才确定，即一个引用变量到底会指向哪个类的实例对象，该引用变量发出的方法调用到底是哪个类中实现的方法，必须在由程序运行期间才能决定。</p><p>多态的实现</p><p>Java实现多态有三个必要条件：继承、重写、向上转型。</p><p>继承：在多态中必须存在有继承关系的子类和父类。</p><p>重写：子类对父类中某些方法进行重新定义，在调用这些方法时就会调用子类的方法。</p><p>向上转型：在多态中需要将子类的对象赋给父类引用，只有这样该引用才能够具备技能调用父类的方法和子类的方法。</p><p>只有满足了上述三个条件，我们才能够在同一个继承结构中使用统一的逻辑实现代码处理不同的对象，从而达到执行不同的行为。</p><p>对于Java而言，它多态的实现机制遵循一个原则：当超类对象引用变量引用子类对象时，被引用对象的类型而不是引用变量的类型决定了调用谁的成员方法，但是这个被调用的方法必须是在超类中定义过的，也就是说被子类覆盖的方法。</p><h4 id="多态的实现方式"><a href="#多态的实现方式" class="headerlink" title="多态的实现方式"></a>多态的实现方式</h4><ul><li>重写/重载</li><li>接口</li><li>抽象类和抽象方法</li></ul><h4 id="面向对象五大基本原则是什么（可选）"><a href="#面向对象五大基本原则是什么（可选）" class="headerlink" title="面向对象五大基本原则是什么（可选）"></a>面向对象五大基本原则是什么（可选）</h4><ul><li>单一职责原则SRP(Single Responsibility Principle)<br>类的功能要单一</li><li>开放封闭原则OCP(Open－Close Principle)<br>增加功能热烈欢迎，想要修改，哼，一万个不乐意。</li><li>里式替换原则LSP(the Liskov Substitution Principle LSP)<br>子类可以替换父类出现在父类能够出现的任何地方。比如你能代表你爸去你姥姥家干活。哈哈~~</li><li>依赖倒置原则DIP(the Dependency Inversion Principle DIP)<br>高层次的模块不应该依赖于低层次的模块，他们都应该依赖于抽象。抽象不应该依赖于具体实现，具体实现应该依赖于抽象。</li><li>接口分离原则ISP(the Interface Segregation Principle ISP)<br>设计时采用多个与特定客户类有关的接口比采用一个通用的接口要好。就比如一个手机拥有打电话，看视频，玩游戏等功能，把这几个功能拆分成不同的接口，比在一个接口里要好的多。</li></ul><h3 id="类与接口"><a href="#类与接口" class="headerlink" title="类与接口"></a>类与接口</h3><h4 id="抽象类和接口的对比"><a href="#抽象类和接口的对比" class="headerlink" title="抽象类和接口的对比"></a>抽象类和接口的对比</h4><p>抽象类是用来捕捉子类的通用特性的。接口是抽象方法的集合。</p><p>从设计层面来说，抽象类是对类的抽象，是一种模板设计，接口是行为的抽象，是一种行为的规范。</p><h4 id="普通类和抽象类有哪些区别？"><a href="#普通类和抽象类有哪些区别？" class="headerlink" title="普通类和抽象类有哪些区别？"></a>普通类和抽象类有哪些区别？</h4><ul><li>普通类不能包含抽象方法，抽象类可以包含抽象方法。</li><li>抽象类不能直接实例化，普通类可以直接实例化。</li></ul><h4 id="抽象类能使用-final-修饰吗？"><a href="#抽象类能使用-final-修饰吗？" class="headerlink" title="抽象类能使用 final 修饰吗？"></a>抽象类能使用 final 修饰吗？</h4><p>final不可继承</p><h4 id="创建一个对象用什么关键字？对象实例与对象引用有何不同？"><a href="#创建一个对象用什么关键字？对象实例与对象引用有何不同？" class="headerlink" title="创建一个对象用什么关键字？对象实例与对象引用有何不同？"></a>创建一个对象用什么关键字？对象实例与对象引用有何不同？</h4><p>new，new创建对象实例（对象实例在堆内存中），对象引用指向对象实例（对象引用存放在栈内存中）。</p><h3 id="变量与方法"><a href="#变量与方法" class="headerlink" title="变量与方法"></a>变量与方法</h3><h4 id="成员变量与局部变量的区别有哪些"><a href="#成员变量与局部变量的区别有哪些" class="headerlink" title="成员变量与局部变量的区别有哪些"></a>成员变量与局部变量的区别有哪些</h4><p>变量：在程序执行的过程中，在某个范围内其值可以发生改变的量。从本质上讲，变量其实是内存中的一小块区域</p><p>成员变量：方法外部，类内部定义的变量</p><p>局部变量：类的方法中的变量。</p><p>成员变量和局部变量的区别</p><p>作用域</p><p>成员变量：整个类<br>局部变量：某个范围。(一般指的就是方法,语句体内)</p><p>存储位置</p><p>成员变量：随着对象的创建而存在，随着对象的消失而消失，存储在堆内存中。<br>局部变量：在方法被调用，或者语句被执行的时候存在，存储在栈内存中。当方法调用完，或者语句结束后，就自动释放。</p><p>生命周期</p><p>成员变量：随着对象的创建而存在，随着对象的消失而消失<br>局部变量：当方法调用完，或者语句结束后，就自动释放。</p><p>初始值</p><p>成员变量：有</p><p>局部变量：无，使用前必须赋值。</p><p>使用原则</p><p>在使用变量时需要遵循的原则为：就近原则<br>首先在局部范围找，有就使用；接着在成员位置找。</p><h4 id="在Java中定义一个不做事且没有参数的构造方法的作用"><a href="#在Java中定义一个不做事且没有参数的构造方法的作用" class="headerlink" title="在Java中定义一个不做事且没有参数的构造方法的作用"></a>在Java中定义一个不做事且没有参数的构造方法的作用</h4><p>Java程序在执行子类的构造方法之前，如果没有用super()来调用父类特定的构造方法，则会调用父类中“没有参数的构造方法”。</p><h4 id="在调用子类构造方法之前会先调用父类没有参数的构造方法，其目的是？"><a href="#在调用子类构造方法之前会先调用父类没有参数的构造方法，其目的是？" class="headerlink" title="在调用子类构造方法之前会先调用父类没有参数的构造方法，其目的是？"></a>在调用子类构造方法之前会先调用父类没有参数的构造方法，其目的是？</h4><p>帮助子类做初始化工作。</p><h4 id="一个类的构造方法的作用是什么？若一个类没有声明构造方法，改程序能正确执行吗？为什么？"><a href="#一个类的构造方法的作用是什么？若一个类没有声明构造方法，改程序能正确执行吗？为什么？" class="headerlink" title="一个类的构造方法的作用是什么？若一个类没有声明构造方法，改程序能正确执行吗？为什么？"></a>一个类的构造方法的作用是什么？若一个类没有声明构造方法，改程序能正确执行吗？为什么？</h4><p>对类对象的初始化工作；可以执行</p><h4 id="构造方法有哪些特性？"><a href="#构造方法有哪些特性？" class="headerlink" title="构造方法有哪些特性？"></a>构造方法有哪些特性？</h4><p>名字与类名相同；</p><p>没有返回值，但不能用void声明构造函数；</p><p>生成类的对象时自动执行，无需调用。</p><h4 id="静态变量和实例变量区别"><a href="#静态变量和实例变量区别" class="headerlink" title="静态变量和实例变量区别"></a>静态变量和实例变量区别</h4><p>静态变量：属于类的，在内存中只会有一份</p><p>实例变量： 实例变量是属于实例对象的，在内存中，创建几次对象，就有几份成员变量。</p><h4 id="静态变量与普通变量区别"><a href="#静态变量与普通变量区别" class="headerlink" title="静态变量与普通变量区别"></a>静态变量与普通变量区别</h4><p>同上</p><h4 id="静态方法和实例方法有何不同？"><a href="#静态方法和实例方法有何不同？" class="headerlink" title="静态方法和实例方法有何不同？"></a>静态方法和实例方法有何不同？</h4><p>静态方法和实例方法的区别主要体现在两个方面：</p><p>调用静态方法：”类名.方法名”、”对象名.方法名” 实例方法只有后面这种方式</p><p>静态方法只允许访问静态成员；实例方法则无此限制</p><h4 id="在一个静态方法内调用一个非静态成员为什么是非法的？"><a href="#在一个静态方法内调用一个非静态成员为什么是非法的？" class="headerlink" title="在一个静态方法内调用一个非静态成员为什么是非法的？"></a>在一个静态方法内调用一个非静态成员为什么是非法的？</h4><p>由于静态方法可以不通过对象进行调用</p><h4 id="什么是方法的返回值？返回值的作用是什么？"><a href="#什么是方法的返回值？返回值的作用是什么？" class="headerlink" title="什么是方法的返回值？返回值的作用是什么？"></a>什么是方法的返回值？返回值的作用是什么？</h4><p>返回值的作用:接收出结果，使得它可以用于其他的操作。</p><h3 id="内部类"><a href="#内部类" class="headerlink" title="内部类"></a>内部类</h3><h4 id="什么是内部类？"><a href="#什么是内部类？" class="headerlink" title="什么是内部类？"></a>什么是内部类？</h4><p>一个类的定义放在另外一个类的定义内部；内部类本身就是类的一个属性，与其他属性定义方式一致。</p><h4 id="内部类的分类有哪些"><a href="#内部类的分类有哪些" class="headerlink" title="内部类的分类有哪些"></a>内部类的分类有哪些</h4><h5 id="成员内部类、局部内部类、匿名内部类和静态内部类。"><a href="#成员内部类、局部内部类、匿名内部类和静态内部类。" class="headerlink" title="成员内部类、局部内部类、匿名内部类和静态内部类。"></a><strong>成员内部类、局部内部类、匿名内部类和静态内部类</strong>。</h5><p>匿名内部类以下特点：</p><ul><li>匿名内部类必须继承一个抽象类或者实现一个接口。</li><li>匿名内部类不能定义任何静态成员和静态方法。</li><li>当所在的方法的形参需要被匿名内部类使用时，必须声明为 final。</li><li>匿名内部类不能是抽象的，它必须要实现继承的类或者实现的接口的所有抽象方法。</li></ul><h4 id="内部类的优点"><a href="#内部类的优点" class="headerlink" title="内部类的优点"></a>内部类的优点</h4><p>因为它有以下优点：</p><ul><li>一个内部类对象可以访问创建它的外部类对象的内容，包括私有数据！</li><li>内部类不为同一包的其他类所见，具有很好的封装性；</li><li>内部类有效实现了“多重继承”，优化 java 单继承的缺陷。</li><li>匿名内部类可以很方便的定义回调。</li></ul><h4 id="内部类有哪些应用场景"><a href="#内部类有哪些应用场景" class="headerlink" title="内部类有哪些应用场景"></a>内部类有哪些应用场景</h4><ol><li>一些多算法场合</li><li>解决一些非面向对象的语句块。</li><li>适当使用内部类，使得代码更加灵活和富有扩展性。</li><li>当某个类除了它的外部类，不再被其他的类使用时。</li></ol><h4 id="局部内部类和匿名内部类访问局部变量的时候，为什么变量必须要加上final？"><a href="#局部内部类和匿名内部类访问局部变量的时候，为什么变量必须要加上final？" class="headerlink" title="局部内部类和匿名内部类访问局部变量的时候，为什么变量必须要加上final？"></a>局部内部类和匿名内部类访问局部变量的时候，为什么变量必须要加上final？</h4><p>因为<strong>生命周期不一致</strong>，加了final，可以确保局部内部类使用的变量与外层的局部变量区分开，解决了这个问题。</p><h3 id="重写与重载"><a href="#重写与重载" class="headerlink" title="重写与重载"></a>重写与重载</h3><h4 id="构造器（constructor）是否可被重写（override）"><a href="#构造器（constructor）是否可被重写（override）" class="headerlink" title="构造器（constructor）是否可被重写（override）"></a>构造器（constructor）是否可被重写（override）</h4><p>构造器不能被继承，因此不能被重写，但可以被重载。</p><h4 id="重载（Overload）和重写（Override）的区别。重载的方法能否根据返回类型进行区分？"><a href="#重载（Overload）和重写（Override）的区别。重载的方法能否根据返回类型进行区分？" class="headerlink" title="重载（Overload）和重写（Override）的区别。重载的方法能否根据返回类型进行区分？"></a>重载（Overload）和重写（Override）的区别。重载的方法能否根据返回类型进行区分？</h4><p>方法的重载和重写都是实现多态的方式，区别在于前者实现的是编译时的多态性，而后者实现的是运行时的多态性。</p><p>重载：发生在同一个类中，方法名相同参数列表不同（参数类型不同、个数不同、顺序不同），与方法返回值和访问修饰符无关，即重载的方法不能根据返回类型进行区分</p><p>重写：发生在父子类中，方法名、参数列表必须相同，返回值小于等于父类，抛出的异常小于等于父类，访问修饰符大于等于父类（里氏代换原则）；如果父类方法访问修饰符为private则子类中就不是重写。</p><h3 id="对象相等判断"><a href="#对象相等判断" class="headerlink" title="对象相等判断"></a>对象相等判断</h3><h4 id="和-equals-的区别是什么"><a href="#和-equals-的区别是什么" class="headerlink" title="== 和 equals 的区别是什么"></a>== 和 equals 的区别是什么</h4><p>== : (基本数据类型 == 比较的是值，引用数据类型 == 比较的是内存地址)</p><p>equals() : 它的作用也是判断两个对象是否相等。但它一般有两种使用情况：</p><p>情况1：类没有覆盖 equals() 方法。则通过 equals() 比较该类的两个对象时，等价于通过“==”比较这两个对象。</p><p>情况2：类覆盖了 equals() 方法。一般，我们都覆盖 equals() 方法来两个对象的内容相等；若它们的内容相等，则返回 true (即，认为这两个对象相等)。</p><h4 id="hashCode-与-equals-重要"><a href="#hashCode-与-equals-重要" class="headerlink" title="hashCode 与 equals (重要)"></a>hashCode 与 equals (重要)</h4><p><strong>hashCode()与equals()的相关规定</strong></p><p>如果两个对象相等，则hashcode一定也是相同的</p><p>两个对象相等，对两个对象分别调用equals方法都返回true</p><p>两个对象有相同的hashcode值，它们也不一定是相等的</p><p><strong>因此，equals 方法被覆盖过，则 hashCode 方法也必须被覆盖</strong></p><h4 id="对象的相等与指向他们的引用相等，两者有什么不同？"><a href="#对象的相等与指向他们的引用相等，两者有什么不同？" class="headerlink" title="对象的相等与指向他们的引用相等，两者有什么不同？"></a>对象的相等与指向他们的引用相等，两者有什么不同？</h4><p>对象的相等 比的是内存中存放的内容是否相等而 引用相等 比较的是他们指向的内存地址是否相等。</p><h3 id="值传递"><a href="#值传递" class="headerlink" title="值传递"></a>值传递</h3><h4 id="当一个对象被当作参数传递到一个方法后，此方法可改变这个对象的属性，并可返回变化后的结果，那么这里到底是值传递还是引用传递"><a href="#当一个对象被当作参数传递到一个方法后，此方法可改变这个对象的属性，并可返回变化后的结果，那么这里到底是值传递还是引用传递" class="headerlink" title="当一个对象被当作参数传递到一个方法后，此方法可改变这个对象的属性，并可返回变化后的结果，那么这里到底是值传递还是引用传递"></a>当一个对象被当作参数传递到一个方法后，此方法可改变这个对象的属性，并可返回变化后的结果，那么这里到底是值传递还是引用传递</h4><p>是值传递。Java 语言的方法调用只支持参数的值传递。当一个对象实例作为一个参数被传递到方法中时，参数的值就是对该对象的引用。对象的属性可以在被调用过程中被改变，但对对象引用的改变是不会影响到调用者的</p><h4 id="为什么-Java-中只有值传递"><a href="#为什么-Java-中只有值传递" class="headerlink" title="为什么 Java 中只有值传递"></a>为什么 Java 中只有值传递</h4><p>按值调用(call by value)表示方法接收的是调用者提供的值，而按引用调用（call by reference)表示方法接收的是调用者提供的变量地址。一个方法可以修改传递引用所对应的变量值，而不能修改传递值调用所对应的变量值。</p><p><strong>Java程序设计语言总是采用按值调用。也就是说，方法得到的是所有参数值的一个拷贝</strong></p><h4 id="值传递和引用传递有什么区别"><a href="#值传递和引用传递有什么区别" class="headerlink" title="值传递和引用传递有什么区别"></a>值传递和引用传递有什么区别</h4><p>值传递：指的是在方法调用时，传递的参数是按值的拷贝传递，传递的是值的拷贝，也就是说传递后就互不相关了。</p><p>引用传递：指的是在方法调用时，传递的参数是按引用进行传递，其实传递的引用的地址，也就是变量所对应的内存空间的地址。传递的是值的引用，也就是说传递前和传递后都指向同一个引用（也就是同一个内存空间）。</p><h3 id="Java包"><a href="#Java包" class="headerlink" title="Java包"></a>Java包</h3><h4 id="JDK-中常用的包有哪些"><a href="#JDK-中常用的包有哪些" class="headerlink" title="JDK 中常用的包有哪些"></a>JDK 中常用的包有哪些</h4><ul><li>java.lang：这个是系统的基础类；</li><li>java.io：这里面是所有输入输出有关的类，比如文件操作等；</li><li>java.nio：为了完善 io 包中的功能，提高 io 包中性能而写的一个新包；</li><li>java.net：这里面是与网络有关的类；</li><li>java.util：这个是系统辅助类，特别是集合类；</li><li>java.sql：这个是数据库操作的类。</li></ul><h4 id="import-java和javax有什么区别"><a href="#import-java和javax有什么区别" class="headerlink" title="import java和javax有什么区别"></a>import java和javax有什么区别</h4><p>刚开始的时候 JavaAPI 所必需的包是 java 开头的包，javax 当时只是扩展 API 包来说使用。然而随着时间的推移，javax 逐渐的扩展成为 Java API 的组成部分。</p><h2 id="IO流"><a href="#IO流" class="headerlink" title="IO流"></a>IO流</h2><h3 id="java-中-IO-流分为几种"><a href="#java-中-IO-流分为几种" class="headerlink" title="java 中 IO 流分为几种?"></a>java 中 IO 流分为几种?</h3><ul><li>按照流的流向分，可以分为输入流和输出流；</li><li>按照操作单元划分，可以划分为字节流和字符流；</li><li>按照流的角色划分为节点流和处理流。</li></ul><h3 id="BIO-NIO-AIO-有什么区别"><a href="#BIO-NIO-AIO-有什么区别" class="headerlink" title="BIO,NIO,AIO 有什么区别?"></a>BIO,NIO,AIO 有什么区别?</h3><p>BIO：Block IO 同步阻塞式 IO，就是我们平常使用的传统 IO，它的特点是模式简单使用方便，并发处理能力低。<br>NIO：Non IO 同步非阻塞 IO，是传统 IO 的升级，客户端和服务器端通过 Channel（通道）通讯，实现了多路复用。<br>AIO：Asynchronous IO 是 NIO 的升级，也叫 NIO2，实现了异步非堵塞 IO ，异步 IO 的操作基于事件和回调机制。</p><h3 id="Files的常用方法都有哪些？"><a href="#Files的常用方法都有哪些？" class="headerlink" title="Files的常用方法都有哪些？"></a>Files的常用方法都有哪些？</h3><p>Files. exists()：检测文件路径是否存在。<br>Files. createFile()：创建文件。<br>Files. createDirectory()：创建文件夹。<br>Files. delete()：删除一个文件或目录。<br>Files. copy()：复制文件。<br>Files. move()：移动文件。<br>Files. size()：查看文件个数。<br>Files. read()：读取文件。<br>Files. write()：写入文件。</p><h2 id="反射"><a href="#反射" class="headerlink" title="反射"></a>反射</h2><h3 id="什么是反射机制？"><a href="#什么是反射机制？" class="headerlink" title="什么是反射机制？"></a>什么是反射机制？</h3><p>JAVA反射机制是在运行状态中，对于任意一个类，都能够知道这个类的所有属性和方法；对于任意一个对象，都能够调用它的任意一个方法和属性；</p><h3 id="反射机制优缺点"><a href="#反射机制优缺点" class="headerlink" title="反射机制优缺点"></a>反射机制优缺点</h3><ul><li><strong>优点：</strong> 运行期类型的判断，动态加载类，提高代码灵活度。</li><li><strong>缺点：</strong> 性能瓶颈：反射相当于一系列解释操作，通知 JVM 要做的事情，性能比直接的java代码要慢很多。</li></ul><h3 id="反射机制的应用场景有哪些？"><a href="#反射机制的应用场景有哪些？" class="headerlink" title="反射机制的应用场景有哪些？"></a>反射机制的应用场景有哪些？</h3><p>Spring／Hibernate 等框架也大量使用到了反射机制、Class.forName()</p><h3 id="Java获取反射的三种方法"><a href="#Java获取反射的三种方法" class="headerlink" title="Java获取反射的三种方法"></a>Java获取反射的三种方法</h3><p>1.通过new对象实现反射机制 2.通过路径实现反射机制 3.通过类名实现反射机制</p><h2 id="常用API"><a href="#常用API" class="headerlink" title="常用API"></a>常用API</h2><h3 id="String相关"><a href="#String相关" class="headerlink" title="String相关"></a>String相关</h3><h4 id="字符型常量和字符串常量的区别"><a href="#字符型常量和字符串常量的区别" class="headerlink" title="字符型常量和字符串常量的区别"></a>字符型常量和字符串常量的区别</h4><p>形式上: 单引号一个字符 双引号若干个字符<br>含义上: 字符常量相当于一个整形值(ASCII值),可以参加表达式运算 字符串常量代表一个地址值(该字符串在内存中存放位置)<br>占内存大小：字符常量一个字节 字符串常量占若干个字节(至少一个字符结束标志)</p><h4 id="什么是字符串常量池？"><a href="#什么是字符串常量池？" class="headerlink" title="什么是字符串常量池？"></a>什么是字符串常量池？</h4><p>字符串常量池位于堆内存中，专门用来存储字符串常量，可以提高内存的使用率，避免开辟多块空间存储相同的字符串，在创建字符串时 JVM 会首先检查字符串常量池，如果该字符串已经存在池中，则返回它的引用，如果不存在，则实例化一个字符串放到池中，并返回其引用。</p><h4 id="String-是最基本的数据类型吗"><a href="#String-是最基本的数据类型吗" class="headerlink" title="String 是最基本的数据类型吗"></a>String 是最基本的数据类型吗</h4><p>不是。Java 中的基本数据类型只有 8 个 ：byte、short、int、long、float、double、char、boolean；除了基本类型（primitive type），剩下的都是引用类型（referencetype）</p><h4 id="String有哪些特性"><a href="#String有哪些特性" class="headerlink" title="String有哪些特性"></a>String有哪些特性</h4><ul><li>不变性：String 是只读字符串，是一个典型的 immutable 对象，对它进行任何操作，其实都是创建一个新的对象，再把引用指向该对象。不变模式的主要作用在于当一个对象需要被多线程共享并频繁访问时，可以保证数据的一致性。</li><li>常量池优化：String 对象创建之后，会在字符串常量池中进行缓存，如果下次创建同样的对象时，会直接返回缓存的引用。</li><li>final：使用 final 来定义 String 类，表示 String 类不能被继承，提高了系统的安全性。</li></ul><h4 id="String为什么是不可变的吗？"><a href="#String为什么是不可变的吗？" class="headerlink" title="String为什么是不可变的吗？"></a>String为什么是不可变的吗？</h4><p>String类利用了final修饰的char类型数组存储字符</p><h4 id="String真的是不可变的吗？"><a href="#String真的是不可变的吗？" class="headerlink" title="String真的是不可变的吗？"></a>String真的是不可变的吗？</h4><p>不可变</p><p><strong>1) String不可变但不代表引用不可以变</strong></p><p><strong>2) 通过反射是可以修改所谓的“不可变”对象</strong></p><h4 id="是否可以继承-String-类"><a href="#是否可以继承-String-类" class="headerlink" title="是否可以继承 String 类"></a>是否可以继承 String 类</h4><p>String 类是 final 类，不可以被继承。</p><h4 id="String-str-”i”与-String-str-new-String-“i”-一样吗？"><a href="#String-str-”i”与-String-str-new-String-“i”-一样吗？" class="headerlink" title="String str=”i”与 String str=new String(“i”)一样吗？"></a>String str=”i”与 String str=new String(“i”)一样吗？</h4><p>不一样，因为内存的分配方式不一样。String str=”i”的方式，java 虚拟机会将其分配到常量池中；而 String str=new String(“i”) 则会被分到堆内存中。</p><h4 id="String-s-new-String-“xyz”-创建了几个字符串对象"><a href="#String-s-new-String-“xyz”-创建了几个字符串对象" class="headerlink" title="String s = new String(“xyz”);创建了几个字符串对象"></a>String s = new String(“xyz”);创建了几个字符串对象</h4><p>两个对象，一个是静态区的”xyz”，一个是用new创建在堆上的对象。</p><h4 id="如何将字符串反转？"><a href="#如何将字符串反转？" class="headerlink" title="如何将字符串反转？"></a>如何将字符串反转？</h4><p>使用 StringBuilder 或者 stringBuffer 的 reverse() 方法。</p><h4 id="数组有没有-length-方法？String-有没有-length-方法"><a href="#数组有没有-length-方法？String-有没有-length-方法" class="headerlink" title="数组有没有 length()方法？String 有没有 length()方法"></a>数组有没有 length()方法？String 有没有 length()方法</h4><p>数组没有 length()方法 ，有 length 的属性。String 有 length()方法</p><h4 id="String-类的常用方法都有那些？"><a href="#String-类的常用方法都有那些？" class="headerlink" title="String 类的常用方法都有那些？"></a>String 类的常用方法都有那些？</h4><p>indexOf()：返回指定字符的索引。<br>charAt()：返回指定索引处的字符。<br>replace()：字符串替换。<br>trim()：去除字符串两端空白。<br>split()：分割字符串，返回一个分割后的字符串数组。<br>getBytes()：返回字符串的 byte 类型数组。<br>length()：返回字符串长度。<br>toLowerCase()：将字符串转成小写字母。<br>toUpperCase()：将字符串转成大写字符。<br>substring()：截取字符串。<br>equals()：字符串比较。</p><h4 id="在使用-HashMap-的时候，用-String-做-key-有什么好处？"><a href="#在使用-HashMap-的时候，用-String-做-key-有什么好处？" class="headerlink" title="在使用 HashMap 的时候，用 String 做 key 有什么好处？"></a>在使用 HashMap 的时候，用 String 做 key 有什么好处？</h4><p>HashMap 内部实现是通过 key 的 hashcode 来确定 value 的存储位置，因为字符串是不可变的，所以当创建字符串时，它的 hashcode 被缓存下来，不需要再次计算，所以相比于其他对象更快。</p><h4 id="String和StringBuffer、StringBuilder的区别是什么？String为什么是不可变的"><a href="#String和StringBuffer、StringBuilder的区别是什么？String为什么是不可变的" class="headerlink" title="String和StringBuffer、StringBuilder的区别是什么？String为什么是不可变的"></a>String和StringBuffer、StringBuilder的区别是什么？String为什么是不可变的</h4><p>可变性</p><p>string对象是不可变的。StringBuilder与StringBuffer都继承自AbstractStringBuilder类，在AbstractStringBuilder中也是使用字符数组保存字符串，char[] value，都是可变的。</p><p>线程安全性</p><p>String常量，线程安全。StringBuffer对方法加了同步锁或者对调用的方法加了同步锁，所以是线程安全的。StringBuilder并没有对方法进行加同步锁，所以是非线程安全的。</p><p>性能</p><p>每次对String 类型进行改变的时候，都会生成一个新的String对象，然后将指针指向新的String 对象。StringBuffer每次都会对StringBuffer对象本身进行操作，而不是生成新的对象并改变对象引用。相同情况下使用StirngBuilder 相比使用StringBuffer 仅能获得10%~15% 左右的性能提升，但却要冒多线程不安全的风险。</p><p><strong>对于三者使用的总结</strong></p><p>如果要操作少量的数据用 = String</p><p>单线程操作字符串缓冲区 下操作大量数据 = StringBuilder</p><p>多线程操作字符串缓冲区 下操作大量数据 = StringBuffer</p><h3 id="Date相关"><a href="#Date相关" class="headerlink" title="Date相关"></a>Date相关</h3><h3 id="包装类相关"><a href="#包装类相关" class="headerlink" title="包装类相关"></a>包装类相关</h3><h4 id="自动装箱与拆箱"><a href="#自动装箱与拆箱" class="headerlink" title="自动装箱与拆箱"></a>自动装箱与拆箱</h4><p><strong>装箱</strong>：将基本类型用它们对应的引用类型包装起来；</p><p><strong>拆箱</strong>：将包装类型转换为基本数据类型；</p><h4 id="int-和-Integer-有什么区别"><a href="#int-和-Integer-有什么区别" class="headerlink" title="int 和 Integer 有什么区别"></a>int 和 Integer 有什么区别</h4><p>Java 是一个近乎纯洁的面向对象编程语言，但是为了编程的方便还是引入了基本数据类型，但是为了能够将这些基本数据类型当成对象操作，Java 为每一个基本数据类型都引入了对应的包装类型（wrapper class），int 的包装类就是 Integer，从 Java 5 开始引入了自动装箱/拆箱机制，使得二者可以相互转换。</p><p>Java 为每个原始类型提供了包装类型：</p><p>原始类型: boolean，char，byte，short，int，long，float，double</p><p>包装类型：Boolean，Character，Byte，Short，Integer，Long，Float，Double</p>]]></content>
      
      
      <categories>
          
          <category> java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> base </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>java集合笔记</title>
      <link href="/2020/06/20/java%E9%9B%86%E5%90%88%E7%AC%94%E8%AE%B0/"/>
      <url>/2020/06/20/java%E9%9B%86%E5%90%88%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<h1 id="java集合笔记"><a href="#java集合笔记" class="headerlink" title="java集合笔记"></a>java集合笔记</h1><h2 id="集合概述"><a href="#集合概述" class="headerlink" title="集合概述"></a>集合概述</h2><h4 id="什么是集合"><a href="#什么是集合" class="headerlink" title="什么是集合"></a>什么是集合</h4><p>集合框架：用于存储数据的容器。</p><p>集合框架是为表示和操作集合而规定的一种统一的标准的体系结构。<br>任何集合框架都包含三大块内容：对外的接口、接口的实现和对集合运算的算法。</p><p>接口：表示集合的抽象数据类型。接口允许我们操作集合时不必关注具体实现，从而达到“多态”。在面向对象编程语言中，接口通常用来形成规范。</p><p>实现：集合接口的具体实现，是重用性很高的数据结构。</p><p>算法：在一个实现了某个集合框架中的接口的对象身上完成某种有用的计算的方法，例如查找、排序等。这些算法通常是多态的，因为相同的方法可以在同一个接口被多个类实现时有不同的表现。事实上，算法是可复用的函数。<br>它减少了程序设计的辛劳。</p><p>集合框架通过提供有用的数据结构和算法使你能集中注意力于你的程序的重要部分上，而不是为了让程序能正常运转而将注意力于低层设计上。<br>通过这些在无关API之间的简易的互用性，使你免除了为改编对象或转换代码以便联合这些API而去写大量的代码。 它提高了程序速度和质量。</p><h4 id="集合的特点"><a href="#集合的特点" class="headerlink" title="集合的特点"></a>集合的特点</h4><p>集合的特点主要有如下两点：</p><ul><li>对象封装数据，对象多了也需要存储。集合用于存储对象。</li><li>对象的个数确定可以使用数组，对象的个数不确定的可以用集合。因为集合是可变长度的。</li></ul><h4 id="集合和数组的区别"><a href="#集合和数组的区别" class="headerlink" title="集合和数组的区别"></a>集合和数组的区别</h4><ul><li>数组是固定长度的；集合可变长度的。</li><li>数组可以存储基本数据类型，也可以存储引用数据类型；集合只能存储引用数据类型。</li><li>数组存储的元素必须是同一个数据类型；集合存储的对象可以是不同数据类型。</li></ul><h4 id="使用集合框架的好处"><a href="#使用集合框架的好处" class="headerlink" title="使用集合框架的好处"></a>使用集合框架的好处</h4><ul><li>容量自增长；</li><li>提供了高性能的数据结构和算法，使编码更轻松，提高了程序速度和质量；</li><li>允许不同 API 之间的互操作，API之间可以来回传递集合；</li><li>可以方便地扩展或改写集合，提高代码复用性和可操作性。</li><li>通过使用JDK自带的集合类，可以降低代码维护和学习新API成本。</li></ul><h4 id="常用的集合类有哪些？"><a href="#常用的集合类有哪些？" class="headerlink" title="常用的集合类有哪些？"></a>常用的集合类有哪些？</h4><p>Map接口和Collection接口是所有集合框架的父接口：</p><p>Collection接口的子接口包括：Set接口和List接口<br>Map接口的实现类主要有：HashMap、TreeMap、Hashtable、ConcurrentHashMap以及Properties等<br>Set接口的实现类主要有：HashSet、TreeSet、LinkedHashSet等<br>List接口的实现类主要有：ArrayList、LinkedList、Stack以及Vector等</p><h4 id="List，Set，Map三者的区别？List、Set、Map-是否继承自-Collection-接口？List、Map、Set-三个接口存取元素时，各有什么特点？"><a href="#List，Set，Map三者的区别？List、Set、Map-是否继承自-Collection-接口？List、Map、Set-三个接口存取元素时，各有什么特点？" class="headerlink" title="List，Set，Map三者的区别？List、Set、Map 是否继承自 Collection 接口？List、Map、Set 三个接口存取元素时，各有什么特点？"></a>List，Set，Map三者的区别？List、Set、Map 是否继承自 Collection 接口？List、Map、Set 三个接口存取元素时，各有什么特点？</h4><p><img src="https://lixiangbetter.github.io/2020/06/20/java%E9%9B%86%E5%90%88%E7%AC%94%E8%AE%B0/aHR0cHM6Ly9pbWcyMDE4LmNuYmxvZ3MuY29tL290aGVyLzE0MDgxODMvMjAxOTExLzE0MDgxODMtMjAxOTExMTkxODQxNDk1NTktMTU3MTU5NTY2OC5qcGc.jpeg" alt="https://lixiangbetter.github.io/2020/06/20/java%E9%9B%86%E5%90%88%E7%AC%94%E8%AE%B0/aHR0cHM6Ly9pbWcyMDE4LmNuYmxvZ3MuY29tL290aGVyLzE0MDgxODMvMjAxOTExLzE0MDgxODMtMjAxOTExMTkxODQxNDk1NTktMTU3MTU5NTY2OC5qcGc.jpeg"></p><h4 id="集合框架底层数据结构"><a href="#集合框架底层数据结构" class="headerlink" title="集合框架底层数据结构"></a>集合框架底层数据结构</h4><p>Collection</p><ol><li>List</li></ol><ul><li>Arraylist： Object数组</li><li>Vector： Object数组</li><li>LinkedList： 双向循环链表</li></ul><ol start="2"><li>Set</li></ol><ul><li>HashSet（无序，唯一）：底层采用 HashMap 来保存元素</li><li>LinkedHashSet： LinkedHashSet 继承于HashSet，并且其内部是通过 LinkedHashMap 来实现的。</li><li>TreeSet（有序，唯一）： 红黑树(自平衡的排序二叉树。)</li></ul><p>Map</p><ul><li>HashMap： JDK1.8之前HashMap由数组+链表组成的；JDK1.8以后链表+红黑树；</li><li>LinkedHashMap：LinkedHashMap 继承自 HashMap，所以它的底层仍然是基于拉链式散列结构即由数组和链表或红黑树组成。另外，LinkedHashMap 在上面结构的基础上，增加了一条双向链表，使得上面的结构可以保持键值对的插入顺序。同时通过对链表进行相应的操作，实现了访问顺序相关逻辑。</li><li>HashTable： 数组+链表组成的，数组是 HashMap 的主体，链表则是主要为了解决哈希冲突而存在</li><li>TreeMap： 红黑树（自平衡的排序二叉树）</li></ul><h4 id="哪些集合类是线程安全的？"><a href="#哪些集合类是线程安全的？" class="headerlink" title="哪些集合类是线程安全的？"></a>哪些集合类是线程安全的？</h4><ul><li>vector：就比arraylist多了个同步化机制（线程安全），因为效率较低，现在已经不太建议使用。在web应用中，特别是前台页面，往往效率（页面响应速度）是优先考虑的。</li><li>statck：堆栈类，先进后出。</li><li>hashtable：就比hashmap多了个线程安全。</li><li>enumeration：枚举，相当于迭代器。</li></ul><h4 id="Java集合的快速失败机制-“fail-fast”？"><a href="#Java集合的快速失败机制-“fail-fast”？" class="headerlink" title="Java集合的快速失败机制 “fail-fast”？"></a>Java集合的快速失败机制 “fail-fast”？</h4><p>是java集合的一种错误检测机制，当多个线程对集合进行结构上的改变的操作时，有可能会产生 fail-fast 机制。</p><p>例如：假设存在两个线程（线程1、线程2），线程1通过Iterator在遍历集合A中的元素，在某个时候线程2修改了集合A的结构（是结构上面的修改，而不是简单的修改集合元素的内容），那么这个时候程序就会抛出 ConcurrentModificationException 异常，从而产生fail-fast机制。</p><p>原因：迭代器在遍历时直接访问集合中的内容，并且在遍历过程中使用一个 modCount 变量。集合在被遍历期间如果内容发生变化，就会改变modCount的值。每当迭代器使用hashNext()/next()遍历下一个元素之前，都会检测modCount变量是否为expectedmodCount值，是的话就返回遍历；否则抛出异常，终止遍历。</p><p>解决办法：</p><p>在遍历过程中，所有涉及到改变modCount值得地方全部加上synchronized。</p><p>使用CopyOnWriteArrayList来替换ArrayList</p><h4 id="怎么确保一个集合不能被修改？"><a href="#怎么确保一个集合不能被修改？" class="headerlink" title="怎么确保一个集合不能被修改？"></a>怎么确保一个集合不能被修改？</h4><p>可以使用 Collections. unmodifiableCollection(Collection c) 方法来创建一个只读集合，这样改变集合的任何操作都会抛出 Java. lang. UnsupportedOperationException 异常。</p><h2 id="Collection接口"><a href="#Collection接口" class="headerlink" title="Collection接口"></a>Collection接口</h2><h3 id="List接口"><a href="#List接口" class="headerlink" title="List接口"></a>List接口</h3><h4 id="迭代器-Iterator-是什么？"><a href="#迭代器-Iterator-是什么？" class="headerlink" title="迭代器 Iterator 是什么？"></a>迭代器 Iterator 是什么？</h4><p>Iterator 接口提供遍历任何 Collection 的接口。迭代器取代了 Java 集合框架中的 Enumeration，迭代器允许调用者在迭代过程中移除元素。</p><h4 id="Iterator-怎么使用？有什么特点？"><a href="#Iterator-怎么使用？有什么特点？" class="headerlink" title="Iterator 怎么使用？有什么特点？"></a>Iterator 怎么使用？有什么特点？</h4><p>Iterator 的特点是只能单向遍历，但是更加安全，因为它可以确保，在当前遍历的集合元素被更改的时候，就会抛出 ConcurrentModificationException 异常。</p><h4 id="如何边遍历边移除-Collection-中的元素？"><a href="#如何边遍历边移除-Collection-中的元素？" class="headerlink" title="如何边遍历边移除 Collection 中的元素？"></a>如何边遍历边移除 Collection 中的元素？</h4><p>边遍历边修改 Collection 的唯一正确方式是使用 Iterator.remove() 方法</p><h4 id="Iterator-和-ListIterator-有什么区别？"><a href="#Iterator-和-ListIterator-有什么区别？" class="headerlink" title="Iterator 和 ListIterator 有什么区别？"></a>Iterator 和 ListIterator 有什么区别？</h4><ul><li>Iterator 可以遍历 Set 和 List 集合，而 ListIterator 只能遍历 List。</li><li>Iterator 只能单向遍历，而 ListIterator 可以双向遍历（向前/后遍历）。</li><li>ListIterator 实现 Iterator 接口，然后添加了一些额外的功能，比如添加一个元素、替换一个元素、获取前面或后面元素的索引位置。</li></ul><h4 id="遍历一个-List-有哪些不同的方式？每种方法的实现原理是什么？Java-中-List-遍历的最佳实践是什么？"><a href="#遍历一个-List-有哪些不同的方式？每种方法的实现原理是什么？Java-中-List-遍历的最佳实践是什么？" class="headerlink" title="遍历一个 List 有哪些不同的方式？每种方法的实现原理是什么？Java 中 List 遍历的最佳实践是什么？"></a>遍历一个 List 有哪些不同的方式？每种方法的实现原理是什么？Java 中 List 遍历的最佳实践是什么？</h4><ol><li>for 循环遍历，基于计数器。在集合外部维护一个计数器，然后依次读取每一个位置的元素，当读取到最后一个元素后停止。</li><li>迭代器遍历，Iterator。Iterator 是面向对象的一个设计模式，目的是屏蔽不同数据集合的特点，统一遍历集合的接口。Java 在 Collections 中支持了 Iterator 模式。</li><li>foreach 循环遍历。foreach 内部也是采用了 Iterator 的方式实现，使用时不需要显式声明 Iterator 或计数器。优点是代码简洁，不易出错；缺点是只能做简单的遍历，不能在遍历过程中操作数据集合，例如删除、替换。</li></ol><p>推荐的做法就是，支持 Random Access 的列表可用 for 循环遍历，否则建议用 Iterator 或 foreach 遍历。</p><h4 id="说一下-ArrayList-的优缺点"><a href="#说一下-ArrayList-的优缺点" class="headerlink" title="说一下 ArrayList 的优缺点"></a>说一下 ArrayList 的优缺点</h4><p>优点：随机访问快</p><p>缺点：插入删除需复制，耗费性能</p><h4 id="如何实现数组和-List-之间的转换？"><a href="#如何实现数组和-List-之间的转换？" class="headerlink" title="如何实现数组和 List 之间的转换？"></a>如何实现数组和 List 之间的转换？</h4><ul><li>数组转 List：使用 Arrays. asList(array) 进行转换。</li><li>List 转数组：使用 List 自带的 toArray() 方法。</li></ul><h4 id="ArrayList-和-LinkedList-的区别是什么？"><a href="#ArrayList-和-LinkedList-的区别是什么？" class="headerlink" title="ArrayList 和 LinkedList 的区别是什么？"></a>ArrayList 和 LinkedList 的区别是什么？</h4><p>数据结构实现：ArrayList 动态数组，而 LinkedList 双向链表<br>随机访问效率：ArrayList 更好<br>增加和删除效率：LinkedList 更好<br>内存空间占用：LinkedList 比 ArrayList 更占内存，因为 LinkedList 的节点除了存储数据，还存储了两个引用<br>线程安全：都不保证线程安全；<br>综合来说，在需要频繁读取集合中的元素时，更推荐使用 ArrayList，而在插入和删除操作较多时，更推荐使用 LinkedList。</p><h4 id="ArrayList-和-Vector-的区别是什么？"><a href="#ArrayList-和-Vector-的区别是什么？" class="headerlink" title="ArrayList 和 Vector 的区别是什么？"></a>ArrayList 和 Vector 的区别是什么？</h4><ul><li>线程安全：Vector 使用了 Synchronized 来实现线程同步，是线程安全的，而 ArrayList 是非线程安全的。</li><li>性能：ArrayList 在性能方面要优于 Vector。</li><li>扩容：ArrayList 和 Vector 都会根据实际的需要动态的调整容量，只不过在 Vector 扩容每次会增加 1 倍，而 ArrayList 只会增加 50%。</li></ul><h4 id="插入数据时，ArrayList、LinkedList、Vector谁速度较快？阐述-ArrayList、Vector、LinkedList-的存储性能和特性？"><a href="#插入数据时，ArrayList、LinkedList、Vector谁速度较快？阐述-ArrayList、Vector、LinkedList-的存储性能和特性？" class="headerlink" title="插入数据时，ArrayList、LinkedList、Vector谁速度较快？阐述 ArrayList、Vector、LinkedList 的存储性能和特性？"></a>插入数据时，ArrayList、LinkedList、Vector谁速度较快？阐述 ArrayList、Vector、LinkedList 的存储性能和特性？</h4><p>ArrayList、Vector 底层数组方式存储数据。</p><p>LinkedList 双向链表，LinkedList 插入速度较快。</p><h4 id="多线程场景下如何使用-ArrayList？"><a href="#多线程场景下如何使用-ArrayList？" class="headerlink" title="多线程场景下如何使用 ArrayList？"></a>多线程场景下如何使用 ArrayList？</h4><p>ArrayList 不是线程安全的，如果遇到多线程场景，可以通过 Collections 的 synchronizedList 方法将其转换成线程安全的容器后再使用</p><h4 id="为什么-ArrayList-的-elementData-加上-transient-修饰？"><a href="#为什么-ArrayList-的-elementData-加上-transient-修饰？" class="headerlink" title="为什么 ArrayList 的 elementData 加上 transient 修饰？"></a>为什么 ArrayList 的 elementData 加上 transient 修饰？</h4><p>每次序列化时，先调用 defaultWriteObject() 方法序列化 ArrayList 中的非 transient 元素，然后遍历 elementData，只序列化已存入的元素，这样既加快了序列化的速度，又减小了序列化之后的文件大小。</p><h4 id="List-和-Set-的区别"><a href="#List-和-Set-的区别" class="headerlink" title="List 和 Set 的区别"></a>List 和 Set 的区别</h4><p>list: 有序 元素可重复 多个null 有索引 for和iterator 检索低效 插入删除高效 </p><p>set: 无序 不可重复 一个null iterator 查找高效 插入删除低效</p><h3 id="Set接口"><a href="#Set接口" class="headerlink" title="Set接口"></a>Set接口</h3><h4 id="说一下-HashSet-的实现原理？"><a href="#说一下-HashSet-的实现原理？" class="headerlink" title="说一下 HashSet 的实现原理？"></a>说一下 HashSet 的实现原理？</h4><p>HashSet: 底层HashMap hashmap的value统一为PRESENT 底层调用hashmap的方法 hashset不允许重复</p><h4 id="HashSet如何检查重复？HashSet是如何保证数据不可重复的？"><a href="#HashSet如何检查重复？HashSet是如何保证数据不可重复的？" class="headerlink" title="HashSet如何检查重复？HashSet是如何保证数据不可重复的？"></a>HashSet如何检查重复？HashSet是如何保证数据不可重复的？</h4><p>检查重复，不仅比较hash值，还要结合equals方法</p><p>值作为hashmap的key，所以不会重复</p><h3 id="Queue"><a href="#Queue" class="headerlink" title="Queue"></a>Queue</h3><h4 id="BlockingQueue是什么？"><a href="#BlockingQueue是什么？" class="headerlink" title="BlockingQueue是什么？"></a>BlockingQueue是什么？</h4><p>阻塞队列  在进行检索或移除一个元素的时候，它会等待队列变为非空；当在添加一个元素时，它会等待队列中的可用空间</p><h4 id="在-Queue-中-poll-和-remove-有什么区别？"><a href="#在-Queue-中-poll-和-remove-有什么区别？" class="headerlink" title="在 Queue 中 poll()和 remove()有什么区别？"></a>在 Queue 中 poll()和 remove()有什么区别？</h4><p>相同点：返回第一元素，并删除</p><p>不同点：没有元素，poll返回null remove抛出异常NoSuchElementException</p><h2 id="Map接口"><a href="#Map接口" class="headerlink" title="Map接口"></a>Map接口</h2><h4 id="说一下-HashMap-的实现原理？"><a href="#说一下-HashMap-的实现原理？" class="headerlink" title="说一下 HashMap 的实现原理？"></a>说一下 HashMap 的实现原理？</h4><p>概述： HashMap是基于哈希表的Map接口的非同步实现</p><p>数组和链表的结合体</p><p>1.用key的hashcode作hash计算下标</p><p>2.(1)key相同，覆盖原始值；(2)key不同（出现冲突），key-value放入链表</p><p>Jdk 1.8中对HashMap的实现做了优化，当链表中的节点数据超过八个之后，该链表会转为红黑树来提高查询效率，从原来的O(n)到O(logn)</p><h4 id="HashMap在JDK1-7和JDK1-8中有哪些不同？HashMap的底层实现"><a href="#HashMap在JDK1-7和JDK1-8中有哪些不同？HashMap的底层实现" class="headerlink" title="HashMap在JDK1.7和JDK1.8中有哪些不同？HashMap的底层实现"></a>HashMap在JDK1.7和JDK1.8中有哪些不同？HashMap的底层实现</h4><p>JDK1.8之前: 数据+链表； 之后：链表长度大于阈值（默认为8），链表转为红黑树</p><h4 id="HashMap的put方法的具体流程？"><a href="#HashMap的put方法的具体流程？" class="headerlink" title="HashMap的put方法的具体流程？"></a>HashMap的put方法的具体流程？</h4><p>①.判断键值对数组table是否为空或为null，否则执行resize()进行扩容；</p><p>②.根据键值key计算hash值得到插入的数组索引i，如果table[i]==null，直接新建节点添加，转向⑥，如果table[i]不为空，转向③；</p><p>③.判断table[i]的首个元素是否和key一样，如果相同直接覆盖value，否则转向④，这里的相同指的是hashCode以及equals；</p><p>④.判断table[i] 是否为treeNode，即table[i] 是否是红黑树，如果是红黑树，则直接在树中插入键值对，否则转向⑤；</p><p>⑤.遍历table[i]，判断链表长度是否大于8，大于8的话把链表转换为红黑树，在红黑树中执行插入操作，否则进行链表的插入操作；遍历过程中若发现key已经存在直接覆盖value即可；</p><p>⑥.插入成功后，判断实际存在的键值对数量size是否超多了最大容量threshold，如果超过，进行扩容。</p><h4 id="HashMap的扩容操作是怎么实现的？"><a href="#HashMap的扩容操作是怎么实现的？" class="headerlink" title="HashMap的扩容操作是怎么实现的？"></a>HashMap的扩容操作是怎么实现的？</h4><p>①.在jdk1.8中，resize方法是在hashmap中的键值对大于阀值时或者初始化时，就调用resize方法进行扩容；</p><p>②.每次扩展的时候，都是扩展2倍；</p><p>③.扩展后Node对象的位置要么在原位置，要么移动到原偏移量两倍的位置。</p><h4 id="HashMap是怎么解决哈希冲突的？"><a href="#HashMap是怎么解决哈希冲突的？" class="headerlink" title="HashMap是怎么解决哈希冲突的？"></a>HashMap是怎么解决哈希冲突的？</h4><p>什么是哈希：<strong>就是把任意长度的输入通过散列算法，变换成固定长度的输出，该输出就是散列值（哈希值）</strong></p><p>基本特性：<strong>根据同一散列函数计算出的散列值如果不同，那么输入值肯定也不同。但是，根据同一散列函数计算出的散列值如果相同，输入值不一定相同</strong></p><p>什么是哈希冲突：<strong>当两个不同的输入值，根据同一散列函数计算出相同的散列值的现象，我们就把它叫做碰撞（哈希碰撞）</strong></p><p>HashMap的数据结构：<strong>数组的特点是：寻址容易，插入和删除困难；链表的特点是：寻址困难，但插入和删除容易</strong></p><p>hash()函数：<strong>与自己右移16位进行异或运算（高低位异或）</strong></p><h4 id="能否使用任何类作为-Map-的-key？"><a href="#能否使用任何类作为-Map-的-key？" class="headerlink" title="能否使用任何类作为 Map 的 key？"></a>能否使用任何类作为 Map 的 key？</h4><p>可以，考虑一下几点：</p><p>1.重写了 equals() 方法，也应该重写 hashCode() 方法。</p><p>2.遵循与 equals() 和 hashCode() 相关的规则。</p><p>3.用户自定义 Key 类最佳实践是使之为不可变的</p><h4 id="为什么HashMap中String、Integer这样的包装类适合作为Key？"><a href="#为什么HashMap中String、Integer这样的包装类适合作为Key？" class="headerlink" title="为什么HashMap中String、Integer这样的包装类适合作为Key？"></a>为什么HashMap中String、Integer这样的包装类适合作为Key？</h4><p>1.都是final类型，即不可变性，保证key的不可更改性，不会存在获取hash值不同的情况</p><p>2.内部已重写了<code>equals()</code>、<code>hashCode()</code>等方法，遵守了HashMap内部的规范</p><h4 id="如果使用Object作为HashMap的Key，应该怎么办呢？"><a href="#如果使用Object作为HashMap的Key，应该怎么办呢？" class="headerlink" title="如果使用Object作为HashMap的Key，应该怎么办呢？"></a>如果使用Object作为HashMap的Key，应该怎么办呢？</h4><p>重写<code>hashCode()</code>和<code>equals()</code>方法</p><h4 id="HashMap为什么不直接使用hashCode-处理后的哈希值直接作为table的下标？"><a href="#HashMap为什么不直接使用hashCode-处理后的哈希值直接作为table的下标？" class="headerlink" title="HashMap为什么不直接使用hashCode()处理后的哈希值直接作为table的下标？"></a>HashMap为什么不直接使用hashCode()处理后的哈希值直接作为table的下标？</h4><p><code>hashCode()</code>方法返回的是int整数类型，其范围为-(2 ^ 31)~(2 ^ 31 - 1)，约有40亿个映射空间；哈希值可能不在数组大小范围内，进而无法匹配存储位置</p><h4 id="HashMap-的长度为什么是2的幂次方"><a href="#HashMap-的长度为什么是2的幂次方" class="headerlink" title="HashMap 的长度为什么是2的幂次方"></a>HashMap 的长度为什么是2的幂次方</h4><p> hash%length==hash&amp;(length-1)的前提是 length 是2的 n 次方；</p><h4 id="HashMap-与-HashTable-有什么区别？"><a href="#HashMap-与-HashTable-有什么区别？" class="headerlink" title="HashMap 与 HashTable 有什么区别？"></a>HashMap 与 HashTable 有什么区别？</h4><p>1.线程安全 hashtable用synchronized修饰</p><p>2.效率 hashmap效率高</p><p>3.对null key的支持 hashmap可以 hashtable报错</p><p>4.Hashtable 默认大小11，之后扩充，容量为原来2n+1。HashMap 默认大小16。扩充，原来的2倍</p><h4 id="如何决定使用-HashMap-还是-TreeMap？"><a href="#如何决定使用-HashMap-还是-TreeMap？" class="headerlink" title="如何决定使用 HashMap 还是 TreeMap？"></a>如何决定使用 HashMap 还是 TreeMap？</h4><p>对于在Map中插入、删除和定位元素，HashMap最好。然而，对一个有序的key集合进行遍历，TreeMap更好</p><h4 id="HashMap-和-ConcurrentHashMap-的区别"><a href="#HashMap-和-ConcurrentHashMap-的区别" class="headerlink" title="HashMap 和 ConcurrentHashMap 的区别"></a>HashMap 和 ConcurrentHashMap 的区别</h4><p>1.JDK1.8之后ConcurrentHashMap启用了一种全新的方式实现,利用CAS算法。</p><p>2.hashmap允许null</p><h4 id="ConcurrentHashMap-和-Hashtable-的区别？"><a href="#ConcurrentHashMap-和-Hashtable-的区别？" class="headerlink" title="ConcurrentHashMap 和 Hashtable 的区别？"></a>ConcurrentHashMap 和 Hashtable 的区别？</h4><p>1.底层数据结构，ConcurrentHashMap：数组+链表/红黑二叉树；Hashtable：数组+链表</p><p>2.<strong>实现线程安全的方式（重要）</strong>：① 在JDK1.7的时候，ConcurrentHashMap（分段锁） 对整个桶数组进行了分割分段(Segment)，每一把锁只锁容器其中一部分数据，多线程访问容器里不同数据段的数据，就不会存在锁竞争，提高并发访问率。（默认分配16个Segment，比Hashtable效率提高16倍。） 到了 JDK1.8 的时候已经摒弃了Segment的概念，而是直接用 Node 数组+链表+红黑树的数据结构来实现，并发控制使用 synchronized 和 CAS 来操作。（JDK1.6以后 对 synchronized锁做了很多优化） 整个看起来就像是优化过且线程安全的 HashMap，虽然在JDK1.8中还能看到 Segment 的数据结构，但是已经简化了属性，只是为了兼容旧版本；② Hashtable(同一把锁) :使用 synchronized 来保证线程安全，效率非常低下。当一个线程访问同步方法时，其他线程也访问同步方法，可能会进入阻塞或轮询状态，如使用 put 添加元素，另一个线程不能使用 put 添加元素，也不能使用 get，竞争会越来越激烈效率越低。</p><h4 id="ConcurrentHashMap-底层具体实现知道吗？实现原理是什么？"><a href="#ConcurrentHashMap-底层具体实现知道吗？实现原理是什么？" class="headerlink" title="ConcurrentHashMap 底层具体实现知道吗？实现原理是什么？"></a>ConcurrentHashMap 底层具体实现知道吗？实现原理是什么？</h4><p>JDK1.7: ConcurrentHashMap采用Segment + HashEntry的方式进行实现</p><p>JDK1.8: synchronized只锁定当前链表或红黑二叉树的首节点</p><h4 id="Array-和-ArrayList-有何区别？"><a href="#Array-和-ArrayList-有何区别？" class="headerlink" title="Array 和 ArrayList 有何区别？"></a>Array 和 ArrayList 有何区别？</h4><p>Array 存储基本数据类型和对象，ArrayList 只能存储对象。<br>Array 是指定固定大小的，而 ArrayList 大小是自动扩展的。<br>Array 内置方法没有 ArrayList 多，比如 addAll、removeAll、iteration 等方法只有 ArrayList 有。</p><h4 id="如何实现-Array-和-List-之间的转换？"><a href="#如何实现-Array-和-List-之间的转换？" class="headerlink" title="如何实现 Array 和 List 之间的转换？"></a>如何实现 Array 和 List 之间的转换？</h4><ul><li>Array 转 List： Arrays. asList(array) ；</li><li>List 转 Array：List 的 toArray() 方法。</li></ul><h4 id="comparable-和-comparator的区别？"><a href="#comparable-和-comparator的区别？" class="headerlink" title="comparable 和 comparator的区别？"></a>comparable 和 comparator的区别？</h4><ul><li>comparable接口实际上是出自java.lang包，它有一个 compareTo(Object obj)方法用来排序</li><li>comparator接口实际上是出自 java.util 包，它有一个compare(Object obj1, Object obj2)方法用来排序</li></ul><h4 id="Collection-和-Collections-有什么区别？"><a href="#Collection-和-Collections-有什么区别？" class="headerlink" title="Collection 和 Collections 有什么区别？"></a>Collection 和 Collections 有什么区别？</h4><ul><li>java.util.Collection 是一个集合接口（集合类的一个顶级接口）。它提供了对集合对象进行基本操作的通用接口方法。Collection接口在Java 类库中有很多具体的实现。Collection接口的意义是为各种具体的集合提供了最大化的统一操作方式，其直接继承接口有List与Set。</li><li>Collections则是集合类的一个工具类/帮助类，其中提供了一系列静态方法，用于对集合中元素进行排序、搜索以及线程安全等各种操作。</li></ul><h4 id="TreeMap-和-TreeSet-在排序时如何比较元素？Collections-工具类中的-sort-方法如何比较元素？"><a href="#TreeMap-和-TreeSet-在排序时如何比较元素？Collections-工具类中的-sort-方法如何比较元素？" class="headerlink" title="TreeMap 和 TreeSet 在排序时如何比较元素？Collections 工具类中的 sort()方法如何比较元素？"></a>TreeMap 和 TreeSet 在排序时如何比较元素？Collections 工具类中的 sort()方法如何比较元素？</h4><ol><li>TreeSet 要求存放的对象所属的类必须实现 Comparable 接口，该接口提供了比较元素的 compareTo()方法，当插入元素时会回调该方法比较元素的大小。TreeMap 要求存放的键值对映射的键必须实现 Comparable 接口从而根据键对元素进 行排 序。</li><li>Collections 工具类的 sort 方法有两种重载的形式，</li></ol><p>第一种要求传入的待排序容器中存放的对象比较实现 Comparable 接口以实现元素的比较；</p><p>第二种不强制性的要求容器中的元素必须可比较，但是要求传入第二个参数，参数是Comparator 接口的子类型（需要重写 compare 方法实现元素的比较），相当于一个临时定义的排序规则，其实就是通过接口注入比较元素大小的算法，也是对回调模式的应用（Java 中对函数式编程的支持）。</p>]]></content>
      
      
      <categories>
          
          <category> java </category>
          
      </categories>
      
      
        <tags>
            
            <tag> Collections </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>初识alluxio</title>
      <link href="/2020/01/12/%E5%88%9D%E8%AF%86alluxio/"/>
      <url>/2020/01/12/%E5%88%9D%E8%AF%86alluxio/</url>
      
        <content type="html"><![CDATA[<p>Alluxio<br>    Tachyon 前身的名字  超光速粒子<br>    以内存为中心的分布式文件系统<br>        HDFS、S3….<br>    介于计算层和存储层之间<br>        计算层：Spark、Flink、MapReduce<br>    存储层在内存中的一个Cache系统<br>    Spark/Alluxio：AMPLab<br>    2012/12 0.1.0<br>    将计算和存储分离    移动计算优于移动数据</p><p>能够为我们带来什么？？？</p><pre><code>Flink能否替代Spark成为第三代/新一代执行引擎？Hadoop真的凉了吗？那我还有必须学习Hadoop吗？Flume吞吐量多少？Spark Application放多少资源？如何保证数据不丢失自动动手测试一下时效性的要求是越来越高的基于内存  Memory is King   Spark Flink两面性</code></pre><p>1） 2 Spark Application 需要共享数据，必须通过写XX操作<br>2）基于JVM对数据进行缓存<br>    Spark Application = 1 Driver + N executor<br>3）2 Spark Application操作相同的数据<br>    HDFS ==&gt; WC ==&gt; SINK<br>    HDFS ==&gt; XXX ==&gt; SINK</p><p>Alluxio不是Apache的顶级项目<br>    <a href="https://www.alluxio.io/" target="_blank" rel="noopener">https://www.alluxio.io/</a><br>    <a href="https://github.com/Alluxio/alluxio" target="_blank" rel="noopener">https://github.com/Alluxio/alluxio</a></p><p>特点：<br>1）原生的API和文件系统的非常类似<br>2）兼容性   Hadoop   Spark  Flink<br>3）列式<br>4）底层文件系统是可插拔的<br>5）Web UI<br>6）Command line interaction<br>    hadoop/hdfs fs -ls …<br>    alluxio fs ….</p><p>Spark 两个不同角度的应用进行实战<br>    Spark 离线<br>    Spark 实时</p><p>Alluxio部署<br>    1）下载<br>    2）解压到app<br>    3）配置到系统环境变量<br>    4）conf/<br>        alluxio-site.properties<br>        masters<br>        workers<br>    5）格式化<br>    6）启动<br>    7）hadoop000:19999 可以看到Alluxio的Web UI</p><p>Alluxio常用的命令行参数<br>    alluxio fs<br>        ls lsr mkdir cat<br>        copyFromLocal copyToLocal mv<br>        pin<br>        count  location</p><p>Alluxio和HDFS整合</p><p>Alluxio和MapReduce整合</p><p>hadoop jar hadoop-mapreduce-examples-2.6.0-cdh5.15.1.jar wordcount -libjars /home/hadoop/app/alluxio-1.8.1/client/alluxio-1.8.1-client.jar alluxio://hadoop000:19998/alluxio/wc/input/hello.txt alluxio://hadoop000:19998/alluxio/wc/output</p><p>Alluxio和Spark整合</p><p>做了这几个与Alluxio的整合，业务逻辑根本没有发生变化，只是:</p><p>1) 环境上变化<br>2) hdfs ==&gt; alluxio</p>]]></content>
      
      
      <categories>
          
          <category> bigdata </category>
          
      </categories>
      
      
        <tags>
            
            <tag> alluxio </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Spark优化笔记</title>
      <link href="/2020/01/12/Spark%E4%BC%98%E5%8C%96%E7%AC%94%E8%AE%B0/"/>
      <url>/2020/01/12/Spark%E4%BC%98%E5%8C%96%E7%AC%94%E8%AE%B0/</url>
      
        <content type="html"><![CDATA[<h5 id="优化杂谈"><a href="#优化杂谈" class="headerlink" title="优化杂谈"></a>优化杂谈</h5><p>优化点一：资源<br>    spark作业在运行的时候能占用多少资源：cpu、memory<br>    分配”足够多“的资源，在一定范围内，增加资源 和 性能提升 成正比的<br>    Spark on YARN 作业跑在规划好的YARN的队列中</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">./bin/spark-submit --class org.apache.spark.examples.SparkPi \</span><br><span class="line">    --master yarn \</span><br><span class="line">    --deploy-mode cluster \</span><br><span class="line">    --driver-memory 4g \    # Driver的内存</span><br><span class="line">    --executor-memory 2g \  # 每个Executor的内存</span><br><span class="line">    --executor-cores 1 \    # Executor的cpu core的数量</span><br><span class="line">    --queue thequeue \      # 运行在YARN的哪个队列上</span><br><span class="line">    --num-executors 3 \     # Executor的数量 </span><br><span class="line">    examples/jars/spark-examples*.jar \</span><br><span class="line">    10</span><br></pre></td></tr></table></figure><pre><code>送你们一句话：尽量将你的作业使用的资源调整到最大YARN: pkspark  400G 100C    50exe ==&gt;         executor-memory = 8G        executor-cores  = 2Cnum-executors + :    task的并行度  num*cores        4exe 2core = 8task    8exe 2core = 16task    100task executor-cores + : task的并行度executor-memory + :    能cache的数据多 ==&gt; 写入disk的次数会降低    shuffle   IO    JVM   GC思考：Spark ETL HBase 运行在YARN之上</code></pre><p>调优之算子的选择<br>    map<br>        def map[U: ClassTag](f: T =&gt; U): RDD[U]</p><pre><code>mapPartitions    def mapPartitions[U: ClassTag](          f: Iterator[T] =&gt; Iterator[U],          preservesPartitioning: Boolean = false): RDD[U]transforamtion:转换算子RDD = 2Partitions (2 * 1w = 2w)    map  2w    mapPartitions  2 </code></pre><p>QA：转换算子能生成Job吗？</p><pre><code>foreach     def foreach(f: T =&gt; Unit)foreachPartitions    def foreachPartition(f: Iterator[T] =&gt; Unit)Action算子送你们一句话：如果涉及到写数据库操作，    建议采用带Partitions的，但是由于mapPartitions是一个transforamtion算子，所以建议采用foreachPartitions    OOM    使用之前：        评估你要处理的RDD的数据量        每个partition的数据量        整个作业使用到的资源</code></pre><p>生产或者面试：Spark自定义排序</p><p>class 和 case class在使用层面有什么区别？？？</p><p>Spark Streaming对接Kafka数据<br>    对于Kafka来说，我们的Spark Streaming应用程序其实就是一个消费者</p><pre><code>1） Spark Streaming挂了，那么就没有办法去消费Kafka中的数据了，Kafka中的数据就会有积压2） 高峰期的时候，由于你作业的资源并没有很好的设置，在某些批次中，很可能数据比较大batch时间到了，那么Spark Streaming就会处理这个批次中的数据假设：batch time 10s  就会出现10s你根本处理不过来整个批次的数据后续批次的作业就会产生挤压，那么时效性就没有办法保证==&gt; Kafka的限速假设限速是10010秒一个批次    topic 是1个分区：10 * 1 * 100 = 1000    topic 是3个分区：10 * 3 * 100 = 3000要提升数据处理的吞吐量：提升Kafka的分区数    </code></pre><p>Spark Streaming对接Kafka数据进行处理时，能否保证仅处理一次的语义<br>    至少一次：可能数据消费重复<br>    至多一次：可能数据有丢失<br>    仅仅一次：不会有数据的丢失，也不会重复消费   ✅</p><pre><code>能？ 怎么做？不能做到？还能用吗？</code></pre><p>广播<br>    join： shuffle/reduce join   mapjoin</p><p>val o = xxxx   // 20M  算子的外部变量<br>rdd.map(x =&gt; {</p><pre><code>//....o</code></pre><p>})    </p><p>每个task都会获得一份变量o的副本</p><p>20executor  500task ==&gt; 500 * 20M = 10G</p><p>如果使用了广播变量：<br>    每个executor保存一个变量o的副本</p><pre><code>20 * 20m = 400M </code></pre>]]></content>
      
      
      <categories>
          
          <category> bigdata </category>
          
      </categories>
      
      
        <tags>
            
            <tag> spark </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>OLTP和OLAP</title>
      <link href="/2020/01/05/OLTP%E5%92%8COLAP/"/>
      <url>/2020/01/05/OLTP%E5%92%8COLAP/</url>
      
        <content type="html"><![CDATA[<p>1 OLTP和OLAP</p><p>online transaction processing，联机事务处理。业务类系统主要供基层人员使用，进行一线业务操作，通常被称为联机事务处理。</p><p>online analytical processing，联机分析处理。数据分析的目标是探索并挖掘数据的价值，作为企业高层进行决策的参考。</p><p>从功能层面上来看，OLTP负责基本业务的正常运转，业务数据积累所产生的价值信息被OLAP所呈现，根据OLAP所产生的价值信息不断优化基本业务。</p><p>2 OLTP</p><p>OLTP负责基本业务的正常运转，因此使用基本的关系型数据库就可以了。比如Mysql。</p><p>3 OLAP</p><p>基本业务生成的数据越来越多，目前流行的是分布式的处理方案，即sql on hadoop。比如百度的关系数据仓储Palo。</p><p>MPP架构的数据仓储是典型的OLAP应用。</p><p>4 MPP</p><p>massively parallel processing，大规模并行处理。比如非共享数据库集群。</p><p>图示<br><img src="https://lixiangbetter.github.io/2020/01/05/OLTP%E5%92%8COLAP/2018072122222525.png" alt="not found"></p><p>图片转载：<a href="https://blog.csdn.net/qq_33414271/article/details/81149966" target="_blank" rel="noopener">https://blog.csdn.net/qq_33414271/article/details/81149966</a></p>]]></content>
      
      
      <categories>
          
          <category> bigdata </category>
          
      </categories>
      
      
        <tags>
            
            <tag> olap </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>浅谈四层和七层负载均衡</title>
      <link href="/2020/01/05/%E6%B5%85%E8%B0%88%E5%9B%9B%E5%B1%82%E5%92%8C%E4%B8%83%E5%B1%82%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1/"/>
      <url>/2020/01/05/%E6%B5%85%E8%B0%88%E5%9B%9B%E5%B1%82%E5%92%8C%E4%B8%83%E5%B1%82%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1/</url>
      
        <content type="html"><![CDATA[<h4 id="浅谈四层和七层负载"><a href="#浅谈四层和七层负载" class="headerlink" title="浅谈四层和七层负载"></a>浅谈四层和七层负载</h4><p>关于负载均衡，经常听到四层负载均衡和七层负载均衡的说法，他们之间有什么关系和区别呢，今天就简单总结概括下。</p><h3 id="概述"><a href="#概述" class="headerlink" title="概述"></a>概述</h3><blockquote><p>用一句话来说，<strong>四层负载均衡就是工作在计算机网络OSI七层分层的第四层（传输层）的，七层负载军和则是工作在第七层（应用层）的</strong>。</p></blockquote><p>也就是说，四层负载均衡是<strong>基于IP+端口</strong>的负载均衡，七层负载均衡是<strong>基于URL</strong>等应用层信息的负载均衡。</p><p>同理，还有基于MAC地址的二层负载均衡和基于IP地址的三层负载均衡。</p><blockquote><ul><li><strong>二层负载均衡(mac)</strong><br> 一般是用<code>虚拟mac地址</code>方式,外部对虚拟MAC地址请求,负载均衡接收后分配后端实际的MAC地址响应。</li><li><strong>三层负载均衡(ip)</strong><br> 一般采用<code>虚拟IP地址</code>方式,外部对虚拟的ip地址请求,负载均衡接收后分配后端实际的IP地址响应。</li><li><strong>四层负载均衡(tcp)</strong><br> 用<code>虚拟ip+port</code>接收请求,再转发到对应的真实机器。</li><li><strong>七层负载均衡(http)</strong><br> 用<code>虚拟的url或主机名</code>接收请求,再转向相应的处理服务器。</li></ul></blockquote><p>在实际应用中,比较常见的就是四层负载及七层负载。这里也重点说下这两种负载。</p><p>  所谓的四到七层负载均衡，就是在对后台的服务器进行负载均衡时，<strong>依据四层的信息或七层的信息来决定怎么样转发流量</strong>。 比如四层的负载均衡，就是通过发布三层的IP地址（VIP），然后加四层的端口号，来决定哪些流量需要做负载均衡，对需要处理的流量进行NAT处理，转发至后台服务器，并记录下这个TCP或者UDP的流量是由哪台服务器处理的，后续这个连接的所有流量都同样转发到同一台服务器处理。七层的负载均衡，就是在四层的基础上（没有四层是绝对不可能有七层的），再考虑应用层的特征，比如同一个Web服务器的负载均衡，除了根据VIP加80端口辨别是否需要处理的流量，还可根据七层的URL、浏览器类别、语言来决定是否要进行负载均衡。举个例子，如果你的Web服务器分成两组，一组是中文语言的，一组是英文语言的，那么七层负载均衡就可以当用户来访问你的域名时，自动辨别用户语言，然后选择对应的语言服务器组进行负载均衡处理。</p><hr><h3 id="具体区别"><a href="#具体区别" class="headerlink" title="具体区别"></a>具体区别</h3><p>负载均衡器通常称为<strong>四层交换机</strong>或<strong>七层交换机</strong>。那么四层和七层两者到底区别在哪里？</p><h5 id="1-技术原理区别"><a href="#1-技术原理区别" class="headerlink" title="1. 技术原理区别"></a>1. 技术原理区别</h5><ul><li>所谓<strong>四层负载均衡</strong>，也就是主要通过报文中的目标地址和端口，再加上负载均衡设备设置的服务器选择方式，决定最终选择的内部服务器。</li></ul><p>  以常见的TCP为例，负载均衡设备在接收到第一个来自客户端的SYN 请求时，即通过上述方式选择一个最佳的服务器，并对报文中目标IP地址进行修改(改为后端服务器IP），直接转发给该服务器。TCP的连接建立，即<strong>三次握手是客户端和服务器直接建立的，负载均衡设备只是起到一个类似路由器的转发动作</strong>。在某些部署情况下，为保证服务器回包可以正确返回给负载均衡设备，在转发报文的同时可能还会对报文原来的源地址进行修改。<br><img src="/Users/lx/Documents/myblog/source/_posts/%E6%B5%85%E8%B0%88%E5%9B%9B%E5%B1%82%E5%92%8C%E4%B8%83%E5%B1%82%E8%B4%9F%E8%BD%BD%E5%9D%87%E8%A1%A1/1038472-618c1fc22f893b96.jpg" alt="not found"></p><p>四层和七层交换机原理</p><ul><li>所谓<strong>七层负载均衡</strong>，也称为“内容交换”，也就是主要通过报文中的真正有意义的应用层内容，再加上负载均衡设备设置的服务器选择方式，决定最终选择的内部服务器。</li></ul><p>  以常见的TCP为例，负载均衡设备如果要根据真正的应用层内容再选择服务器，只能先代理最终的服务器和客户端建立连接(三次握手)后，才可能接受到客户端发送的真正应用层内容的报文，然后再根据该报文中的特定字段，再加上负载均衡设备设置的服务器选择方式，决定最终选择的内部服务器。<strong>负载均衡设备在这种情况下，更类似于一个代理服务器</strong>。负载均衡和前端的客户端以及后端的服务器会分别建立TCP连接。所以从这个技术原理上来看，七层负载均衡明显的对负载均衡设备的要求更高，处理七层的能力也必然会低于四层模式的部署方式。</p><h5 id="2-应用场景区别"><a href="#2-应用场景区别" class="headerlink" title="2.应用场景区别"></a>2.应用场景区别</h5><p>  七层因为可以代理任意修改和处理用户的请求，所以可以使整个应用更加智能化和安全，代价就是设计和配置会更复杂。所以是否有必要使用七层负载均衡是一个需要权衡的问题。</p><p>  现在的7层负载均衡，主要还是着重于应用HTTP协议，所以其应用范围主要是众多的网站或者内部信息平台等基于B/S开发的系统。 4层负载均衡则对应其他TCP应用，例如基于C/S开发的ERP等系统。</p><p>原文链接：<a href="https://www.jianshu.com/p/04518b017c90" target="_blank" rel="noopener">https://www.jianshu.com/p/04518b017c90</a></p>]]></content>
      
      
      <categories>
          
          <category> linux </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 负载均衡 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>storm创建DRPC远程客户端</title>
      <link href="/2019/12/11/storm%E5%88%9B%E5%BB%BAdrpc%E8%BF%9C%E7%A8%8B%E5%AE%A2%E6%88%B7%E7%AB%AF/"/>
      <url>/2019/12/11/storm%E5%88%9B%E5%BB%BAdrpc%E8%BF%9C%E7%A8%8B%E5%AE%A2%E6%88%B7%E7%AB%AF/</url>
      
        <content type="html"><![CDATA[<h4 id="创建DRPC远程客户端"><a href="#创建DRPC远程客户端" class="headerlink" title="创建DRPC远程客户端"></a>创建DRPC远程客户端</h4><p>DRPCClient创建方法如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment">//conf map, drpc server, port no, timeout for the call</span></span><br><span class="line"><span class="keyword">new</span> DRPCClient(conf, <span class="string">"192.168.0.217"</span>, <span class="number">3772</span>, <span class="number">5000</span>);</span><br></pre></td></tr></table></figure><p>conf如下：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">Config conf = <span class="keyword">new</span> Config();</span><br><span class="line">conf.setDebug(<span class="keyword">false</span>);</span><br></pre></td></tr></table></figure><p>这将产生下列这个错误：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">java.lang.NullPointerException</span><br><span class="line">java.lang.RuntimeException: java.lang.NullPointerException</span><br><span class="line">at backtype.storm.security.auth.AuthUtils.GetTransportPlugin(AuthUtils.java:<span class="number">230</span>)</span><br><span class="line">at backtype.storm.security.auth.ThriftClient.reconnect(ThriftClient.java:<span class="number">91</span>)</span><br></pre></td></tr></table></figure><p>如何添加下列这句话：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">conf.put(<span class="string">"storm.thrift.transport"</span>, <span class="string">"backtype.storm.security.auth.SimpleTransportPlugin"</span>);</span><br></pre></td></tr></table></figure><p>这将继续报这个错误：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">Don<span class="string">'t know how to convert null to int</span></span><br><span class="line"><span class="string">java.lang.IllegalArgumentException: Don'</span>t know how to convert <span class="keyword">null</span> to <span class="keyword">int</span></span><br><span class="line">at backtype.storm.utils.Utils.getInt(Utils.java:<span class="number">420</span>)</span><br><span class="line">at backtype.storm.security.auth.ThriftClient.reconnect(ThriftClient.java:<span class="number">100</span>)</span><br></pre></td></tr></table></figure><p>在storm0.10之后就已经做了改进，使用map来传递配置参数。</p><p>正确做法：</p><figure class="highlight java"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">Config conf = <span class="keyword">new</span> Config();</span><br><span class="line">Map defaultConfig = Utils.readDefaultConfig();</span><br><span class="line"></span><br><span class="line">defaultConfig.put(<span class="string">"storm.thrift.transport"</span>,<span class="string">"org.apache.storm.security.auth.SimpleTransportPlugin"</span>);</span><br><span class="line">defaultConfig.put(Config.STORM_EXHIBITOR_RETRY_TIMES, <span class="number">3</span>);</span><br><span class="line">defaultConfig.put(Config.STORM_EXHIBITOR_RETRY_INTERVAL, <span class="number">10</span>);</span><br><span class="line">defaultConfig.put(Config.STORM_EXHIBITOR_RETRY_INTERVAL_CEILING, <span class="number">20</span>);</span><br><span class="line">defaultConfig.put(Config.DRPC_MAX_BUFFER_SIZE, <span class="number">1048576</span>);</span><br><span class="line"></span><br><span class="line">conf.putAll(defaultConfig);</span><br><span class="line"></span><br><span class="line">DRPCClient drpcClient = <span class="keyword">new</span> DRPCClient(conf,<span class="string">"localhost"</span>, <span class="number">3772</span>,<span class="number">5000</span>);</span><br></pre></td></tr></table></figure>]]></content>
      
      
      <categories>
          
          <category> bigdata </category>
          
      </categories>
      
      
        <tags>
            
            <tag> storm </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>研一前的暑假，深度学习初体验</title>
      <link href="/2019/08/10/%E7%A0%94%E4%B8%80%E5%89%8D%E7%9A%84%E6%9A%91%E5%81%87%EF%BC%8C%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%88%9D%E4%BD%93%E9%AA%8C/"/>
      <url>/2019/08/10/%E7%A0%94%E4%B8%80%E5%89%8D%E7%9A%84%E6%9A%91%E5%81%87%EF%BC%8C%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E5%88%9D%E4%BD%93%E9%AA%8C/</url>
      
        <content type="html"><![CDATA[<h1 id="选导师"><a href="#选导师" class="headerlink" title="选导师"></a>选导师</h1><p>&nbsp;&nbsp;&nbsp;&nbsp;在五月二十七号左右，再次来到一所新的城市，即将在这里度过我研究生的三年，之所以来这么早，是因为希望研究生阶段，能够跟着一个研究方向，自己比较感兴趣的导师。找的第一个导师比较偏学术一些，而我又希望自己在研究生阶段能够有一定的项目经历，所以在经过思考后，我和老师表明了我的想法，老师也表示理解，和我推荐了其他的导师。在找导师的过程中，发现比较热门的导师的特点有以下几点。在校刚来的几位导师中，比较年轻的导师无论能力还是学术都比较强，所有也很快就定下了人选。在来之前，自己也大致确立了方向，大数据。学校有个大数据院，看了相关的介绍，这一方向在校长的带领下，是发展的非常不错的。但是对于我看似来的早，实际上并不早的人来说，是轮不到我了。。此处省略一万字吧。最终找到了我现在的导师，导师管理的公司主要是做图像识别。也就属于当前的计算机视觉方向，被分配在公司的算法组，所以也就不得不走向了研究深度学习的方向。</p><h1 id="做事情"><a href="#做事情" class="headerlink" title="做事情"></a>做事情</h1><p>&nbsp;&nbsp;&nbsp;&nbsp;在简单的熟悉了公司之后，由于初来公司，并没有分配什么很具体的工作，所以自己在没事的时候，我选择继续考研之后做的事情，那就是学习springboot. 由于在公司里，没有很具体的工作，每天自己看视频自学，一个视频教程边看自己边跟着实践，也很快就完成了。 就这样，看了springboot企业微信点餐，springcloud升级企业微信点餐两个教程。就在这个时候，公司有准备让我写一些简单的接口。这时候随着对微服务的了解之后，我发现我对这个概念非常感兴趣。所以我不断去深入了解这一块的知识。</p><h1 id="深度学习相关"><a href="#深度学习相关" class="headerlink" title="深度学习相关"></a>深度学习相关</h1><p>&nbsp;&nbsp;&nbsp;&nbsp;在机器学习中，我们通常使用梯度下降来更新模型参数从而求解。损失函数关于模型参数的梯度指向一个可以降低损失函数值的方向，我们不断地沿着梯度的方向更新模型从而最小化损失函数。</p><h2 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h2><p>&nbsp;&nbsp;&nbsp;&nbsp;线性回归输出是一个连续值，因此适用于回归问题。回归问题在实际中很常见，如预测房屋价格、气温、销售额等连续值的问题。与回归问题不同，分类问题中模型的最终输出是一个离散值。我们所说的图像分类、垃圾邮件识别、疾病检测等输出为离散值的问题都属于分类问题的范畴。softmax回归则适用于分类问题。</p><h3 id="线性回归的基本要素"><a href="#线性回归的基本要素" class="headerlink" title="线性回归的基本要素"></a>线性回归的基本要素</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;我们以一个简单的房屋价格预测作为例子来解释线性回归的基本要素。这个应用的目标是预测一栋房子的售出价格（元）。我们知道这个价格取决于很多因素，如房屋状况、地段、市场行情等。为了简单起见，这里我们假设价格只取决于房屋状况的两个因素，即面积（平方米）和房龄（年）。接下来我们希望探索价格与这两个因素的具体关系。</p><h3 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;设房屋的面积为x1，房龄为x2，售出价格为y。我们需要建立基于输入x1和x2来计算输出y的表达式，也就是模型（model）。顾名思义，线性回归假设输出与各个输入之间是线性关系：<br>&nbsp;&nbsp;&nbsp;&nbsp;y^=x1w1+x2w2+b,<br>&nbsp;&nbsp;&nbsp;&nbsp;其中w1和w2是权重（weight），b是偏差（bias），且均为标量。它们是线性回归模型的参数（parameter）。模型输出ˆy是线性回归对真实价格y的预测或估计。我们通常允许它们之间有一定误差。</p><h3 id="模型训练"><a href="#模型训练" class="headerlink" title="模型训练"></a>模型训练</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;接下来我们需要通过数据来寻找特定的模型参数值，使模型在数据上的误差尽可能小。这个过程叫作模型训练（model training）。下面我们介绍模型训练所涉及的3个要素。</p><h4 id="训练数据"><a href="#训练数据" class="headerlink" title="训练数据"></a>训练数据</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;我们通常收集一系列的真实数据，例如多栋房屋的真实售出价格和它们对应的面积和房龄。我们希望在这个数据上面寻找模型参数来使模型的预测价格与真实价格的误差最小。在机器学习术语里，该数据集被称为训练数据集（training data set）或训练集（training set），一栋房屋被称为一个样本（sample），其真实售出价格叫作标签（label），用来预测标签的两个因素叫作特征（feature）。特征用来表征样本的特点。<br>&nbsp;&nbsp;&nbsp;&nbsp;假设我们采集的样本数为n，索引为i的样本的特征为x(i)1和x(i)2，标签为y(i)。对于索引为i的房屋，线性回归模型的房屋价格预测表达式为<br>&nbsp;&nbsp;&nbsp;&nbsp;y(i)=x1(i)w1+x2(i)w2+b.</p><h4 id="损失函数"><a href="#损失函数" class="headerlink" title="损失函数"></a>损失函数</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;在模型训练中，我们需要衡量价格预测值与真实值之间的误差。通常我们会选取一个非负数作为误差，且数值越小表示误差越小。一个常用的选择是平方函数。<br>    给定训练数据集，这个误差只与模型参数相关，因此我们将它记为以模型参数为参数的函数。在机器学习里，将衡量误差的函数称为损失函数（loss function）。<br>&nbsp;&nbsp;&nbsp;&nbsp;通常，我们用训练数据集中所有样本误差的平均来衡量模型预测的质量.<br>&nbsp;&nbsp;&nbsp;&nbsp;在模型训练中，我们希望找出一组模型参数，记为w∗1,w∗2,b∗，来使训练样本平均损失最小.</p><h4 id="优化算法"><a href="#优化算法" class="headerlink" title="优化算法"></a>优化算法</h4><p>&nbsp;&nbsp;&nbsp;&nbsp;当模型和损失函数形式较为简单时，上面的误差最小化问题的解可以直接用公式表达出来。这类解叫作解析解（analytical solution）。本节使用的线性回归和平方误差刚好属于这个范畴。然而，大多数深度学习模型并没有解析解，只能通过优化算法有限次迭代模型参数来尽可能降低损失函数的值。这类解叫作数值解（numerical solution）。<br>&nbsp;&nbsp;&nbsp;&nbsp;在求数值解的优化算法中，小批量随机梯度下降（mini-batch stochastic gradient descent）在深度学习中被广泛使用。它的算法很简单：先选取一组模型参数的初始值，如随机选取；接下来对参数进行多次迭代，使每次迭代都可能降低损失函数的值。在每次迭代中，先随机均匀采样一个由固定数目训练数据样本所组成的小批量（mini-batch）β，然后求小批量中数据样本的平均损失有关模型参数的导数（梯度），最后用此结果与预先设定的一个正数的乘积作为模型参数在本次迭代的减小量。<br>&nbsp;&nbsp;&nbsp;&nbsp;在迭代的过程中，β代表每个小批量中的样本个数（批量大小，batch size），η称作学习率（learning rate）并取正数。需要强调的是，这里的批量大小和学习率的值是人为设定的，并不是通过模型训练学出的，因此叫作超参数（hyperparameter）。我们通常所说的“调参”指的正是调节超参数，例如通过反复试错来找到超参数合适的值。</p><h3 id="模型预测"><a href="#模型预测" class="headerlink" title="模型预测"></a>模型预测</h3><p>&nbsp;&nbsp;&nbsp;&nbsp;模型训练完成后，我们将模型参数w1, w2, b在优化算法停止时的值分别记作w<sub>1</sub>, w<sub>2</sub>, b。注意，这里我们得到的并不一定是最小化损失函数的最优解w*<sub>1</sub>, w*<sub>2</sub>，b*，而是对最优解的一个近似。然后，我们就可以使用学出的线性回归模型x1w1+ x2w2 + b来估算训练数据集以外任意一栋面积（平方米）为x1、房龄（年）为x2的房屋的价格了。这里的估算也叫作模型预测、模型推断或模型测试。</p>]]></content>
      
      
      <categories>
          
          <category> 计算机视觉 </category>
          
          <category> 机器学习 </category>
          
      </categories>
      
      
        <tags>
            
            <tag> 深度学习 </tag>
            
        </tags>
      
    </entry>
    
    
    
    <entry>
      <title>Hello World</title>
      <link href="/2019/03/11/hello-world/"/>
      <url>/2019/03/11/hello-world/</url>
      
        <content type="html"><![CDATA[<p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p><h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p><h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p><h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p><h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure><p>More info: <a href="https://hexo.io/docs/deployment.html" target="_blank" rel="noopener">Deployment</a></p>]]></content>
      
      
      
    </entry>
    
    
  
  
</search>
